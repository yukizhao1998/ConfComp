Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Supercedes),Inward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
NotifyCheckpointAbortedITCase failed due to timeout,FLINK-20816,13348417,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,mapohl,mapohl,30/Dec/20 07:28,29/Jun/21 03:03,13/Jul/23 08:11,12/Apr/21 18:52,1.12.2,1.13.0,,,,1.13.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=152&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4245] failed caused by a failing of {{NotifyCheckpointAbortedITCase}} due to a timeout.
{code}
2020-12-29T21:48:40.9430511Z [INFO] Running org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase
2020-12-29T21:50:28.0087043Z [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 107.062 s <<< FAILURE! - in org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase
2020-12-29T21:50:28.0087961Z [ERROR] testNotifyCheckpointAborted[unalignedCheckpointEnabled =true](org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase)  Time elapsed: 104.044 s  <<< ERROR!
2020-12-29T21:50:28.0088619Z org.junit.runners.model.TestTimedOutException: test timed out after 100000 milliseconds
2020-12-29T21:50:28.0088972Z 	at java.lang.Object.wait(Native Method)
2020-12-29T21:50:28.0089267Z 	at java.lang.Object.wait(Object.java:502)
2020-12-29T21:50:28.0089633Z 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
2020-12-29T21:50:28.0090458Z 	at org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.verifyAllOperatorsNotifyAborted(NotifyCheckpointAbortedITCase.java:200)
2020-12-29T21:50:28.0091313Z 	at org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.testNotifyCheckpointAborted(NotifyCheckpointAbortedITCase.java:183)
2020-12-29T21:50:28.0091819Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-12-29T21:50:28.0092199Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-12-29T21:50:28.0092675Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-12-29T21:50:28.0093095Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-12-29T21:50:28.0093495Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-12-29T21:50:28.0093980Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-12-29T21:50:28.0094444Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-12-29T21:50:28.0094917Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-12-29T21:50:28.0095663Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-12-29T21:50:28.0096221Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-12-29T21:50:28.0096675Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-12-29T21:50:28.0097022Z 	at java.lang.Thread.run(Thread.java:748)
{code}

The branch contained changes from FLINK-20594 and FLINK-20595. These issues remove code that is not used anymore and should have had only affects on unit tests. [The previous build|https://dev.azure.com/mapohl/flink/_build/results?buildId=151&view=results] containing all the changes accept for [9c57c37|https://github.com/XComp/flink/commit/9c57c37c50733a1f592a4fc5e492b22be80d8279] passed.",,AHeise,dwysakowicz,kezhuw,maguowei,mapohl,pnowojski,rmetzger,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18335,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/21 16:19;mapohl;flink-20816-failure.log;https://issues.apache.org/jira/secure/attachment/13023258/flink-20816-failure.log","31/Mar/21 16:19;mapohl;flink-20816-success.log;https://issues.apache.org/jira/secure/attachment/13023259/flink-20816-success.log",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 18:52:21 UTC 2021,,,,,,,,,,"0|z0lxag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/20 16:34;yunta;After searching in the artifact of [logs-ci_build-tests|https://dev.azure.com/mapohl/flink/_build/results?buildId=152&view=artifacts&pathAsName=false&type=publishedArtifacts], the error is happened with unaligned checkpoint enabled. And happened at line [verifyAllOperatorsNotifyAborted()|https://github.com/apache/flink/blob/55b46d8b8deb2103af4d57dfbd4fc2c0d4d8e948/flink-tests/src/test/java/org/apache/flink/test/checkpointing/NotifyCheckpointAbortedITCase.java#L183]


{code:java}
log.info(""Verifying whether all operators have been notified of checkpoint-1 aborted."");
verifyAllOperatorsNotifyAborted();
log.info(""Verified that all operators have been notified of checkpoint-1 aborted."");
{code}

I think this might be somehow un-expected as the notification should not be held for 10 seconds, could this problem reproduce locally in your branch [~mapohl]?
;;;","04/Jan/21 08:29;mapohl;I ran the tests ~3200 times locally without running into that issue. I started [another build|https://dev.azure.com/mapohl/flink/_build/results?buildId=153&view=results] on that branch. ...not being confident though about being able to reproduce it.;;;","04/Jan/21 09:18;trohrmann;Have you tried looping this test on AZP [~mapohl]?;;;","04/Jan/21 12:31;mapohl;Thanks for the hint. I committed [cdf59dc|https://github.com/XComp/flink/commit/cdf59dc973ef4e692ba2b9766c3f91c7bca3705d] looping over the test 2000 times. Let's see whether the [corresponding build|https://dev.azure.com/mapohl/flink/_build/results?buildId=155&view=results] reveals something.;;;","16/Feb/21 09:06;dwysakowicz;It failed on 1.12 branch as well: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13357&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0;;;","11/Mar/21 12:38;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8966&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a;;;","22/Mar/21 00:35;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9412;;;","22/Mar/21 05:13;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15137&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4367;;;","26/Mar/21 15:33;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15528&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9680;;;","29/Mar/21 18:27;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=371&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=9591;;;","29/Mar/21 18:51;mapohl;Not sure why I missed that one. But I started a new looped run for the test on -[AzureC #376|https://dev.azure.com/mapohl/flink/_build/results?buildId=376&view=results]- -[AzureCI #377|https://dev.azure.com/mapohl/flink/_build/results?buildId=377&view=results]- [AzureCI #380|https://dev.azure.com/mapohl/flink/_build/results?buildId=380&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b] (I had to restart it because I forgot to add the actual test to the Maven command (n)). Let's see...;;;","29/Mar/21 19:02;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=363&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=9585;;;","31/Mar/21 16:22;mapohl;I was able to reproduce the timeout on AzureCI. I wasn't able to pinpoint the reason by diffing the successful and failed run log files. But it looks like in this specific case, the waiting for the abort notification of checkpoint #2 in the DeclineSink does not get triggered. [~yunta] are you able to get anything more out of the logs?;;;","01/Apr/21 03:47;yunta;From the comparison of success and failure logs, I think the root cause is that {{DeclineSink}} did not execute sync phase of snapshot of checkpoint-2. However, we expect to fail the checkpoint-2 during async phase and that's why the test timeout as we did not wait for the expect checkpoint failure.

If we extract all related logs of {{DeclineSink}} from failure log:
{code:java}
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - found barrier 2, lastSeenBarrier = 1 (COMPLETED) @ InputChannelInfo{gateIdx=0, inputChannelIdx=0}
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler [] - DeclineSink (1/1)#0 (7457bf515844f409738c9929fffc54f7): Received barrier from channel InputChannelInfo{gateIdx=0, inputChannelIdx=0} @ 2.
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl [] - DeclineSink (1/1)#0 starting checkpoint 2 (CheckpointOptions {checkpointType = CHECKPOINT, targetLocation = (default), isExactlyOnceMode = true, isUnalignedCheckpoint = true, alignmentTimeout = 9223372036854775807})
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - startPersisting 2, lastSeenBarrier = 2 (BARRIER_RECEIVED) @ InputChannelInfo{gateIdx=0, inputChannelIdx=0}
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler [] - DeclineSink (1/1)#0 (7457bf515844f409738c9929fffc54f7): Triggering checkpoint 2 on the barrier announcement at 1617138259258.
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Starting checkpoint (2) CHECKPOINT on task DeclineSink (1/1)#0
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl [] - DeclineSink (1/1)#0 finishing output data, checkpoint 2
21:04:19,272 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl [] - DeclineSink (1/1)#0 requested write result, checkpoint 2
21:04:19,273 [Channel state writer DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter [] - complete output, input completed: false
21:05:58,298 [flink-akka.actor.default-dispatcher-12] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DeclineSink (1/1) (7457bf515844f409738c9929fffc54f7) switched from RUNNING to CANCELING.
{code}

Since I am not familiar with unaligned checkpoint, I think {{Channel state writer DeclineSink}} should wait for input completed as true and then [code below|https://github.com/apache/flink/blob/04bbf03a0cdb2f455c1b06569dea95ace6fa7e7c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java#L305-L317] could execute:

{code:java}
        // Step (4): Take the state snapshot. This should be largely asynchronous, to not impact
        // progress of the
        // streaming topology

        Map<OperatorID, OperatorSnapshotFutures> snapshotFutures =
                new HashMap<>(operatorChain.getNumberOfOperators());
        try {
            if (takeSnapshotSync(
                    snapshotFutures, metadata, metrics, options, operatorChain, isRunning)) {
                finishAndReportAsync(snapshotFutures, metadata, metrics, isRunning);
            } else {
                cleanup(snapshotFutures, metadata, metrics, new Exception(""Checkpoint declined""));
            }
        } catch (Exception ex) {
            cleanup(snapshotFutures, metadata, metrics, ex);
            throw ex;
        }
{code}


;;;","01/Apr/21 07:19;mapohl;Thanks [~yunta] for your analysis. [~AHeise] could you have a look at this?;;;","07/Apr/21 17:24;arvid;[~yunta]'s analysis is spot on; there is not much to add as the interesting debug statements are currently not in.

Interestingly, a line like 

{noformat}
21:04:11,714 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.runtime.state.SnapshotStrategyRunner        [] - StuckAsyncSnapshotStrategy (FsCheckpointStorageLocation {fileSystem=org.apache.flink.core.fs.SafetyNetWrapperFileSystem@3f0e55b0, checkpointDirectory=file:/tmp/junit7287967740618809656/junit2918432964059421469/663881dcecc7cc89be722ae89e3384ab/chk-2, sharedStateDirectory=file:/tmp/junit7287967740618809656/junit2918432964059421469/663881dcecc7cc89be722ae89e3384ab/shared, taskOwnedStateDirectory=file:/tmp/junit7287967740618809656/junit2918432964059421469/663881dcecc7cc89be722ae89e3384ab/taskowned, metadataFilePath=file:/tmp/junit7287967740618809656/junit2918432964059421469/663881dcecc7cc89be722ae89e3384ab/chk-2/_metadata, reference=(default), fileStateSizeThreshold=20480, writeBufferSize=4096}, synchronous part) in thread Thread[DeclineSink (1/1)#0,5,Flink Task Threads] took 0 ms.
{noformat}

is missing from the failed log, which indicates that we never successfully execute {{SubtaskCheckpointCoordinatorImpl#buildOperatorSnapshotFutures}}. The part of unaligned checkpoint before Yun's fragment is executed normally, so I don't immediately see a connection to unaligned checkpoints.
;;;","07/Apr/21 17:26;arvid;Indeed https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15528&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9680 shows that the same occurs for aligned checkpoints.;;;","09/Apr/21 09:59;arvid;With some more echo debugging, it's most likely caused by

{noformat}
       OperatorSnapshotFutures snapshotInProgress =
                checkpointStreamOperator(
                        op, checkpointMetaData, checkpointOptions, storage, isRunning);
{noformat}

hanging in the sync phase.;;;","09/Apr/21 10:14;arvid;That is actually be design of the test
{noformat}
            if (context.getCheckpointId() == DECLINE_CHECKPOINT_ID) {
                DeclineSink.waitLatch.await();
            }
{noformat}
DeclineSink is not supposed to complete it until the abortion of the first checkpoint is verified.

However, there is no log statement that indicate that, there is an abortion call happening at all.
In the attached success.log we have

{noformat}
21:04:11,624 [Source: NormalSource (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 1 for task Source: NormalSource (1/1)#0
21:04:11,624 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 1 for task DeclineSink (1/1)#0
21:04:11,625 [   NormalMap (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 1 for task NormalMap (1/1)#0
21:04:11,739 [Source: NormalSource (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 2 for task Source: NormalSource (1/1)#0
21:04:11,739 [   NormalMap (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 2 for task NormalMap (1/1)#0
21:04:11,739 [ DeclineSink (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 2 for task DeclineSink (1/1)#0
{noformat}

while in failure.log, I can only find

{noformat}
21:04:19,260 [Source: NormalSource (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 1 for task Source: NormalSource (1/1)#0
21:04:19,268 [   NormalMap (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 1 for task NormalMap (1/1)#0
21:05:58,297 [Source: NormalSource (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 2 for task Source: NormalSource (1/1)#0
21:05:58,297 [   NormalMap (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Notification of aborted checkpoint 2 for task NormalMap (1/1)#0
{noformat}

It might be some race condition, where the {{SubtaskCheckpointCoordinatorImpl}} does not know that the {{DeclineSink}} is already running.

{noformat}
21:04:19,037 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DeclineSink (1/1) (7457bf515844f409738c9929fffc54f7) switched from DEPLOYING to RUNNING.
{noformat}

;;;","09/Apr/21 17:05;arvid;I found the root cause: the test assumes implicitly that abortion of chk-1 happens before sync phase of chk-2. If abortion is late, the mail that handles the abortion is never executed since the waitLatch blocks the mailbox thread.

It's easy to reproduce by adding some sleep into 

{noformat}
    private void CheckpointCoordinator#sendAbortedMessages(
            List<ExecutionVertex> tasksToAbort, long checkpointId, long timeStamp) {
        // send notification of aborted checkpoints asynchronously.
        executor.execute(
                () -> {
                    try {
                        Thread.sleep(1000);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
{noformat}
;;;","10/Apr/21 12:36;arvid;Downgraded to Major as it's a test-only issue.;;;","10/Apr/21 13:08;yunta;[~AHeise] Thanks for your troubleshooting and sorry for making this unstable case.;;;","12/Apr/21 18:52;arvid;Merged into master as fad4874f9866de7d3c2f5fb3a473f4df744c8159 and into 1.12 as a1ee66d9ef9a14414b9c0fee9288a94685740471.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client should create modules with user class loader,FLINK-20813,13348389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,lirui,lirui,30/Dec/20 06:05,15/Apr/21 08:56,13/Jul/23 08:12,15/Apr/21 08:56,,,,,,1.13.0,,,Table SQL / Client,,,,,0,,,,,,,,fsk119,jark,lirui,Paul Lin,qingyue,ZhaoWeiNan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 08:56:30 UTC 2021,,,,,,,,,,"0|z0lx48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/20 08:20;jark;cc [~fsk119];;;","12/Apr/21 14:09;ZhaoWeiNan;cc [~fsk119],i will do it.;;;","15/Apr/21 08:56;fsk119;Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Version mismatch between spotless-maven-plugin and google-java-format plugin,FLINK-20803,13348284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Dec/20 11:08,28/May/21 06:58,13/Jul/23 08:12,29/Dec/20 12:01,1.11.4,1.12.1,1.13.0,,,1.11.4,1.12.1,1.13.0,Build System,,,,,0,pull-request-available,,,,,"The spotless-maven-plugin uses version 1.7 of the google-java-format, while the IntelliJ google-java-format plugin uses 1.9, resulting in inconsistent formatting.

We cannot bump the version used by the spotless plugin because it requires java 11, so instead we have to downgrade the intellij plugin to 1.7.0.5 .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21106,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 29 12:01:59 UTC 2020,,,,,,,,,,"0|z0lwgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/20 12:01;chesnay;master: 1ec147c2812d561624d37639f8faeb5ccfe52e1e
1.12: 9c2cdb43605cfb9408eae8a97aa748c078bd71fc
1.11: d400a9b6ece28a486fd83345fa0294d3aa382109 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using PVC as high-availability.storageDir could not work,FLINK-20798,13348239,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hayden zhou,hayden zhou,29/Dec/20 06:58,28/May/21 07:02,13/Jul/23 08:12,11/Jan/21 15:31,1.12.0,,,,,1.12.2,1.13.0,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"
When deploying standalone Flink on Kubernetes and configure the {{high-availability.storageDir}} to a mounted PVC directory, the Flink webui could not be visited normally. It shows that ""Service temporarily unavailable due to an ongoing leader election. Please refresh"".

 


The following is related logs from JobManager.

{code}
2020-12-29T06:45:54.177850394Z 2020-12-29 14:45:54,177 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Leader election started
 2020-12-29T06:45:54.177855303Z 2020-12-29 14:45:54,177 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Attempting to acquire leader lease 'ConfigMapLock: default - mta-flink-resourcemanager-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'...
 2020-12-29T06:45:54.178668055Z 2020-12-29 14:45:54,178 DEBUG io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager [] - WebSocket successfully opened
 2020-12-29T06:45:54.178895963Z 2020-12-29 14:45:54,178 INFO org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver\{configMapName='mta-flink-resourcemanager-leader'}.
 2020-12-29T06:45:54.179327491Z 2020-12-29 14:45:54,179 DEBUG io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager [] - Connecting websocket ... io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@6d303498
 2020-12-29T06:45:54.230081993Z 2020-12-29 14:45:54,229 DEBUG io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager [] - WebSocket successfully opened
 2020-12-29T06:45:54.230202329Z 2020-12-29 14:45:54,230 INFO org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver\{configMapName='mta-flink-dispatcher-leader'}.
 2020-12-29T06:45:54.230219281Z 2020-12-29 14:45:54,229 DEBUG io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager [] - WebSocket successfully opened
 2020-12-29T06:45:54.230353912Z 2020-12-29 14:45:54,230 INFO org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Starting DefaultLeaderElectionService with KubernetesLeaderElectionDriver\{configMapName='mta-flink-resourcemanager-leader'}.
 2020-12-29T06:45:54.237004177Z 2020-12-29 14:45:54,236 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Leader changed from null to 6f6479c6-86cc-4d62-84f9-37ff968bd0e5
 2020-12-29T06:45:54.237024655Z 2020-12-29 14:45:54,236 INFO org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector [] - New leader elected 6f6479c6-86cc-4d62-84f9-37ff968bd0e5 for mta-flink-restserver-leader.
 2020-12-29T06:45:54.237027811Z 2020-12-29 14:45:54,236 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Successfully Acquired leader lease 'ConfigMapLock: default - mta-flink-restserver-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'
 2020-12-29T06:45:54.237297376Z 2020-12-29 14:45:54,237 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender [http://mta-flink-jobmanager:8081|http://mta-flink-jobmanager:8081/] with session ID 9587e13f-322f-4cd5-9fff-b4941462be0f.
 2020-12-29T06:45:54.237353551Z 2020-12-29 14:45:54,237 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - [http://mta-flink-jobmanager:8081|http://mta-flink-jobmanager:8081/] was granted leadership with leaderSessionID=9587e13f-322f-4cd5-9fff-b4941462be0f
 2020-12-29T06:45:54.237440354Z 2020-12-29 14:45:54,237 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Confirm leader session ID 9587e13f-322f-4cd5-9fff-b4941462be0f for leader [http://mta-flink-jobmanager:8081|http://mta-flink-jobmanager:8081/].
 2020-12-29T06:45:54.254555127Z 2020-12-29 14:45:54,254 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Leader changed from null to 6f6479c6-86cc-4d62-84f9-37ff968bd0e5
 2020-12-29T06:45:54.254588299Z 2020-12-29 14:45:54,254 INFO org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector [] - New leader elected 6f6479c6-86cc-4d62-84f9-37ff968bd0e5 for mta-flink-resourcemanager-leader.
 2020-12-29T06:45:54.254628053Z 2020-12-29 14:45:54,254 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Successfully Acquired leader lease 'ConfigMapLock: default - mta-flink-resourcemanager-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'
 2020-12-29T06:45:54.254871569Z 2020-12-29 14:45:54,254 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: StandaloneResourceManager with session ID b1730dc6-0f94-49f4-b519-56917f3027b7.
 2020-12-29T06:45:54.256608291Z 2020-12-29 14:45:54,256 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Attempting to renew leader lease 'ConfigMapLock: default - mta-flink-resourcemanager-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'...
 2020-12-29T06:45:54.259155793Z 2020-12-29 14:45:54,258 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Leader changed from null to 6f6479c6-86cc-4d62-84f9-37ff968bd0e5
 2020-12-29T06:45:54.259176091Z 2020-12-29 14:45:54,258 INFO org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector [] - New leader elected 6f6479c6-86cc-4d62-84f9-37ff968bd0e5 for mta-flink-dispatcher-leader.
 2020-12-29T06:45:54.25918096Z 2020-12-29 14:45:54,259 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Successfully Acquired leader lease 'ConfigMapLock: default - mta-flink-dispatcher-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'
 2020-12-29T06:45:54.259362149Z 2020-12-29 14:45:54,259 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: DefaultDispatcherRunner with session ID fbbaa883-69f6-43df-9ca0-c646bc1baad1.
 2020-12-29T06:45:54.260301799Z 2020-12-29 14:45:54,260 DEBUG org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - Create new DispatcherLeaderProcess with leader session id fbbaa883-69f6-43df-9ca0-c646bc1baad1.
 2020-12-29T06:45:54.266724597Z 2020-12-29 14:45:54,266 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
 2020-12-29T06:45:54.267718418Z 2020-12-29 14:45:54,267 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Attempting to renew leader lease 'ConfigMapLock: default - mta-flink-dispatcher-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'...
 2020-12-29T06:45:54.26786349Z 2020-12-29 14:45:54,267 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
 2020-12-29T06:45:54.267976912Z 2020-12-29 14:45:54,267 DEBUG org.apache.flink.runtime.jobmanager.DefaultJobGraphStore [] - Retrieving all stored job ids from KubernetesStateHandleStore\{configMapName='mta-flink-dispatcher-leader'}.
 2020-12-29T06:45:54.277681598Z 2020-12-29 14:45:54,277 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - ResourceManager akka.tcp://flink@mta-flink-jobmanager:6123/user/rpc/resourcemanager_0 was granted leadership with fencing token b51956917f3027b7b1730dc60f9449f4
 2020-12-29T06:45:54.280411279Z 2020-12-29 14:45:54,280 INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Starting the SlotManager.
 2020-12-29T06:45:54.281367931Z 2020-12-29 14:45:54,281 DEBUG org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver [] - Successfully wrote leader information: Leader=[http://mta-flink-jobmanager:8081|http://mta-flink-jobmanager:8081/], session ID=9587e13f-322f-4cd5-9fff-b4941462be0f.
 2020-12-29T06:45:54.281528772Z 2020-12-29 14:45:54,281 DEBUG io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector [] - Attempting to renew leader lease 'ConfigMapLock: default - mta-flink-restserver-leader (6f6479c6-86cc-4d62-84f9-37ff968bd0e5)'...
 2020-12-29T06:45:54.286191344Z 2020-12-29 14:45:54,286 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:45:54.286304807Z 2020-12-29 14:45:54,286 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:45:54.286438227Z 2020-12-29 14:45:54,286 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Confirm leader session ID b1730dc6-0f94-49f4-b519-56917f3027b7 for leader akka.tcp://flink@mta-flink-jobmanager:6123/user/rpc/resourcemanager_0.
 2020-12-29T06:45:54.309361096Z 2020-12-29 14:45:54,309 DEBUG org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver [] - Successfully wrote leader information: Leader=akka.tcp://flink@mta-flink-jobmanager:6123/user/rpc/resourcemanager_0, session ID=b1730dc6-0f94-49f4-b519-56917f3027b7.
 2020-12-29T06:45:54.320673232Z 2020-12-29 14:45:54,320 INFO org.apache.flink.runtime.jobmanager.DefaultJobGraphStore [] - Retrieved job ids [] from KubernetesStateHandleStore\{configMapName='mta-flink-dispatcher-leader'}
 2020-12-29T06:45:54.3206989Z 2020-12-29 14:45:54,320 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
 2020-12-29T06:45:54.324829616Z 2020-12-29 14:45:54,324 DEBUG org.apache.flink.runtime.rpc.akka.SupervisorActor [] - Starting FencedAkkaRpcActor with name dispatcher_1.
 2020-12-29T06:45:54.325343659Z 2020-12-29 14:45:54,325 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
 2020-12-29T06:45:54.33778039Z 2020-12-29 14:45:54,337 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Confirm leader session ID fbbaa883-69f6-43df-9ca0-c646bc1baad1 for leader akka.tcp://flink@mta-flink-jobmanager:6123/user/rpc/dispatcher_1.
 2020-12-29T06:45:54.36249763Z 2020-12-29 14:45:54,362 DEBUG org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver [] - Successfully wrote leader information: Leader=akka.tcp://flink@mta-flink-jobmanager:6123/user/rpc/dispatcher_1, session ID=fbbaa883-69f6-43df-9ca0-c646bc1baad1.
 2020-12-29T06:46:04.298366262Z 2020-12-29 14:46:04,297 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:46:04.298442695Z 2020-12-29 14:46:04,298 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:46:14.318174464Z 2020-12-29 14:46:14,317 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:46:14.318256849Z 2020-12-29 14:46:14,318 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:46:24.337694477Z 2020-12-29 14:46:24,337 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:46:24.337816516Z 2020-12-29 14:46:24,337 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request.
 2020-12-29T06:46:26.044624193Z 2020-12-29 14:46:26,044 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf [] - -Dorg.apache.flink.shaded.netty4.io.netty.buffer.checkAccessible: true
{code}",FLINK 1.12.0,hayden zhou,oceanxie,tbnguyen1407,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20797,,,,,,,,,,,,,,,,,,"30/Dec/20 04:35;hayden zhou;flink.log;https://issues.apache.org/jira/secure/attachment/13017871/flink.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 15:31:16 UTC 2021,,,,,,,,,,"0|z0lw6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/20 03:26;wangyang0918;Could you share the complete logs for JobManager? From the pieces of logs you provided, it seems that the rest endpoint is granted leadership successfully.;;;","30/Dec/20 04:35;hayden zhou;[^flink.log] this is the log

 

my flink-conf.yaml

 

kubernetes.cluster-id: mta-flink
 high-availability: org.apache.flink.kubernetes.highavailability. KubernetesHaServicesFactory
 high-availability.storageDir: file:///opt/flink/nfs/ha

 

 

can I just use  FileSystemHaServiceFactory to replace the KubernetesHaServicesFactory in the configMap？ ;;;","30/Dec/20 04:36;hayden zhou;can we chat on wechat？ my wechat account `zmfk2009`;;;","30/Dec/20 07:55;wangyang0918;[~hayden zhou] I do not find any suspicious logs in the attached log file. The rest endpoint ""http://mta-flink-jobmanager:8081"" is granted leadership successfully. After the leadership is granted, I believe that if you visit the webui normally.

 

For {{FileSystemHaServiceFactory}}, it is not supported in Flink right now. But I think you could have your own implementation and configure the HA mode. Please remember to replace the deployment with statefulset for JobManager.;;;","30/Dec/20 09:34;wangyang0918;After offline discussion with [~hayden zhou], we find the root cause is {{kubernetes.namespace}} is not configured correctly.

 

cc [~hayden zhou] would you like to update the K8s HA document[1]. I believe it could help other users a lot.

 

[1]. https://ci.apache.org/projects/flink/flink-docs-master/deployment/ha/kubernetes_ha.html;;;","30/Dec/20 10:01;hayden zhou;great thanks to [~fly_in_gis], so patiently to help me debug then find the problem ;;;","30/Dec/20 12:21;trohrmann;Does the wrongly configured {{kubernetes.namespace}} explains why the {{DefaultLeaderRetrievalService.notifyLeaderAddress}} is not being called (differently said {{KubernetesLeaderRetrievalDriver}} not seing the ConfigMap change)?;;;","05/Jan/21 13:56;wangyang0918;I am going to reopen this ticket. After a deep dive into the codes and logs, I believe this is a bug. The root cause is we do not set the namespace when creating a watch on the ConfigMap[1].

 

In [~hayden zhou]'s use case, he deploys the JobManager/TaskManager to namespace flink. But the leader ConfigMaps are created in the namespace default. The JobManager/TaskManager pods are using the same namespace as they are running in(aka default). So they could not retrieve the leader address even though the leaders have been granted.

 

Currently for standalone Flink on K8s, we could set the {{kubernetes.namespace}} same as the namespace we are deploying the JobManager and TaskManager pods to work around. For native Flink on K8s, we do not have this issue. Because we are always creating the HA ConfigMaps in the same namespace as the JobManager/TaskManager pods.

 

cc [~trohrmann]

 

[1]. https://github.com/apache/flink/blob/master/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/Fabric8FlinkKubeClient.java#L344;;;","05/Jan/21 14:53;trohrmann;Thanks for the explanation. The fix should be easy for this problem, right? We need to set the proper namespace for the ConfigMap watch.;;;","06/Jan/21 12:31;wangyang0918;Yes. It is easy to fix. Instead of directly set the namespace for ConfigMap watch, I prefer to use a namespaced Kubernetes client when creating {{FlinkKubeClient}} in {{DefaultKubeClientFactory}}. After then, we will not need to always set the namespace when creating kubernetes resources(e.g. deployment, pods, configmap, watch, etc.).

 

I will attach a PR soon.;;;","07/Jan/21 06:50;oceanxie;The HA configmap would't be deleted when I delete the flink cluster(sts and configmap). I don't think this is appropriate. They were generated automatically, not created by me.;;;","07/Jan/21 08:52;trohrmann;[~oceanxie] I think this is intended behaviour as described in the [K8s HA documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/kubernetes_ha.html#high-availability-data-clean-up].;;;","11/Jan/21 15:31;trohrmann;Fixed via 

1.13.0: 460e1799d2b9069835ecae295c75f571c7d6b65a
1.12.2: 8d29d8873b00df983917257c1e577f4f25b3baa2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NamesTest due to code style refactor,FLINK-20793,13348210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,29/Dec/20 02:52,28/May/21 06:58,13/Jul/23 08:12,29/Dec/20 08:57,1.11.0,1.12.0,1.13.0,,,1.11.4,1.12.1,1.13.0,API / Core,,,,,0,pull-request-available,test-stability,,,,"Due to the [FLINK-20651|https://issues.apache.org/jira/browse/FLINK-20651], the NameTest failed

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11403&view=results]

I will fix it asap",,dian.fu,hxbks2ks,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 29 08:57:17 UTC 2020,,,,,,,,,,"0|z0lw0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/20 08:57;dian.fu;Fixed in:
- master via 679c974a24de55d5a12d1fa58cb679dd242098ba
- release-1.12 via c19d049aa1fe197d9b9b5a91d6af5acc0b7ac2dd
- release-1.11 via ca2ac0108bf4050ba7efc4fa729e5f7fdf3da459;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-playgrounds Git branch(release-1.12) does not exist ,FLINK-20786,13348099,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yzcloud_billy,yzcloud_billy,28/Dec/20 10:01,04/Feb/21 10:51,13/Jul/23 08:12,04/Feb/21 10:51,1.12.0,,,,,,,,Examples,,,,,0,playground,,,,,"Based on the official document ,we can use docer-compose to show the test environment.

[Flink Operations Playground
|[https://ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/flink-operations-playground.html]]|

!image-2020-12-28-17-57-20-829.png|width=980,height=293!

but we can't find release-1.12 branch . 

!image-2020-12-28-17-55-42-117.png|width=655,height=214!

{color:#ff0000}> git clone --branch release-1.12 [https://github.com/apache/flink-playgrounds.git]{color}
 {color:#ff0000}Cloning into 'flink-playgrounds'...{color}
 {color:#ff0000}fatal: Remote branch release-1.12 not found in upstream origin{color}

 ",,billyxie,yzcloud_billy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/20 09:55;yzcloud_billy;image-2020-12-28-17-55-42-117.png;https://issues.apache.org/jira/secure/attachment/13017747/image-2020-12-28-17-55-42-117.png","28/Dec/20 09:57;yzcloud_billy;image-2020-12-28-17-57-20-829.png;https://issues.apache.org/jira/secure/attachment/13017746/image-2020-12-28-17-57-20-829.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 02:39:12 UTC 2021,,,,,,,,,,"0|z0lvc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/21 02:39;billyxie;have tested with version 1.12.1 , this issue was resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase failure caused by NullPointerException,FLINK-20781,13348069,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,mapohl,mapohl,28/Dec/20 06:55,28/May/21 07:02,13/Jul/23 08:12,07/Jan/21 17:03,1.12.0,1.13.0,,,,1.12.1,1.13.0,,,,,,,0,pull-request-available,test-stability,,,,"{{UnalignedCheckpointITCase}} fails due to {{NullPointerException}} (e.g. in [this build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11083&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=8798]):
{code:java}
[ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 152.186 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointITCase
[ERROR] execute[Parallel cogroup, p = 10](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 34.869 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
	at akka.dispatch.OnComplete.internal(Future.scala:264)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:214)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:205)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:577)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:420)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.streaming.api.operators.SourceOperator.notifyCheckpointAborted(SourceOperator.java:299)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:311)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:968)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:977)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:91)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:412)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:585)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.afterInvoke(SourceOperatorStreamTask.java:128)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:547)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748) {code}

This might be caused by work related to FLINK-20492 due to [168124f|https://github.com/apache/flink/commit/168124f99c75e873adc81437c700f85f703e2248#diff-eba14821fb3e96f6f20e3116ceab893c2f18cff605b177dad485aadc43ac4f56R240] setting {{sourceReader}} to {{null}}.",,becket_qin,hxbks2ks,leonard,mapohl,pnowojski,rmetzger,xtsong,xwang51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20492,,,,FLINK-20389,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 07 17:02:42 UTC 2021,,,,,,,,,,"0|z0lv5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/20 07:02;mapohl;FLINK-20492 was linked as it might have been caused by changes related to this ticket. [~becket_qin] may you have a look?;;;","28/Dec/20 07:04;mapohl;FLINK-20389 was linked as the most recent comments in this issue refer to the same failure that is described here.;;;","28/Dec/20 07:51;xtsong;Looks like a real and severe bug to me. Tagging it as release blocker of 1.12.1 for now.

We can downgrade it if it's proven not a real bug later.;;;","28/Dec/20 10:41;becket_qin;Yes, this is indeed a bug. I think the quick fix would be just skipping the {{SourceReader.notifyCheckpointAborted()}} call if the SourceReader is null. It would have been better to not invoke any method on an operator if the operator has closed, but that has been a behavior for a while. So we probably don't have to solve that in this ticket.;;;","28/Dec/20 11:00;xtsong;[~becket_qin],

Any chance that `null` check is also needed in other methods? E.g., `emitNext()`, `handleOperatorEvent()`, `getAvailableFuture()`?

IIUC, `sourceReader` was guaranteed non-null as long as `open()` is called on the operator, which FLINK-20492 has changed. Any chance other methods are invoked after `close` due to concurrency and is not aware of the operator being closed?;;;","28/Dec/20 12:33;becket_qin;[~xintongsong] Yes, unfortunately all the methods are subject to this problem.

The correct fix here have to be more systematic. The root cause to the problem is that the order of the mailbox task  shutdown sequence is not thread safe and not atomic. For example, as [~kezhuw] pointed out in FLINK-20389. The chained futures are not guaranteed to be executed in a consecutive manner. We will have to construct the entire chained futures in one shot and pass the whole chained futures to the mailbox.

BTW, personally I don't think this issue is a blocker for release-1.12.1. The problem only occurs occasionally when a task exits. When hitting this problem, the job will just failover. I am happy to put a band-aid here for the {{SourceOperator}}. But it seems we don't have to make such a long pending issue of the mailbox task shutdown sequence as a blocker of release 1.12.1.;;;","28/Dec/20 12:46;xtsong;[~becket_qin],

Thanks for the reply.

+1 on putting a band-aid for 1.12.x and targeting a systematic fix in the future release.

Downgrading it to critical.;;;","31/Dec/20 01:46;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11498&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2;;;","31/Dec/20 01:50;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11501&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","31/Dec/20 02:23;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11516&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56]

 ;;;","01/Jan/21 13:07;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11530&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","01/Jan/21 16:24;xwang51;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11546&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9380;;;","01/Jan/21 16:25;xwang51;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11557&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9459;;;","02/Jan/21 10:11;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11564&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc;;;","03/Jan/21 03:57;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11568&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","04/Jan/21 02:33;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11579&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185]

 ;;;","05/Jan/21 06:58;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=156&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=3997;;;","05/Jan/21 07:01;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11627&view=logs&j=21408240-6569-5a01-c099-3adfe83ce651&t=b2761bb8-3852-5a0d-bc43-6a1d327b63cb]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11629&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc]

 ;;;","05/Jan/21 14:27;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11638&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11640&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]

 ;;;","06/Jan/21 01:49;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11654&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 ;;;","06/Jan/21 02:35;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11671&view=logs&j=77009337-525b-5cd6-c533-94b7ca20542a&t=7ab53e5a-ac96-590c-10c5-9a31a346f79d]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11671&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11671&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc]

 ;;;","06/Jan/21 08:01;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11670&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2;;;","06/Jan/21 08:04;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11673&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0;;;","07/Jan/21 02:35;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11704&view=logs&j=af96ba69-1e60-500c-b8d1-43a47801b668&t=5af1cce8-b79c-5474-167c-f4e5395858de]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11704&view=logs&j=02c4e775-43bf-5625-d1cc-542b5209e072&t=e5961b24-88d9-5c77-efd3-955422674c25]

 ;;;","07/Jan/21 10:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11711&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","07/Jan/21 16:40;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=165&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=8964;;;","07/Jan/21 17:02;becket_qin;Patch merged.

master: 9720e5633cf6b85f78a3b6d6f92ed22a7479658e
release-1.12: 7c7f0839f5d13147c24a9e33fb93a7a6758e844b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Hive partition is not added when there is a lot of data,FLINK-20771,13347861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hehuiyuan,hehuiyuan,25/Dec/20 10:17,28/Jan/21 08:21,13/Jul/23 08:12,28/Jan/21 08:21,,,,,,,,,Connectors / Hive,,,,,0,,,,,,"Hive partition is not added when the data is huge .

!image-2020-12-25-18-09-42-707.png|width=437,height=115!

  Before partition commit, *inProgressPart* will be reinitialize .

the current bucket is active , so the  notifyBucketInactive is  not executed.

!image-2020-12-25-18-15-07-519.png|width=574,height=192!

 

!image-2020-12-25-18-19-53-746.png|width=697,height=56!",,hehuiyuan,jark,leonard,lirui,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20213,,,,,,,,,,,,,,,,,,,,,,,,,"25/Dec/20 10:09;hehuiyuan;image-2020-12-25-18-09-42-707.png;https://issues.apache.org/jira/secure/attachment/13017650/image-2020-12-25-18-09-42-707.png","25/Dec/20 10:15;hehuiyuan;image-2020-12-25-18-15-07-519.png;https://issues.apache.org/jira/secure/attachment/13017649/image-2020-12-25-18-15-07-519.png","25/Dec/20 10:19;hehuiyuan;image-2020-12-25-18-19-53-746.png;https://issues.apache.org/jira/secure/attachment/13017651/image-2020-12-25-18-19-53-746.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 26 08:35:49 UTC 2020,,,,,,,,,,"0|z0ltv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/20 17:09;ZhuShang;Hi [~hehuiyuan],this is dumplicated to https://issues.apache.org/jira/browse/FLINK-20213;;;","26/Dec/20 08:35;hehuiyuan;Hi  [~ZhuShang] , fix , it is good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalarOperatorGens doesn't set proper nullability for result type of generated expressions,FLINK-20765,13347807,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,lirui,lirui,25/Dec/20 02:35,17/Jun/22 11:37,13/Jul/23 08:12,17/Jun/22 11:37,,,,,,1.16.0,,,Table SQL / Runtime,,,,,1,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,stale-major,,"Suppose we have a function that takes a BOOLEAN NOT NULL parameter:
{code}
	/**
	 * A function that takes BOOLEAN NOT NULL.
	 */
	public static class BoolEcho extends ScalarFunction {
		public Boolean eval(@DataTypeHint(""BOOLEAN NOT NULL"") Boolean b) {
			return b;
		}
	}
{code}
Then the following test case will fail:
{code}
CREATE TABLE SourceTable(x INT NOT NULL,y INT) WITH ('connector' = 'COLLECTION');
SELECT BoolEcho(x=1 and y is null) FROM SourceTable;
{code}
with exception:
{noformat}
org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of function's argument data type 'BOOLEAN NOT NULL' and actual argument type 'BOOLEAN'.

	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:323)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:320)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.verifyArgumentTypes(BridgingFunctionGenUtil.scala:320)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCallWithDataType(BridgingFunctionGenUtil.scala:95)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCall(BridgingFunctionGenUtil.scala:65)
	at org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.generate(BridgingSqlFunctionCallGen.scala:62)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:832)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:529)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:155)
{noformat}",,godfreyhe,jark,leonard,libenchao,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25095,FLINK-26363,,,,,,FLINK-25095,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 17 11:37:35 UTC 2022,,,,,,,,,,"0|z0ltj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/20 02:41;lirui;It seems that {{ScalarOperatorGens}} always generates expressions will nullable return type, which can be different from the inferred type in Calcite.;;;","22/Apr/21 10:56;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 11:22;godfreyhe;cc [~lzljs3620320];;;","21/May/21 00:53;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","29/Jul/21 03:06;lzljs3620320;FLINK-22106 is a similar issue.;;;","04/Feb/22 10:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Feb/22 10:37;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/Apr/22 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","17/Jun/22 11:37;jark;Fixed in master: 3ae4c6f5a48105d00807e8ce02e70d4c092cbf40
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchGroupedReduceOperator does not emit results for singleton inputs,FLINK-20764,13347790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,aljoscha,aljoscha,24/Dec/20 20:59,25/Dec/20 09:47,13/Jul/23 08:12,25/Dec/20 09:47,1.12.0,,,,,1.12.1,1.13.0,,API / DataStream,,,,,0,pull-request-available,,,,,"This was reported on the ML: https://lists.apache.org/thread.html/rf24ff56408d3f07a02a3726336229a5919e62c4179c9eb8361a5b76b%40%3Cuser.flink.apache.org%3E.

The problem is that the logic for setting up the ""final"" timer does not kick in when there is only on input.

I'll cut a PR for that and also add tests for the operator.",,aljoscha,leonard,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 25 09:47:26 UTC 2020,,,,,,,,,,"0|z0ltfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/20 09:47;xtsong;Fixed via:
 * master (1.13): b474d2872f4a82d838ac23252254253df4ca2309
 * release-1.12: bdbbb3907865342a7da5be899313422f23d386ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot read hive table/partition whose location path contains comma,FLINK-20761,13347730,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,24/Dec/20 08:43,28/May/21 09:08,13/Jul/23 08:12,16/Apr/21 01:29,,,,,,1.12.3,1.13.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,We probably need to call hadoop {{StringUtils::escapeString}} to escape the input path string.,,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 01:29:57 UTC 2021,,,,,,,,,,"0|z0lt20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 01:29;ykt836;1.13.0: 45e2fb584579952d6abc9a4507cbc3b09b071c12

1.12.3: 902a0f54b487ee39a8adb0e04c826bcc4a9dcab7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken doc link in Apache Flink Code Style and Quality Guide,FLINK-20760,13347721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Tommmmmm,fsk119,fsk119,24/Dec/20 07:53,12/Jun/22 05:15,13/Jul/23 08:12,12/Jun/22 05:15,1.16.0,,,,,1.16.0,,,Documentation,Project Website,,,,0,auto-deprioritized-major,auto-deprioritized-minor,auto-unassigned,pull-request-available,,"The link in pic is broken. The [usage of Java Optional|https://flink.apache.org/contributing/code-style-and-quality-java.md#java-optional] should be linked to [here|https://flink.apache.org/contributing/code-style-and-quality-java.html#java-optional]. !image-2020-12-24-15-56-04-643.png!",,fsk119,jark,liqi25,Tommmmmm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/20 07:56;fsk119;image-2020-12-24-15-56-04-643.png;https://issues.apache.org/jira/secure/attachment/13017608/image-2020-12-24-15-56-04-643.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 12 05:15:11 UTC 2022,,,,,,,,,,"0|z0lt00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/20 03:24;liqi25;Hi，can you assign this issue to me?;;;","25/Dec/20 04:20;fsk119;[~liqi25] Sure.;;;","16/Apr/21 10:48;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:49;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","29/May/21 11:25;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Jun/21 10:52;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","30/Jun/21 22:37;flink-jira-bot;This issue was marked ""stale-assigned"" 7 days ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","29/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Jan/22 10:39;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/22 02:50;Tommmmmm;Hi Shengkai, can you assign this issue to me;;;","08/Jun/22 09:38;fsk119;[~Tommmmmm] Only the committer has the right to assign the ticket. You can just open a PR for it now.;;;","12/Jun/22 05:15;jark;Fixed in flink-web: 29dbb9e8aa5b67d645e8f412fb831acb4d3030dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonCalcSplitConditionRule is not working as expected,FLINK-20756,13347673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,zhongwei,zhongwei,24/Dec/20 02:39,28/May/21 06:57,13/Jul/23 08:12,28/Dec/20 09:52,,,,,,1.12.1,1.13.0,,API / Python,,,,,0,pull-request-available,,,,,"Currently if users write such a SQL:

`SELECT pyFunc5(f0, f1) FROM (SELECT e.f0, e.f1 FROM (SELECT pyFunc5(a) as e FROM MyTable) where e.f0 is NULL)`

It will be optimized to:

`FlinkLogicalCalc(select=[pyFunc5(pyFunc5(a)) AS f0])
+- FlinkLogicalCalc(select=[a], where=[IS NULL(pyFunc5(a).f0)])
 +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])`

The optimized plan is not runnable, we need to fix this.",,dian.fu,hxbks2ks,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 28 13:45:26 UTC 2020,,,,,,,,,,"0|z0lspc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/20 02:04;hxbks2ks;[~zhongwei] Thanks a lot for reporting this issue. I will look into it asap;;;","28/Dec/20 09:52;dian.fu;Fixed in master via 7d6715c27c782f467089d43b8c0c4dfc32dea156;;;","28/Dec/20 13:45;dian.fu;Fixed in release-1.12 via 3176d76b75b341eb2920a303285bf608723939c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FailureRateRestartBackoffTimeStrategy allows one less restart than configured,FLINK-20752,13347594,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,23/Dec/20 11:49,01/Apr/21 07:47,13/Jul/23 08:12,23/Dec/20 17:10,1.10.0,,,,,1.11.4,1.12.3,1.13.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The {{FailureRateRestartBackoffTimeStrategy}} maintains a list of failure timestamps, keeping N timestamps where N is the configured of failures per interval.

The timestamp is added when #notifyFailure() is called, and later evaluated within #canRestart().

To determine whether a restart should be allowed we first check whether we are already storing N timestamps, and if so check whether the earliest failure still falls within the current interval. If it does, we reject the restart.

The problem is that we check whether we have already stored exactly N timestamps. If we have exactly N timestamps, and we allow N failures per interval, then we should not reject the restart by definition. We should instead be checking whether N+1 timestamps have been stored.

For example, let's say we allow 2 exceptions, and 2 have occurred so far. Regardless of what the timestamps are, we should still allow a restart in this case.
Only once a third exception occurs should we be looking at the timestamps, and we should furthermore only look at the exception exceeding the allowed failure count; in this example it is the very first exception.

I don't know why this went unnoticed for so long; the relevant tests fail rather reliably for me locally. ({{FailureRateRestartBackoffTimeStrategyTest}}, {{SimpleRecoveryFailureRateStrategyITBase}})

 ",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 07:47:20 UTC 2021,,,,,,,,,,"0|z0ls7s:",9223372036854775807,The Failure Rate Restart Strategy was allowing 1 less restart per interval as configured. Users wishing to keep the current behavior should reduce the maximum number of allowed failures per interval by 1.,,,,,,,,,,,,,,,,,,,"23/Dec/20 17:10;chesnay;master: 342ec9b9e7e4021b5fb90220d098590d80620005;;;","31/Mar/21 11:13;trohrmann;I think we should backport this to 1.12 and 1.11 as well.;;;","31/Mar/21 11:19;trohrmann;I will do this as part of FLINK-21609.;;;","01/Apr/21 07:47;trohrmann;1.12.3: bd055ed03690b5646f7263ba040fa59c3437daa2
1.11.4: a6d802dde65476b1ebc7f2d9698dfb374575b4d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip deployment of StateFun example artifacts,FLINK-20742,13347508,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,23/Dec/20 07:05,22/Jan/21 06:41,13/Jul/23 08:12,21/Jan/21 13:27,,,,,,statefun-3.0.0,,,Stateful Functions,,,,,0,pull-request-available,,,,,"Starting from the next Stateful Functions release, we'd like to stop publishing Maven artifacts for the examples.

We never expect users to be trying out examples through this artifacts, and therefore releasing them is not required.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 13:27:09 UTC 2021,,,,,,,,,,"0|z0lroo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/21 13:27;tzulitai;Merged to flink-statefun/master: d5da910a6bdbc97a4b191d9ad4429a3b8c265c7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateFun's DataStream interop SDK module is missing a valid NOTICE file,FLINK-20741,13347477,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,23/Dec/20 05:03,23/Dec/20 05:23,13/Jul/23 08:12,23/Dec/20 05:23,,,,,,statefun-2.2.2,,,Stateful Functions,,,,,0,,,,,,"The {{statefun-flink-datastream}} module bundles a fat jar, just like the {{statefun-flink-distribution}} module. Likewise, for legal purposes, all bundled dependencies need to be acknowledged in the NOTICE file.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 05:23:14 UTC 2020,,,,,,,,,,"0|z0lrhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/20 05:23;tzulitai;statefun/master: 265a8a8593f6082a9ac49afec9e100ec4080c23f
statefun/release-2.2: 80ce1b006cbadd8bad34e79d2f07faa206cecf32;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testCassandraPojoNoAnnotatedKeyspaceAtLeastOnceSink failed due to NoHostAvailableException,FLINK-20723,13347360,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,22/Dec/20 13:39,01/Jun/21 07:07,13/Jul/23 08:12,23/Apr/21 14:59,1.11.3,1.12.2,1.13.0,,,1.13.0,,,Connectors / Cassandra,,,,,0,pull-request-available,stale-assigned,test-stability,,,"[Build 20201221.17|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11137&view=results] failed due to {{NoHostAvailableException}}:
{code}
[ERROR] Tests run: 17, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 167.927 s <<< FAILURE! - in org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase
[ERROR] testCassandraPojoNoAnnotatedKeyspaceAtLeastOnceSink(org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase)  Time elapsed: 12.234 s  <<< ERROR!
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/127.0.0.1] Timed out waiting for server response))
	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:37)
	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:39)
	at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.createTable(CassandraConnectorITCase.java:221)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/127.0.0.1] Timed out waiting for server response))
	at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:218)
{code}",,dwysakowicz,hxbks2ks,maguowei,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 07:07:48 UTC 2021,,,,,,,,,,"0|z0lqrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/21 03:21;maguowei;another instance

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13031&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051;;;","15/Feb/21 08:03;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13325&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=eb5f4d19-2d2d-5856-a4ce-acf5f904a994;;;","17/Feb/21 08:26;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13385&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","22/Feb/21 02:56;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13531&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","03/Mar/21 06:02;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14014&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9;;;","22/Mar/21 02:40;maguowei;CassandraConnectorITCase.createTable also fail because of NoHostAvailableException
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13470;;;","23/Mar/21 08:28;chesnay;Let's just increase the client<->cassandra timeouts. We can see from the logs that subsequent tests are running fine again.;;;","01/Apr/21 07:39;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15936&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=13891;;;","13/Apr/21 03:02;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16396&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20
;;;","20/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","21/Apr/21 07:22;dwysakowicz;different test same exception: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16906&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=14754;;;","21/Apr/21 14:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16947&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13953;;;","23/Apr/21 14:59;chesnay;The tests will now be retried 2 times if a NoHostAvailableException occurs.

master: 2cbaf4e6ca87e96c4215818fe9c238215aa8de1a

1.13: 2413649b0c04b72a38e57d0a57f71d9ff5919274 ;;;","24/Apr/21 03:12;maguowei;There are some failed tests on the 1.11 brach because of the same reason.
 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17130&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051&l=14808

;;;","01/Jun/21 07:07;dwysakowicz;There are also failures on 1.12: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18466&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14429

Shall we backport the changes?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSink should copy the record when converting RowData to Row,FLINK-20722,13347352,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,22/Dec/20 13:10,28/Aug/21 11:17,13/Jul/23 08:12,21/Apr/21 03:13,1.12.2,,,,,1.12.3,1.13.0,1.14.0,Connectors / Hive,,,,,0,pull-request-available,,,,,"Add the following test in {{TableEnvHiveConnectorITCase}} to reproduce the issue:
{code}
	@Test
	public void test() throws Exception {
		TableEnvironment tableEnv = getTableEnvWithHiveCatalog();
		tableEnv.executeSql(""create table src1(key string, val string)"");
		tableEnv.executeSql(""create table src2(key string, val string)"");
		tableEnv.executeSql(""create table dest(key string, val string)"");
		HiveTestUtils.createTextTableInserter(hiveCatalog, ""default"", ""src1"")
				.addRow(new Object[]{""1"", ""val1""})
				.addRow(new Object[]{""2"", ""val2""})
				.addRow(new Object[]{""3"", ""val3""})
				.commit();
		HiveTestUtils.createTextTableInserter(hiveCatalog, ""default"", ""src2"")
				.addRow(new Object[]{""3"", ""val4""})
				.addRow(new Object[]{""4"", ""val4""})
				.commit();
		tableEnv.executeSql(""INSERT OVERWRITE dest\n"" +
				""SELECT j.*\n"" +
				""FROM (SELECT t1.key, p1.val\n"" +
				""      FROM src2 t1\n"" +
				""      LEFT OUTER JOIN src1 p1\n"" +
				""      ON (t1.key = p1.key)\n"" +
				""      UNION ALL\n"" +
				""      SELECT t2.key, p2.val\n"" +
				""      FROM src2 t2\n"" +
				""      LEFT OUTER JOIN src1 p2\n"" +
				""      ON (t2.key = p2.key)) j"").await();
	}
{code}",,dwysakowicz,godfreyhe,jark,kezhuw,libenchao,lirui,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 03:13:02 UTC 2021,,,,,,,,,,"0|z0lqq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/20 13:12;lirui;Execution plan is:
{noformat}
== Abstract Syntax Tree ==
LogicalSink(table=[test-catalog.default.dest], fields=[key, val])
+- LogicalProject(key=[$0], val=[$1])
   +- LogicalUnion(all=[true])
      :- LogicalProject(key=[$0], val=[$3])
      :  +- LogicalJoin(condition=[=($0, $2)], joinType=[left])
      :     :- LogicalTableScan(table=[[test-catalog, default, src2]])
      :     +- LogicalTableScan(table=[[test-catalog, default, src1]])
      +- LogicalProject(key=[$0], val=[$3])
         +- LogicalJoin(condition=[=($0, $2)], joinType=[left])
            :- LogicalTableScan(table=[[test-catalog, default, src2]])
            +- LogicalTableScan(table=[[test-catalog, default, src1]])

== Optimized Physical Plan ==
Sink(table=[test-catalog.default.dest], fields=[key, val])
+- Union(all=[true], union=[key, val])
   :- Calc(select=[key, val])
   :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(key, key0)], select=[key, key0, val], build=[left])
   :     :- Exchange(distribution=[hash[key]])
   :     :  +- TableSourceScan(table=[[test-catalog, default, src2, project=[key]]], fields=[key])
   :     +- Exchange(distribution=[hash[key]])
   :        +- TableSourceScan(table=[[test-catalog, default, src1]], fields=[key, val])
   +- Calc(select=[key, val])
      +- HashJoin(joinType=[LeftOuterJoin], where=[=(key, key0)], select=[key, key0, val], build=[left])
         :- Exchange(distribution=[hash[key]])
         :  +- TableSourceScan(table=[[test-catalog, default, src2, project=[key]]], fields=[key])
         +- Exchange(distribution=[hash[key]])
            +- TableSourceScan(table=[[test-catalog, default, src1]], fields=[key, val])

== Optimized Execution Plan ==
Sink(table=[test-catalog.default.dest], fields=[key, val])
+- MultipleInput(readOrder=[0,1], members=[\nUnion(all=[true], union=[key, val])\n:- Calc(select=[key, val])(reuse_id=[1])\n:  +- HashJoin(joinType=[LeftOuterJoin], where=[(key = key0)], select=[key, key0, val], build=[left])\n:     :- [#1] Exchange(distribution=[hash[key]])\n:     +- [#2] Exchange(distribution=[hash[key]])\n+- Reused(reference_id=[1])\n])
   :- Exchange(distribution=[hash[key]])
   :  +- TableSourceScan(table=[[test-catalog, default, src2, project=[key]]], fields=[key])
   +- Exchange(distribution=[hash[key]])
      +- TableSourceScan(table=[[test-catalog, default, src1]], fields=[key, val])
{noformat}
Stack trace is:
{noformat}
Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to org.apache.flink.table.data.RowData
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
	at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.pushToOperator(OneInputStreamOperatorOutput.java:77)
	at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.collect(OneInputStreamOperatorOutput.java:62)
	at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.collect(OneInputStreamOperatorOutput.java:32)
	at org.apache.flink.table.runtime.operators.multipleinput.output.BroadcastingOutput.collect(BroadcastingOutput.java:74)
	at org.apache.flink.table.runtime.operators.multipleinput.output.BroadcastingOutput.collect(BroadcastingOutput.java:37)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at BatchExecCalc$16.processElement(Unknown Source)
	at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.pushToOperator(OneInputStreamOperatorOutput.java:77)
	at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.collect(OneInputStreamOperatorOutput.java:62)
	at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.collect(OneInputStreamOperatorOutput.java:32)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.table.runtime.util.StreamRecordCollector.collect(StreamRecordCollector.java:44)
	at org.apache.flink.table.runtime.operators.join.HashJoinOperator.collect(HashJoinOperator.java:201)
	at org.apache.flink.table.runtime.operators.join.HashJoinOperator.innerJoin(HashJoinOperator.java:184)
	at org.apache.flink.table.runtime.operators.join.HashJoinOperator$BuildOuterHashJoinOperator.join(HashJoinOperator.java:331)
	at org.apache.flink.table.runtime.operators.join.HashJoinOperator.joinWithNextKey(HashJoinOperator.java:178)
	at org.apache.flink.table.runtime.operators.join.HashJoinOperator.processElement2(HashJoinOperator.java:147)
	at org.apache.flink.table.runtime.operators.multipleinput.input.SecondInputOfTwoInput.processElement(SecondInputOfTwoInput.java:41)
	at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamMultipleInputProcessorFactory.java:259)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:184)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:157)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
	at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:83)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
{noformat};;;","22/Dec/20 13:18;lirui;I did a little debugging and I think it's because in {{BroadcastingOutput::collect}} we broadcast the same StreamRecord to all the outputs. However, HiveTableSink [converts|https://github.com/apache/flink/blob/release-1.12.0/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java#L209] RowData to Row. Therefore when we pass the record to the 2nd output in BroadcastingOutput, the output finds out the data is a Row and thus the exception.;;;","23/Dec/20 02:48;godfreyhe;HiveTableSink should copy the record if object reuse enabled.;;;","23/Dec/20 07:41;lirui;Thanks [~godfreyhe] for your suggestions. I'll open a PR accordingly.;;;","01/Feb/21 06:24;jark;Sorry, but shouldn't this be fixed in {{multipleinput.output.BroadcastingOutput}}? Otherwise, all the connectors may need this special fix which is introduced by multi-input. IMO, multi-input should create a copying {{BroadcastingOutput}} when object reuse is enabled, just like how runtime operator chian does, {{CopyingBroadcastingOutputCollector}} vs. {{BroadcastingOutputCollector}}. What do you think [~godfreyhe]?;;;","02/Apr/21 02:53;jark;ping [~godfreyhe];;;","02/Apr/21 03:00;godfreyhe;[~lirui] [~jark], we should use CopyingBroadcastingOutput if isObjectReuseEnabled is true, just like line #586 in OperatorChain. ;;;","02/Apr/21 05:55;jark;Yes. Assigned to you [~godfreyhe].;;;","12/Apr/21 06:36;dwysakowicz;Do you plan to include it in 1.13 [~jark]  [~godfreyhe] ?;;;","21/Apr/21 03:13;godfreyhe;Fixed in 1.140: 65827041830e1332aeb373eb64ae31505a49f268
Fixed in 1.13.0: d58db3b6d0129273ae592e541c3c25b6b379ba37
Fixed in 1.12.3: aada320d9027450e915b5791d86552bc9a30b386;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter enumeration value error in ES doc,FLINK-20711,13347248,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jackwangcs,sunzheng,sunzheng,22/Dec/20 04:12,28/May/21 07:01,13/Jul/23 08:12,06/Jan/21 07:22,1.12.0,,,,,1.13.0,,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"[https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/elasticsearch.html]

 
|h5. failure-handler|optional|fail|String|Failure handling strategy in case a request to Elasticsearch fails. Valid strategies are:
 * {{fail}}: throws an exception if a request fails and thus causes a job failure.
 * {{ignore}}: ignores failures and drops the request.
 * {{retry_rejected}}: re-adds requests that have failed due to queue capacity saturation.
 * custom class name: for failure handling with a ActionRequestFailureHandler subclass.|

 

The retry_rejected parameter is wrong , I check the source code should be 

{{retry-rejected}}",,jackwangcs,jark,sunzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 07:22:49 UTC 2021,,,,,,,,,,"0|z0lq2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/20 09:08;jackwangcs;Hi [~sunzheng] and [~jark], I'd like to take this, could you assign this to me?

 ;;;","06/Jan/21 07:22;jark;Fixed in master: fd448ef1e175d01bcc2e8867372fb0721c53f4cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableInputFormat and HiveSourceFileEnumerator will throw IllegalArgumentException when creating splits for tables with partitions on different hdfs nameservices,FLINK-20710,13347240,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,jackwangcs,jackwangcs,22/Dec/20 03:19,06/Jan/21 04:42,13/Jul/23 08:12,06/Jan/21 04:42,1.11.2,1.11.3,1.12.0,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"HiveTableInputFormat will throws the IllegalArgumentException if partitions of a table are in different HDFS nameservices:
{code:java}
Caused by: java.lang.IllegalArgumentException: Wrong FS: hdfs://ns1/hive/warehouse/test.db/flink_test/day=4, expected: hdfs://ns2
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:662)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:222)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:114)
        at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1266)
        at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1262)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1262)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1418)
        at org.apache.flink.connectors.hive.read.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:299)
        at org.apache.flink.connectors.hive.read.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:282)
        at org.apache.flink.connectors.hive.HiveTableSource.createBatchSource(HiveTableSource.java:214)
        at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:188) {code}
The FLINK-16197 introduced this issue.

The HiveSourceFileEnumerator also has this issue",Flink 1.11.2 on Yarn.,jackwangcs,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 04:42:24 UTC 2021,,,,,,,,,,"0|z0lq14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/20 03:26;jackwangcs;Currently, the Hive connector does not have full support for tables with partitions on different Filesystems as far as I know. But we can fix this bug and make Flink be able to query the data.

 

I'd like to submit a PR for this issue. ;;;","22/Dec/20 04:21;jackwangcs;Hi [~lirui], could you please take a look at this? Thank you;;;","06/Jan/21 04:42;jackwangcs;Fixed in master: 5ffcfcafe4fa40988a7a3a03b45b5960792416c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some rel data type does not implement the digest correctly,FLINK-20704,13347228,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,danny0405,danny0405,22/Dec/20 02:51,28/May/21 06:59,13/Jul/23 08:12,31/Dec/20 03:04,1.11.3,1.12.0,,,,1.12.1,,,Table SQL / Legacy Planner,,,,,0,pull-request-available,,,,,"Some of the rel data types for legacy planner:

- {{GenericRelDataType}}
- {{ArrayRelDataType}}
- {{MapRelDataType}}
- {{MultisetRelDataType}}

Does not implement the digest correctly, especially for {{GenericRelDataType}} , the {{RelDataTypeFactory}} caches the type instances based on its digest, a wrong digest impl would mess up the type instance creation, e.g. without this patch, all the {{GenericRelDataType}} instances have same digest `ANY`.",,danny0405,godfreyhe,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20785,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 31 03:04:30 UTC 2020,,,,,,,,,,"0|z0lpyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Dec/20 03:04;godfreyhe;Fixed in 1.12.1: 712becde6f8f771a000f50a4ed0963d381431900
Fixed in 1.13.0: 64d9ea339fe6360eb9c59a6fd1946948c2fecbf9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveSinkCompactionITCase test timeout,FLINK-20703,13347227,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Dec/20 02:33,22/Dec/20 06:46,13/Jul/23 08:12,22/Dec/20 06:46,,,,,,1.12.1,1.13.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11096&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11136&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf]

The phenomenon is:
 * All failure are related to the hive tests.
 * All tests timeout when fail.",,godfreyhe,hxbks2ks,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20665,,,,FLINK-20028,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 06:46:33 UTC 2020,,,,,,,,,,"0|z0lpy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/20 02:35;godfreyhe;same failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11123&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","22/Dec/20 03:28;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11138&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91;;;","22/Dec/20 03:37;lzljs3620320;This due to FLINK-20665, I will fix this soon.;;;","22/Dec/20 06:46;lzljs3620320;release-1.12: 80bd16917f42a9cd31189fd7473ac803a4738363

master (1.13): 0041ef3de7afa25ebba652d556f5743b5b30a50f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Declarative resource management can get stuck in a loop,FLINK-20694,13347098,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,21/Dec/20 08:10,23/Dec/20 10:00,13/Jul/23 08:12,23/Dec/20 10:00,1.13.0,,,,,1.13.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,We've seen a few tests where the coordination layer gets stuck in a request slot -> offer slot -> reject slot -> request slot loop.,,guoyangze,hxbks2ks,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20688,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 10:00:51 UTC 2020,,,,,,,,,,"0|z0lp5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/20 10:00;chesnay;master:

337f8dc3e35401f5b798bf17d5d0f6d8fc5969f0

44280a44b3394e0adb417684df40eb710cc016c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fails to call var-arg function with no parameters,FLINK-20680,13346911,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,19/Dec/20 09:45,28/May/21 07:13,13/Jul/23 08:12,25/Jan/21 10:32,,,,,,1.11.4,1.12.2,1.13.0,Table SQL / API,,,,,0,pull-request-available,,,,,,,jark,libenchao,lirui,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 10:32:12 UTC 2021,,,,,,,,,,"0|z0lo00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/21 10:32;twalthr;Fixed in 1.13.0: 1ee13f261a04b3a5bdb223381348bb7789e60687 59454b8f1f3d5738ea4ab5bc4c5717af1979e0ee
Fixed in 1.12.2: 923863316601564cfa5ba65a3982fd6a1b437bcf
Fixed in 1.11.4: ca3f080068266676ee5d3ff43918463dc794a99c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterTest.testSlotRequestTimeoutWhenNoSlotOffering test failed,FLINK-20679,13346884,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,hxbks2ks,hxbks2ks,19/Dec/20 03:01,22/Dec/20 09:40,13/Jul/23 08:12,22/Dec/20 09:40,1.13.0,,,,,1.13.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11055&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6]
{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterTest.testSlotRequestTimeoutWhenNoSlotOffering:972 
Expected: a collection with size <1>
     but: collection size was <0>
{code}",,dian.fu,godfreyhe,hxbks2ks,jark,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10404,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 09:40:54 UTC 2020,,,,,,,,,,"0|z0lnu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/20 14:45;trohrmann;I think this test instability is caused by FLINK-10404. The problem is that the {{SlotPool}} no longer eagerly accepts newly offered slots. With the declarative resource management, it can happen that we are offering the slots just in the moment when the job fails over. Since we are decreasing the required resources when we are failing over, the offered slots can be rejected.

cc [~chesnay].;;;","21/Dec/20 14:51;trohrmann;Looking at the test case, I think we should rather test the behaviour on the {{SchedulerNG}} instead of on the {{JobMaster}}.;;;","21/Dec/20 15:12;trohrmann;The scheduling part should be covered by {{DefaultSchedulerTest.restartVerticesOnSlotAllocationTimeout}} which ensures that vertices are restarted on allocation timeouts. The allocation timeout part should be covered by {{PhysicalSlotRequestBulkCheckerImplTest.testUnfulfillableBulkIsCancelled}} which ensures that {{PhysicalSlotRequestBulks}} are timed out after a configured allocation timeout.;;;","22/Dec/20 02:06;godfreyhe;same failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11118&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","22/Dec/20 03:02;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11126&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","22/Dec/20 06:39;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11151&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","22/Dec/20 09:40;trohrmann;Fixed via 2aa598b7e505bd0553d636726b1588fcf5e850c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManagerRunnerImpl does not notice unexpected JobMasterService termination,FLINK-20678,13346818,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,18/Dec/20 16:18,23/Dec/20 10:16,13/Jul/23 08:12,23/Dec/20 10:16,1.13.0,,,,,1.13.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,The {{JobManagerRunnerImpl}} does not notice unexpected terminations of the {{JobMasterService}}. This puts the system stability at risk and I propose to monitor the liveliness of the {{JobMasterService}} and then to fail if it should terminate unexpectedly.,,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 10:16:02 UTC 2020,,,,,,,,,,"0|z0lnfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/20 10:16;trohrmann;Fixed via 60b0745c56476e88bc99abf64b4efe60150a1d05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Asynchronous checkpoint failure would not fail the job anymore,FLINK-20675,13346813,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,yunta,yunta,18/Dec/20 15:49,03/Mar/21 13:02,13/Jul/23 08:12,29/Jan/21 08:10,1.10.2,1.11.3,1.12.0,1.9.3,,1.11.4,1.12.2,1.13.0,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"After FLINK-12364, no mater how many times of asynchronous part of checkpoint on task failed, the job itself would not fail by default:
| Default behavior|Flink-1.5 —> Flink-1.8||Flink-1.9 -> Flink-1.12||
|Synchronous part of checkpoint at task failed|Job failed|Job failed|
|Asynchronous part of checkpoint at task failed| Job failed| Job would not fail|

 This error was because {{StreamTask}} use {{Exception}} instead of {{CheckpointException}} [when async part failed|https://github.com/apache/flink/blob/5125b1123dfcfff73b5070401dfccb162959080c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L1118] as decline message. Thus checkpoint coordinator would call {{failPendingCheckpointDueToTaskFailure(pendingCheckpoint, CheckpointFailureReason.JOB_FAILURE, cause, executionAttemptID)}} to [process the declined checkpoint|https://github.com/apache/flink/blob/release-1.9/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1316-L1323]:
{code:java}
if (cause == null) {
	failPendingCheckpointDueToTaskFailure(pendingCheckpoint, CheckpointFailureReason.CHECKPOINT_DECLINED, executionAttemptID);
} else if (cause instanceof CheckpointException) {
	CheckpointException exception = (CheckpointException) cause;
	failPendingCheckpointDueToTaskFailure(pendingCheckpoint, exception.getCheckpointFailureReason(), cause, executionAttemptID);
} else {
	failPendingCheckpointDueToTaskFailure(pendingCheckpoint, CheckpointFailureReason.JOB_FAILURE, cause, executionAttemptID);
}
{code}
However, {{CheckpointFailureManager}} would [ignore the JOB_FAILURE reason|https://github.com/apache/flink/blob/5125b1123dfcfff73b5070401dfccb162959080c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointFailureManager.java#L108] and not count this failed checkpoint, which causes asynchronous checkpoint failure would not fail the job anymore.

 

FLINK-16753 corrects the misleading message of JOB_FAILURE but the asynchronous checkpoint failure still cannot fail the job.

 

As this bug exists too long, I decide to set it as critical instead of blocker level. 

 ",,kezhuw,klion26,liyu,pnowojski,stevenz3wu,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12364,,FLINK-21215,FLINK-21244,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 08:10:46 UTC 2021,,,,,,,,,,"0|z0lne8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/20 15:50;yunta;cc [~pnowojski];;;","08/Jan/21 10:13;pnowojski;Hey [~yunta], do you have any progress working on this issue?;;;","11/Jan/21 04:00;yunta;[~pnowojski], thanks for your reminder. I tried to [modify locally|https://github.com/Myasuka/flink/tree/async-checkpoint-failure] before 2021 new year's day but found it was a bit complicated than what I thought. I'll submit the PR in this week.;;;","28/Jan/21 10:07;yunta;Merged in master with three commits:
f421c6f72b91b7897ff2246bdf4f408e3609f6f0
c7ed2e818a4ead70c6081117f771c54a1bbc0fc5
be6eb95af5eaa866ead85ebfa89bdbc7c57c6edc;;;","29/Jan/21 02:18;yunta;Merged in release-1.12 with two commits:
d1f8f41b773ec160d4ccee8f6b82e63afd78c901
4d6b952a82facc838d5ea73bc8fe20e6dc66e1cf;;;","29/Jan/21 08:10;yunta;Merged into release-1.11 with two commits:
43fa9bfeb3768368da80011b0b41cc9ed28d98d0
4ad5c811c075901ee90c64e3272effe7d66dc7fe

Private CI success: https://myasuka.visualstudio.com/flink/_build/results?buildId=245&view=results;;;","01/Feb/21 08:10;pnowojski;FYI: this change caused UnalignedCheckpointITCase to become unstable.

https://issues.apache.org/jira/browse/FLINK-21215

Cancellation of Async part of the checkpoint is still not working correctly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the jzlib LICENSE file in flink-python module,FLINK-20669,13346704,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Dec/20 03:36,18/Dec/20 05:09,13/Jul/23 08:12,18/Dec/20 05:09,1.12.0,1.13.0,,,,1.12.1,1.13.0,,API / Python,,,,,0,pull-request-available,,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 18 05:09:25 UTC 2020,,,,,,,,,,"0|z0lmq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/20 05:09;dian.fu;Fixed in
- master via 0a5632fd6de2dcab3e248ed1284830ef420614e5
- release-1.12 via 8755b33bc1da3ba9b368aaa3aaf6f09ab422e525;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the deserialized Row losing the field_name information in PyFlink,FLINK-20666,13346697,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Dec/20 02:58,19/Dec/20 02:32,13/Jul/23 08:12,19/Dec/20 02:32,1.11.2,1.12.0,,,,1.11.4,1.12.1,1.13.0,API / Python,,,,,0,pull-request-available,,,,,"Now, the deserialized Row loses the field_name information.
{code:java}
@udf(result_type=DataTypes.STRING())
def get_string_element(my_list):
    my_string = 'xxx'
    for element in my_list:
        if element.integer_element == 2:  # element lost the field_name information
            my_string = element.string_element
    return my_string

t = t_env.from_elements(
    [(""1"", [Row(3, ""flink"")]), (""3"", [Row(2, ""pyflink"")]), (""2"", [Row(2, ""python"")])],
    DataTypes.ROW(
        [DataTypes.FIELD(""Key"", DataTypes.STRING()),
         DataTypes.FIELD(""List_element"",
                         DataTypes.ARRAY(DataTypes.ROW(
                             [DataTypes.FIELD(""integer_element"", DataTypes.INT()),
                              DataTypes.FIELD(""string_element"", DataTypes.STRING())])))]))
print(t.select(get_string_element(t.List_element)).to_pandas())
{code}
element lost the field_name information",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 19 02:32:40 UTC 2020,,,,,,,,,,"0|z0lmog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/20 02:32;dian.fu;Fixed in
- master via 2061b720f6995c0701df390dce767d1e2a9645b2
- release-1.12 via cd4d3f1bed19c5d3223f493a200922734e7f87d3
- release-1.11 via 3c0253936f2e6cf8d4fcb3d5b4c6050de2626d9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException when restore from latest Checkpoint,FLINK-20665,13346696,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ZhuShang,ZhuShang,ZhuShang,18/Dec/20 02:45,22/Dec/20 03:36,13/Jul/23 08:12,21/Dec/20 02:34,1.12.0,,,,,1.12.1,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"reproduce steps:

1.a kafka to hdfs job,open `auto-compaction`

2.when the job have done a successful checkpoint then cancel the  job.

3.restore from the latest checkpoint.

4.after the first checkpoint has done ,the exception will appear
{code:java}
2020-12-18 10:40:58java.io.UncheckedIOException: java.io.FileNotFoundException: File does not exist: hdfs://xxxx/day=2020-12-18/hour=10/.uncompacted-part-84db54f8-eda9-4e01-8e85-672144041642-0-0    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.lambda$coordinate$1(CompactCoordinator.java:160)    at org.apache.flink.table.runtime.util.BinPacking.pack(BinPacking.java:41)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.lambda$coordinate$2(CompactCoordinator.java:169)    at java.util.HashMap.forEach(HashMap.java:1289)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.coordinate(CompactCoordinator.java:166)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.commitUpToCheckpoint(CompactCoordinator.java:147)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.processElement(CompactCoordinator.java:137)    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:193)    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:179)    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:152)    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)    at java.lang.Thread.run(Thread.java:748)Caused by: java.io.FileNotFoundException: File does not exist: hdfs://xxxx/day=2020-12-18/hour=10/.uncompacted-part-84db54f8-eda9-4e01-8e85-672144041642-0-0    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1441)    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:85)    at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.getFileStatus(SafetyNetWrapperFileSystem.java:64)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.lambda$coordinate$1(CompactCoordinator.java:158)    ... 17 more

{code}
DDL
{code:java}
CREATE TABLE cpc_bd_recall_log_hdfs (    log_timestamp BIGINT,    ip STRING,    `raw` STRING,    `day` STRING, `hour` STRING) PARTITIONED BY (`day` , `hour`) WITH (    'connector'='filesystem',    'path'='hdfs://xxx',    'format'='parquet',    'parquet.compression'='SNAPPY',    'sink.partition-commit.policy.kind' = 'success-file',    'auto-compaction' = 'true',    'compaction.file-size' = '128MB');
{code}",,jark,leonard,lzljs3620320,xiaozilong,xtsong,yunta,zhisheng,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20703,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 21 02:34:43 UTC 2020,,,,,,,,,,"0|z0lmo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/20 03:35;ZhuShang;When the file is big,it will be rename directly.So when restore from latest checkpoint,the file is lost.
Maybe,it is the root casue.WDYT [~jark],[~lzljs3620320];;;","18/Dec/20 03:48;lzljs3620320;Hi [~ZhuShang], yes, we should keep old temp file for big files.
 * Simple solution: We can remove the optimization of HDFS. Just copy the bytes to new file.
 * Better solution: I don't know if HDFS supports adding soft links to avoid copying data.

I think we can choose simple solution to fix this problem in 1.12.1. WDYT, [~ZhuShang];;;","18/Dec/20 03:49;lzljs3620320;[~ZhuShang] Do you want to fix this?;;;","18/Dec/20 03:53;ZhuShang;[~lzljs3620320],I'd like to do this job.
 BTW,I find that if some small files are compacted,when restore, they will be compacted again.
How can we ensure data consistency.;;;","18/Dec/20 03:57;lzljs3620320;Assigned to u [~ZhuShang]. I marked this as a Blocker. We should fix this before 1.12.1.

> BTW,I find that if some small files are compacted,when restore, they will be compacted again.

If it has been completely compacted, it should not be compacted again, unless it is not completed.;;;","18/Dec/20 05:33;ZhuShang;[~lzljs3620320],yes you are write,if it has been compacted, the target file is exists,then will not compact.
sorry for misunderstanding;;;","21/Dec/20 02:34;lzljs3620320;master (1.13): 94e4d2af6e963d6d97ba685b5fa0c556c26376d5

release-1.12: 774d92da33bae9c5df7110adb3ae29c1d0f3bc8c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support setting service account for TaskManager pod,FLINK-20664,13346695,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lublinsky,wangyang0918,wangyang0918,18/Dec/20 02:42,25/Dec/20 09:05,13/Jul/23 08:12,25/Dec/20 09:05,1.12.0,,,,,1.12.1,1.13.0,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"Currently, we only set the service account for JobManager. The TaskManager is using the default service account. Before the KubernetesHAService is introduced, it works because the TaskManager does not need to access the K8s resource(e.g. ConfigMap) directly. But now the TaskManager needs to watch ConfigMap and retrieve leader address. So if the default service account does not have enough permission, users could not specify a valid service account for TaskManager.

 

We should introduce a new config option for TaskManager service account. 

{{kubernetes.taskmanager.service-account}}

 

 ",,lublinsky,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 25 09:05:45 UTC 2020,,,,,,,,,,"0|z0lmo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/20 03:33;lublinsky;Do you mean:

kubernetes.taskmanager.service-account ?

I think a better solution is to use the same service account as job manager. In this case one role and one binding is sufficient. If you make them different, then a user will have to create two bindings ;;;","18/Dec/20 03:56;wangyang0918;Since the user code is executing on the TaskManager, I am afraid some users do not want to give TaskManager pods a service account with too many permissions. Currently, only for ConfigMap is enough.

Why do we need to create two binding? I think the {{kubernetes.taskmanager.service-account}} could be set same as {{kubernetes.jobmanager.service-account}} if users want. Two config options are more flexible. Right?;;;","18/Dec/20 16:22;wangyang0918;[~lublinsky] Do you think adding a new config option {{kubernetes.taskmanager.service-account}} makes sense to you?;;;","18/Dec/20 16:47;lublinsky;I think so. This gives me the flexibility. I can decide to make it the same as job manager or different;;;","21/Dec/20 01:52;lublinsky;PR is created - [https://github.com/apache/flink/pull/14447] 

 ;;;","21/Dec/20 02:46;wangyang0918;[~lublinsky] Thanks for this PR. Actually, I am already working on this. Since you are interested in the fixing, [~xintongsong] could you please re-assign this ticket to Boris. I could help with reviewing.;;;","22/Dec/20 09:29;trohrmann;Thanks for driving this effort. What in a situation like this always worked best is to introduce a common configuration option like {{kubernetes.service-account}} which is used for both JM and TM unless a more specific configuration option {{kubernetes.service-account.jobmanager}} or {{kubernetes.service-account.taskmanager}} has been configured. That way you don't force people to configure two options if they are ok with using a single service account for both processes.;;;","22/Dec/20 10:06;wangyang0918;Thanks for the nice suggestion. [~lublinsky] would you like to integrate the comments in the PR via introducing a common config option key {{kubernetes.service-account}}?;;;","25/Dec/20 09:05;xtsong;Fixed via
 * master (1.13): 4e5448cc332090d7bd2664f60278534d945e1211
 * release-1.12: d3ec48b3cdc7aaf6cef8978bf7c0545be7ce43e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed memory may not be released in time when operators use managed memory frequently,FLINK-20663,13346693,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,TsReaper,TsReaper,18/Dec/20 02:31,09/Apr/21 11:43,13/Jul/23 08:12,15/Feb/21 15:24,1.12.0,,,,,1.12.2,1.13.0,,Runtime / Task,,,,,0,pull-request-available,,,,,"Some batch operators (like sort merge join or hash aggregate) use managed memory frequently. When these operators are chained together and the cluster load is a bit heavy, it is very likely that the following exception occurs:

{code:java}
2020-12-18 10:04:32
java.lang.RuntimeException: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
	at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:85)
	at org.apache.flink.runtime.io.disk.SimpleCollectingOutputView.<init>(SimpleCollectingOutputView.java:49)
	at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap$RecordArea.<init>(BytesHashMap.java:297)
	at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.<init>(BytesHashMap.java:103)
	at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.<init>(BytesHashMap.java:90)
	at LocalHashAggregateWithKeys$209161.open(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:401)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:506)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:834)
	Suppressed: java.lang.NullPointerException
		at LocalHashAggregateWithKeys$209161.close(Unknown Source)
		at org.apache.flink.table.runtime.operators.TableStreamOperator.dispose(TableStreamOperator.java:46)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:739)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:719)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:642)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:551)
		... 3 more
		Suppressed: java.lang.NullPointerException
			at LocalHashAggregateWithKeys$209766.close(Unknown Source)
			... 8 more
Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
	at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:231)
	at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:83)
	... 13 more
Caused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 16777216 bytes, only 9961487 bytes are remaining. This usually indicates that you are requesting more memory than you have reserved. However, when running an old JVM version it can also be caused by slow garbage collection. Try to upgrade to Java 8u72 or higher if running on an old Java version.
	at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:164)
	at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:80)
	at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:229)
	... 14 more
{code}

It seems that this is caused by relying on GC to release managed memory, as {{System.gc()}} may not trigger GC in time. See {{UnsafeMemoryBudget.java}}.",,hxbks2ks,jingzhang,kezhuw,libenchao,liyu,trohrmann,TsReaper,wangm92,wind_ljy,xtsong,ym,zhou_yb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15758,FLINK-13985,,,,,,,,,,,,,,"08/Apr/21 03:25;zhou_yb;exception;https://issues.apache.org/jira/secure/attachment/13023530/exception","09/Apr/21 02:17;zhou_yb;summary.py;https://issues.apache.org/jira/secure/attachment/13023587/summary.py","08/Apr/21 08:27;zhou_yb;taskmanager.log;https://issues.apache.org/jira/secure/attachment/13023542/taskmanager.log",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 11:43:47 UTC 2021,,,,,,,,,,"0|z0lmnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/21 10:34;xtsong;cc [~trohrmann],

According to [~ykt836], this has become a severe problem for batch processing, especially the OLAP scenarios.

I traced the discussions and changes in FLINK-15758 and other related issues. It seems to me that we do not concretely see any problem in accessing the memory after the segment being released, except for the verifications in `MemorySegmentTestBase#testByteBufferWrapping`.

I'm currently working with [~ykt836]'s team on some internal experiments see if actively deallocate memory on the segment being freed indeed cause any trouble. Do you think it would be possible that, if the experiments go well, we bring the active releasing back at least for the use cases that is currently suffering from this issue?;;;","27/Jan/21 10:50;ykt836;It's not necessarily be OLAP scenario, but normal batch job will trigger this problem, if the TM will be reused by several tasks. The root cause is relying on GC to release memory is not only slow but time consuming. We have observed performance regression, high full gc counts and also causing OOM triggering unnecessary job failure. 

Most importantly, we didn't get benefit from such complex design... I would suggest we fallback to a simpler way, just like Flink did in the old days. (AFAIK, we didn't suffer any mis using issue before we changed this mechanism). 

cc [~sewen];;;","27/Jan/21 18:38;trohrmann;Thanks for reporting this issue. Which Java version are you using when this problem occurs? Versions below jdk8u72 don't reliably work (FLINK-18581).

Can you reproduce the same problem when using the DataSet API? I just want to make sure that we don't have a resource leak somewhere. Maybe drawing a heap dump could help shine some light on the problem.

Just to give a bit of background: The problem FLINK-15758 is supposed to solve is FLINK-14894. Since we don't do ref counting for the {{MemorySegments}} it is hard to know when we can truly release a {{MemorySegment}}. If we release a segment and we still have an owner which accesses this segment, then this can potentially lead to memory corruption. It is best described here: https://issues.apache.org/jira/browse/FLINK-14894?focusedCommentId=17023015&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17023015.;;;","28/Jan/21 01:25;ykt836;IIRC in the past, users of managed memory (operators, data structures) are required to know the contract when using memory segment, which is when they release the memory segments, they can just return the object back to memory manager, and don't access it ever again. 

And {{HybridMemorySegment}} and {{MemorySegment}} also provided some protection after being released in their free() method. The only exception is wrap() method, which caused FLINK-14894. But wrap() method is not that popular, and half of them are used by network buffer which is not even managed (they are unpooled segments). Other than this, all other methods are safe to use and will raise exception if being misused.

So my thought is, instead of relying on such inefficient and complex way (Full GC) to protect only one usage about memory segment, we can change this to much simpler way (like the old behaviors) and we pay enough attention of the wrap() method, e.g. adding some comments or find some way to protect the wrapped ByteBuffer. ;;;","28/Jan/21 10:45;xtsong;bq. If we release a segment and we still have an owner which accesses this segment, then this can potentially lead to memory corruption.

I understand the concern and thus the current GC-based memory releasing. And I'm also not sure whether the segment may indeed get accessed after being released. However, if a segment is accessed after being freed, wouldn't it already cause problems when previously heap/direct memory segments are reused (with the memory pre-allocation) across jobs/tasks?;;;","28/Jan/21 14:27;trohrmann;I understand that the current approach is not ideal in terms of performance. Before jumping to conclusions, can we verify whether we don't have a resource leak somewhere? I just wanna make sure that we don't overlook something in this discussion.

In general, I think that implicit contracts such as when using {{wrap()}} you have to make sure that you don't access the {{ByteBuffer}} after releasing the owning {{MemorySegment}} are bound to break eventually. Also comments won't really help against the problem. I also think that it does not make a difference whether the segment is pooled or unpooled. The only proper way I see at the moment is to make sure that accesses to {{ByteBuffer}} fail when the owning {{MemorySegment}} is released. 

At the moment, the {{MemorySegmentTestBase.testByteBufferWrapping}} ensures explicitly that we can access {{ByteBuffers}} after the {{MemorySegment}} has been released. Hence, I cannot rule out that we don't rely on this contract somewhere. We would have to hunt these places down and change the behaviour with a proper lifecycle management of the underlying memory.

If I remember correctly, then the problem is that we wouldn't necessarily see an illegal access because the underlying memory could have been allocated by a different {{MemorySegment}} in the same JVM. In this case, the access could silently corrupt some data w/o us ever realizing. I also think that we no longer allocate all the memory from the get go but only once the {{TaskSlots}} are created.;;;","29/Jan/21 01:58;ykt836;{quote}Before jumping to conclusions, can we verify whether we don't have a resource leak somewhere? I just wanna make sure that we don't overlook something in this discussion.
{quote}
Yes, we should definitely do this. We have modified this internally and running a bunch of cases to see whether there are some memory rats.
{quote}I understand that the current approach is not ideal in terms of performance. 
{quote}
From our experience, it only only affects performance but also cause instability. For streaming jobs, it might be okay, since they only allocate / free memory occasionally,  like when starting or stopping the job. For batch jobs, there will be much more frequent allocation/free. It will cause much more full GC, sometimes it will throw OOM exception if JVM can't free the memory enough quickly, and we also see full GC blocking some RPC thread and causing a bunch of timeout.

I also see the potential dangerous of ""unmanaged"" wrap() call will cause risk in the future. The ideal way would be we make it simple and straightforward for those memory users who follow the contract correctly (no more relying on GC for these), and we also have some safety net to hunt down the bad guys.  

Right now, we are using the safety net as the only option. No matter how correct you follow the contract, you still need to wait for GC to really free the memory you want to return, causing issues & instability for the job. Thus I think it's kind of unfair. ;;;","01/Feb/21 07:47;TsReaper;Hi,

I've tested the patch [~xintongsong] provided on some large batch SQL jobs and the memory problem didn't occur anymore (these jobs will frequently fail with memory problem every 10 minutes or so without the patch), so from the test results I think this is a decent solution.;;;","01/Feb/21 12:52;ykt836;Thanks [~TsReaper] for the testing.

[~trohrmann] I see following choices we have now:
 # Revert the GC based memory free mechanism and simply rely on the memory usage contract
 # Choose 1 and find a way to protect the exception (wrap() method in this case)
 # Further improve current mechanism to avoid performance & stability regression

I personally don't want to go with option 3 because it's already quite complex now. And most importantly, such mechanism doesn't create any value for now, only regression. What's your opinion?;;;","02/Feb/21 02:15;xtsong;I think option 2) might be a good way to go.

It looks like the underlying native memory of a segment is always accessed through the wrapped ByteBuffers. We could implement something like a WrappedByteBuffer. All read / write operations should be forwarded to the original ByteBuffer before the underlying memory is released, and fail after the memory is released. A ReadWriteLock is needed to make sure the freeing the segment is visible to all the wrapped ByteBuffers immediately.

I'm still looking into this. There're a few things to be checked.
- Impact on performance due to the lock-based synchronization
- Potential read/write API changes on ByteBuffer between different Java versions

If this can work, I would suggest the following actions.
1. Fail explicitly on attempts to access the memory after it's freed
2. By default release the memory when the segment is freed. This would make the potential access-after-release issues surface.
3. Keep the configuration option to disable releasing memory when the segment is freed and fallback to GC, in case users indeed run into access-after-release issues.
4. Remove the configuration option once we are confidence that there's no more access-after-release issues. This is optional. We could also keep it incase new access-after-release issues are introduced in future.;;;","02/Feb/21 02:32;ykt836;[~xintongsong] It doesn't need to be such complex. We can still use `DirectByteBuffer` after being wrapped, and record down it into a list. When free then parent segment, we can iterate the wrapped list to either:
 # call limit(0)
 # set address to -1 with reflection

 ;;;","02/Feb/21 03:10;xtsong;[~ykt836],
I don't think that works.

{code:java}
ByteBuffer bb1 = segment.wrap(offset, size);
ByteBuffer bb2 = bb1.duplicate();
{code}
In the above example, {{bb2}} will not be added to the recording list, thus can still access the memory. With our own `WrappedByteBuffer` implementation, we can make sure calling `duplicate()` on it will also return a `WrappedByteBuffer`.

Also, I don't think `DirectByteBuffer` is thread safe, thus changing its `limit` or `address` will not be visible to other threads immediately.;;;","02/Feb/21 03:32;ykt836;[~xintongsong] You are correct. 

Regarding to the lock part, do we really need this? Neither MemorySegment or ByteBuffer is thread safe, why suddenly we have to provide a thread-safe version? ;;;","02/Feb/21 03:56;xtsong;bq. Regarding to the lock part, do we really need this? Neither MemorySegment or ByteBuffer is thread safe, why suddenly we have to provide a thread-safe version?

That's a good point. I'd be glad to see if we don't need the lock, which solves the performance concern. However, I would need a bit more investigation to confirm that.;;;","02/Feb/21 04:15;ykt836;One more comment to hint that we are pretty safe when using managed memory.

With Flink <= 1.9, the default managed memory type [1] is heap memory, which will use HeapMemorySegment. With HeapMemorySegment, even if it is wrapped, they will still share the underlying byte[]. We haven't heard any memory illegal access before. 

[1] https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/config.html#taskmanager-memory-off-heap;;;","02/Feb/21 10:07;trohrmann;Thanks for driving the discussion. I feel that I lack a bit of context here. 

* Which patch did you test and what does it do [~TsReaper]? 
* Does this test ensure that we don't have a memory leak? 
* If not, have we checked that we don't have a memory leak somewhere in our code? 
* Have we been able to reproduce the problem with the {{DataSet/DataStream}} API only? 

Again, before jumping to conclusions and action plans, I would like to make sure that we don't overlook things here.

Concerning the action plan, I agree that option 2) is the best option here. But maybe there is also a fourth option: Try to establish a clear memory ownership where we don't hand ByteBuffers into components and then release the memory in a different component.

Concerning the thread-safety I think this might be needed if we hand a {{ByteBuffer}} to a different thread and release the underlying {{MemorySegment}} in another thread. That is the problem of unclear ownership we might have in the code base. Maybe the check behaviour could be made configurable if we realize that this a bottleneck for performance.

Just because the existing code base didn't make the {{MemorySegment}} and {{ByteBuffer}} thread safe, does not necessarily mean that this is correct. Moreover, just because something didn't crash before, does not mean that it was working either.


;;;","02/Feb/21 10:49;ykt836;[~trohrmann] Things are becoming more interesting. If I understand this correctly, I think the old design is exactly your forth option. Even the old design wasn't perfect, but it had clear memory ownership (at least to my understanding). Whoever allocate the memory should take care the whole lifecycle of these memory segments, including the wrapped ones. In this case, all allocators should be careful enough, and should never deliver them to, let's say another thread and loose the control ship. It caused some extra burden of the memory users but I think that's the price one have to pay for using managed memory of our own. ;;;","02/Feb/21 16:41;trohrmann;I am not sure whether the fourth option is really the old design. Looking again at {{MemorySegmentTestBase.testByteBufferWrapping}}, we guard that we can reuse a {{ByteBuffer}} after its underlying {{MemorySegment}} has been freed. Hence, I cannot tell for sure that we don't rely on this contract in our code base.

In general I also think that we should have an assertion making sure that nobody breaks our contract. This assertion should at least be active in our test runs. Given this, my fourth option is actually Xintong's second option. Sorry for the confusion.;;;","03/Feb/21 01:09;ykt836;I'm fine to add safety net for WrappedBuffer, read-write lock, memory fencing, GC or whatever you think is necessary to protect illegal memory access. But I'm still against to protect other memory de-allocation with GC:
 # We already have enough protection for these in MemorySegment#free() and HybridMemorySegment#free()
 # It slows down memory circulation between tasks, and maybe operators if we want memory be used more dynamically in the future
 # It creates extra complexity for nothing
 # It causes instability which we are currently suffering;;;","03/Feb/21 10:18;trohrmann;I understand the problems you are facing and I also see that the current solution (relying on GC) can potentially cause the problems. In order to move forward, could you please answer my questions I've posted to this thread? I really would like to rule out that we have a memory leak somewhere by accident and I would be happy to have the same context as you guys seem to have.;;;","03/Feb/21 10:42;ykt836;{quote}Which patch did you test and what does it do [~TsReaper]?
{quote}
Xingtong gave us a patch to release memory directly, just like before we change it to relying on GC.

 
{quote}{color:#172b4d}Does this test ensure that we don't have a memory leak?{color}
{quote}
I don't think so. We only observed there are no out-of-managed-memory anymore. 
{quote}{color:#172b4d}If not, have we checked that we don't have a memory leak somewhere in our code?{color}
{quote}
I have checked the codes, didn't find any suspicious places. As discussed, the only possible leak is *memory segment owned by memory manager* and *wrapped memory segments*. All these wrapped segments are actually just want to turn memory segment into ByteBuffer and then call some nio functions. 
{quote}{color:#172b4d}Have we been able to reproduce the problem with the {color}{{DataSet/DataStream}}{color:#172b4d} API only?{color}
{quote}
DataStream won't be affected since it doesn't use managed memory. We didn't check DataSet but according to the usage of wrapped memory segment, I don't see a problem also. ;;;","03/Feb/21 13:30;trohrmann;Thanks for answering my questions. So to sum it up, we don't really know for sure whether there isn't a component which is kept alive and keeps a reference to a {{ByteBuffer}} which prevents it from being garbage collected later. Could we maybe create a heap dump of a run where it fails? This could help with the analysis.

Other than that, we can look into how to introduce the safety net and investigate which components need to be changed if we remove the contract that {{ByteBuffers}} cannot be used after their originating {{MemorySegment}} is released.;;;","04/Feb/21 01:31;xtsong;Regarding the safety net, I still have concern on the performance. Practically, we need some kind of cross-thread synchronization on every read / write operations on the byte buffers.

I can try to provide a patch with the safety net. I think we can make a decision after measuring the performance differences with the patch.;;;","04/Feb/21 01:51;ykt836;[~xintongsong]  Will it affect all interfaces of MemorySegment or it only affects the ByteBuffer which is wrapped on MemorySegment through wrap()?;;;","04/Feb/21 01:57;xtsong;[~ykt836], I'm afraid it also affects `MemorySegment` interfaces.;;;","04/Feb/21 02:17;ykt836;[~xintongsong] Then I think the result will be even worse. ;;;","05/Feb/21 09:04;xtsong;For the record, [~trohrmann], [~ykt836] and I had a offline discussion, and we decided on the following action items.

* Check whether there's in-proper cleared threads/references that keep the segments from being GC-ed, by generating a heap dump on hitting the managed memory budget.
* Accessing to a ByteBuffer wrapped from an unsafe memory segment should be forbidden after the segment is freed.
* If the synchronization (whether segment has been freed) is too heavy, we should at least activate it with a configuration options for the CI tests.
* Further optimizations may be considered later.
** Migrating more ByteBuffer accesses to direct segment accesses.
** Removing GC-based memory free if we're sure about no reference leaking.
;;;","08/Feb/21 05:22;ym;[~ykt836] mentioned it is better to include this ticket in 1.12.2; do we have a rough estimation of how long this will take?;;;","08/Feb/21 06:15;xtsong;[~ym],

I don't have a good estimation atm. I should be able to open a PR by tomorrow. However, we need to do some memory safety and performance evaluations for the changes. Anything discovered during the evaluation may increase the time needed.

I would not block 1.12.2 on this ticket. However, if including this ticket only delays the release for a few days, it might worth to include it. We should have a better ETA very soon.;;;","08/Feb/21 10:27;xtsong;[~trohrmann], [~ykt836],

I'm afraid implementing our own {{ByteBuffer}} is a dead end. {{ByteBuffer}} is an abstract class, not an interface. It has only package-visibility constructors and many final methods. I don't find a good way to extend the class with our own implementation, even with reflection. Any suggestions on that?

Shall we try the other approach, that do not allow wrapping unsafe buffers from the segment, and migrate existing wrappings to direct operations on the segment?;;;","08/Feb/21 12:58;trohrmann;Sounds as the next best option here.;;;","08/Feb/21 16:20;trohrmann;What we could also try in order to see whether the problem disappears is to increase the number of {{UnsafeMemoryBudget.MAX_SLEEPS}}. Increasing this value should increase the overall waiting time.

;;;","15/Feb/21 15:24;xtsong;Fixed via
* master (1.13): 75fc592187a3a84163e262c6b1ba2a286cec4be4
* release-1.12: 1f8be1fd7b2b37a124e4d2b8080d08e259bdf095;;;","07/Apr/21 11:13;zhou_yb;the problem is still exist on flink1.12.2,what should I do?;;;","07/Apr/21 11:37;xtsong;[~zhou_yb],

Could you share some more information about the problem you encountered? It would be helpful if you can provide error stack, or complete jobmanager/taskmanager logs which are even better.;;;","07/Apr/21 11:54;zhou_yb; here is script (the data set amount is small ) :


 /space/airflow/dags/chloe # /space/flink/bin/flink run -m yarn-cluster -yjm 26096 -ytm 46789 -ynm collocation --pyFiles /space/airflow/dags/ -py /space/airflow/dags/chloe/material_collocation_summary.py --date=2021-04-01 --env=pro
{quote}

File ""/space/airflow/dags/chloe/material_collocation_summary.py"", line 181, in <module>
 main(date)
 File ""/space/airflow/dags/chloe/material_collocation_summary.py"", line 129, in main
 result.execute().print()
 File ""/space/flink/opt/python/pyflink.zip/pyflink/table/table_result.py"", line 219, in print
 File ""/space/flink/opt/python/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1286, in __call__
 File ""/space/flink/opt/python/pyflink.zip/pyflink/util/exceptions.py"", line 147, in deco
 File ""/space/flink/opt/python/py4j-0.10.8.1-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o1428.print.
: java.lang.RuntimeException: Failed to fetch next result
 at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
 at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
 at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:117)
 at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:350)
 at org.apache.flink.table.utils.PrintUtils.printAsTableauForm(PrintUtils.java:149)
 at org.apache.flink.table.api.internal.TableResultImpl.print(TableResultImpl.java:154)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
 at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
 at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
 at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
 at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
 at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to fetch job execution result
 at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)
 at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)
 at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
 ... 16 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: ef1a60be8f725a192a72b12cbcc2769c)
 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
 at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)
 ... 18 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: ef1a60be8f725a192a72b12cbcc2769c)
 at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:125)
 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
 at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$22(RestClusterClient.java:665)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
 at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:394)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
 at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 ... 1 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
 at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
 at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:123)
 ... 19 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:233)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:224)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:215)
 at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:669)
 at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
 at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:447)
 at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
 at akka.actor.Actor.aroundReceive(Actor.scala:517)
 at akka.actor.Actor.aroundReceive$(Actor.scala:515)
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
 at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:84)
 at org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.getNextBuffer(BaseHybridHashTable.java:254)
 at org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.nextSegment(BaseHybridHashTable.java:313)
 at org.apache.flink.table.runtime.hashtable.LongHashPartition.<init>(LongHashPartition.java:166)
 at org.apache.flink.table.runtime.hashtable.LongHashPartition.<init>(LongHashPartition.java:136)
 at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.createPartitions(LongHybridHashTable.java:276)
 at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.<init>(LongHybridHashTable.java:89)
 at LongHashJoinOperator$893$LongHashTable$877.<init>(Unknown Source)
 at LongHashJoinOperator$893.open(Unknown Source)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:428)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:543)
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:533)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
 at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
 at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:235)
 at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:82)
 ... 16 more
Caused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 16777216 bytes, only 0 bytes are remaining. This usually indicates that you are requesting more memory than you have reserved. However, when running an old JVM version it can also be caused by slow garbage collection. Try to upgrade to Java 8u72 or higher if running on an old Java version.
 at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:170)
 at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:84)
 at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:232)
 ... 17 more

org.apache.flink.client.program.ProgramAbortException
 at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:124)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:349)
 at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:219)
 at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
 at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
 at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
 at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Exception in thread ""Thread-9"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
 at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
 at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
 at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2803)
 at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3059)
 at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3018)
 at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2991)
 at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2871)
 at org.apache.hadoop.conf.Configuration.get(Configuration.java:1223)
 at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1835)
 at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
 at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
 at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
 at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
 at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
{quote}
 

 ;;;","07/Apr/21 12:51;xtsong;[~zhou_yb],

This indeed looks similar.

Is it possible for you to share the complete jobmanager and taskmanager logs? And does the problem happen at the very beginning of the task execution, or it happens when upstream tasks are finished and new tasks are executed?.;;;","08/Apr/21 03:25;zhou_yb; logs in attachment,it a long text: [^exception];;;","08/Apr/21 07:25;xtsong;[~zhou_yb],

Findings from the JM logs:
- The TM where the failure happen is newly started, registered at 19:14:30, with plenty of managed memory (1.7G).
- In the first stage, 3 source tasks are deployed onto the TM (19:14:30), and successfully finished (19:14:51)
- In the second stage, 3 hash join tasks are deployed onto the (19:14:52), and soon failed (19:14:55) due to not being able to allocate managed memory.
- From the task name ( {{Source: HiveSource-chloe.chloe_bus_hive_log -> Calc(select=[dataobj], where=[(bustype = 12)]) -> (BatchExecPythonCalc, BatchExecPythonCalc}} ), it seems the first stage tasks do not use memory segments. They should only use managed memory for the python process.

I think this is a different problem. The previous problem reported in this ticket was caused due to memory segment not being GC-ed timely. In your case, the first stage tasks do not use memory segments. 

With the provided JM lobs, I cannot explain how this happened. Checking the codes, memory for python operators should have been released by the time the first stage tasks finish.

Would you be able to provide the complete task manager logs? And one more question, can the problem be reproduced, steady or occasionally?;;;","08/Apr/21 08:27;zhou_yb;the problem is  steady. taskmanager log :[^taskmanager.log];;;","08/Apr/21 08:54;trohrmann;If the problem is reproducible could you maybe share the complete job with some example data which reproduces the problem [~zhou_yb]? That way it will be a lot easier to figure the problem out.;;;","09/Apr/21 02:03;xtsong;I haven't find anything useful from the task manager logs.

+1 on [~trohrmann]'s advice. It would be helpful if we can reproduce the problem locally.;;;","09/Apr/21 02:26;zhou_yb;here is my code script:[^summary.py]

^when I do sceond union all operator ,the exception occured. the code is correctly on flink1.11. when the code executed faild on flink1.12.1, I change the grammar that pyflink1.12.1 supported.^

^so you will see the code cant run on flink1.11. but if you change the code grammar that pyflink1.11 supported ,the code can executed correctly on flink1.11^

 
{code:java}
 result = result1.union_all(result2)
 result = result.union_all(result3)
 result = result.union_all(result4)
 result.execute().print(){code}
 ;;;","09/Apr/21 04:33;zhou_yb;I have another script have same problems,but I just change little sql grammar .the code executed correctly, I had pasted some different codes,not complete codes 

Is the planner bug? 

first code:
{code:java}
sql = ""select ds, bustype,dataobj from chloe_bus_hive_log where ds='%s' and bustype=11 "" % date
bus_result1 = st_env.sql_query(sql)
sql = ""select ds, bustype,dataobj from chloe_bus_hive_log where ds='%s' and bustype=4 "" % date 
bus_result2 = st_env.sql_query(sql)


{code}
changed code executed correctly :
{code:java}

sql = ""select ds, bustype,dataobj from chloe_bus_hive_log where ds='%s' and (bustype=11 or bustype=4) "" % date
bus_result = st_env.sql_query(sql)

type1_result = bus_result.where(""bustype==11"")
type2_result = bus_result.where(""bustype==4"")

{code};;;","09/Apr/21 05:15;xtsong;I'm not sure about this being a planner bug.

My gut feeling this is some runtime/operator bug that happens to be triggered by certain task execution pattern. Changing of the sql query also changes the compiled execution plan, thus the bug is not triggered with the new plan.

It seems we cannot easily reproduce your problem. We cannot execute the scripts locally, due to missing the ""chloe"" packages. And even we have the packages, I assume we cannot access the data sources anyway.

Would it be possible for you to run the scripts with a debug version of Flink? I can provide you a github branch based on 1.12.1, with some additional logs that help us understand whats going on with the memory. You would need to checkout the branch, manually build and install the package. Would that work for you?;;;","09/Apr/21 05:31;zhou_yb;ok,but Im not sure when I will do it done. I need some spare time to do it ;;;","09/Apr/21 06:36;xtsong;I think I've found out the problem. I'm afraid this is a pyflink bug, which causes the python worker not terminated and the memory not released.

Python UDFs are executed in python VM processes. A python VM is only started when there's python UDFs to execute, and is expected terminated when the UDFs finish, releasing the managed memory for other executions. If the VM is not terminated, new coming tasks will not be able to use the memory. That's the error we met.

When there're multiple python UDFs within the same slot, they share the same VM. Consider the python VM as a resource, when it is first requested the resource is created (starting the VM), and a second request will get the same resource without creating a new one. The resource is disposed (terminating the VM) only when it is released as many times as it is requested.

The bug is that, this reference counting logic is implemented twice, in {{SharedResource.release()}} and {{BeamPythonFunctionRunner.close}}. Consequently, when there's multiple python UDFs in the slot, {{BeamPythonFunctionRunner}} requests {{SharedResource}} many times but only release once, while {{SharedResource}} expects to be released as many times as requested.

This bug can be triggered when 1) there are multiple python UDFs in the same slot, and 2) another operator tries to allocate non-python managed memory afterwards. If there's only 1) without 2), there are still python process leaks, which might easily get overlooked.

[~zhou_yb], the above bug matches observations from your job.
- There are 2 python UDFs in the same slot. That can be seen from the operator name {{(BatchExecPythonCalc, BatchExecPythonCalc)}}, and that the log ""Obtained shared Python process ..."" are printed as many times as twice of the slots.
- The error happens on the same TM after the python UDFs are executed.
- Changing of the SQL codes probably have changed the number of underlying python UDFs, thus no longer triggers the problem.

[~dianfu], [~hxbks2ks], could you confirm this issue? And before the bug is fixed, is there any way to help the user workaround the problem? E.g., any way to prevent multiple python UDFs in the same slot?;;;","09/Apr/21 06:53;zhou_yb;Im glad to you found the problem，expected to fix it .;;;","09/Apr/21 07:00;xtsong;[~zhou_yb], 
Thanks for reporting this issue and helping us locating the bug. This is a very valuable finding.
FYI, a new ticket (FLINK-22172) has been opened to track this bug.;;;","09/Apr/21 08:42;hxbks2ks;Thanks a lot for the analysis. I agree with @Xintong's analysis. I'm fixing it in FLINK-22172. 

Regarding to the work around, just as @Xintong said, this issue only happens if there are multiple Python operators in the same slot, so one work around in my mind is to split the python operators into different slot sharing group, e.g. 
{code}
t_env # StreamTableEnvironment
t = # Table
t1 = t.select(pyFunc1(col('a')))
ds = t_env.to_append_stream(t1, type_info=...).map(lambda x: x).slot_sharing_group(""another2"") # default slot sharing group is default
t = t_env.from_data_stream(ds, col('a'))
t.select(pyFunc2('a'))
{code};;;","09/Apr/21 08:53;zhou_yb;each udf calling split to different slot? the progress is : table > dataStream/dataSet > newTable?;;;","09/Apr/21 10:29;hxbks2ks;[~zhou_yb] If some udfs can be chained together, they will be executed in the same Python Operator. In this situation, we don't need to split them. So what needs to be split are multiple Python Operators in a slot, and then separate them into different slot sharing group. the progress is : table -> datastream.map.slot_sharing_group -> table;;;","09/Apr/21 11:43;zhou_yb;how to  know multiple Python Operators in a slot. ;;;"
Unaligned checkpoint recovery may lead to corrupted data stream,FLINK-20654,13346592,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,arvid,AHeise,17/Dec/20 13:07,22/Jun/21 14:06,13/Jul/23 08:12,22/Apr/21 06:05,1.12.0,1.12.1,,,,1.12.3,1.13.0,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"Fix of FLINK-20433 shows potential corruption after recovery for all variations of UnalignedCheckpointITCase.

To reproduce, run UCITCase a couple hundreds times. The issue showed for me in:
- execute [Parallel union, p = 5]
- execute [Parallel union, p = 10]
- execute [Parallel cogroup, p = 5]
- execute [parallel pipeline with remote channels, p = 5]
with decreasing frequency.

The issue manifests as one of the following issues:
- stream corrupted exception
- EOF exception
- assertion failure in NUM_LOST or NUM_OUT_OF_ORDER
- (for union) ArithmeticException overflow (because the number that should be [0;100000] has been mis-deserialized)",,AHeise,Bo Cui,dwysakowicz,guoyangze,hxbks2ks,kezhuw,klion26,leonard,lwlin,mapohl,Ming Li,pnowojski,roman,trohrmann,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20662,FLINK-20309,FLINK-20744,,,,,,,FLINK-21104,,,,,,,,FLINK-20960,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 13:13:02 UTC 2021,,,,,,,,,,"0|z0lm14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/20 16:48;arvid;For my investigation, I added a bunch of info statements to track which buffers are written.
https://github.com/AHeise/flink/tree/FLINK-20654

For [Parallel union, p = 5], I noticed that the issues arises only when multiple buffers of the same channel are recovered.


{noformat}
19749 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 8 bytes
19749 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=1, inputChannelIdx=3} recovered 9 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=2, inputChannelIdx=3} recovered 1 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=3, inputChannelIdx=3} recovered 14 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4096 bytes
19750 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79)/InputChannelInfo{gateIdx=0, inputChannelIdx=3} recovered 4 bytes
19750 [Flat Map (4/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=1, inputChannelIdx=3} prepareSnapshot 9 bytes
19750 [Flat Map (4/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=2, inputChannelIdx=3} prepareSnapshot 1 bytes
19750 [Flat Map (4/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=3, inputChannelIdx=3} prepareSnapshot 14 bytes
19750 [Flat Map (1/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=0, inputChannelIdx=0} prepareSnapshot 17 bytes
19750 [Flat Map (1/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=1, inputChannelIdx=0} prepareSnapshot 13 bytes
19750 [Flat Map (1/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=2, inputChannelIdx=0} prepareSnapshot 18 bytes
19750 [Flat Map (1/5)#5] INFO  org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} prepareSnapshot 14 bytes
19751 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19751 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19751 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19752 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19752 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19752 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19752 [Channel state writer Flat Map (4/5)#5] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Flat Map (4/5)#5 discarding 0 drained requests
19752 [Flat Map (1/5)#5] INFO  org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister [] - InputChannelInfo{gateIdx=3, inputChannelIdx=0} maybePersist 4096 bytes
19752 [Flat Map (4/5)#5] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Flat Map (4/5)#5 discarding 1 drained requests
19753 [Source: source1 (1/5)#5] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase [] - Snapshotted LongSplit{increment=5, nextNumber=21525, numCompletedCheckpoints=4} @ 0 subtask (5 attempt)
19753 [Source: source1 (4/5)#5] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase [] - Snapshotted LongSplit{increment=5, nextNumber=21528, numCompletedCheckpoints=4} @ 3 subtask (5 attempt)
19753 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (3/5)#5 (0bf62fa35c5a671073338ea1f1730953)/InputChannelInfo{gateIdx=0, inputChannelIdx=2} recovered 2 bytes
19753 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (3/5)#5 (0bf62fa35c5a671073338ea1f1730953)/InputChannelInfo{gateIdx=1, inputChannelIdx=2} recovered 12 bytes
19753 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Flat Map (4/5)#5 - asynchronous part of checkpoint 11 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:1.8.0_222]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_222]
	at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:583) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:59) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:115) [classes/:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_222]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_222]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
19753 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (3/5)#5 (0bf62fa35c5a671073338ea1f1730953)/InputChannelInfo{gateIdx=2, inputChannelIdx=2} recovered 4 bytes
19753 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (3/5)#5 (0bf62fa35c5a671073338ea1f1730953)/InputChannelInfo{gateIdx=3, inputChannelIdx=2} recovered 14 bytes
19753 [channel-state-unspilling-thread-1] INFO  org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel [] - Flat Map (3/5)#5 (0bf62fa35c5a671073338ea1f1730953)/InputChannelInfo{gateIdx=2, inputChannelIdx=2} recovered 4096 bytes
19753 [Flat Map (4/5)#5] WARN  org.apache.flink.runtime.taskmanager.Task [] - Flat Map (4/5)#5 (de5cfe0d40797af545b28a5c2994ca79) switched from RUNNING to FAILED.
java.lang.ArithmeticException: integer overflow
	at java.lang.Math.toIntExact(Math.java:1011) ~[?:1.8.0_222]
	at java.lang.StrictMath.toIntExact(StrictMath.java:813) ~[?:1.8.0_222]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$CountingMapFunction.flatMap(UnalignedCheckpointITCase.java:414) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$CountingMapFunction.flatMap(UnalignedCheckpointITCase.java:401) ~[test-classes/:?]
	at org.apache.flink.streaming.api.operators.StreamFlatMap.processElement(StreamFlatMap.java:50) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:193) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:185) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:158) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [classes/:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
{noformat}
;;;","20/Dec/20 03:51;hxbks2ks;another instance

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11071&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

 ;;;","20/Dec/20 04:39;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11069&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a]

 ;;;","21/Dec/20 06:07;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11085&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0;;;","21/Dec/20 09:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11091&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","21/Dec/20 09:54;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11025&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","21/Dec/20 10:59;pnowojski;Test was temporarily disabled as d42c333 on the master;;;","22/Dec/20 03:30;hxbks2ks;Instance in 1.12 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11138&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56]

 ;;;","22/Dec/20 06:44;xtsong;Hi [~roman_khachatryan],

How are things going with this issue? Is this a pure test instability, or it's a severe production bug that should block the next bugfix release?;;;","22/Dec/20 07:24;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11145&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","22/Dec/20 07:35;pnowojski;[~xintongsong], I suspect this being a real bug.;;;","22/Dec/20 09:45;xtsong;[~pnowojski],

Thanks for the reply.

Among all the 1.12.1 release blockers, it seems this is the most uncertain one since the cause is not found yet.

Looking forward to further updates.;;;","23/Dec/20 05:07;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11205&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56]

 ;;;","23/Dec/20 07:06;mapohl;[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=140&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=3882] failed due to a {{EOFException}}.;;;","28/Dec/20 06:35;mapohl;[Build failed|https://dev.azure.com/mapohl/flink/_build/results?buildId=142&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=8809] due to {{IndexOutOfBoundsException}} similar to what is listed in FLINK-20662.;;;","28/Dec/20 06:44;mapohl;[Build #20201221.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11090&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=3994] contained a test failure of {{UnalignedCheckpointITCase}} due to an {{EOFException}}.;;;","28/Dec/20 13:01;roman;The staus so far:

Even with high number of allowed restarts, there are still failures present. Which means that the issue is with persisting state, rather than reading it.

The issue doesn't manifest itself with legacy sources. Which means:
1. The issue is likely in MultipleInput task code or is related to different rates in such tasks
2. ""git bisect"" doesn't make sense (and bisecting until API changes didn't help)

Flattening the topology from
{code}
(src0, src1) -> map1
    (map1, src2) -> map2
        (map2, src3) -> map3
{code}
to
{code}
(src0, src1) -> map1
(src2, src3) -> map2
(map1, map2) -> map3
{code}
so that each task combines inputs of only a single type significantly reduces failure rate (which also confirms (1)):

The following ruled out:
* StreamTaskSourceInput - wrong (fake) gate/channels ids - removing didn't help
* Custom partitioner - replacing e.g. with keyby doesn't help
* Thread safety of SingleCheckpointBarrierHandler.allBarriersReceivedFuture - checked with synchronized, didn't help
* Gate alignment (EndOfChannelStateEvent) - doesn't fail with gate alignment always enabled
* Gate index stability - inspected the code, those shouldn't change;;;","28/Dec/20 13:50;mapohl;For the sake of completeness: [This Build|https://dev.azure.com/mapohl/flink/_build/results?buildId=143&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=8813] and [that build|https://dev.azure.com/mapohl/flink/_build/results?buildId=144&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a&l=8807]due to {{IndexOutOfBoundsException}} (FLINK-20662).;;;","30/Dec/20 08:47;pnowojski;A couple of fixes for the corrupted recovery were merged as  6ab2a89..b137bbd. Most of the UnalignedCheckpointITCase was re-enabled again. Union test case is still .failing sporadically ([~roman_khachatryan] is still investigating it) and remains disabled for the time being.;;;","30/Dec/20 18:14;trohrmann;I've seen the {{UnalignedCheckpointITCase}} failing again:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11501&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11500&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","30/Dec/20 20:17;roman;The fixes above were also merged to release-1.12 as d61a36...026a86.

 

As for the failures, the first one is FLINK-20781, but the second one is again IOException in co-group (thanks for reporting  them!)

 

I think we can leave the test enabled (without union) as failure rate should be low.;;;","31/Dec/20 01:52;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11500&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","04/Jan/21 15:45;trohrmann;Another failure: https://dev.azure.com/tillrohrmann/flink/_build/results?buildId=378&view=logs&j=6e55a443-5252-5db5-c632-109baf464772&t=9df6efca-61d0-513a-97ad-edb76d85786a;;;","08/Jan/21 07:08;pnowojski;merged commit 45f8577 into apache:release-1.12
merged commit c6786ab into apache:master;;;","15/Jan/21 09:52;hxbks2ks;The failed instance with ""ArithmeticException: integer overflow"" happened again. Do we reopen this issue?
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12083&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2]
{code:java}
2021-01-15T03:39:12.3771952Z Caused by: java.lang.ArithmeticException: integer overflow
2021-01-15T03:39:12.3772559Z 	at java.lang.Math.toIntExact(Math.java:1011)
2021-01-15T03:39:12.3773309Z 	at java.lang.StrictMath.toIntExact(StrictMath.java:813)
2021-01-15T03:39:12.3774293Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$CountingMapFunction.flatMap(UnalignedCheckpointITCase.java:488)
2021-01-15T03:39:12.3775410Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$CountingMapFunction.flatMap(UnalignedCheckpointITCase.java:474)
2021-01-15T03:39:12.3776427Z 	at org.apache.flink.streaming.api.operators.StreamFlatMap.processElement(StreamFlatMap.java:47)
2021-01-15T03:39:12.3777379Z 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:191)
2021-01-15T03:39:12.3778351Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:205)
2021-01-15T03:39:12.3779297Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:175)
2021-01-15T03:39:12.3780231Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-01-15T03:39:12.3781114Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:399)
2021-01-15T03:39:12.3782078Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:190)
2021-01-15T03:39:12.3783079Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:608)
2021-01-15T03:39:12.3783947Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:572)
2021-01-15T03:39:12.3784646Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:749)
2021-01-15T03:39:12.3785361Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:564)
2021-01-15T03:39:12.3785993Z 	at java.lang.Thread.run(Thread.java:748)
{code};;;","15/Jan/21 10:07;roman;Yes, let's keep it open.;;;","22/Jan/21 18:52;arvid;Merged a fix into master as 5d1c1b145fe2f5d34a692b3b13b731e9a26f4c0 and into 1.12 as 155301ecb1f6f2b6be140a2eeafcd8b0427d9ab2.;;;","25/Jan/21 08:09;dwysakowicz;Hey [~AHeise] I think there is a mistake in the master commit hash. I think the respective commits are:

* master
** 25d1c1b145fe2f5d34a692b3b13b731e9a26f4c0
* 1.12.2
** 155301ecb1f6f2b6be140a2eeafcd8b0427d9ab2;;;","30/Jan/21 10:56;pnowojski;yet another fix merged to master as a23744795d9
Back-ported to 1.12 as f460a57c5e3;;;","07/Mar/21 06:35;lwlin;hi [~pnowojski] , does this also affect Flink 1.11.3 ?

And another thing we'd like to know is, is unaligned checkpoint production-ready as of Flink 1.11.3 or 1.12.2 ?

thanks!  :);;;","07/Mar/21 08:01;pnowojski;Hi [~lwlin]. We are not aware of issues with 1.11.x branch and unaligned checkpoints. Most problems in this ticket with 1.12.x were caused by some preliminary changes for FLINK-19681, so 1.11.x shouldn't be affected by it. As of 1.12.2, we are not aware of any problems with unaligned checkpoints, last one as you can see was reported/merged over a month ago. So both 1.11.3 and 1.12.2 should be stable in this regard.

Having said that, as a result of those bugs, 1.12.x and 1.13.x branches are more thoroughly tested, so if you have an option to choose 1.11.3 or 1.12.2, I would suggest 1.12.2.;;;","08/Mar/21 05:33;lwlin;Hi [~pnowojski], very appreciated for your detailed explanation! Exactly what we'd like to know! (y);;;","09/Mar/21 15:52;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14342&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2;;;","26/Mar/21 13:02;arvid;Merged another set of test tunings + improved logging into master as f0d5d3be89c9762fdf147077a90831e365cf6ba0..913ea8e398e8396d044c14a0911d8e134ec4377d. I'm closing this ticket as there have been no other failures in the past two weeks. When another issue occurs the new logging should help us to drill it down.;;;","22/Apr/21 06:03;pnowojski;Buffers recycling fix merged:
to master as 3fc77ad3ffc
to release-1.13 as 860a716574c
to release-1.12 as 9a256ec6b64
to release-1.11 as 603a3c3ef93 (probably not needed);;;","05/May/21 08:58;mapohl;[~AHeise] what's the plan with the trace logging being enabled for UnalignedCheckpoints? Shall we have a Jira issue for disabling it again after a certain amount of time? I came across it when investigating the file size of the Maven logs (the one I looked at was 7.9G). 25.458.266 out of 26.712.993 lines were due to the trace logging of the {{NetworkActionsLogger}}.;;;","05/May/21 09:05;trohrmann;I'd also be in favor of decreasing the verbosity of these logs. Due to the TRACE logging, the logs are super huge and harder to analyze.;;;","06/May/21 08:31;pnowojski;[~mapohl] and [~trohrmann], could you elaborate what's the problem exactly? Why do you need to analyse UCITcase/UCStressITCase/UCRescaingITCase logs?;;;","06/May/21 12:54;mapohl;I didn't have to analyze the UCITCase. I looked into another issue and was initially puzzled by the size of the Maven logs (only one of the two). I'm just concerned that the big file size for {{mvn*.log}} would make analyzing any issue a bit annoying. Just grepping for a String in the Maven logs takes already much longer.

I totally understand the necessity of why we set the log level to trace. I was just wondering whether we have some plan to disable it again. I could imagine the case where we just don't run into issues with the UC anymore. At what point in time do we decide to set the log level back to {{INFO}}? Alternatively, we could move those logs into it's own file? Not sure how easy it is with our current AzureCI setup.;;;","06/May/21 13:13;pnowojski;Maybe let's wait a bit more time, and let's re-evaluate it in a couple of weeks? If we won't be using those logs (no new bugs in this area), maybe indeed it would be the best to decrease the logging level.;;;",,,,,,,,,,,,,
Unable to restore job from savepoint when using Kubernetes based HA services,FLINK-20648,13346541,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,dmvk,dmvk,17/Dec/20 09:33,28/May/21 06:57,13/Jul/23 08:12,25/Dec/20 01:31,1.12.0,,,,,1.12.1,1.13.0,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"When restoring job from savepoint, we always end up with following error:
{code}
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not instantiate JobManager.
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:463) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?]
	... 3 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Stopped retrying the operation because the error is not retryable.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2063) ~[?:?]
	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.addAndLock(KubernetesStateHandleStore.java:150) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.addCheckpoint(DefaultCompletedCheckpointStore.java:211) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1479) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:325) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:266) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:238) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:108) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:323) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:310) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:96) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:41) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:141) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:80) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:450) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?]
	... 3 more
Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Stopped retrying the operation because the error is not retryable.
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperation$1(FutureUtils.java:166) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
	... 3 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.kubernetes.kubeclient.resources.KubernetesException: Cannot retry checkAndUpdateConfigMap with configMap pipelines-runner-fulltext-6e99e672-4af29f0768624632839835717898b08d-jobmanager-leader because it does not exist.
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$6(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.Optional.orElseThrow(Optional.java:401) ~[?:?]
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?]
	... 3 more
Caused by: org.apache.flink.kubernetes.kubeclient.resources.KubernetesException: Cannot retry checkAndUpdateConfigMap with configMap pipelines-runner-fulltext-6e99e672-4af29f0768624632839835717898b08d-jobmanager-leader because it does not exist.
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$6(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.Optional.orElseThrow(Optional.java:401) ~[?:?]
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?]
{code}

Cause of the issue is following:
- We construct `jobMasterServices` prior starting `leaderElectionService` (in `JobManagerRunnerImpl`)
- During `jobMasterServices` initialization `tryRestoreExecutionGraphFromSavepoint` gets called. This calls `KubernetesStateHandleStore.addAndLock` interally.
- `KubernetesStateHandleStore.addAndLock` expects configmap for JM leadership to be already present, which is wrong, because `leaderElectionService` which is responsible for its creation has not started yet

Possible fixes:
- Start `leaderElectionService` before `jobMasterServices`
- Fix `KubernetesStateHandleStore`, so it can handle the case, when leader hasn't been elected
",,dianer17,dmvk,guoyangze,klion26,stevenz3wu,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 25 06:10:38 UTC 2020,,,,,,,,,,"0|z0llps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/20 09:37;dmvk;Possible solution is outlined here: [https://github.com/dmvk/flink/commit/ae8133016df76594af008fac8d91fc98efe5297d]

I don't feel confident enough with these internals to form this into proper PR.;;;","18/Dec/20 02:04;xtsong;[~fly_in_gis], could you help look into this?;;;","18/Dec/20 05:32;wangyang0918;[~dmvk] Thanks for creating this issue and debugging the root cause. I think you are right. Currently, when recovering from savepoint, Flink will add a new checkpoint to the HA storage. So it needs to update the ConfigMap. However, the ConfigMap has not be created since the leader election service is not started.

 

In the Kubernetes HA implementation, we have a very important assumption, only the active leader could update the HA ConfigMap. I do not tend to let {{KubernetesStateHandleStore}} could support adding checkpoints before leader is granted. It will cause some issues when we have multiple JobManagers. But I am also not sure whether we could start the leader election service before jobmaster service. I will dig more and hope to find a more reasonable solution.;;;","18/Dec/20 09:22;trohrmann;I am currently working on FLINK-11719 which would resolve the problem because we only start the {{JobMaster}} once we have obtained the leadership.

As a short term fix, we could try to do the {{JobManagerRunnerImpl}} initialization under the {{lock}} and then already start the leader election before we create the {{JobMasterService}}. At the moment, we always call {{JobManagerRunnerImpl.start()}} directly after the {{JobManagerRunnerImpl}} has been created anyways.;;;","18/Dec/20 09:49;wangyang0918;[~trohrmann] Thanks for your comments. I am afraid we could not simply start the leader election before creating {{JobMasterService}}. Because we could not get the leader address and then {{DefaultLeaderElectionService}} could not write it to the ConfigMap. 

 

FLINK-11719 is the cleanest way. But I assume it may not be resolved very soon.;;;","18/Dec/20 15:22;trohrmann;The address of the contender does not need to be known when starting the leader election service. Only when confirming the leader session one needs to specify the leader address.;;;","18/Dec/20 16:30;wangyang0918;Yes. We are justing using the {{leaderContenderDescription}}(aka leader address) for logging in {{ZooKeeperLeaderElectionDriver}}.;;;","21/Dec/20 09:50;trohrmann;As a quick update, I don't intend to fix FLINK-11719 for Flink {{1.12.1}}. Hence, we need at least a quick fix for this problem in the {{1.12.x}} release branch.;;;","21/Dec/20 10:18;wangyang0918;Thanks for your information. But it seems that we could not have a quick fix via starting leader election service before {{jobMasterFactory.createJobMasterService}} in the constructor. Because we could not guarantee that the ConfigMap has been created even though the leader election service is started successfully.

 

Currently, I am working to delay starting JobManager until leadership is granted.;;;","21/Dec/20 14:00;trohrmann;If it turns out to be the same as what I did for FLINK-11719, then it might be easier to try to backport it. Currently, I am bit hesitant doing it because I want to avoid introducing behavioural changes with a bug fix release.;;;","21/Dec/20 14:02;trohrmann;Is it because the ConfigMap gets created lazily when the leader election service is started [~fly_in_gis]?;;;","22/Dec/20 04:14;wangyang0918;Yes, the {{KubernetesLeaderElector#run}} is started in a separate thread. Once started, it tries to create the ConfigMap immediately. Since we have http IO operation here, we could not guarantee that the ConfigMap is created in given time. In most cases, it could be done in one round of loop(5 seconds).

 

I quickly gone though the PR of FLINK-11719. And actually we have the similar implementation[1]. I verified it could works and recover from the savepoint successfully. But just as you said, we introduce some behavior changes, and it seems that we need to fix some more tests.

 

[1]. https://github.com/wangyang0918/flink/commit/3be9df9063280693526c1b4446ec499a024a9059;;;","22/Dec/20 06:39;xtsong;Just to add my two cents.

IIUC, the reason we convert a savepoint into a checkpoint, is to reuse the checkpoint recovery logics for restoring jobs from savepoints. For that purpose, it is not necessary to write this savepoint-converted checkpoint to the external HA service, because if this checkpoint gets lost the job can simply recover from the original savepoint. If we don't need to write this checkpoint to the external HA service, there's no need to wait for the leadership.

Therefore, we might consider to extend {{CompletedCheckpointStore}} with {{addCheckpointLocally}} and {{StatehandleStore}} with {{addLocally}} for the savepoint-converted checkpoint. We can keep the local checkpoints in memory for {{KubernetesStateHandleStore}}, while keep the original implementation for {{ZooKeeperStateHandleStore}}. For recovery, we would need to compare the local checkpoints (if any) with the remote ones retrieved from external HA service, and recover from the most recent one.

Ideally, this should bring no behavior changes to the ZooKeeper HA service, while allow restoring jobs from savepoints with K8s HA service. It might complicate the HA service interfaces a bit, but should be less invasive/risky compared to changing job master start up logics.

I'm proposing this as a potential quick fix for the 1.12 branch. I agree that FLINK-11719 is our first choice for resolving the problem on the master branch.

[~trohrmann] [~fly_in_gis] WDYT?;;;","22/Dec/20 10:04;wangyang0918;Thanks [~xintongsong] for suggestion. Maybe we do not need to store the initial savepoint to HA storage at all. We just need to check whether it is initial savepoint and then add to a local list cache {{initialSavepoints}}. When recovering, we merge the {{initialSavepoints}} and {{completedCheckpoints}}. This[1] is a simple PoC of the thoughts and I am verifying whether it could work.

 

[1]. https://github.com/wangyang0918/flink/commit/ee75083487741b508579717644eca291632f98db;;;","22/Dec/20 10:30;trohrmann;How would we handle resetting the checkpoint counter to the checkpoint id = savepoint id + 1? For this operation, we would also need the leader ConfigMap being present if I am not mistaken. Not resetting the checkpoint id could violate the monotonicity of checkpoint ids.

Another idea could be block the constructor of {{KubernetesLeaderElectionDriver}} until a {{leaderElector}} has created the leader {{ConfigMap}}.;;;","22/Dec/20 11:23;wangyang0918;I think we could have the similar logics in {{CheckpointCoordinator}} . When restoring from savepoint, we do not need to update the ConfigMap/ZooKeeper. Instead, we use a private long variable for the storage, and compare it with {{checkpointIdCounter.getAndIncrement()}} in the {{initializeCheckpoint}} . Does it make sense? Or you prefer blocking the constructor of {{KubernetesLeaderElectionDriver}}.

 

IIUC, the checkpoint and counter from initial savepoint do not need to be stored in the HA storage. And they should not be stored in the HA storage since the leader has not been elected.;;;","22/Dec/20 13:49;trohrmann;My concern is that introducing special case logic into the {{CheckpointCoordinator}} is usually more difficult than blocking until the {{ConfigMap}} has been created by the {{KubernetesLeaderElectionDriver}}. Independent of this, I think that changing how initial savepoints are treated can work as well.;;;","24/Dec/20 04:02;wangyang0918;I have attached a PR to fix this issue via starting leader election before creating {{JobManagerService}} and making constructor of {{KubernetesLeaderElectionDriver}} blocking until ConfigMap exists.;;;","25/Dec/20 01:31;xtsong;Fixed on release-1.12 via:
 * 368b204d82ea274be9a47d7f1b9d24c7365c3b2d

Fixed on master (1.13) by FLINK-11719, added it case via:
 * 2c92ea9a06fc13d8adedd4cd5737ab89cd0a7334 ;;;","25/Dec/20 06:10;wangyang0918;[~dmvk] Could you help to verify that this issue is fixed in your situation? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReduceTransformation does not work with RocksDBStateBackend,FLINK-20646,13346532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,xtsong,xtsong,17/Dec/20 09:00,13/Jan/21 09:02,13/Jul/23 08:12,21/Dec/20 08:29,1.12.0,,,,,1.12.1,1.13.0,,API / DataStream,,,,,0,pull-request-available,,,,,"The intra-slot managed memory sharing (FLIP-141) requires transformations to properly declare their managed memory use cases.

For RocksDB state backend, it requires all {{Transformation}}-s on a keyed stream (with non-null {{KeySelector}}) to call {{Transformation#updateManagedMemoryStateBackendUseCase}}, which the newly introduced {{ReduceTransformation}} did not.

As a result, Flink will not reserve managed memory for operators converted from {{ReduceTransformation}} (FLINK-19931), leading to the following failure when RocksDB state backend is used.

{code}
16:58:49,373 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamGroupedReduceOperator_c27dcf7b54ef6bfd6cff02ca8870b681_(1/1) from alternative (1/1), will retry while more alternatives are available.
java.io.IOException: Failed to acquire shared cache resource for RocksDB
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:264) ~[flink-statebackend-rocksdb_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:535) ~[flink-statebackend-rocksdb_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-statebackend-rocksdb_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-streaming-java_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-runtime_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-runtime_2.11-1.12.0.jar:1.12.0]
	at java.lang.Thread.run(Thread.java:832) [?:?]
Caused by: java.lang.IllegalArgumentException: The fraction of memory to allocate should not be 0. Please make sure that all types of managed memory consumers contained in the job are configured with a non-negative weight via `taskmanager.memory.managed.consumer-weights`.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:164) ~[flink-core-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.memory.MemoryManager.validateFraction(MemoryManager.java:631) ~[flink-runtime_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.memory.MemoryManager.computeMemorySize(MemoryManager.java:612) ~[flink-runtime_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:499) ~[flink-runtime_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:260) ~[flink-statebackend-rocksdb_2.11-1.12.0.jar:1.12.0]
	... 16 more
{code}

The problem is reported on the user-zh mailing list. (In Chinese though.)
http://apache-flink.147419.n8.nabble.com/flink-1-12-RocksDBStateBackend-td9504.html",,aljoscha,klion26,xtsong,yunta,zhisheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 09:02:50 UTC 2021,,,,,,,,,,"0|z0llns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/20 09:03;xtsong;[~dwysakowicz],
Could you inform how widely is {{ReduceTransformation}} used? I'm afraid we would need to have a quick bugfix release if it's very widely used.;;;","17/Dec/20 09:16;xtsong;I think this bug reveals two problems.
* Absence of common abstraction for ""stateful transformations"". Relying on various concrete transformation implementations to maintain the key selectors and declare the memory use case is fragile. New transformation implementations can easily overlook them, which is what's happening now.
* Lack of testings. I'm wondering how this problem escaped from our release testing. Reduce operation + RocksDB state backend should not be a that rare case. Obviously we don't have test coverage for such scenarios.

As for a quick fix, we can simply call {{updateManagedMemoryStateBackendUseCase}} in {{ReduceTransformation}}. I reviewed the type hierarchy of {{Transformation}} and do not see other sub-classes with this problem. ;;;","17/Dec/20 09:36;aljoscha;A very quick fix for this is to just call
{code}
updateManagedMemoryStateBackendUseCase(true);
{code}

in {{ReduceTransformation}}. I wouldn't add a test for it now and it seems we also don't have equivalent tests for other transformations.

One thing I noticed is that there are some conflicts in how we assign the memory requirements. We allow operators/transformations to do it but we also hard-set some things for batch execution mode. This comes from the fact that the Blink-derived Table runner basically re-implements the newer BATCH-execution machinery we now have for general operators/transformations. In the long run, we should get rid of this complication.;;;","17/Dec/20 09:54;aljoscha;I think the windowless {{reduce()}} is not a common operation because it's not super useful in {{STREAMING}} mode. You just get an infinite stream of updates.

Do you know how they discovered the bug?;;;","17/Dec/20 10:01;xtsong;[~aljoscha],

I think they are just playing with 1.12.0. The user posted his codes when running into this problem. It's just a word count.
https://paste.ubuntu.com/p/9WrBz3Xrc6/

Also, I'm wondering does it make sense to you to have something like {{AbstractKeyedTransformation}}? Not saying we should do it right now. We can do it after the quick fix. It should help improve the testability and prevent similar problems if we introduce new transformations later.;;;","13/Jan/21 09:02;xtsong;Fixed via:
* master (1.13): cf71008984dd68b25db3612a9ca39f197b7e09c8
* release-1.12: 4d1c9927f77b11f6990e447856fac6627a46bdcf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink with Scala 2.12 that scala shell is not supported still provides start-scala-shell.sh,FLINK-20634,13346479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,paul8263,paul8263,17/Dec/20 02:46,28/May/21 07:00,13/Jul/23 08:12,05/Jan/21 12:25,1.12.0,,,,,1.13.0,,,Scala Shell,,,,,0,pull-request-available,,,,,Flink scala shell does not work with Scala 2.12. But it still provides start-scala-shell.sh in bin folder. Need to hide it in scala 2.12 edition.,,paul8263,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 12:25:55 UTC 2021,,,,,,,,,,"0|z0llc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/21 12:25;chesnay;master: 2c48e4ae35f852576a86f41ef86ae380ee27f753;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_configuration.test_add_all test failed in py35,FLINK-20633,13346477,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,17/Dec/20 02:29,07/Mar/22 02:59,13/Jul/23 08:12,07/Mar/22 02:59,1.12.5,1.13.0,1.14.0,,,1.13.7,1.14.5,1.15.0,API / Python,,,,,0,pull-request-available,stale-assigned,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10946&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490]
{code:java}
2020-12-17T01:07:04.9062839Z _______________________ ConfigurationTests.test_add_all ________________________
2020-12-17T01:07:04.9063107Z 
2020-12-17T01:07:04.9063436Z self = <pyflink.common.tests.test_configuration.ConfigurationTests testMethod=test_add_all>
2020-12-17T01:07:04.9063719Z 
2020-12-17T01:07:04.9063951Z     def test_add_all(self):
2020-12-17T01:07:04.9064224Z >       conf = Configuration()
2020-12-17T01:07:04.9064411Z 
2020-12-17T01:07:04.9064665Z pyflink/common/tests/test_configuration.py:85: 
2020-12-17T01:07:04.9065074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-12-17T01:07:04.9065474Z pyflink/common/configuration.py:43: in __init__
2020-12-17T01:07:04.9065765Z     gateway = get_gateway()
2020-12-17T01:07:04.9066065Z pyflink/java_gateway.py:62: in get_gateway
2020-12-17T01:07:04.9066352Z     _gateway = launch_gateway()
2020-12-17T01:07:04.9066671Z pyflink/java_gateway.py:104: in launch_gateway
2020-12-17T01:07:04.9076442Z     p = launch_gateway_server_process(env, args)
2020-12-17T01:07:04.9076987Z pyflink/pyflink_gateway_server.py:197: in launch_gateway_server_process
2020-12-17T01:07:04.9079207Z     download_apache_avro()
2020-12-17T01:07:04.9079558Z pyflink/pyflink_gateway_server.py:129: in download_apache_avro
2020-12-17T01:07:04.9163662Z     cwd=flink_source_root).decode(""utf-8"")
2020-12-17T01:07:04.9164205Z dev/.conda/envs/3.5/lib/python3.5/subprocess.py:316: in check_output
2020-12-17T01:07:04.9164558Z     **kwargs).stdout
2020-12-17T01:07:04.9164887Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-12-17T01:07:04.9165168Z 
2020-12-17T01:07:04.9165396Z input = None, timeout = None, check = True
2020-12-17T01:07:04.9166036Z popenargs = (['mvn', 'help:evaluate', '-Dexpression=avro.version'],)
2020-12-17T01:07:04.9166615Z kwargs = {'cwd': '/__w/1/s', 'stdout': -1}
2020-12-17T01:07:04.9166959Z process = <subprocess.Popen object at 0x7f0ff8a7a320>
2020-12-17T01:07:04.9168169Z stdout = b""[INFO] Scanning for projects...\nDownloading: https://repo.maven.apache.org/maven2/org/apache/apache/20/apache-20.po...R] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :flink-parent\n""
2020-12-17T01:07:04.9169079Z stderr = None, retcode = 1
2020-12-17T01:07:04.9169259Z 
2020-12-17T01:07:04.9169600Z     def run(*popenargs, input=None, timeout=None, check=False, **kwargs):
2020-12-17T01:07:04.9170061Z         """"""Run command with arguments and return a CompletedProcess instance.
2020-12-17T01:07:04.9170373Z     
2020-12-17T01:07:04.9170683Z         The returned instance will have attributes args, returncode, stdout and
2020-12-17T01:07:04.9171117Z         stderr. By default, stdout and stderr are not captured, and those attributes
2020-12-17T01:07:04.9171577Z         will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them.
2020-12-17T01:07:04.9171895Z     
2020-12-17T01:07:04.9172409Z         If check is True and the exit code was non-zero, it raises a
2020-12-17T01:07:04.9172852Z         CalledProcessError. The CalledProcessError object will have the return code
2020-12-17T01:07:04.9173305Z         in the returncode attribute, and output & stderr attributes if those streams
2020-12-17T01:07:04.9173662Z         were captured.
2020-12-17T01:07:04.9173879Z     
2020-12-17T01:07:04.9174175Z         If timeout is given, and the process takes too long, a TimeoutExpired
2020-12-17T01:07:04.9174537Z         exception will be raised.
2020-12-17T01:07:04.9174773Z     
2020-12-17T01:07:04.9175040Z         There is an optional argument ""input"", allowing you to
2020-12-17T01:07:04.9175663Z         pass a string to the subprocess's stdin.  If you use this argument
2020-12-17T01:07:04.9176301Z         you may not also use the Popen constructor's ""stdin"" argument, as
2020-12-17T01:07:04.9176645Z         it will be used internally.
2020-12-17T01:07:04.9176884Z     
2020-12-17T01:07:04.9177163Z         The other arguments are the same as for the Popen constructor.
2020-12-17T01:07:04.9177469Z     
2020-12-17T01:07:04.9177777Z         If universal_newlines=True is passed, the ""input"" argument must be a
2020-12-17T01:07:04.9178337Z         string and stdout/stderr in the returned object will be strings rather than
2020-12-17T01:07:04.9178779Z         bytes.
2020-12-17T01:07:04.9178981Z         """"""
2020-12-17T01:07:04.9179206Z         if input is not None:
2020-12-17T01:07:04.9179677Z             if 'stdin' in kwargs:
2020-12-17T01:07:04.9180236Z                 raise ValueError('stdin and input arguments may not both be used.')
2020-12-17T01:07:04.9180719Z             kwargs['stdin'] = PIPE
2020-12-17T01:07:04.9180942Z     
2020-12-17T01:07:04.9181203Z         with Popen(*popenargs, **kwargs) as process:
2020-12-17T01:07:04.9181505Z             try:
2020-12-17T01:07:04.9181820Z                 stdout, stderr = process.communicate(input, timeout=timeout)
2020-12-17T01:07:04.9182159Z             except TimeoutExpired:
2020-12-17T01:07:04.9182636Z                 process.kill()
2020-12-17T01:07:04.9182936Z                 stdout, stderr = process.communicate()
2020-12-17T01:07:04.9183280Z                 raise TimeoutExpired(process.args, timeout, output=stdout,
2020-12-17T01:07:04.9183718Z                                      stderr=stderr)
2020-12-17T01:07:04.9183997Z             except:
2020-12-17T01:07:04.9184232Z                 process.kill()
2020-12-17T01:07:04.9184491Z                 process.wait()
2020-12-17T01:07:04.9184716Z                 raise
2020-12-17T01:07:04.9184967Z             retcode = process.poll()
2020-12-17T01:07:04.9185256Z             if check and retcode:
2020-12-17T01:07:04.9185573Z                 raise CalledProcessError(retcode, process.args,
2020-12-17T01:07:04.9185943Z >                                        output=stdout, stderr=stderr)
2020-12-17T01:07:04.9186749Z E               subprocess.CalledProcessError: Command '['mvn', 'help:evaluate', '-Dexpression=avro.version']' returned non-zero exit status 1
{code}
 ",,dian.fu,gaoyunhaii,hxbks2ks,maguowei,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 07 02:59:18 UTC 2022,,,,,,,,,,"0|z0llbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/20 06:23;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11306&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","28/Dec/20 13:56;mapohl;Test failed for Python 3.8.5: [Build|https://dev.azure.com/mapohl/flink/_build/results?buildId=143&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=443dc6bf-b240-56df-6acf-c882d4b238da&l=19972];;;","14/Apr/21 03:55;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16480&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=45a89cfc-9ff2-5909-6443-6c732efcf06b&l=20665;;;","07/Dec/21 11:40;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27618&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22136;;;","07/Dec/21 11:40;trohrmann;[~hxbks2ks] do you have time looking at this problem?;;;","14/Dec/21 01:55;hxbks2ks;[~trohrmann] Sure. I will take a look.;;;","14/Dec/21 06:43;gaoyunhaii;Hi [~hxbks2ks] Do we have progress with this issue~?;;;","14/Dec/21 06:55;hxbks2ks;[~gaoyunhaii] The existing python tests will mvn download the avro package when it starts, and the reason for the error is that the download failed due to occasional network reasons. The reason for the need to download the avro package is recorded in https://issues.apache.org/jira/browse/FLINK-17417.
In view of the very low frequency of download failures, I think this is not a serious problem.;;;","14/Dec/21 07:41;trohrmann;Can we retry the downloads [~hxbks2ks]?;;;","15/Dec/21 11:30;hxbks2ks;[~trohrmann] Yes. I think adding retry times can solve this problem. I will prepare a PR to fix it.;;;","16/Dec/21 10:38;trohrmann;Cool, thanks [~hxbks2ks] :-);;;","11/Feb/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","07/Mar/22 02:59;hxbks2ks;Merged into master via 34e6895af8587b62a692f76591882f9a061fdb34
Merged into release-1.14 via a76ced517e1705527bc0f518f953f8ced9f2289a
Merged into release-1.13 via  f7548702c8f33f1e7c14853ac9a07c1eb5d01929;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing docker images for 1.12 release,FLINK-20632,13346419,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,peperg,peperg,16/Dec/20 19:15,28/May/21 07:03,13/Jul/23 08:12,08/Jan/21 12:17,1.12.0,,,,,,,,Deployment / Kubernetes,,,,,1,pull-request-available,,,,,"Images for Flink 1.12 are missing in Docker hub https://hub.docker.com/_/flink. As a result Kubernetes deployment as in the documentation example is not working.

https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/native_kubernetes.html",,dian.fu,nvolynets,peperg,rmetzger,tbnguyen1407,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20786,,,,,,,FLINK-20631,FLINK-20754,FLINK-20775,FLINK-20939,,,,,,,,FLINK-20631,,,,,,FLINK-20650,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 17:54:12 UTC 2021,,,,,,,,,,"0|z0lkyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/20 02:03;xtsong;Hi [~peperg],

Docker Hub requires all official images to go through a review process. The official image for Flink 1.12 is still in review. You can track the progress on the following pull request.
https://github.com/docker-library/official-images/pull/9249;;;","17/Dec/20 02:05;dian.fu;FYI: there is already a PR: https://github.com/docker-library/official-images/pull/9249

It will be available once the PR is merged.;;;","17/Dec/20 02:10;dian.fu;Oh, just found that [~xintongsong] has replied. Thanks a lot ~;;;","17/Dec/20 06:28;rmetzger;As a temporary workaround, you can build the docker image yourself, with these steps:

git clone https://github.com/apache/flink-docker
cd flink-docker/
cd 1.12/scala_2.12-java8-debian/
docker build -t flink:1.12.0 .

I also pushed an image to DockerHub as: ""rmetzger/flink:1.12.0-unofficial"". This is an unofficial image, not an official apache release!;;;","17/Dec/20 10:40;trohrmann;As a meta comment: Should we wait with release announcements until the docker image is approved in the future?;;;","17/Dec/20 11:02;xtsong;bq. As a meta comment: Should we wait with release announcements until the docker image is approved in the future?
+1;;;","17/Dec/20 12:16;rmetzger;If it does not delay the release too much, yes.

What we could also consider hosting the Flink images in another registry (for example GitHub). The command for pulling the image would be something like docker pull ghcr.io/apache/flink:1.12.0.
We could even extend this to nightly builds as well.;;;","17/Dec/20 12:54;trohrmann;If this works better then this could also be the way forward. The current problem with releasing before the docker image is available is that the native K8s integration does not work.

Providing snapshot docker images is a very good improvement imo.;;;","17/Dec/20 13:08;dian.fu;Providing snapshot docker images seems a good idea. Otherwise, I'm afraid that the release process may still not be rolled back even if we find that there are some problems during publishing to official images as artifacts such as the jars may have already been deployed to the maven central repo.;;;","18/Dec/20 05:37;wangyang0918;I remember [~chesnay] said in the ML that apache does not allow publishing snapshot images. Correct me if I am wrong.;;;","23/Dec/20 18:32;chesnay;[~fly_in_gis] It's not that we can't have snapshot images, but that we can't advertise them to users; so it wouldn't really help in this situation.

We do have to find a way though to ensure we can release the docker images on the day of the release, but it's a bit of a chicken-egg problem. We can't open a proper PR against the official images repo before the release is out (because the image couldn't download the corresponding release), but we don't to release before the image being accepted.

I'm wondering whether we could just move all of the docker logic into a script that lives in the distribution, and we just defer to that script in the image. If we could make this work we could save ourselves a lot of hassle from keeping Flink and flink-docker in sync.
;;;","28/Dec/20 09:09;trohrmann;[~chesnay] maybe it is enough to postpone the release announcement until the Docker PR is merged. That way we can first complete the Flink release, then update the Docker image and lastly announce the release which makes it effectively available to our users.;;;","28/Dec/20 09:45;xtsong;Does our PMC has the authority to remove artifacts from the apache *release* svn repository, say if we have not announced it?

I'm asking because, IIUC building docker images requires downloading the release from the *release* svn repository. And if the official-image review process requires updating Flink's source codes, we would need to remove the invalid release from the repository.;;;","28/Dec/20 09:56;trohrmann;Ideally, we wouldn't have to modify Flink because of the Docker image. I guess that the main part which is being reviewed are the {{Dockerfile}} and the entrypoint start script.

If a change to Flink's source code is needed, then we could also create a new bug fix release right away.;;;","28/Dec/20 10:05;trohrmann;But maybe hosting the Flink docker images somewhere else will decouple our release process from Dockerhub which already does the trick. In any case, I think we need to solve this problem for the next release, because it is now already 2 weeks since the 1.12.0 release and people trying out the K8s integration will fail because of the missing Docker image.;;;","28/Dec/20 11:39;chesnay;??maybe it is enough to postpone the release announcement until the Docker PR is merged. That way we can first complete the Flink release, then update the Docker image and lastly announce the release which makes it effectively available to our users.??

Would we have delayed the release for 2+ weeks because the docker images aren't being released?

??Does our PMC has the authority to remove artifacts from the apache release svn repository, say if we have not announced it???

Yes, we can do that.

??But maybe hosting the Flink docker images somewhere else will decouple our release process from Dockerhub which already does the trick.??

While it would solve the issue I do somewhat like going through the official-images review process as an additional QA. We aren't _really_ familiar with docker conventions, and the issue they pointed out does make sense after all.;;;","28/Dec/20 11:56;xtsong;{quote}While it would solve the issue I do somewhat like going through the official-images review process as an additional QA. We aren't really familiar with docker conventions, and the issue they pointed out does make sense after all.
{quote}
I kind of agree with thinking the official image as an additional QA. Technically speaking the official docker image is not even part of the Apache Flink release.
{quote}Note:

Docker Official Images are an intellectual property of Docker. Distributing Docker Official Images without a prior agreement can constitute a violation of Docker Terms of Service.

--- https://docs.docker.com/docker-hub/official_images/
{quote}
 

 ;;;","28/Dec/20 12:02;trohrmann;Given that Flink's K8s integration does not work out of the box w/o an official Flink Docker image, I do believe that the Docker image is kind of part of the Apache Flink release. At least from the user perspective it seems odd to me that Flink 1.12 has been released but it does not work out of the box with K8s. This is also illustrated by three duplicated tickets which have been opened for the missing Docker images.;;;","28/Dec/20 12:42;xtsong;True, it's hard to explain to users that the official docker image is not part of the release.

Is it possible to include an image (e.g., a tar file) in our release, so that the K8s integration can work out of box? That would make the docker hub official image an available choice rather than a release dependency. I guess it's a trade-off between the complexity to maintain and ship our own image and the risk of depending our release on a third-party process.;;;","28/Dec/20 13:20;trohrmann;I think that Robert's proposal with hosting the Docker image ourselves could work and solve the problem of having the release and the Docker image currently separated. The down side of this approach is that we might lose visibility which we currently have with https://hub.docker.com/ as a well known Docker image registry. But we could still publish our images to this registry as well at the cost of a duplicated Docker image which might confuse users.

Alternatively we could keep things as they are and hope that the future review process won't take that long. With Chesnay's proposal to move most of the entrypoint script into the Flink repository we might be able to reduce the number of things maintainers of https://github.com/docker-library/official-images might comment on.;;;","31/Dec/20 10:26;xtsong;The docker guys are still not responding to our 1.12.0 image PR, neither the [previous one|https://github.com/docker-library/official-images/pull/9249] nor the [new one|https://github.com/docker-library/official-images/pull/9345].

I'm not sure if this is because of the Christmas vacation or not. I do see there are new PRs got merged recently.

Shall we consider hosting the Docker image ourselves starting 1.12.1?;;;","04/Jan/21 09:09;trohrmann;I think the lack of responsiveness is due to Christmas and New Years. However, it shows our dependence on these guys. Hence, I am more and more inclined to host the Docker images on a registry which we control or can at least upload images at our need. So maybe the answer is to not host the Flink images as official images on hub.docker.com.;;;","04/Jan/21 09:46;xtsong;+1 on hosting the image on a registry that we control.

[~fly_in_gis] and I were also discussing this. We noticed that we might need to change {{KubernetesConfigOptions#getDefaultFlinkImage}} to point to our registry by default. I'd like to include this code change in 1.12.1, unless the 1.12.0 image got approved before this (indicating we probably won't have problems with the 1.12.1 image).

Of course we need to first investigate, decide and setup the new register.;;;","04/Jan/21 14:10;wangyang0918;I am also in favor of hosting the docker image on a self-controlled registry(e.g. github or Alibaba Cloud Container Registry). The only advantage of docker hub official image is that users could save the words when specifying image name(e.g. flink:1.11.3 vs ghcr.io/apache/flink:1.11.3).;;;","05/Jan/21 12:04;rmetzger;I will investigate hosting our images on GitHub.;;;","05/Jan/21 15:37;peperg;Hi
It might be worth hosting the image somwhere else so that it is available on the day of the release. This will allow the developers use it right away.

At the same time for some production systems only verified official images may be allowed, so having an official image will bring value. However, these systems will wait for the offcial image and won't even use the latest version anyway.
;;;","06/Jan/21 11:31;rmetzger;I've pushed the 1.12.0 release to GitHub Packages for apache/flink: https://github.com/apache/flink/packages/561356/versions
Pushing there will be part of the official release process.;;;","06/Jan/21 11:48;xtsong;Thanks [~rmetzger],

I got the following error when trying to pull the image.
{code:java}
$ docker pull docker.pkg.github.com/apache/flink/flink:1.11-scala_2.11-java11
Error response from daemon: Head https://docker.pkg.github.com/v2/apache/flink/flink/manifests/1.11-scala_2.11-java11: no basic auth credentials
{code}

And I noticed that Github puts a `Latest version` green tag besides `1.11-scala_2.11-java11 Latest version`, on the all versions page.
Does that mean people get this image when pulling with `docker.pkg.github.com/apache/flink/flink:latest`?
;;;","06/Jan/21 12:39;rmetzger;[~xintongsong] Thanks for trying this out. The latest tag will work similarly to packages hosted on Docker Hub.

I just realized that the images pushed to the legacy GitHub Packages are not pullable by unauthenticated users. You need to be logged in to GitHub to pull the images, even if they are public...
GitHub Container Registry supports pulling images without authentication. But GitHub Container Registry is not supported by the ASF GitHub Account: INFRA-20959;;;","06/Jan/21 13:18;rmetzger;I don't think that we will be able to use the GitHub Container Registry with the official Apache GitHub account, because Infra won't allow it.

I see the following options (ordered by my preference)
1. Use GitHub Container Registry from any GitHub account (project-flink), or ""flinkbot"" which is an account controlled by the PMC
Downsides: Flink committers won't be able to push images there: They need to be added manually to the account. Also, the name of the container will be a bit weird: docker.pkg.github.com/project-flink/flink/flink:1.12.0-scala_2.11.

2. Use a 3rd party DockerHub account (create new ""flink"" account)
Downsides: There are potentially confusions because there will be a real official and a semi official image on Docker Hub. Committers won't have access immediately (like approach 1). 

3. (current) Use GitHub Packages of apache/flink: Users need to provide Image Pull Credentials with their GitHub Account
Downsides: users have a poor experience, because GitHub credentials are needed to pull.

;;;","06/Jan/21 13:50;chesnay;Well 3 is simply not an option then is it; we can't require uses to need github credentials.
I'm currently leaning towards 2).

IMO, once we start going down the road of 1 or 2, we should just stop dealing with official-images imo to avoid confusion and overhead.;;;","06/Jan/21 15:52;rmetzger;There's actually another option: We can request an ""apache/flink"" DockerHub repository from the ""apache"" user: https://hub.docker.com/u/apache (via INFRA)
That's probably the cleanest solution.

I understand your argument Chesnay, but [~peperg] raised the point that some companies are only using official images. For that reason, I believe we should, at least for now, go with the ugly approach of publishing the images to two locations.

I will request an ""apache/flink"" DockerHub repo.

Edit: INFRA-21276
;;;","08/Jan/21 12:12;rmetzger;Merged updated docker image location to release-1.12 in https://github.com/apache/flink/commit/104feeba1946f018388345ffe56633507540721c
Merged updated docker image location to master in https://github.com/apache/flink/commit/03ca39937b406c6db21938e95843b6b290c4a81c

Merged release script to flink-docker: https://github.com/apache/flink-docker/commit/f47f4a2e257279c4c28667e3ceff12bc5eb70afb;;;","08/Jan/21 12:19;rmetzger;NOTE TO USERS LOOKING FOR FLINK 1.12 DOCKER IMAGES:

They are available here: https://hub.docker.com/r/apache/flink

The image is available as ""apache/flink"" instead of ""flink"".
Running ""docker run -it apache/flink:1.12 jobmanager"" starts a JobManager;;;","09/Jan/21 11:10;xtsong;Thanks for driving the efforts, [~rmetzger].
 I'm wondering, shall we also secure the `apache/flink` images with our release manager's signatures?
 I noticed that the previous official-images are signed with administrative keys, while our new images are not.
{code:java}
$ docker trust inspect flink:1.11.3
[
    {
        ""Name"": ""flink:1.11.3"",
        ""SignedTags"": [],
        ""Signers"": [],
        ""AdministrativeKeys"": [
            {
                ""Name"": ""Root"",
                ""Keys"": [
                    {
                        ""ID"": ""84dec8c7bc650432d83c84920cb15bc016100d846320377fe43a0c5722e31740""
                    }
                ]
            },
            {
                ""Name"": ""Repository"",
                ""Keys"": [
                    {
                        ""ID"": ""00916cd7c07d6bf463c4b71e5bb119c3a9350a0637dee74416d34852c18fc03a""
                    }
                ]
            }
        ]
    }
]

$ docker trust inspect apache/flink:1.12
[]
No signatures or cannot access apache/flink:1.12
{code};;;","25/Jan/21 12:32;rmetzger;I agree that we should sign the images! I wasn't even are that this is possible.;;;","26/Jan/21 00:42;xtsong;I tried sign the images for 1.12.1, but couldn't find a way to sign with my PGP key.
I can sign an image with a key generated by the command `docker trust key generate`, but that would be different from what we put in the KEYS file.;;;","24/Feb/21 01:51;peperg;Hi.

It might be worth hosting the image somwhere else so that it is available
on the day of the release. This will allow the developers use it right away.
At the same time for some production systems only veridied official images
may be allowed, so having an official image will bring value. However,
these systems will wait for the offcial image and won't even use the latest
version anyway.

pon., 28 gru 2020, 13:43 użytkownik Xintong Song (Jira) <jira@apache.org>

;;;","04/Mar/21 17:54;chesnay;[~peperg] This is not an issue with our distribution channels, but our release process.;;;",,,,,,,,,,,,,,
[Kinesis][DynamoDB] DynamoDB Streams Consumer fails to consume from Latest,FLINK-20630,13346369,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny.cranmer,danny.cranmer,danny.cranmer,16/Dec/20 14:00,21/Dec/20 06:32,13/Jul/23 08:12,21/Dec/20 06:32,1.12.0,,,,,1.12.1,1.13.0,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"*Background*
When consuming from {{LATEST}}, the {{KinesisDataFetcher}} converts the shard iterator type into an {{AT_TIMESTAMP}} to ensure all shards start from the same position. When {{LATEST}} is used each shared would effectively start from a different point in the time.

DynamoDB streams do not support {{AT_TIMESTAMP}} iterator type.

*Scope*
Remove shard iterator type transform for DynamoDB streams consumer. 

*Reproduction Steps*
Create a simple application that consumer from {{LATEST}} using {{FlinkDynamoDBStreamsConsumer}}

*Expected Results*
Consumer starts reading records from the head of the stream

*Actual Results*
An exception is thrown:

{code}
Caused by: org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: 1 validation error detected: Value 'AT_TIMESTAMP' at 'shardIteratorType' failed to satisfy constraint: Member must satisfy enum value set: [AFTER_SEQUENCE_NUMBER, LATEST, AT_SEQUENCE_NUMBER, TRIM_HORIZON] (Service: AmazonDynamoDBStreams; Status Code: 400; Error Code: ValidationException; Request ID: AFQ8KCJAP74IN5MR5KD2FP0CTBVV4KQNSO5AEMVJF66Q9ASUAAJG)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1799)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1383)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1359)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreamsClient.doInvoke(AmazonDynamoDBStreamsClient.java:686)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreamsClient.invoke(AmazonDynamoDBStreamsClient.java:653)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreamsClient.invoke(AmazonDynamoDBStreamsClient.java:642)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreamsClient.executeGetShardIterator(AmazonDynamoDBStreamsClient.java:544)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreamsClient.getShardIterator(AmazonDynamoDBStreamsClient.java:515)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.dynamodbv2.streamsadapter.AmazonDynamoDBStreamsAdapterClient.getShardIterator(AmazonDynamoDBStreamsAdapterClient.java:355)
	at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.getShardIterator(KinesisProxy.java:311)
	at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.getShardIterator(KinesisProxy.java:302)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.getShardIterator(PollingRecordPublisher.java:173)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.<init>(PollingRecordPublisher.java:93)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherFactory.create(PollingRecordPublisherFactory.java:85)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisherFactory.create(PollingRecordPublisherFactory.java:36)
	at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.createRecordPublisher(KinesisDataFetcher.java:469)
	at org.apache.flink.streaming.connectors.kinesis.internals.DynamoDBStreamsDataFetcher.createShardConsumer(DynamoDBStreamsDataFetcher.java:107)
	at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.runFetcher(KinesisDataFetcher.java:540)
	at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.run(FlinkKinesisConsumer.java:348)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:215)
{code}",,danny.cranmer,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 21 06:32:48 UTC 2020,,,,,,,,,,"0|z0lknk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/20 06:32;tzulitai;Merged:

flink/master: 1fcbdb1dbd6704d765c71f9ca113c789fc2648da
flink/release-1.12: 90a51a3989efaed6a06823a699b03723aa16a88b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Canceling a job when it is failing will result in job hanging in CANCELING state,FLINK-20626,13346347,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,16/Dec/20 11:50,17/Dec/20 07:54,13/Jul/23 08:12,17/Dec/20 07:54,1.11.2,1.12.0,,,,1.11.4,1.12.1,1.13.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"If user manually cancels a job when the job is failing(here failing means the job encounters unrecoverable failure and is about to fail),  the job will hang in CANCELING state and cannot terminate. The cause is that DefaultScheduler currently will always try to transition from `FAILING` to `FAILED` to terminate the job. However, job canceling will change job status to `CANCELING` so that the transition to `FAILED` will not success.",,kaibo.zhou,Thesharing,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 07:54:27 UTC 2020,,,,,,,,,,"0|z0lkio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/20 07:54;zhuzh;Fixed via
master: 58cc2a5fbd419d6a9e4f9c251ac01ecf59a8c5a2
release-1.12: f1fe8b3c446065b1276a98ba8b905f2f6c2cb2e0
release-1.11: 52f9bd93fed764309010f56680fb6d05e489f098;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Local recovery and sticky scheduling end-to-end test timeout with ""IOException: Stream Closed""",FLINK-20615,13346237,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,hxbks2ks,hxbks2ks,16/Dec/20 03:54,28/May/21 07:00,13/Jul/23 08:12,04/Jan/21 16:20,1.12.0,1.13.0,,,,1.12.1,1.13.0,,Runtime / State Backends,Runtime / Task,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10905&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610]

It tried to restart many times, and the final error was following:
{code:java}
2020-12-15T23:54:00.5067862Z Dec 15 23:53:42 2020-12-15 23:53:41,538 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.
2020-12-15T23:54:00.5068392Z Dec 15 23:53:42 java.io.IOException: Stream Closed
2020-12-15T23:54:00.5068767Z Dec 15 23:53:42 	at java.io.FileInputStream.readBytes(Native Method) ~[?:?]
2020-12-15T23:54:00.5069223Z Dec 15 23:53:42 	at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]
2020-12-15T23:54:00.5070150Z Dec 15 23:53:42 	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5071217Z Dec 15 23:53:42 	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5072295Z Dec 15 23:53:42 	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5072967Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]
2020-12-15T23:54:00.5073483Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]
2020-12-15T23:54:00.5074535Z Dec 15 23:53:42 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5075847Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5077187Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5078495Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5079802Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5081013Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5082215Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5083500Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5084899Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5086342Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5087601Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5088924Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5090261Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5091459Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5092604Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5093748Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5094866Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5095912Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5096875Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5097814Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5098373Z Dec 15 23:53:42 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-12-15T23:54:00.5099549Z Dec 15 23:53:42 2020-12-15 23:53:41,557 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/4) from alternative (1/1), will retry while more alternatives are available.
2020-12-15T23:54:00.5100556Z Dec 15 23:53:42 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
2020-12-15T23:54:00.5101480Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5102669Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5103763Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5104723Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5105700Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5106630Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5107587Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5108581Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5109505Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5110456Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5111316Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5112175Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5113012Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5113787Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5114521Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5115209Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5115635Z Dec 15 23:53:42 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-12-15T23:54:00.5115949Z Dec 15 23:53:42 Caused by: java.io.IOException: Stream Closed
2020-12-15T23:54:00.5116246Z Dec 15 23:53:42 	at java.io.FileInputStream.readBytes(Native Method) ~[?:?]
2020-12-15T23:54:00.5116589Z Dec 15 23:53:42 	at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]
2020-12-15T23:54:00.5117284Z Dec 15 23:53:42 	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5118080Z Dec 15 23:53:42 	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5118894Z Dec 15 23:53:42 	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5119392Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]
2020-12-15T23:54:00.5119808Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]
2020-12-15T23:54:00.5120605Z Dec 15 23:53:42 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5121576Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5122579Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5123543Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5124476Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5124994Z Dec 15 23:53:42 	... 16 more
{code}
 ",,dian.fu,hxbks2ks,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15416,,,,FLINK-19688,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 16:20:03 UTC 2021,,,,,,,,,,"0|z0lju8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/20 01:55;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10944&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","17/Dec/20 03:28;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10924&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","30/Dec/20 10:25;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11467&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","30/Dec/20 10:43;trohrmann;Looking at the log output 

{code}
[] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688) switched from DEPLOYING to RUNNING.
2020-12-29T21:11:45.6842078Z Dec 29 21:11:04 2020-12-29 21:10:56,757 DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Initializing Source: Custom Source (3/4)#255.
2020-12-29T21:11:45.6843136Z Dec 29 21:11:04 2020-12-29 21:10:56,758 DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Invoking Source: Custom Source (3/4)#255
2020-12-29T21:11:45.6844432Z Dec 29 21:11:04 2020-12-29 21:10:56,758 DEBUG org.apache.flink.runtime.state.TaskLocalStateStoreImpl       [] - Did not find registered local state for checkpoint 3 in subtask (e22dc2b9ff59ed3b5f9f444745153baa - bc764cd8ddf7a0cff126f51c16239658 - 2)
2020-12-29T21:11:45.6849247Z Dec 29 21:11:04 2020-12-29 21:10:56,758 DEBUG org.apache.flink.runtime.state.TaskStateManagerImpl          [] - Operator bc764cd8ddf7a0cff126f51c16239658 has remote state SubtaskState{operatorStateFromBackend=StateObjectCollection{[OperatorStateHandle{stateNameToPartitionOffsets={currentKey=StateMetaInfo{offsets=[192], distributionMode=SPLIT_DISTRIBUTE}}, delegateStateHandle=ByteStreamStateHandle{handleName='file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-33991570506/local_recovery_test/checkpoints/e22dc2b9ff59ed3b5f9f444745153baa/chk-3/0e4a0721-a9a5-41d8-b6e4-94023ec5a1c8', dataBytes=200}}]}, operatorStateFromStream=StateObjectCollection{[]}, keyedStateFromBackend=StateObjectCollection{[]}, keyedStateFromStream=StateObjectCollection{[]}, inputChannelState=StateObjectCollection{[]}, resultSubpartitionState=StateObjectCollection{[]}, stateSize=200} from job manager and local state alternatives [] from local state store TaskLocalStateStore{jobID=e22dc2b9ff59ed3b5f9f444745153baa, jobVertexID=bc764cd8ddf7a0cff126f51c16239658, allocationID=fef8e1740736efdffe751fdc602fd0e1, subtaskIndex=2, localRecoveryConfig=LocalRecoveryConfig{localRecoveryMode=true, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/tmp/localState/aid_fef8e1740736efdffe751fdc602fd0e1], jobID=e22dc2b9ff59ed3b5f9f444745153baa, jobVertexID=bc764cd8ddf7a0cff126f51c16239658, subtaskIndex=2}}, storedCheckpointIDs=[]}.
2020-12-29T21:11:45.6852863Z Dec 29 21:11:04 2020-12-29 21:10:56,758 DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSource_bc764cd8ddf7a0cff126f51c16239658_(3/4) and restoring with state from alternative (1/1).
2020-12-29T21:11:45.6854442Z Dec 29 21:11:04 2020-12-29 21:10:56,804 DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Finished task Source: Custom Source (3/4)#255
2020-12-29T21:11:45.6855660Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Closed operators for task Source: Custom Source (3/4)#255
2020-12-29T21:11:45.6857718Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Finished PipelinedSubpartition#0 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6868241Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Finished PipelinedSubpartition#1 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6870179Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Finished PipelinedSubpartition#2 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6872073Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Finished PipelinedSubpartition#3 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6873629Z Dec 29 21:11:04 2020-12-29 21:10:56,805 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688) switched from RUNNING to FINISHED.
2020-12-29T21:11:45.6874935Z Dec 29 21:11:04 2020-12-29 21:10:56,805 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688).
2020-12-29T21:11:45.6876227Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.taskmanager.Task                    [] - Release task Source: Custom Source (3/4)#255 network resources (state: FINISHED).
2020-12-29T21:11:45.6877441Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.io.network.TaskEventDispatcher      [] - unregistering d39c5df41fb51c57ac0e340421e62374#2@90cbf40ffa5c2a89a8cc1a6c5ea9b688
2020-12-29T21:11:45.6878938Z Dec 29 21:11:04 2020-12-29 21:10:56,805 DEBUG org.apache.flink.runtime.taskmanager.Task                    [] - Ensuring all FileSystem streams are closed for task Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688) [FINISHED]
2020-12-29T21:11:45.6880460Z Dec 29 21:11:04 2020-12-29 21:10:56,805 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source (3/4)#255 90cbf40ffa5c2a89a8cc1a6c5ea9b688.
2020-12-29T21:11:45.6882952Z Dec 29 21:11:04 2020-12-29 21:10:56,811 INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation [] - Starting to restore from state handle: KeyGroupsStateHandle{groupRangeOffsets=KeyGroupRangeOffsets{keyGroupRange=KeyGroupRange{startKeyGroup=2, endKeyGroup=2}, offsets=[371]}, stateHandle=RelativeFileStateHandle State: file:/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-33991570506/local_recovery_test/checkpoints/e22dc2b9ff59ed3b5f9f444745153baa/chk-3/d7cfa567-a7f6-42d4-8027-826d0d872f65, d7cfa567-a7f6-42d4-8027-826d0d872f65 [2232041 bytes]}.
2020-12-29T21:11:45.6884825Z Dec 29 21:11:04 2020-12-29 21:10:59,077 DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received heartbeat request from b21775cb8c75328e6f3c66b79630f66f.
2020-12-29T21:11:45.6886529Z Dec 29 21:11:04 2020-12-29 21:11:00,175 DEBUG org.apache.flink.runtime.io.network.partition.ResultPartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Releasing PipelinedResultPartition d39c5df41fb51c57ac0e340421e62374#2@90cbf40ffa5c2a89a8cc1a6c5ea9b688 [PIPELINED_BOUNDED, 4 subpartitions, 4 pending consumptions].
2020-12-29T21:11:45.6888372Z Dec 29 21:11:04 2020-12-29 21:11:00,176 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Released PipelinedSubpartition#0 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6890284Z Dec 29 21:11:04 2020-12-29 21:11:00,176 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Released PipelinedSubpartition#1 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6892309Z Dec 29 21:11:04 2020-12-29 21:11:00,176 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Released PipelinedSubpartition#2 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6894366Z Dec 29 21:11:04 2020-12-29 21:11:00,176 DEBUG org.apache.flink.runtime.io.network.partition.PipelinedSubpartition [] - Source: Custom Source (3/4)#255 (90cbf40ffa5c2a89a8cc1a6c5ea9b688): Released PipelinedSubpartition#3 [number of buffers: 2 (0 bytes), number of buffers in backlog: 1, finished? true, read view? false].
2020-12-29T21:11:45.6895936Z Dec 29 21:11:04 2020-12-29 21:11:00,176 DEBUG org.apache.flink.runtime.io.network.partition.ResultPartitionManager [] - Released partition d39c5df41fb51c57ac0e340421e62374#2 produced by 90cbf40ffa5c2a89a8cc1a6c5ea9b688.
2020-12-29T21:11:45.6897246Z Dec 29 21:11:04 2020-12-29 21:11:00,176 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Flat Map -> Sink: Unnamed (3/4)#255 (bc78e4bef29187b8464c79560bbe8306).
2020-12-29T21:11:45.6898580Z Dec 29 21:11:04 2020-12-29 21:11:00,176 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Flat Map -> Sink: Unnamed (3/4)#255 (bc78e4bef29187b8464c79560bbe8306) switched from RUNNING to CANCELING.
2020-12-29T21:11:45.6899960Z Dec 29 21:11:04 2020-12-29 21:11:00,176 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Flat Map -> Sink: Unnamed (3/4)#255 (bc78e4bef29187b8464c79560bbe8306).
2020-12-29T21:11:45.6901668Z Dec 29 21:11:04 2020-12-29 21:11:00,199 DEBUG org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate [] - Flat Map -> Sink: Unnamed (3/4)#255 (bc78e4bef29187b8464c79560bbe8306): Releasing SingleInputGate{owningTaskName='Flat Map -> Sink: Unnamed (3/4)#255 (bc78e4bef29187b8464c79560bbe8306)', gateIndex=0}.
2020-12-29T21:11:45.6903009Z Dec 29 21:11:04 2020-12-29 21:11:00,202 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.
2020-12-29T21:11:45.6903541Z Dec 29 21:11:04 java.io.IOException: Stream Closed
2020-12-29T21:11:45.6903949Z Dec 29 21:11:04 	at java.io.FileInputStream.readBytes(Native Method) ~[?:1.8.0_275]
2020-12-29T21:11:45.6904425Z Dec 29 21:11:04 	at java.io.FileInputStream.read(FileInputStream.java:255) ~[?:1.8.0_275]
2020-12-29T21:11:45.6905393Z Dec 29 21:11:04 	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6906632Z Dec 29 21:11:04 	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:60) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6907835Z Dec 29 21:11:04 	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:52) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6908501Z Dec 29 21:11:04 	at java.io.DataInputStream.readFully(DataInputStream.java:195) ~[?:1.8.0_275]
2020-12-29T21:11:45.6909188Z Dec 29 21:11:04 	at java.io.DataInputStream.readFully(DataInputStream.java:169) ~[?:1.8.0_275]
2020-12-29T21:11:45.6910284Z Dec 29 21:11:04 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:82) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6911706Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:229) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6913102Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6914761Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6916050Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:284) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6917307Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:587) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6918558Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:93) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6919989Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6921281Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6922514Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6923780Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:345) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6925068Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6926315Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:272) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6927495Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:425) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6928612Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:535) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6929764Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6930849Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:525) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6931986Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:565) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6932962Z Dec 29 21:11:04 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6933872Z Dec 29 21:11:04 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6934651Z Dec 29 21:11:04 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]
2020-12-29T21:11:45.6935871Z Dec 29 21:11:04 2020-12-29 21:11:00,202 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(3/4) from alternative (1/1), will retry while more alternatives are available.
2020-12-29T21:11:45.6936711Z Dec 29 21:11:04 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
2020-12-29T21:11:45.6937763Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:361) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6938970Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:587) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6940179Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:93) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6941454Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6942720Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6943954Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6945206Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:345) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6946495Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6947738Z Dec 29 21:11:04 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:272) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6948909Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:425) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6950041Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:535) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6951236Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6952341Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:525) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6953377Z Dec 29 21:11:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:565) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6954661Z Dec 29 21:11:04 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6955680Z Dec 29 21:11:04 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6956264Z Dec 29 21:11:04 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]
2020-12-29T21:11:45.6956676Z Dec 29 21:11:04 Caused by: java.io.IOException: Stream Closed
2020-12-29T21:11:45.6957325Z Dec 29 21:11:04 	at java.io.FileInputStream.readBytes(Native Method) ~[?:1.8.0_275]
2020-12-29T21:11:45.6957778Z Dec 29 21:11:04 	at java.io.FileInputStream.read(FileInputStream.java:255) ~[?:1.8.0_275]
2020-12-29T21:11:45.6958728Z Dec 29 21:11:04 	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6959787Z Dec 29 21:11:04 	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:60) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6960919Z Dec 29 21:11:04 	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:52) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6961563Z Dec 29 21:11:04 	at java.io.DataInputStream.readFully(DataInputStream.java:195) ~[?:1.8.0_275]
2020-12-29T21:11:45.6962092Z Dec 29 21:11:04 	at java.io.DataInputStream.readFully(DataInputStream.java:169) ~[?:1.8.0_275]
2020-12-29T21:11:45.6963151Z Dec 29 21:11:04 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:82) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6964419Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:229) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6965751Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6967017Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6968252Z Dec 29 21:11:04 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:284) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-29T21:11:45.6968852Z Dec 29 21:11:04 	... 16 more
{code}

It looks as if one of the source operators finishes before the consumer has been started. [~chesnay] do you have an idea how this can happen?;;;","30/Dec/20 14:00;trohrmann;One interesting aspect is that all failing test are run with the following configuration:

{code}
Running local recovery test with configuration:
         parallelism: 4
         max attempts: 10
         backend: rocks
         incremental checkpoints: false
         kill JVM: true
{code};;;","30/Dec/20 14:06;trohrmann;It seems that there is NPE problem:

{code}
2020-12-16T13:21:17.7113187Z Dec 16 13:21:11 2020-12-16 13:20:51,705 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Unnamed (4/4) (0482623be404744f26814f94b5a7c57d) switched from RUNNING to FAILED on 10.1.0.4:39159-675ed6 @ fv-az678-525.internal.cloudapp.net (dataPort=34301).
2020-12-16T13:21:17.7114297Z Dec 16 13:21:11 org.apache.flink.runtime.io.network.partition.consumer.PartitionConnectionException: Connection for partition 978de1ce6220888c7dbee5b86005adde#1@c9f3f7cef91057da79f7127d6db79fa0 not reachable.
2020-12-16T13:21:17.7115533Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:167) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7183057Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:314) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7184604Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:286) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7185782Z Dec 16 13:21:11 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7187421Z Dec 16 13:21:11 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7188923Z Dec 16 13:21:11 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7190036Z Dec 16 13:21:11 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7191218Z Dec 16 13:21:11 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7192323Z Dec 16 13:21:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7193381Z Dec 16 13:21:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7194378Z Dec 16 13:21:11 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7195311Z Dec 16 13:21:11 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7195879Z Dec 16 13:21:11 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_275]
2020-12-16T13:21:17.7197115Z Dec 16 13:21:11 Caused by: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/10.1.0.4:43351' has failed. This might indicate that the remote task manager has been lost.
2020-12-16T13:21:17.7198593Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:95) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7200067Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7201317Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:164) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7201913Z Dec 16 13:21:11 	... 12 more
2020-12-16T13:21:17.7203022Z Dec 16 13:21:11 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/10.1.0.4:43351' has failed. This might indicate that the remote task manager has been lost.
2020-12-16T13:21:17.7203894Z Dec 16 13:21:11 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_275]
2020-12-16T13:21:17.7204480Z Dec 16 13:21:11 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_275]
2020-12-16T13:21:17.7205598Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:88) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7253273Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7254806Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:164) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7255395Z Dec 16 13:21:11 	... 12 more
2020-12-16T13:21:17.7256376Z Dec 16 13:21:11 Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/10.1.0.4:43351' has failed. This might indicate that the remote task manager has been lost.
2020-12-16T13:21:17.7257664Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:134) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7258920Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7260246Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7261522Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7262775Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:164) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7263377Z Dec 16 13:21:11 	... 12 more
2020-12-16T13:21:17.7282838Z Dec 16 13:21:11 Caused by: java.lang.NullPointerException
2020-12-16T13:21:17.7283969Z Dec 16 13:21:11 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7285126Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.<init>(NettyPartitionRequestClient.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7286372Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:126) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7288061Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7289594Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7290898Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7292147Z Dec 16 13:21:11 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:164) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-16T13:21:17.7292751Z Dec 16 13:21:11 	... 12 more
{code};;;","30/Dec/20 16:14;trohrmann;Ok a quick update on the analysis. In one of the logs I have seen the following happening:

The test kills TaskManager processes and restarts them to observe whether the tasks are deployed to locations where they have run before. It will try this 10 times. During the first 10 times there was a TM {{a}} which has its data server registered under 10.1.0.4:43351. At a later point a new TM {{b}} will be started which by chance also picks 43351 as its data port. Then there is a third TM {{c}} which tried to connect to {{a}} when {{a}} was killed. This connection attempt failed. Now, when it tries to connect to {{b}} (which has the same address as {{a}}), then it will never succeed and always fail with the above seen NPE.;;;","30/Dec/20 16:22;trohrmann;I think I found the problem. The problem seems to be that the {{PartitionRequestClientFactory}} caches the {{NettyPartitionRequestClient}} for each {{ConnectionID}}. If the connection could not be created for a {{ConnectionID}}, then it seems that we will never clean this entry up. Hence, if at a later point we try to connect to the same {{ConnectionID}} (e.g. because a new process on the same machine with the same data port is started), then we won't be able to talk to this TM.

The underlying problem seems to be that the {{PartitionRequestClientFactory}} does not keep its invariant when {{PartitionRequestClientFactory.createPartitionRequestClient}} fails. Instead of leaving an exceptionally completed future in the {{clients}} map, it should remove it.

cc [~pnowojski], [~roman_khachatryan].;;;","30/Dec/20 16:26;trohrmann;I think the problem has been introduced with FLINK-15416.;;;","30/Dec/20 16:34;trohrmann;Actually, FLINK-19688 solves the exact same problem but unfortunately only for {{InterruptedExceptions}} instead for the general case.;;;","04/Jan/21 16:20;trohrmann;Fixed via

1.13.0: df631f478cbd3de4f8b234e421e4fc7f1b67ef2e
1.12.2: 895ecfb7fb21bc11b91e99df14596032667272b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a wrong example in udfs page.,FLINK-20607,13346030,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tinny,tinny,tinny,15/Dec/20 04:45,17/Dec/20 12:18,13/Jul/23 08:12,17/Dec/20 12:18,1.11.1,1.12.0,,,,1.12.1,1.13.0,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"Demonstration error of multiple input types in FunctionHint: 
{code:java}
@FunctionHint(
  input = [@DataTypeHint(""INT""), @DataTypeHint(""INT"")],
  output = @DataTypeHint(""INT"")
)
{code}

should be 
{code:java}
@FunctionHint(
  input = {@DataTypeHint(""INT""), @DataTypeHint(""INT"")},
  output = @DataTypeHint(""INT"")
)
{code}",,jark,tinny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 12:18:15 UTC 2020,,,,,,,,,,"0|z0lik8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/20 12:18;jark;Fixed in 
 - master: ec2bc631423febc5f9cde6f3d82336b2ee3e9a7f
 - releaes-1.12: 71de5a02cc67395c080c98cb1ea53f7d03831cc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql cli with hive catalog cannot create function using user classes from jar which specified by  -j option   ,FLINK-20606,13345992,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akisaya,akisaya,akisaya,14/Dec/20 23:24,28/May/21 06:59,13/Jul/23 08:12,30/Dec/20 09:26,1.10.2,1.11.2,1.12.0,,,1.13.0,,,Connectors / Hive,Table SQL / API,Table SQL / Client,,,0,pull-request-available,,,,,"with flink version 1.12.0(versions before also affected)

I started a sql cli  with a hive catalog and specified a user jar file with -j option like this:
{code:java}
bin/sql-client.sh embedded -j /Users/akis/Desktop/flink-func/myfunc.jar
{code}
{color:#ff0000}when i tried to create a custom function using class from myfunc.jar，cli reported ClassNotFoundException.{color}

 
{code:java}
Flink SQL> use catalog myhive;

Flink SQL> create function myfunc1 as 'me.aki.flink.flinkudf.MyFunc';
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: me.aki.flink.flinkudf.MyFunc
{code}
 

 

me.aki.flink.flinkudf.MyFunc is the identifier of udf，which defined like this

 
{code:java}
package me.aki.flink.flinkudf;

import org.apache.flink.table.functions.ScalarFunction;

public class MyFunc extends ScalarFunction {
    public String eval(String s) {
        return ""myfunc_"" + s;
    }
}
{code}
 

 

 

after walking through the related code, I believe this is a bug caused by wrong classloader

 

when using a hive catalog, flink will use  {color:#ff0000}CatalogFunctionImpl{color}  to wrap the function。 The

isGeneric() methed  uses {color:#ff0000}Class.forName(String clazzName){color} which will use a current classloader(classloader loads flink/lib) to determine the class。

 

however with -j option, user jar is set to the ExecutionContext and loaded by another userClassLoader

 

and the fix can be easy to pass a classloader to the Class.forName method.
{code:java}
ClassLoader cl = Thread.currentThread().getContextClassLoader();
Class c = Class.forName(className, true, cl);
{code}
after do such fix and build a new flink dist，create function behaves right

 
{code:java}
Flink SQL> select myfunc1('1');

// output
     EXPR$0
     myfunc_1
{code}
 

 

 

 

 

 

 ",,akisaya,fsk119,godfreyhe,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 30 09:26:14 UTC 2020,,,,,,,,,,"0|z0libs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/20 23:28;akisaya;[~jark]，I would like make a  pr to this issue, can you assign this to me?;;;","15/Dec/20 15:10;akisaya;[~lithium147]，what do you think and could you assign it to me?
h3.  ;;;","28/Dec/20 09:09;akisaya;seems pending for long, sorry to bother [~jark] [~lirui]，not sure who to @;;;","30/Dec/20 09:26;jark;Fixed in master: f17562e78526eb27398ab9699e8e4fb60501cc99
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typos in `CREATE Statements` docs,FLINK-20582,13345628,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaozilong,xiaozilong,xiaozilong,12/Dec/20 04:03,18/Dec/20 12:27,13/Jul/23 08:12,18/Dec/20 12:27,1.12.0,,,,,1.12.1,1.13.0,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,,,jark,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 18 12:27:02 UTC 2020,,,,,,,,,,"0|z0lg2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/20 12:27;jark;Fixed in 
 - master: 67203f79a5954f7a41c259f942f5ccabbd5a95fb
 - release-1.12: 8ec883cdd781be2c72ec7e89e7d417820ffe405d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing null value handling for SerializedValue's getByteArray() ,FLINK-20580,13345516,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kezhuw,mapohl,mapohl,11/Dec/20 13:08,26/Feb/21 23:00,13/Jul/23 08:12,18/Feb/21 18:07,1.13.0,,,,,1.12.2,1.13.0,,API / Type Serialization System,,,,,0,pull-request-available,starter,,,,"{{SerializedValue}} allows to wrap {{null}} values. Because of this, {{SerializedValue.getByteArray()}} might return {{null}} which is not properly handled in different locations (it's probably the best to use the IDEs ""Find usages"" to identify these locations). The most recent findings (for now) are listed in the comments.

We should add null handling in these cases and add tests for these cases.",,jackwangcs,kezhuw,mapohl,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 18 18:07:21 UTC 2021,,,,,,,,,,"0|z0lfe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/20 15:54;trohrmann;The problematic code locations are {{BlobWriter}} and {{SerializedValueSerializer}}.;;;","11/Dec/20 16:21;jackwangcs;Hi [~trohrmann] , I also find the  {{SerializedValue.getByteArray()}} is called in {{AkkaRpcActor}} and {{RemoteRpcInvocation}}. And they both has some prolematic code to check the length of serializedData by: {{SerializedValue.getByteArray().length}}.;;;","11/Dec/20 16:44;trohrmann;True, you are right [~jackwangcs]. I did not mention the {{AkkaRpcActor}} because I am fixing this problem via FLINK-20521.;;;","11/Feb/21 16:29;kezhuw;[~mapohl] [~trohrmann]  May be better to not nullable but empty array ?;;;","11/Feb/21 17:26;trohrmann;I think this is the better solution [~kezhuw]. Do you wanna take a stab at this problem?;;;","11/Feb/21 17:31;kezhuw;[~trohrmann] Yeh, I could give it a try.;;;","12/Feb/21 12:41;kezhuw;Hi [~trohrmann], after digging a bit, I think we had third option: not supporting nullable value for {{SerializedValue(T value)}}.

Caller should resort to nullable/optional variable of {{SerializedValue}} if null-ability is required, just like {{JobCheckpointingSettings.defaultStateBackend}}.

I saw two null-ability dependencies of {{SerializedValue(T value)}} so far:
 * {{rpc.akka}}. As an rpc module, I think it would be better to use its own version for self-contained. If there is an rpc returning {{SerializedValue}}, it will fail as {{AkkaInvocationHandler.deserializeValueIfNeeded}} treats the value as wrapped result. Not sure whether this is a valid case, but there are already cases where {{SerializedValue}} used as parameter.
 * {{TaskExecutorOperatorEventHandlingTest.eventHandlingInTaskFailureFailsTask}}. It is easy to fix.

To sum up, the third option should be:
 * Forbid null value in {{SerializedValue(T value)}}.
 * Migrate existing null-ability dependent code.
 
[~trohrmann] Which approach do you prefer ?;;;","12/Feb/21 13:26;kezhuw;I am kind of prefer third option.

If we allow nullable value in {{SerializedValue(T value)}}, then {{SerializedValue.deserializeValue}} will be nullabe, while we all know that it is almost always not null. But when that happens, it may be too far from its cause.;;;","12/Feb/21 13:45;trohrmann;Thanks for further looking into the problem [~kezhuw]. I think the answer depends on whether we want to support {{null}} values or not in {{SerializedValue}}.

If we want to support {{null}} values, then returning an empty {{byte[]}} does not work because it would not deserialize to {{null}}. This would be somewhat surprising given that {{SerializedValue.getByteArray}} should return the bytes of the serialized value.

Since {{null}} values are always a bit problematic I think it would be good to get rid of them as much as possible. Hence, I like your idea of only accepting non-null values for the {{SerializedValue}}. So if there are only few places where {{SerializedValue}} is used with {{null}} values, then this sounds like my preferred solution. 

Moreover, I think you are completely right that the {{RpcService}} should use their own abstraction to send serialized data over the wire. That way it would be decoupled from what the user actually returns. With {{SerializedValue}} this can lead currently to incorrect results in the case of local communication and a RPC returning a {{SerializedValue}}.;;;","12/Feb/21 14:29;kezhuw;{quote}
If we want to support null values, then returning an empty byte[] does not work because it would not deserialize to null. This would be somewhat surprising given that SerializedValue.getByteArray should return the bytes of the serialized value.
{quote}

Yeh, a bit surprising for {{SerializedValue.getByteArray}} not deserializable for not null array. I planed to deserialize to null for empty array in {{SerializedValue.deserializeValue}}. But if callers resort to deserialize themselves, then it could be really surprising.

I will go through making value not nullable in {{SerializedValue(T value)}}.;;;","18/Feb/21 18:07;trohrmann;Fixed via

1.13.0:
bbdd769253b25b4093a1759c835c6ff1d99d390d
ed981be6601d3600d23b301d70d2bffff8aad3e6

1.12.3:
c8c790f6d726fd6942a2569dd97ed7a116092c4e
1d8a8e3529956f930725777499155d51aeb79045

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testKafkaSourceSinkWithMetadata hangs,FLINK-20569,13345419,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,hxbks2ks,hxbks2ks,11/Dec/20 03:26,03/Jan/22 07:24,13/Jul/23 08:12,03/Jan/22 07:24,1.12.0,1.13.0,,,,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,auto-deprioritized-major,auto-unassigned,stale-minor,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10781&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b]
{code:java}
2020-12-10T23:10:46.7788275Z Test testKafkaSourceSinkWithMetadata[legacy = false, format = csv](org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase) is running.
2020-12-10T23:10:46.7789360Z --------------------------------------------------------------------------------
2020-12-10T23:10:46.7790602Z 23:10:46,776 [                main] INFO  org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl [] - Creating topic metadata_topic_csv
2020-12-10T23:10:47.1145296Z 23:10:47,112 [                main] WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
2020-12-10T23:10:47.1683896Z 23:10:47,166 [Sink: Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp]) (1/1)#0] WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Using AT_LEAST_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-12-10T23:10:47.2087733Z 23:10:47,206 [Sink: Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp]) (1/1)#0] INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic metadata_topic_csv
2020-12-10T23:10:47.5157133Z 23:10:47,513 [Source: TableSourceScan(table=[[default_catalog, default_database, kafka]], fields=[physical_1, physical_2, physical_3, topic, partition, headers, leader-epoch, timestamp, timestamp-type]) -> Calc(select=[physical_1, physical_2, CAST(timestamp-type) AS timestamp-type, CAST(timestamp) AS timestamp, leader-epoch, CAST(headers) AS headers, CAST(partition) AS partition, CAST(topic) AS topic, physical_3]) -> SinkConversionToTuple2 -> Sink: Select table sink (1/1)#0] INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase [] - Consumer subtask 0 has no restore state.
2020-12-10T23:10:47.5233388Z 23:10:47,521 [Source: TableSourceScan(table=[[default_catalog, default_database, kafka]], fields=[physical_1, physical_2, physical_3, topic, partition, headers, leader-epoch, timestamp, timestamp-type]) -> Calc(select=[physical_1, physical_2, CAST(timestamp-type) AS timestamp-type, CAST(timestamp) AS timestamp, leader-epoch, CAST(headers) AS headers, CAST(partition) AS partition, CAST(topic) AS topic, physical_3]) -> SinkConversionToTuple2 -> Sink: Select table sink (1/1)#0] INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase [] - Consumer subtask 0 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='metadata_topic_csv', partition=0}]
2020-12-10T23:10:47.5387239Z 23:10:47,537 [Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, kafka]], fields=[physical_1, physical_2, physical_3, topic, partition, headers, leader-epoch, timestamp, timestamp-type]) -> Calc(select=[physical_1, physical_2, CAST(timestamp-type) AS timestamp-type, CAST(timestamp) AS timestamp, leader-epoch, CAST(headers) AS headers, CAST(partition) AS partition, CAST(topic) AS topic, physical_3]) -> SinkConversionToTuple2 -> Sink: Select table sink (1/1)#0] INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase [] - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='metadata_topic_csv', partition=0}=-915623761775}.
2020-12-11T02:34:02.6860452Z ##[error]The operation was canceled.
{code}
This test started at 2020-12-10T23:10:46.7788275Z and has not been finished at 2020-12-11T02:34:02.6860452Z",,dwysakowicz,hxbks2ks,jark,joemoe,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 03 07:24:12 UTC 2022,,,,,,,,,,"0|z0leso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/21 07:16;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14362&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","10/Mar/21 07:17;dwysakowicz;cc [~twalthr] Do you mind having a look if this is an issue or just instability in Kafka?;;;","30/Mar/21 14:25;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15756&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=10280;;;","29/Apr/21 23:47;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/May/21 00:53;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 10:13;joemoe;[~twalthr] will take a look.;;;","22/Jun/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","30/Jun/21 22:37;flink-jira-bot;This issue was marked ""stale-assigned"" 7 days ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","29/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Jan/22 07:24;martijnvisser;Closing this ticket because no new issues have been reported in a long time anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Error in UDTF section,FLINK-20567,13345317,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,appleyuchi,appleyuchi,appleyuchi,10/Dec/20 15:40,23/Dec/20 09:58,13/Jul/23 08:12,23/Dec/20 09:58,1.13.0,,,,,1.13.0,,,Documentation,Table SQL / Ecosystem,,,,0,,,,,,"||item||Content||
|Document|[Link|https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tableApi.html]|
|part|Inner Join with Table Function (UDTF)|
|origin|TableFunction<String> split = new MySplitUDTF();|
|change to|TableFunction<Tuple3<String,String,String>> split = new MySplitUDTF();|


I have run the following the codes successfully 
that contain all the contents from the above.
①[InnerJoinwithTableFunction.java|https://paste.ubuntu.com/p/MMXJPrfRWC]
②[MySplitUDTF.java|https://paste.ubuntu.com/p/Q6fDHxw4Td/]

Reason:
In this part, 
it says:
joinLateral(call(""split"", $(""c"")).as(""s"", ""t"", ""v""))

it means:
the udtf has 1 input ""c"",
and 3 outputs ""s"", ""t"", ""v""

So:
these outputs should have 3 types.
such as TableFunction<Tuple3<String,String,String>>
instead of only 
-TableFunction<String> split-
 !screenshot-1.png! 








",,appleyuchi,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 15:46;appleyuchi;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13016859/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 09:58:27 UTC 2020,,,,,,,,,,"0|z0le60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/20 16:38;jark;Do you want to fix this? [~appleyuchi];;;","11/Dec/20 02:32;appleyuchi;yes;;;","23/Dec/20 09:58;chesnay;master: 9e30806ccfe87c41c2fc295306dff31a608479b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in EXPLAIN Statements docs.,FLINK-20565,13345278,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaozilong,xiaozilong,xiaozilong,10/Dec/20 11:31,15/Jan/21 07:03,13/Jul/23 08:12,15/Jan/21 07:03,1.11.0,1.12.0,,,,1.12.2,,,Documentation,,,,,0,pull-request-available,,,,,Fix typo in EXPLAIN Statements docs.,,jark,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 08:08:58 UTC 2020,,,,,,,,,,"0|z0ldxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/20 08:08;jark;[~xiaozilong], could you open another PR for master branch? Usually we should first fix the master branch. ;;;","11/Dec/20 08:08;jark;Fixed in 
 - release-1.12: b212c886dfec982775294199941bd5a19dc4b194;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetAvroStreamingFileSinkITCase.testWriteParquetAvroSpecific test failure,FLINK-20558,13345222,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,mapohl,mapohl,10/Dec/20 08:35,25/Aug/21 11:34,13/Jul/23 08:12,25/Aug/21 11:33,1.13.0,1.14.0,,,,1.14.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,testability,test-stability,,,,"[Build|https://dev.azure.com/mapohl/flink/_build/results?buildId=135&view=results] failed due to failing test \{{ParquetAvroStreamingFileSinkITCase.testWriteParquetAvroSpecific}}.
{code:java}
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.193 s <<< FAILURE! - in org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase
[ERROR] testWriteParquetAvroSpecific(org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase)  Time elapsed: 0.561 s  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase.validateResults(ParquetAvroStreamingFileSinkITCase.java:160)
	at org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase.testWriteParquetAvroSpecific(ParquetAvroStreamingFileSinkITCase.java:95)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748) {code}
The assertion was caused by [this assert call checking the number of files in the bucket|https://github.com/apache/flink/blob/fdea3cdc47052d59fc20611e1be019d223d77501/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/avro/ParquetAvroStreamingFileSinkITCase.java#L160].",,gaoyunhaii,jark,kezhuw,knaufk,mapohl,sewen,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22710,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 11:33:06 UTC 2021,,,,,,,,,,"0|z0ldkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/21 02:27;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15900&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20

Different test method, but the similar error. 
{code}
ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.064 s <<< FAILURE! - in org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase
[ERROR] testWriteParquetAvroReflect(org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase)  Time elapsed: 3.213 s  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase.validateResults(ParquetAvroStreamingFileSinkITCase.java:161)
	at org.apache.flink.formats.parquet.avro.ParquetAvroStreamingFileSinkITCase.testWriteParquetAvroReflect(ParquetAvroStreamingFileSinkITCase.java:152)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}
;;;","01/Apr/21 17:37;sewen;One more: https://dev.azure.com/sewen0794/Flink/_build/results?buildId=253&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=777327ab-6d4e-582e-3e76-4a9391c57e59&l=12843;;;","23/Apr/21 08:19;knaufk;As part of https://issues.apache.org/jira/browse/FLINK-22029 the ""Test"" Issue Type is removed. I migrated this one to a ""Bug"" and added the label ""test-stability"". If you think this should rather be an ""Improvement"", ""New Feature"" or ""Technical Debt"", feel free to change the issue type.;;;","23/May/21 22:51;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","31/May/21 23:24;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","12/Aug/21 03:15;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21934&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9&l=12891;;;","25/Aug/21 11:33;gaoyunhaii;Fixed on master via 5390e91bd47219adde15d5d515a4f5baf4231fc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
404 url dependency error in Flink Website,FLINK-20555,13345204,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,DaiXinyu,DaiXinyu,10/Dec/20 06:22,08/Jun/21 14:24,13/Jul/23 08:12,08/Jun/21 14:24,,,,,,,,,Project Website,,,,,0,auto-deprioritized-major,,,,,"in [Download page|https://flink.apache.org/downloads.html#apache-flink-1112] !image-2020-12-10-14-21-42-372.png!!image-2020-12-10-14-22-12-678.png!",,DaiXinyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 06:21;DaiXinyu;image-2020-12-10-14-21-42-372.png;https://issues.apache.org/jira/secure/attachment/13016822/image-2020-12-10-14-21-42-372.png","10/Dec/20 06:22;DaiXinyu;image-2020-12-10-14-22-12-678.png;https://issues.apache.org/jira/secure/attachment/13016821/image-2020-12-10-14-22-12-678.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 14:24:22 UTC 2021,,,,,,,,,,"0|z0ldgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/21 07:29;DaiXinyu;https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/deployment/hadoop.html  

This page is not currently accessible ;;;","22/Apr/21 10:58;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:01;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 14:24;chesnay;The link was removed some time ago.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Checkpointed Data Size of the Latest Completed Checkpoint is incorrectly displayed on the Overview page of the UI,FLINK-20554,13345194,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,10/Dec/20 04:03,11/Dec/20 07:18,13/Jul/23 08:12,11/Dec/20 07:18,1.11.0,,,,,1.11.3,1.12.1,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"The {{Checkpointed Data Size}} of the {{Latest Completed Checkpoint}} always shows '-' in the {{Overview}} of the UI.

!image-2020-12-10-11-57-56-888.png|width=862,height=104!

I think it should be {{state_size}} instead of {{checkpointed_data_size}} in the code([https://github.com/apache/flink/blob/master/flink-runtime-web/web-dashboard/src/app/pages/job/checkpoints/job-checkpoints.component.html#L52]), which should fix this problem.",,Ming Li,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 03:57;Ming Li;image-2020-12-10-11-57-56-888.png;https://issues.apache.org/jira/secure/attachment/13016812/image-2020-12-10-11-57-56-888.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 07:17:46 UTC 2020,,,,,,,,,,"0|z0ldeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/20 04:06;Ming Li;Hi, [~yunta], What do you think?;;;","10/Dec/20 08:06;yunta;I think you're right and I have assigned this ticket to you [~Ming Li];;;","11/Dec/20 07:17;yunta;Merged
master: 91e81eee9b3096580f6ff830b6b6401cdfd594a4
release-1.11: 1ead23dc2ce4da209cba6e2869a53e2088a8334b
release-1.12: c46b06b74313bcdea2e1c2de043c63a29d693ef7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcDynamicTableSink doesn't sink buffered data on checkpoint,FLINK-20552,13345191,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,meijies,meijies,meijies,10/Dec/20 03:56,25/Dec/20 02:21,13/Jul/23 08:12,25/Dec/20 02:21,,,,,,1.13.0,,,Connectors / JDBC,Table SQL / Ecosystem,,,,0,pull-request-available,starter,,,,"JdbcBatchingOutputFormat  is wrapped to OutputFormatSinkFunction``` when createSinkTransformation at CommonPhysicalSink class. but OutputFormatSinkFunction don't implement CheckpointedFunction interface, so the flush method of JdbcBatchingOutputFormat can't be called  when checkpoint",,hailong wang,jark,leonard,meijies,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 25 02:21:15 UTC 2020,,,,,,,,,,"0|z0lde0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/20 04:04;ZhuShang;Maybe OutputFormatSinkFunction should implements CheckpointedFunction;;;","10/Dec/20 04:15;leonard;Hi, [~meijies] 
 Thanks for the report， A simple way to fix this is wrapping the `JdbcBatchingOutputFormat` with `GenericJdbcSinkFunction` [here|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java#L87%C2%A0] 
{code:java}
return OutputFormatProvider.of(builder.build());
=>
return SinkFunctionProvider.of(new GenericJdbcSinkFunction<>(builder.build()));
{code}
And we can add a test for this change, [~meijies] Do you like to help fix this ?

 ;;;","10/Dec/20 05:47;meijies;Hi, [~Leonard Xu]

I will fix this by the method you mentioned
;;;","10/Dec/20 05:54;jark;+1 to [~Leonard Xu]'s proposal.

We should also add a proper testing for this.  An easier way to test this is using {{OneInputStreamOperatorTestHarness}}. 
You can have a look at the {{GroupAggregateHarnessTest#testAggregateWithRetraction}} about how to create an operator harness by using SQL. And have a look at {{RowTimeDeduplicateFunctionTest#testRowTimeDeduplicateKeepLastRow(boolean, boolean, java.util.List<java.lang.Object>)}} about how to snapshot and restore. ;;;","18/Dec/20 06:18;jark;Hi [~meijies], are you still working on this? ;;;","18/Dec/20 17:35;meijies;Hi, [~jark] 
Too busy these days, I will  fix this at this weekend.;;;","19/Dec/20 16:50;meijies;Hi, [~jark]

It seems unable to create SinkOperator by using insert into sql. I have tryied to triger source operator do snapshot, but the snapshot function of sink operator didn't be trigered.  Do you haven any other ideas ?

;;;","21/Dec/20 03:33;jark;[~meijies], how did you create SinkOperator? ;;;","21/Dec/20 15:43;meijies;Hi, [~jark]
Here is my test code, it seems unable create SinkOperator from TableResult. so i think maybe try to use JdbcDynamicTableSink to create SinkOperator. Look forward your advices, I will try it again tomorrow night.

{code:java}
		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		List<Long> data = new LinkedList<>();
		data.add(1l);
		data.add(2l);
		DataStream<Long> stream = env.fromCollection(data);

//		Transformation<Long> transformation = stream.getTransformation();
//		StreamSource<Long, ?> operator = ((LegacySourceTransformation<Long>) transformation).getOperator();
//		operator.setup(
//			new SourceOperatorStreamTask<Long>(SourceOperatorStateContextUtil.getTestingEnvironment()),
//			new MockStreamConfig(new Configuration(), 1),
//			new MockOutput<>(new ArrayList<>()));

		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
		Table t = tEnv.fromDataStream(stream, $(""id""));
		tEnv.createTemporaryView(""data_table"", t);

		tEnv.executeSql(""CREATE TABLE checkpoint_sink (\n"" +
			"" id BIGINT\n"" +
			"") WITH (\n"" +
			""  'connector' = 'jdbc',\n"" +
			""  'url' = '"" + DB_URL + ""',\n"" +
			""  'table-name' = '"" + OUTPUT_TABLE5 + ""',\n"" +
			""  'sink.buffer-flush.interval' = '0'\n"" + // disable async flush
			"")"");

		tEnv.executeSql(""INSERT INTO checkpoint_sink \n"" +
			""SELECT id from data_table"");


//		operator.initializeState(new StreamTaskStateInitializerImpl(
//			SourceOperatorStateContextUtil.getTestingEnvironment(), new MemoryStateBackend()));
//		operator.snapshotState(new StateSnapshotContextSynchronousImpl(100L, 100L));

		check(new Row[]{Row.of(1l), Row.of(2l)}, DB_URL, OUTPUT_TABLE5, new String[]{""id""});
{code}
;;;","22/Dec/20 04:29;jark;[~meijies], yes, I also find it's not easy to get SinkOperator from the SQL. I think you can try to use JdbcDynamicTableSink. ;;;","25/Dec/20 02:21;jark;Fixed in master: 21a08ea5696c92505f85f3cba0e47e32d7c1e21d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New ExecutionContext doesn't inherit classloader from previous context,FLINK-20549,13345079,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,lirui,lirui,09/Dec/20 13:14,15/Apr/21 08:53,13/Jul/23 08:12,15/Apr/21 08:53,,,,,,1.13.0,,,Table SQL / Client,,,,,0,,,,,,"When users change session properties from SQL client, {{LocalExecutor}} creates a new {{ExecutionContext}}. The new {{ExecutionContext}} inherits {{SessionState}} from previous session, which means the loaded Catalog/Module instances are reused. Since Catalog/Module are pluggable, the classes may come from user jars. However the new {{ExecutionContext}} doesn't inherit classloader from previous session. That means when the Catalog/Module instances are used, the thread context classloader and the defining classloader are different.

This can cause problems such as:
# Define a {{HiveModule}} in yaml file. Don't put the hive connector jar under lib folder, but add it through the {{""-l""}} option when launching the SQL client.
# Run some query using a hive built-in function.
# Change some session property to trigger a re-creation of {{ExecutionContext}}.
# Run the same query again and it would fail because the hive built-in function cannot be instantiated.",,akisaya,fsk119,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/21 08:51;fsk119;image-2021-04-15-16-51-16-765.png;https://issues.apache.org/jira/secure/attachment/13023897/image-2021-04-15-16-51-16-765.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 08:53:14 UTC 2021,,,,,,,,,,"0|z0lcpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/21 02:57;jark;cc [~fsk119] [~lirui];;;","15/Apr/21 08:53;fsk119;!image-2021-04-15-16-52-38-799.png!

Verified in my local env.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch job fails due to the exception in network stack,FLINK-20547,13345051,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,Thesharing,Thesharing,09/Dec/20 11:17,24/Mar/21 07:26,13/Jul/23 08:12,24/Mar/21 07:26,1.12.0,1.12.1,1.12.2,1.13.0,,1.12.3,1.13.0,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,,"I run a simple batch job with only two job vertices: a source and a sink.

The parallelisms of them are both 8000. They are connected via all-to-all blocking edges.

During the running of sink tasks, an exception raises:
{code:java}
2020-12-09 18:43:48,981 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Sink 1 (1595/8000) (08bd4214d6e0dc144e9654f1faaa3b28) switched from RUNNING to FAILED on [masked container name] @ [masked address] (dataPort=47872).
java.io.IOException: java.lang.IllegalStateException: Inconsistent availability: expected true
	at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.checkError(InputChannel.java:232) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.getNextBuffer(RecoveredInputChannel.java:165) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:626) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:603) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:591) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:109) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:157) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:834) ~[?:1.8.0_102]
Caused by: java.lang.IllegalStateException: Inconsistent availability: expected true
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:198) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.checkConsistentAvailability(LocalBufferPool.java:434) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.setNumBuffers(LocalBufferPool.java:564) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.redistributeBuffers(NetworkBufferPool.java:509) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.tryRedistributeBuffers(NetworkBufferPool.java:438) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:166) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:60) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.requestExclusiveBuffers(BufferManager.java:131) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.setup(RemoteInputChannel.java:148) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteRecoveredInputChannel.toInputChannelInternal(RemoteRecoveredInputChannel.java:76) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.toInputChannel(RecoveredInputChannel.java:91) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.convertRecoveredInputChannels(SingleInputGate.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:285) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	... 5 more
{code}
 It seems to be an exception in network stack.

The full log of the job is attached below.

 ",,gaoyunhaii,kevin.cyj,pnowojski,roman,Thesharing,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20824,,,,,,,,,,,,,FLINK-20824,,,,,"09/Dec/20 11:14;Thesharing;inconsistent.tar.gz;https://issues.apache.org/jira/secure/attachment/13016764/inconsistent.tar.gz",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 07:26:24 UTC 2021,,,,,,,,,,"0|z0lcj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/20 05:41;kevin.cyj;I also encountered this issue these days when testing the new optimization for sort-merge shuffle but I was suspecting that this exception is caused by my new added code. From this case, I suspect that it is a bug from master branch. 

cc [~AHeise].;;;","10/Dec/20 09:40;roman;Can you share your configuration please [~kevin.cyj]?

(in particular, TM memory configuration);;;","10/Dec/20 10:08;kevin.cyj;[~roman_khachatryan] I encountered this exception when running TPCDS tests, but the exception is hard to reproduce. Here is the relevant configuration:

 
{code:java}
taskmanager.memory.process.size: 30000m
taskmanager.memory.jvm-metaspace.size: 512m
# leave this to page cache
containerized.heap-cutoff-ratio: 0.3
taskmanager.memory.managed.size: 12000m

taskmanager.network.memory.fraction: 0.99
taskmanager.network.memory.min: 64mb
taskmanager.network.memory.max: 4096mb

taskmanager.numberOfTaskSlots: 10{code}
 

[~Thesharing] Could you also share more  information about the issue? For example, can the issue be reproduced?

 ;;;","10/Dec/20 10:25;roman;Thanks [~kevin.cyj].

Can you please also confirm that the issue appears only since 1.13?;;;","10/Dec/20 11:10;Thesharing;[~roman_khachatryan] This issue can be reproduced. 

The configurations related to the memory are:
{code:java}
jobmanager.memory.heap.size: 40960 m
env.java.opts: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:ParallelGCThreads=4 -XX:+PrintPromotionFailure -XX:+PrintGCCause
env.java.opts.jobmanager: -Xmx25600m -Xms25600m -Xmn15360m -XX:ParallelGCThreads=32 -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70
env.java.opts.taskmanager: -Xloggc:<LOG_DIR>/taskmanager-gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=512M

taskmanager.numberOfTaskSlots: 20
taskmanager.memory.segment-size: 4 k
taskmanager.memory.process.size: 8192 m
taskmanager.memory.task.off-heap.size: 134217728b
taskmanager.memory.framework.off-heap.size: 1073741824b
taskmanager.memory.managed.size: 134217728b
taskmanager.memory.network.min: 1073741824b
taskmanager.memory.network.max: 1073741824b
taskmanager.network.netty.transport: nio
{code}
 ;;;","10/Dec/20 12:47;kevin.cyj;[~roman_khachatryan] To be honest, I am not sure whether 1.12 has the same issue or not. But we didn't encountered the exception when running TPCDS test with 1.12. I offline discussed with [~Thesharing], he confirmed that his configuration can consistently reproduce the issue. He will confirm whether the issue can be reproduced with 1.12 and update the result latter.;;;","10/Dec/20 12:58;roman;Thanks [~kevin.cyj],

I'm asking because the first suspicion was that RecoveredInputChannels aren't needed in Batch mode.  But after checking the code, I see they are created uniformly since 1.11.

 

Do I understand correctly, that you need the DOP of at least 8000 to reproduce the issue?;;;","10/Dec/20 13:21;kevin.cyj;[~roman_khachatryan] We are using 8000 because we are testing the scheduling performance of Flink (preparing for migrating large scale batch job from Blink to Flink). I think 8000 parallelism is not a must for reproducing the issue, but I guess parallelism may influence the probability of reproduction.;;;","15/Dec/20 10:11;roman;[~kevin.cyj], [~Thesharing] did you manage to reproduce it in previous Flink releases?;;;","17/Dec/20 08:22;Thesharing;Sorry for the late reply, [~roman_khachatryan]. I need to clarify that we discovered this issue in the latest master branch. At present, we have managed to reproduce this issue in release-1.12. And we are still working on reproducing it in release-1.11.;;;","28/Dec/20 19:56;roman;Debugging an unrelated issue FLINK-20654, I see there is probably a memory leak when running UnalignedCheckpointITCase multiple times:
{code:java}
122805 [ForkJoinPool.commonPool-worker-5] WARN  org.apache.flink.runtime.minicluster.MiniClusterJobClient [] - Shutdown of MiniCluster failed.
org.apache.flink.util.FlinkException: Error while shutting the TaskExecutor down.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.handleOnStopException(TaskExecutor.java:461) ~[classes/:?]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$onStop$2(TaskExecutor.java:443) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_271]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$17(FutureUtils.java:678) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_271]
	at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217) ~[classes/:?]
	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:765) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_271]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$22(FutureUtils.java:1323) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:704) ~[?:1.8.0_271]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_271]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:442) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:209) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:159) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the TaskManager services.
	at org.apache.flink.runtime.taskexecutor.TaskManagerServices.shutDown(TaskManagerServices.java:235) ~[classes/:?]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.stopTaskExecutorServices(TaskExecutor.java:484) ~[classes/:?]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$17(FutureUtils.java:672) ~[classes/:?]
	... 37 more
Caused by: java.lang.IllegalStateException: NetworkBufferPool is not empty after destroying all LocalBufferPools
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.destroyAllBufferPools(NetworkBufferPool.java:431) ~[classes/:?]
	at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.close(NettyShuffleEnvironment.java:346) ~[classes/:?]
	at org.apache.flink.runtime.taskexecutor.TaskManagerServices.shutDown(TaskManagerServices.java:191) ~[classes/:?]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.stopTaskExecutorServices(TaskExecutor.java:484) ~[classes/:?]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$17(FutureUtils.java:672) ~[classes/:?]
	... 37 mor
{code};;;","19/Mar/21 07:44;pnowojski;[~kevin.cyj] have you managed to make any progress? I'm not sure if trying to reproduce the problem is worth it. It seems like an internal bug of {{LocalBufferPool}} and how {{availabilityHelper}} gets out of sync from {{availableMemorySegments}} and {{unavailableSubpartitionsCount}}, so it should be easy to just analyse the code what code paths can lead to that inconsistency.;;;","19/Mar/21 11:25;kevin.cyj;[~kevin.cyj] No much progress so far. Previously, I was trying to reproduce it with a unit test because it is hard to debug the large parallelism real job, unfortunately, the error did not reproduce. These days, I am busy with the blocking shuffle things and is not working it. I agree with your suggestion, I will spend some time dig into the code and see if I can find something.;;;","22/Mar/21 04:24;kevin.cyj;[~pnowojski] After reading the code, I think I have find the bug. The checkAvailability method will allocate buffer from the global, I may return false even when it already allocate some buffers and the available future will be set as unavailable which leads to the consistency check fail.

Things happen in the following order can lead to the error:
 # The setNumBuffers method is called.
 # The checkAvailability method is called.
 # The requestMemorySegmentFromGlobal is called and no buffer is returned which means the global pool does not have any available buffers.
 # The requestMemorySegmentFromGlobalWhenAvailable method is called and the global pool now has available buffers and the registered call back is called directly, after which new buffers are allocated and the buffer pool becomes available.
 # However, the checkAvailability call in 2 returns false and the else branch in setNumBuffers is ran, the buffer pool is set unavailable and the inconsistency happens.

Could you please verify if I am right? If I am right, I can prepare a fix PR soon, but the test is hard to simulate the scenario. I wonder if we can fix it without test coverage (I can verify the fix with the 8000 * 8000 real world job).;;;","22/Mar/21 08:13;pnowojski;Good catch [~kevin.cyj]. I think you are right. 

Isn't it quite easy to test on a unit test level with passing a mock implementation of `NetworkBufferPool`, that is first unavailable and doesn't have any buffers when requesting it for the first time. But when `getAvailableFuture` is called, one buffer becomes available and always returns a completed future?;;;","22/Mar/21 08:29;kevin.cyj;[~pnowojski] Thanks for the confirmation. You are right, a mock NetworkBufferPool can do the work. I will submit a fix PR  soon.;;;","23/Mar/21 04:00;kevin.cyj;[~pnowojski] I have opened a PR for this issue, could you please take a look?;;;","24/Mar/21 07:26;pnowojski;Thanks for fixing the issue [~kevin.cyj]

merged commit bc78f6b into apache:master
merged f90bce94cbc into release-1.12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in upsert kafka docs,FLINK-20543,13344980,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaozilong,xiaozilong,xiaozilong,09/Dec/20 04:47,09/Dec/20 07:24,13/Jul/23 08:12,09/Dec/20 07:15,1.12.0,,,,,1.12.1,1.13.0,,Documentation,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,,,jark,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 07:15:57 UTC 2020,,,,,,,,,,"0|z0lc3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/20 07:15;jark;Fixed in 
 - master: cf881b48d67de4033cf94ed1dddcd0c90d453ca6
 - release-1.12: ca4e02d1684d8f87983ff13fc24256e72ffea901;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcCatalog throws connection exception when baseUrl doesn't end with slash,FLINK-20540,13344958,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kgzhang,kgzhang,kgzhang,09/Dec/20 01:44,28/May/21 06:38,13/Jul/23 08:12,28/Dec/20 07:08,1.11.1,1.12.0,,,,1.13.0,,,Connectors / JDBC,Documentation,Table SQL / Ecosystem,,,0,pull-request-available,,,,," 
{code:java}
//代码占位符
import org.apache.flink.connector.jdbc.catalog.JdbcCatalog
new JdbcCatalog(name, defaultDatabase, username, password, baseUrl){code}
 

The baseUrl must be endswith / when instantiate JdbcCatalog.

But according to [Flink document|https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/connectors/jdbc.html#usage-of-postgrescatalog] and code comments, baseUrl should be support  format {{""jdbc:postgresql://<ip>:<port>""}}

 

When i use baseUrl ""{{jdbc:postgresql://<ip>:<port>}}"", the error stack is:
{code:java}
//代码占位符
org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$1(JarRunHandler.java:103)
java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
java.util.concurrent.FutureTask.run(FutureTask.java:266)
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:748)\\nCaused by: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkRuntimeException: Could not execute application.
java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)\\n\\t... 7 more\\nCaused by: org.apache.flink.util.FlinkRuntimeException: Could not execute application.
org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:81)
org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:67)
org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:100)
java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\\n\\t... 7 more\\nCaused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Failed connecting to jdbc:postgresql://flink-postgres.cdn-flink:5432flink via JDBC.
org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)
org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)
org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:78)\\n\\t... 10 more\\nCaused by: org.apache.flink.table.api.ValidationException: Failed connecting to jdbc:postgresql://flink-postgres.cdn-flink:5432flink via JDBC.
org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.open(AbstractJdbcCatalog.java:100)
org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:191)
org.apache.flink.table.api.internal.TableEnvImpl.registerCatalog(TableEnvImpl.scala:267)
com.upai.jobs.TableBodySentFields.registerCatalog(TableBodySentFields.scala:25)
com.upai.jobs.FusionGifShow$.run(FusionGifShow.scala:28)
com.upai.jobs.FlinkTask$.delayedEndpoint$com$upai$jobs$FlinkTask$1(FlinkTask.scala:41)
com.upai.jobs.FlinkTask$delayedInit$body.apply(FlinkTask.scala:11)
scala.Function0$class.apply$mcV$sp(Function0.scala:34)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
scala.App$$anonfun$main$1.apply(App.scala:76)
scala.App$$anonfun$main$1.apply(App.scala:76)
scala.collection.immutable.List.foreach(List.scala:392)
scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
scala.App$class.main(App.scala:76)
com.upai.jobs.FlinkTask$.main(FlinkTask.scala:11)
com.upai.jobs.FlinkTask.main(FlinkTask.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)\\n\\t... 13 more\\nCaused by: java.sql.SQLException: No suitable driver found for jdbc:postgresql://flink-postgres.cdn-flink:5432flink
java.sql.DriverManager.getConnection(DriverManager.java:689)
java.sql.DriverManager.getConnection(DriverManager.java:247)
org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.open(AbstractJdbcCatalog.java:97)\\n\\t... 33 more\\n\""]}""
{code}
 

 

 

相关代码以及

 ",,jark,kgzhang,leonard,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 28 07:08:18 UTC 2020,,,,,,,,,,"0|z0lbyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/20 01:48;kgzhang;I think the bug should be fixed by:

 
{code:java}
//代码占位符

package org.apache.flink.connector.jdbc.catalog;

public AbstractJdbcCatalog(String catalogName, String defaultDatabase, String username, String pwd, String baseUrl) {
   super(catalogName, defaultDatabase);

   checkArgument(!StringUtils.isNullOrWhitespaceOnly(username));
   checkArgument(!StringUtils.isNullOrWhitespaceOnly(pwd));
   checkArgument(!StringUtils.isNullOrWhitespaceOnly(baseUrl));

   JdbcCatalogUtils.validateJdbcUrl(baseUrl);

   this.username = username;
   this.pwd = pwd;
   this.baseUrl = baseUrl.endsWith(""/"") ? baseUrl : baseUrl + ""/"";

   // old code: this.defaultUrl = baseUrl + defaultDatabase;
   this.defaultUrl = this.baseUrl + defaultDatabase;
}
{code}
 

 ;;;","09/Dec/20 03:48;leonard;[~kgzhang] Thanks for your report.

 The default dabase name is necessary, so I think the document is wrong, the base url used in catalog should be ""jdbc:postgresql://<ip>:<port>/"", and the [tests|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalogTestBase.java#L70] in project also use these parameters in this way.

Do you want to help fix this issue? 

 

 ;;;","10/Dec/20 06:31;kgzhang;Ok, i will submit 2 pull requests to fix document and the tests this week.;;;","10/Dec/20 06:51;leonard;Happy to hear you're willing to contribute, I think we can fix the doc and tests in one PR, please ping me if you need a reviewer. ;;;","12/Dec/20 09:14;kgzhang;[~Leonard Xu], I have updated the PR. This is PR [address|https://github.com/apache/flink/pull/14362];;;","15/Dec/20 06:38;kgzhang;[~Leonard Xu], Can Merge the PR?  This is PR [address. |https://github.com/apache/flink/pull/14362];;;","28/Dec/20 07:08;jark;Fixed in master: 8da64d7982de7287766de658f6d6f03037be94e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generate-stackbrew-library.sh in flink-docker doesn't properly prune the java11 tag,FLINK-20532,13344781,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,08/Dec/20 10:42,18/Dec/20 10:19,13/Jul/23 08:12,18/Dec/20 10:19,1.12.0,,,,,1.11.3,1.12.0,,flink-docker,,,,,0,pull-request-available,,,,,The output of {generate-stackbrew-library.sh} contains two {java11} tags.,,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 11:30:26 UTC 2020,,,,,,,,,,"0|z0lavc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/20 11:30;rmetzger;Resolved in flink-docker/master in https://github.com/apache/flink-docker/commit/11221b1fc63ccacff701ad5b86be7f9063aacee7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink docs are not building anymore due to builder change,FLINK-20531,13344768,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,08/Dec/20 09:46,10/Dec/20 15:13,13/Jul/23 08:12,10/Dec/20 15:13,,,,,,,,,Documentation,,,,,0,,,,,,"The Flink docs are not building anymore, due to 
{code}
r1068824 | dfoulks | 2020-12-07 18:53:38 +0100 (Mon, 07 Dec 2020) | 1 line
Moved bb-slave1 jobs to bb-slave7 and bb-slave2 jobs to bb-slave8
{code}

bb-slave2 has ""rvm"" installed, ""bb-slave8"" doesn't: https://ci.apache.org/builders/flink-docs-release-1.11/builds/161",,dian.fu,hxbks2ks,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 15:12:55 UTC 2020,,,,,,,,,,"0|z0lasg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/20 13:43;rmetzger;Issues should be fixed in https://github.com/apache/infrastructure-p6/pull/552 and https://github.com/apache/infrastructure-puppet/pull/1833;;;","09/Dec/20 02:07;hxbks2ks;Hi, [~rmetzger], the link of first PR seems inaccessible. The second PR has been merged yesterday. But Our build-doc bot still fails to build doc, the current error is that it failed when rvm install 2.6.3.

[https://ci.apache.org/builders/flink-docs-master/builds/2061/steps/Build%20docs/logs/stdio]
{code:java}
+ cd docs
+ pwd
+ DIR=/home/buildslave/slave/flink-docs-master/build/docs
+ DOCS_SRC=/home/buildslave/slave/flink-docs-master/build/docs
+ DOCS_DST=/home/buildslave/slave/flink-docs-master/build/docs/content
+ export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/usr/share/rvm/bin/
+ . /home/buildslave/.rvm/scripts/rvm
+ [ -n  -o -n  ]
+ rvm install 2.6.3
Segmentation fault (core dumped)
{code};;;","09/Dec/20 07:46;rmetzger;I'm still discussing with apache infra to resolve the problem :(;;;","10/Dec/20 07:30;rmetzger;Problem is still not resolved.

Medium and long term approaches to fix the problem:
1. Spark/Kafka approach: The docs are copied from the source repo into the website repository https://cwiki.apache.org/confluence/display/KAFKA/Release+Process#ReleaseProcess-Websiteupdateprocess / http://spark.apache.org/release-process.html

We could push the docs to an s3 bucket using azure (where we also validate the docs), and use ASFs buildbot only to push the docs to the website?! 

Side note for the flink website: Apache Airflow is using GitHub Actions to automatically publish their website: https://github.com/apache/airflow-site/blob/master/.github/workflows/build.yml#L65

We could also use GH Actions to build the full website (including the documentation).;;;","10/Dec/20 15:12;rmetzger;Fixed by Infra.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamArrowPythonGroupWindowAggregateFunctionOperator doesn't handle rowtime and proctime properly,FLINK-20525,13344716,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,08/Dec/20 04:31,09/Dec/20 01:51,13/Jul/23 08:12,09/Dec/20 01:51,1.12.0,1.13.0,,,,1.12.1,1.13.0,,API / Python,,,,,0,pull-request-available,,,,,Fix StreamArrowPythonGroupWindowAggregateFunctionOperator's incorrect handling of rowtime and proctime fields,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 01:51:36 UTC 2020,,,,,,,,,,"0|z0lagw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/20 01:51;dian.fu;Fixed in:
- master: 5376254c49555c7211f17ab5c4cfda6cb0ed7971
- release-1.12: cac9c1a5197e602774fb2cbe4a19361eea13a717;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null result values are being swallowed by RPC system,FLINK-20521,13344583,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,07/Dec/20 13:55,14/Dec/20 12:35,13/Jul/23 08:12,14/Dec/20 12:35,1.11.2,1.12.0,,,,1.12.1,1.13.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"If an RPC method returns a {{null}} value, then it seems that the request future won't get completed as reported in FLINK-17921.

We should either not allow to return {{null}} values as responses or make sure that a {{null}} value is properly transmitted to the caller.",,mapohl,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17921,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 12:35:22 UTC 2020,,,,,,,,,,"0|z0l9nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/20 12:54;mapohl;Just for documentation purposes as this issue is already in the process of being resolved: [This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=137&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87] failed due to a timeout which looks like being related to this issue.
{code:java}
2020-12-11T04:08:07.7372503Z [ERROR] Tests run: 10, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 554.133 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointITCase
2020-12-11T04:08:07.7373566Z [ERROR] execute[Parallel cogroup, p = 10](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 300.162 s  <<< ERROR!
2020-12-11T04:08:07.7374137Z org.junit.runners.model.TestTimedOutException: test timed out after 300 seconds
2020-12-11T04:08:07.7381510Z 	at sun.misc.Unsafe.park(Native Method)
2020-12-11T04:08:07.7381840Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-12-11T04:08:07.7382291Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-12-11T04:08:07.7382782Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-12-11T04:08:07.7383266Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-12-11T04:08:07.7383743Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-12-11T04:08:07.7384367Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1842)
2020-12-11T04:08:07.7384982Z 	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:70)
2020-12-11T04:08:07.7385641Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1822)
2020-12-11T04:08:07.7386146Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1804)
2020-12-11T04:08:07.7386810Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:122)
2020-12-11T04:08:07.7387382Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:159)
2020-12-11T04:08:07.7387800Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-12-11T04:08:07.7388165Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-12-11T04:08:07.7388744Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-12-11T04:08:07.7389130Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-12-11T04:08:07.7389538Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-12-11T04:08:07.7389993Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-12-11T04:08:07.7390446Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-12-11T04:08:07.7391069Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-12-11T04:08:07.7391463Z 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2020-12-11T04:08:07.7391831Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-12-11T04:08:07.7392483Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-12-11T04:08:07.7393011Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-12-11T04:08:07.7393443Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-12-11T04:08:07.7393776Z 	at java.lang.Thread.run(Thread.java:748)
2020-12-11T04:08:07.7393952Z 
2020-12-11T04:08:07.7789797Z [ERROR] execute[Parallel union, p = 10](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 118.969 s  <<< ERROR!
2020-12-11T04:08:07.7790405Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-12-11T04:08:07.7790949Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-12-11T04:08:07.7791480Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
2020-12-11T04:08:07.7791989Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-12-11T04:08:07.7792447Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-12-11T04:08:07.7792901Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-12-11T04:08:07.7793328Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-12-11T04:08:07.7793836Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-12-11T04:08:07.7794479Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-12-11T04:08:07.7794942Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-12-11T04:08:07.7795419Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-12-11T04:08:07.7795846Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-12-11T04:08:07.7796304Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
2020-12-11T04:08:07.7796692Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-12-11T04:08:07.7797023Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-12-11T04:08:07.7797385Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-12-11T04:08:07.7797735Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-12-11T04:08:07.7798094Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-12-11T04:08:07.7798554Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-12-11T04:08:07.7799011Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-12-11T04:08:07.7799424Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-12-11T04:08:07.7799831Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-12-11T04:08:07.7800253Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-12-11T04:08:07.7800754Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-12-11T04:08:07.7801295Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-12-11T04:08:07.7801680Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-12-11T04:08:07.7802052Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-12-11T04:08:07.7802490Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-12-11T04:08:07.7802980Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-12-11T04:08:07.7803490Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-12-11T04:08:07.7804007Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-12-11T04:08:07.7804454Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-12-11T04:08:07.7804997Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-12-11T04:08:07.7805350Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-12-11T04:08:07.7805739Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-12-11T04:08:07.7806160Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-12-11T04:08:07.7806515Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-12-11T04:08:07.7806871Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-12-11T04:08:07.7807250Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-12-11T04:08:07.7807713Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2020-12-11T04:08:07.7808257Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-12-11T04:08:07.7809035Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-12-11T04:08:07.7809618Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-12-11T04:08:07.7810154Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-12-11T04:08:07.7810748Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-12-11T04:08:07.7811280Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:610)
2020-12-11T04:08:07.7811914Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:60)
2020-12-11T04:08:07.7812578Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1743)
2020-12-11T04:08:07.7813106Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1313)
2020-12-11T04:08:07.7813579Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1257)
2020-12-11T04:08:07.7814024Z 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1091)
2020-12-11T04:08:07.7814480Z 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$11(Execution.java:770)
2020-12-11T04:08:07.7814961Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-12-11T04:08:07.7815421Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-12-11T04:08:07.7815908Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-12-11T04:08:07.7816357Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:404)
2020-12-11T04:08:07.7816816Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:197)
2020-12-11T04:08:07.7817373Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-12-11T04:08:07.7817853Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-12-11T04:08:07.7818264Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-12-11T04:08:07.7818651Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-12-11T04:08:07.7819026Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-12-11T04:08:07.7819414Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-12-11T04:08:07.7819828Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-12-11T04:08:07.7820219Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-12-11T04:08:07.7820603Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-12-11T04:08:07.7820985Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-12-11T04:08:07.7821339Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-12-11T04:08:07.7821791Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-12-11T04:08:07.7822098Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-12-11T04:08:07.7822388Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-12-11T04:08:07.7822666Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-12-11T04:08:07.7822949Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-12-11T04:08:07.7823143Z 	... 4 more
2020-12-11T04:08:07.7823792Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
2020-12-11T04:08:07.7824597Z 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
2020-12-11T04:08:07.7824977Z 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
2020-12-11T04:08:07.7825377Z 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
2020-12-11T04:08:07.7825796Z 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
2020-12-11T04:08:07.7826203Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-12-11T04:08:07.7826597Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-12-11T04:08:07.7827037Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:227)
2020-12-11T04:08:07.7827489Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-12-11T04:08:07.7827894Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-12-11T04:08:07.7828295Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-12-11T04:08:07.7828712Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-12-11T04:08:07.7829111Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:994)
2020-12-11T04:08:07.7829452Z 	at akka.dispatch.OnComplete.internal(Future.scala:263)
2020-12-11T04:08:07.7829762Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-12-11T04:08:07.7830056Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-12-11T04:08:07.7830359Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-12-11T04:08:07.7830690Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-12-11T04:08:07.7831209Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-12-11T04:08:07.7831713Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-12-11T04:08:07.7832072Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-12-11T04:08:07.7832434Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)
2020-12-11T04:08:07.7832780Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-12-11T04:08:07.7833128Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-12-11T04:08:07.7833507Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-12-11T04:08:07.7833891Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-12-11T04:08:07.7834299Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-12-11T04:08:07.7834755Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-12-11T04:08:07.7835225Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-12-11T04:08:07.7835647Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-12-11T04:08:07.7835970Z 	at java.lang.Thread.run(Thread.java:748)
2020-12-11T04:08:07.7836624Z Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
2020-12-11T04:08:07.7837361Z 	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72)
2020-12-11T04:08:07.7837812Z 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756)
2020-12-11T04:08:07.7838211Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2020-12-11T04:08:07.7838582Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-12-11T04:08:07.7838932Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-12-11T04:08:07.7839340Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2020-12-11T04:08:07.7839910Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2020-12-11T04:08:07.7840519Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-12-11T04:08:07.7841136Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-12-11T04:08:07.7841473Z 	... 1 more
2020-12-11T04:08:07.7842865Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_87#1921737344]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-12-11T04:08:07.7843696Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-12-11T04:08:07.7844119Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-12-11T04:08:07.7844556Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-12-11T04:08:07.7844993Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-12-11T04:08:07.7845421Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-12-11T04:08:07.7845892Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-12-11T04:08:07.7846368Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-12-11T04:08:07.7846874Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-12-11T04:08:07.7847464Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-12-11T04:08:07.7848079Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-12-11T04:08:07.7848604Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-12-11T04:08:07.7848964Z 	... 1 more{code};;;","11/Dec/20 15:57;trohrmann;I am not entirely sure whether this failure is caused by not being able to return a {{null}} value from the RPC. The {{TaskExecutor.submitTask}} method should not return a {{null}} value.;;;","14/Dec/20 12:35;trohrmann;Fixed via

1.13.0: 8d8125037adb678ad31b47809786b03628319db0
1.12.1: e2b3e86041df74e5e80a35a79e1a6066218083a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertKafkaTableITCase.testTemporalJoin test failed,FLINK-20500,13344468,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,hxbks2ks,hxbks2ks,07/Dec/20 02:27,28/May/21 07:11,13/Jul/23 08:12,20/Jan/21 12:04,1.12.0,1.13.0,,,,1.12.2,1.13.0,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10555&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518]

 
{code:java}
2020-12-06T23:09:20.3744822Z [ERROR]   UpsertKafkaTableITCase.testTemporalJoin:111->temporalJoinUpsertKafka:545 
2020-12-06T23:09:20.3746021Z Expected: <10001,100,2020-08-15T00:00:02,Bob,BEIJING,2020-08-15T00:00:01>
2020-12-06T23:09:20.3747548Z <10002,101,2020-08-15T00:00:03,Alice,SHANGHAI,2020-08-15T00:00:02>
2020-12-06T23:09:20.3748539Z <10002,102,2020-08-15T00:00:04,Greg,BERLIN,2020-08-15T00:00:03>
2020-12-06T23:09:20.3749294Z <10002,101,2020-08-16T00:02:01,Alice,WUHAN,2020-08-16T00:02>
2020-12-06T23:09:20.3749799Z <10004,104,2020-08-16T00:04,null,null,null>
2020-12-06T23:09:20.3789757Z <10003,101,2020-08-16T00:04:06,Alice,WUHAN,2020-08-16T00:02>
2020-12-06T23:09:20.3790891Z <10004,104,2020-08-16T00:05:06,Tomato,HONGKONG,2020-08-16T00:05:05>
2020-12-06T23:09:20.3791758Z      but: was <10002,102,2020-08-15T00:00:04,null,null,null>
2020-12-06T23:09:20.3792494Z <10004,104,2020-08-16T00:04,null,null,null>
2020-12-06T23:09:20.3793282Z <10001,100,2020-08-15T00:00:02,Bob,BEIJING,2020-08-15T00:00:01>
2020-12-06T23:09:20.3794134Z <10002,101,2020-08-15T00:00:03,Alice,SHANGHAI,2020-08-15T00:00:02>
2020-12-06T23:09:20.3794928Z <10002,101,2020-08-16T00:02:01,Alice,WUHAN,2020-08-16T00:02>
2020-12-06T23:09:20.3795512Z <10003,101,2020-08-16T00:04:06,Alice,WUHAN,2020-08-16T00:02>
2020-12-06T23:09:20.3796059Z <10004,104,2020-08-16T00:05:06,Tomato,HONGKONG,2020-08-16T00:05:05>
{code}",,fsk119,hxbks2ks,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 03:19:07 UTC 2021,,,,,,,,,,"0|z0l8y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/20 02:29;hxbks2ks;cc [~fsk119] Could you help take a look. Thanks.;;;","08/Dec/20 03:37;leonard;I'd like to look into this. ;;;","09/Dec/20 04:29;jark;Fixed in 
 - master: b82585fb78309f45f421d932ad0b43ebf62150ea
 - release-1.12: 3454d4367a68b40859bb9ff24f5130c46a303524;;;","15/Jan/21 12:03;hxbks2ks;The failed test happened again. Do we reopen this issue?

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12100&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5]
{code:java}
2021-01-15T09:10:21.7739782Z [ERROR]   UpsertKafkaTableITCase.testTemporalJoin:119->temporalJoinUpsertKafka:803 
2021-01-15T09:10:21.7740683Z Expected: <10001,100,2020-08-15T00:00:02,Bob,BEIJING,2020-08-15T00:00:01>
2021-01-15T09:10:21.7741354Z <10002,101,2020-08-15T00:00:03,Alice,SHANGHAI,2020-08-15T00:00:02>
2021-01-15T09:10:21.7741965Z <10002,102,2020-08-15T00:00:04,Greg,BERLIN,2020-08-15T00:00:03>
2021-01-15T09:10:21.7743174Z <10002,101,2020-08-16T00:02:01,Alice,WUHAN,2020-08-16T00:02>
2021-01-15T09:10:21.7743879Z <10004,104,2020-08-16T00:04,null,null,null>
2021-01-15T09:10:21.7744391Z <10003,101,2020-08-16T00:04:06,Alice,WUHAN,2020-08-16T00:02>
2021-01-15T09:10:21.7744984Z <10004,104,2020-08-16T00:05:06,Tomato,HONGKONG,2020-08-16T00:05:05>
2021-01-15T09:10:21.7745606Z      but: was <10001,100,2020-08-15T00:00:02,Bob,BEIJING,2020-08-15T00:00:01>
2021-01-15T09:10:21.7746262Z <10002,101,2020-08-15T00:00:03,Alice,SHANGHAI,2020-08-15T00:00:02>
2021-01-15T09:10:21.7746999Z <10002,101,2020-08-16T00:02:01,Alice,WUHAN,2020-08-16T00:02>
2021-01-15T09:10:21.7747678Z <10003,101,2020-08-16T00:04:06,Alice,WUHAN,2020-08-16T00:02>
2021-01-15T09:10:21.7748244Z <10002,102,2020-08-15T00:00:04,Greg,BERLIN,2020-08-15T00:00:03>
2021-01-15T09:10:21.7748943Z <10004,104,2020-08-16T00:04,null,null,null>
2021-01-15T09:10:21.7749417Z <10004,104,2020-08-16T00:05:06,null,null,null>
{code};;;","15/Jan/21 12:11;leonard;I think we can reopen this ticket, CC: [~fsk119] ;;;","20/Jan/21 03:19;jark;Fixed in
 - master: 59905140b7d6a31f8c07fc8151f33f978e475bae
 - release-1.12:  d18a700c98b3b90737d3490ae2711cd72d62a95d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientSchemaRegistryITCase.testReading test timed out after 120000 milliseconds,FLINK-20498,13344397,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxbks2ks,hxbks2ks,06/Dec/20 05:08,17/Jan/22 15:25,13/Jul/23 08:12,17/Jan/22 15:25,1.12.1,1.12.3,1.13.0,1.14.0,,1.14.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Client,,,,0,auto-deprioritized-major,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10548&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b]
{code:java}
2020-12-06T02:06:38.6416440Z Dec 06 02:06:38 org.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds
2020-12-06T02:06:38.6417052Z Dec 06 02:06:38 	at java.lang.Object.wait(Native Method)
2020-12-06T02:06:38.6417586Z Dec 06 02:06:38 	at java.lang.Thread.join(Thread.java:1252)
2020-12-06T02:06:38.6418170Z Dec 06 02:06:38 	at java.lang.Thread.join(Thread.java:1326)
2020-12-06T02:06:38.6418788Z Dec 06 02:06:38 	at org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:541)
2020-12-06T02:06:38.6419463Z Dec 06 02:06:38 	at org.apache.kafka.clients.admin.Admin.close(Admin.java:96)
2020-12-06T02:06:38.6420277Z Dec 06 02:06:38 	at org.apache.kafka.clients.admin.Admin.close(Admin.java:79)
2020-12-06T02:06:38.6420973Z Dec 06 02:06:38 	at org.apache.flink.tests.util.kafka.KafkaContainerClient.createTopic(KafkaContainerClient.java:76)
2020-12-06T02:06:38.6421797Z Dec 06 02:06:38 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.testReading(SQLClientSchemaRegistryITCase.java:109)
2020-12-06T02:06:38.6422517Z Dec 06 02:06:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-12-06T02:06:38.6423173Z Dec 06 02:06:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-12-06T02:06:38.6423990Z Dec 06 02:06:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-12-06T02:06:38.6424656Z Dec 06 02:06:38 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-12-06T02:06:38.6425321Z Dec 06 02:06:38 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-12-06T02:06:38.6426057Z Dec 06 02:06:38 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-12-06T02:06:38.6426766Z Dec 06 02:06:38 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-12-06T02:06:38.6427478Z Dec 06 02:06:38 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-12-06T02:06:38.6428232Z Dec 06 02:06:38 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-12-06T02:06:38.6428999Z Dec 06 02:06:38 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-12-06T02:06:38.6429707Z Dec 06 02:06:38 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-12-06T02:06:38.6430292Z Dec 06 02:06:38 	at java.lang.Thread.run(Thread.java:748)
{code}",,dian.fu,dwysakowicz,gaoyunhaii,hxbks2ks,kezhuw,maguowei,mapohl,martijnvisser,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20956,,,,,,,,,,,,,,,,,FLINK-23556,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 17 15:25:26 UTC 2022,,,,,,,,,,"0|z0l8i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/20 05:12;hxbks2ks;KafkaAdminClient did not succeed in close after creating topics within the test time of 2minus

cc [~dwysakowicz] Could you help take a look? Thanks.;;;","10/Jan/21 03:34;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11848&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b]

 ;;;","03/May/21 04:47;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17496&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27216;;;","03/May/21 04:50;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17494&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b&l=27495;;;","04/May/21 07:44;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17527&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=16805;;;","10/May/21 18:03;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17804&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","11/May/21 09:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17565&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=26752;;;","20/May/21 02:13;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18152&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610&l=17745
;;;","21/May/21 06:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18197&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=26409;;;","20/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Jun/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Jul/21 08:23;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20589&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b&l=28168;;;","20/Jul/21 13:07;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20729&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27874;;;","28/Jul/21 14:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21076&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=25153;;;","15/Aug/21 10:10;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22148&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24179;;;","23/Aug/21 13:27;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22524&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24253;;;","21/Nov/21 11:30;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26785&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=16877];;;","21/Nov/21 11:30;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26785&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610&l=17693]

This is a case for SQLClientSchemaRegistryITCase#testWriting, but it seems to be the same issue.;;;","17/Jan/22 15:25;martijnvisser;Most likely fixed via https://issues.apache.org/jira/browse/FLINK-23556;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLClientSchemaRegistryITCase failed with ""Could not build the flink-dist image""",FLINK-20493,13344322,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,hxbks2ks,hxbks2ks,05/Dec/20 02:49,07/Dec/20 19:51,13/Jul/23 08:12,07/Dec/20 19:51,1.13.0,,,,,1.12.1,1.13.0,,Table SQL / Client,,,,,0,test-stability,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10535&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610]
{code:java}
[ERROR] testReading(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 0.02 s  <<< ERROR!
2020-12-05T00:02:40.7985264Z Dec 05 00:02:40 java.lang.RuntimeException: Could not build the flink-dist image
2020-12-05T00:02:40.7986437Z Dec 05 00:02:40 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:283)
2020-12-05T00:02:40.7988073Z Dec 05 00:02:40 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.<init>(SQLClientSchemaRegistryITCase.java:91)
2020-12-05T00:02:40.7989022Z Dec 05 00:02:40 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-12-05T00:02:40.7990237Z Dec 05 00:02:40 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-12-05T00:02:40.7991510Z Dec 05 00:02:40 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-12-05T00:02:40.7992240Z Dec 05 00:02:40 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
2020-12-05T00:02:40.7992918Z Dec 05 00:02:40 	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
2020-12-05T00:02:40.7993949Z Dec 05 00:02:40 	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
2020-12-05T00:02:40.7994754Z Dec 05 00:02:40 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-12-05T00:02:40.7996033Z Dec 05 00:02:40 	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
2020-12-05T00:02:40.7996680Z Dec 05 00:02:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-12-05T00:02:40.7997240Z Dec 05 00:02:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-12-05T00:02:40.7997749Z Dec 05 00:02:40 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-12-05T00:02:40.7998231Z Dec 05 00:02:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-12-05T00:02:40.7998923Z Dec 05 00:02:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-12-05T00:02:40.7999435Z Dec 05 00:02:40 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-12-05T00:02:40.7999936Z Dec 05 00:02:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-12-05T00:02:40.8000665Z Dec 05 00:02:40 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-12-05T00:02:40.8001298Z Dec 05 00:02:40 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-12-05T00:02:40.8002411Z Dec 05 00:02:40 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2020-12-05T00:02:40.8002948Z Dec 05 00:02:40 	at java.base/java.lang.Thread.run(Thread.java:834)
2020-12-05T00:02:40.8003757Z Dec 05 00:02:40 Caused by: java.util.concurrent.TimeoutException: java.util.concurrent.TimeoutException
2020-12-05T00:02:40.8004387Z Dec 05 00:02:40 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:47)
2020-12-05T00:02:40.8005131Z Dec 05 00:02:40 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.buildBaseImage(FlinkContainer.java:314)
2020-12-05T00:02:40.8005731Z Dec 05 00:02:40 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:266)
2020-12-05T00:02:40.8006180Z Dec 05 00:02:40 	... 20 more
{code}",,dian.fu,dwysakowicz,hxbks2ks,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 19:51:03 UTC 2020,,,,,,,,,,"0|z0l81k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/20 02:54;hxbks2ks;Now the timeout is set to 1 minute. I'm not sure if increasing this time will alleviate the problem.

cc [~dwysakowicz] Could you help take a look at it.;;;","06/Dec/20 04:04;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10546&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610;;;","07/Dec/20 02:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10555&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610;;;","07/Dec/20 02:38;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10553&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610;;;","07/Dec/20 10:45;dwysakowicz;I think the problem is indeed the Timeout. All three failures happened with jdk 11. The test uses base image based on the java version that runs the test. Therefore if run with jdk 8 the container will use java 8 for running Flink cluster, if jdk 11 java 11. I guess no other tests download openjdk:11 image before contrary to the openjdk:8, therefore the test run on Java 11 requires more time.

I will increase the timeout.;;;","07/Dec/20 19:51;dwysakowicz;Timeouts increased in:
* master:
** 700e4c2afc53fcf23a40ef9bf7d73726e6113b2d
* 1.12.1
** 5dc5cf7e4af5e1b864da6e08db9074cce95126f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SourceOperatorStreamTask should implement cancelTask() and finishTask(),FLINK-20492,13344318,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,05/Dec/20 01:19,28/Dec/20 07:05,13/Jul/23 08:12,10/Dec/20 00:49,1.11.3,1.12.0,,,,1.11.3,1.12.1,1.13.0,Connectors / Common,,,,,0,pull-request-available,,,,,"The {{SourceOperatorStreamTask}} does not implement {{cancelTask}} and {{finishTask}} at this point. This causes resource leak on job cancellation of finish.

Currently there are three cases that a Job may exit.
 # Canceled - {{cancelTask()}} method will be called to close the SourceOperator.
 # Stopped with a savepoint - {{finishTask()}} method will be called to close the SourceOperator.
 # Runs to the EndOfInput -  the SourceOperator is closed in {{StreamTask.afterInvoke()}}.

 ",,becket_qin,kezhuw,stevenz3wu,yuehan124,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20781,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 00:48:50 UTC 2020,,,,,,,,,,"0|z0l80o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/20 00:48;becket_qin;Per discussion in the [PR|https://github.com/apache/flink/pull/14314]. The correct solution here is closing the {{SourceReader}} in {{SourceOperator.dispose()}} which will be called in all cases.

The PR has been merged:

master: 168124f99c75e873adc81437c700f85f703e2248
release-1.12: c22bb49443e6d70be5d312acee065fc9b4d95bb2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive temporal join should allow monitor interval smaller than 1 hour,FLINK-20486,13344193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,lirui,lirui,04/Dec/20 08:58,08/Dec/20 03:07,13/Jul/23 08:12,08/Dec/20 03:07,,,,,,1.12.1,1.13.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Currently hive temporal join requires the monitor interval to be at least 1h, which may not fit everyone's needs. Although we recommend a relatively large monitor interval, we shouldn't make such decision for users. A warning log is a better option for a small interval.",,jark,leonard,libenchao,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 12:52:05 UTC 2020,,,,,,,,,,"0|z0l78w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/20 09:05;lirui;cc [~Leonard Xu];;;","04/Dec/20 09:39;jark;Big +1.
I think a feature must can be tried in testing environment. An interval > 1hour is hard to try out this feature. ;;;","04/Dec/20 11:36;leonard;Thanks [~lirui] for the report, I agree that unlock the limitation may help user in simply test case. I've create a PR.;;;","07/Dec/20 12:52;jark;Fixed in
 - master (1.13.0): 09ec2ae626acac0c17374a77d49f290ae5289f7d
 - release-1.12: 54cb13366d1366b22a77af0c7722715a3b4d4766;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MissingNode can't be casted to ObjectNode when deserializing JSON,FLINK-20470,13344017,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,jark,jark,03/Dec/20 13:36,09/Dec/20 05:42,13/Jul/23 08:12,09/Dec/20 05:42,1.11.2,1.12.0,,,,1.12.1,1.13.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"{code}
Caused by: java.io.IOException: Failed to deserialize JSON ''.
        at
org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:126)
~[flink-json-1.11.2.jar:1.11.2]
        at
org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:76)
~[flink-json-1.11.2.jar:1.11.2]
        at
org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:81)
~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.connectors.kafka.internals.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56)
~[flink-sql-connector-kafka_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:181)
~[flink-sql-connector-kafka_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141)
~[flink-sql-connector-kafka_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755)
~[flink-sql-connector-kafka_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at
org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
~[flink-dist_2.11-1.11.2.jar:1.11.2]
Caused by: java.lang.ClassCastException:
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.MissingNode
cannot be cast to
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode
{code}

Currently, we only check {{jsonNode == null || jsonNode.isNull()}} for nullable node, I think we should also take MissingNode into account. 

",,caozhen1937,jark,libenchao,zhisheng,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 05:42:49 UTC 2020,,,,,,,,,,"0|z0l65s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/20 02:37;ZhuShang;Hi [~jark],I want to take this.;;;","07/Dec/20 06:09;ZhuShang;[~jark],The legacy  JsonRowDeserializationSchema also has this problem,do we need to fix it?

 ;;;","09/Dec/20 05:42;jark;Fixed in 
 - master: 29e122d539caebe9dd5717439eda12b252e26d41
 - releaes-1.12: 9a2d4c2e23de074e0aafebf72c23d5a95cf76614;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the Example in Python DataStream Doc,FLINK-20467,13343961,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,hxbks2ks,hxbks2ks,03/Dec/20 09:42,03/Dec/20 12:27,13/Jul/23 08:12,03/Dec/20 12:26,1.12.0,1.13.0,,,,1.12.1,1.13.0,,API / Python,Documentation,,,,0,pull-request-available,,,,,Currently the example of MapFunction can't work. We need to fix it.,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 12:26:56 UTC 2020,,,,,,,,,,"0|z0l5tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/20 12:26;dian.fu;Fixed in
- master via a411abf132e10484c674d8dfc0ede04f80c8a9f3
- release-1.12 via de53c34d54dca99e5e7cd8af1596829832b16ffa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some Table examples are not built correctly,FLINK-20464,13343946,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,03/Dec/20 09:03,03/Dec/20 15:45,13/Jul/23 08:12,03/Dec/20 15:45,1.12.0,,,,,1.12.1,,,Examples,,,,,0,pull-request-available,,,,,"Some examples were moved to the {{org.apache.flink.table.examples.scala.basics}} package but the pom.xml was not updated. This means the example jars are not built correctly and do not contain the classes.

Examples that I noticed:
* org.apache.flink.table.examples.scala.basics.StreamTableExample
* org.apache.flink.table.examples.scala.basics.TPCHQuery3Table

We should update the {{includes}} sections e.g.:

{code}
<execution>
	<id>StreamTableExample</id>
	<phase>package</phase>
	<goals>
		<goal>jar</goal>
	</goals>
	<configuration>
		<classifier>StreamTableExample</classifier>

<!--- The sections below should be updated -->

		<archive>
			<manifestEntries>
				<program-class>org.apache.flink.table.examples.scala.StreamTableExample</program-class>
			</manifestEntries>
		</archive>
		<includes>
			<include>org/apache/flink/table/examples/scala/StreamTableExample*</include>
		</includes>
	</configuration>
</execution>
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 15:45:24 UTC 2020,,,,,,,,,,"0|z0l5q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/20 15:45;dwysakowicz;Fixed in:
* master
** e10e548feb2bedf54c3863bbd49ed4f9140546cf
* 1.12
** 2ad1e7b21eb523e3ee727cdf99c6a72c4e0709c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MailboxOperatorTest.testAvoidTaskStarvation,FLINK-20462,13343908,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,hxbks2ks,hxbks2ks,03/Dec/20 05:11,22/Jun/21 14:04,13/Jul/23 08:12,16/Jan/21 20:47,1.12.0,1.13.0,,,,1.12.2,1.13.0,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10450&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada
{code:java}
[ERROR] testAvoidTaskStarvation(org.apache.flink.streaming.runtime.operators.MailboxOperatorTest) Time elapsed: 1.142 s <<< FAILURE! 


java.lang.AssertionError: 

 

Expected: is <[0, 2, 4]> 


 but: was <[0, 2, 516]> 


 at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) 


 at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8) 


 at org.apache.flink.streaming.runtime.operators.MailboxOperatorTest.testAvoidTaskStarvation(MailboxOperatorTest.java:85)

{code}",,AHeise,hxbks2ks,mapohl,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 16 20:47:15 UTC 2021,,,,,,,,,,"0|z0l5hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/20 08:08;mapohl;I verified locally that the test is unstable. It failed after 2016 runs:
{code:java}
java.lang.AssertionError: 
Expected: is <[0, 2, 4]>
     but: was <[0, 42, 44]>
Expected :is <[0, 2, 4]>
Actual   :<[0, 42, 44]>
<Click to see difference>




	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.apache.flink.streaming.runtime.operators.MailboxOperatorTest.testAvoidTaskStarvation(MailboxOperatorTest.java:85)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:52)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53) {code};;;","25/Dec/20 06:27;hxbks2ks;instance in Master

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11304&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada]

 ;;;","09/Jan/21 05:18;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11829&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada]

 ;;;","16/Jan/21 20:47;arvid;Merged into master as cc44a5533d2bf984e601a26759812f4d7097fdb6.
Merged into 1.12 as 069e05a3513d0eeba078e90f9d1a228b1c3a616b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix license documentation mistakes in flink-python.jar,FLINK-20442,13343547,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,rmetzger,rmetzger,01/Dec/20 12:22,05/Dec/20 12:48,13/Jul/23 08:12,05/Dec/20 12:48,1.12.0,,,,,1.11.3,1.12.0,,API / Python,,,,,0,pull-request-available,,,,,"Issues reported by Chesnay:

-The flink-python jar contains 2 license files in the root directory and another 2 in the META-INF directory. This should be reduced down to 1 under META-INF. I'm inclined to block the release on this because the root license is BSD.
- The flink-python jar appears to bundle lz4 (native libraries under win32/, linux/ and darwin/), but this is neither listed in the NOTICE nor do we have an explicit license file for it.

Other minor things that we should address in the future:
- opt/python contains some LICENSE files that should instead be placed under licenses/
- licenses/ contains a stray ""ASM"" file containing the ASM license. It's not a problem (because it is identical with our intended copy), but it indicates that something is amiss. This seems to originate from the flink-python jar, which bundles some beam stuff, which bundles bytebuddy, which bundles this license file. From what I can tell bytebuddy is not actually bundling ASM though; they just bundle the license for whatever reason. It is not listed as bundled in the flink-python NOTICE though, so I wouldn't block the release on it.",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20455,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 05 12:48:23 UTC 2020,,,,,,,,,,"0|z0l39c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/20 07:27;dian.fu;Merged to
- master via 7818b7a8290c84b99e809773b8c91fb028670865
- release-1.12 via 58ccb941803107afd08f72762072c7467b7bbd01;;;","02/Dec/20 07:30;dian.fu;I found that part of the problems described in this JIRA also exists in 1.11.x. I'll keep this JIRA open for now and prepare a PR for release-1.11. ;;;","05/Dec/20 12:48;dian.fu;Backport to release-1.11 via af80ba57ade23f881af21beb93178495bbb7e500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testKafkaTemporalJoinChangelog failed with unexpected results,FLINK-20429,13343435,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,dian.fu,dian.fu,01/Dec/20 01:33,07/Dec/20 12:36,13/Jul/23 08:12,07/Dec/20 12:36,1.13.0,,,,,1.12.1,1.13.0,,Connectors / Kafka,Table SQL / Planner,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10349&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

{code}
2020-11-30T18:41:15.9353975Z Test testKafkaTemporalJoinChangelog[legacy = false, format = csv](org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase) failed with:
2020-11-30T18:41:15.9388679Z java.lang.AssertionError: expected:<[o_001,2020-10-01T00:01,p_001,1970-01-01T00:00,11.1100,Alice,scooter,1,11.1100, o_002,2020-10-01T00:02,p_002,1970-01-01T00:00,23.1100,Bob,basketball,1,23.1100, o_003,2020-10-01T12:00,p_001,2020-10-01T12:00,12.9900,Tom,scooter,2,25.9800, o_004,2020-10-01T12:00,p_002,2020-10-01T12:00,19.9900,King,basketball,2,39.9800, o_005,2020-10-01T18:00,p_001,2020-10-01T18:00,11.9900,Leonard,scooter,10,119.9000, o_006,2020-10-01T18:00,null,null,null,Leonard,null,10,null]> but was:<[o_001,2020-10-01T00:01,p_001,1970-01-01T00:00,11.1100,Alice,scooter,1,11.1100, o_002,2020-10-01T00:02,p_002,1970-01-01T00:00,23.1100,Bob,basketball,1,23.1100, o_003,2020-10-01T12:00,p_001,2020-10-01T12:00,12.9900,Tom,scooter,2,25.9800, o_004,2020-10-01T12:00,p_002,1970-01-01T00:00,23.1100,King,basketball,2,46.2200, o_005,2020-10-01T18:00,p_001,2020-10-01T18:00,11.9900,Leonard,scooter,10,119.9000, o_006,2020-10-01T18:00,null,null,null,Leonard,null,10,null]>
2020-11-30T18:41:15.9392921Z 	at org.junit.Assert.fail(Assert.java:88)
2020-11-30T18:41:15.9403705Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-11-30T18:41:15.9404670Z 	at org.junit.Assert.assertEquals(Assert.java:118)
2020-11-30T18:41:15.9405477Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-11-30T18:41:15.9406471Z 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testKafkaTemporalJoinChangelog(KafkaTableITCase.java:633)
{code}",,dian.fu,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 12:36:39 UTC 2020,,,,,,,,,,"0|z0l2kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/20 01:34;dian.fu;cc [~Leonard Xu];;;","01/Dec/20 01:40;leonard;Thanks [~dian.fu] for the report, Could you help assign this to me ?;;;","01/Dec/20 01:43;dian.fu;[~Leonard Xu] Thanks a lot! Have assigned it to you~;;;","07/Dec/20 12:36;jark;Fixed in
 - master (1.13.0): beb08a29d5eeb1ee075132597f89270d524db152
 - release-1.12 (1.12.1): 4e8f01050d90d2e95fc3520af1e5a3aa088a6d48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ZooKeeperLeaderElectionConnectionHandlingTest.testConnectionSuspendedHandlingDuringInitialization failed with ""No result is expected since there was no leader elected before stopping the server, yet""",FLINK-20428,13343432,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,dian.fu,dian.fu,01/Dec/20 01:26,02/Dec/20 15:31,13/Jul/23 08:12,02/Dec/20 15:31,1.13.0,,,,,1.12.1,1.13.0,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10327&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
2020-11-30T13:22:47.5981970Z [ERROR] testConnectionSuspendedHandlingDuringInitialization(org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest)  Time elapsed: 2.16 s  <<< FAILURE!
2020-11-30T13:22:47.5984731Z java.lang.AssertionError: 
2020-11-30T13:22:47.5985220Z No result is expected since there was no leader elected before stopping the server, yet.
2020-11-30T13:22:47.5985546Z Expected: is null
2020-11-30T13:22:47.5985866Z      but: was <java.util.concurrent.CompletableFuture@53f3bdbd[Completed normally]>
2020-11-30T13:22:47.5986376Z 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
2020-11-30T13:22:47.5986752Z 	at org.junit.Assert.assertThat(Assert.java:956)
2020-11-30T13:22:47.5990853Z 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.testConnectionSuspendedHandlingDuringInitialization(ZooKeeperLeaderElectionConnectionHandlingTest.java:106)
{code}",,dian.fu,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 15:31:31 UTC 2020,,,,,,,,,,"0|z0l2js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/20 04:09;wangyang0918;I will have a look on the failed test.;;;","01/Dec/20 06:01;wangyang0918;When the ZooKeeper connection is suspended, the {{LeaderRetrievalEventHandler}} will be notified with an empty leader information. In the failed test {{ZooKeeperLeaderElectionConnectionHandlingTest.testConnectionSuspendedHandlingDuringInitialization}}, we expect no leader will be notified. This is wrong. The reason why it could work is the timeout is set to very small(50ms) and it is too fast to get the leader information.

 

I will attach to fix the test.;;;","02/Dec/20 15:31;trohrmann;Fixed via

1.13.0: 674d61033a98e2b9b7913c3ed1359b9404e019bd 
1.12.1: 884765e0f0a1f257b3ffdf00e19958791e49cf35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove CheckpointConfig.setPreferCheckpointForRecovery because it can lead to data loss,FLINK-20427,13343351,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Nicolaus Weidner,trohrmann,trohrmann,30/Nov/20 16:30,03/Sep/21 09:12,13/Jul/23 08:12,03/Sep/21 09:12,1.12.0,,,,,1.14.0,,,API / DataStream,Runtime / Checkpointing,,,,0,auto-deprioritized-critical,pull-request-available,,,,"The {{CheckpointConfig.setPreferCheckpointForRecovery}} allows to configure whether Flink prefers checkpoints for recovery if the {{CompletedCheckpointStore}} contains savepoints and checkpoints. This is problematic because due to this feature, Flink might prefer older checkpoints over newer savepoints for recovery. Since some components expect that the always the latest checkpoint/savepoint is used (e.g. the {{SourceCoordinator}}), it breaks assumptions and can lead to {{SourceSplits}} which are not read. This effectively means that the system loses data. Similarly, this behaviour can cause that exactly once sinks might output results multiple times which violates the processing guarantees. Hence, I believe that we should remove this setting because it changes Flink's behaviour in some very significant way potentially w/o the user noticing.",,dwysakowicz,kezhuw,klion26,Liwen Liu,liyu,Ming Li,nkruber,pnowojski,rex-remind,sewen,stevenz3wu,trohrmann,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,FLINK-20441,,,,,,,,,,,,,,,,,,,FLINK-11159,,,,FLINK-20290,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 03 09:12:18 UTC 2021,,,,,,,,,,"0|z0l21s:",9223372036854775807,"Remove deprecated CheckpointConfig#setPreferCheckpointForRecovery

Deprecated method CheckpointConfig#setPreferCheckpointForRecovery was removed, because preferring older checkpoints over newer savepoints for recovery can lead to data loss.",,,,,,,,,,,,,,,,,,,"30/Nov/20 16:31;trohrmann;The feature has been originally introduced with FLINK-11159.;;;","30/Nov/20 16:32;trohrmann;fyi [~NicoK] in case you want to discuss it.;;;","30/Nov/20 17:22;sewen;+1 to remove it, until the semantics we better defined.

I would also vote to deprecate/discourage it in 1.12 already.;;;","30/Nov/20 17:53;trohrmann;I'd also be in favour of deprecating this option in {{1.12.0}} to discourage users from using this feature.;;;","01/Dec/20 11:43;nkruber;I'm not sure we should touch this before the rework of the checkpoint/savepoint semantics - if this is still planned, I am talking about not having checkpoints vs. savepoints but rather snapshots with certain properties. In that case, we could have user-triggered snapshots with statebackend-native format and then I don't see any downside of using these for recovery just like using checkpoints-only now.
From what we learned, RocksDB-recovery from a savepoint can actually take a considerable amount of time and especially in at-least-once sink use cases with low latency requirements, this could be a problem!;;;","01/Dec/20 12:31;trohrmann;Are users aware of the consequences when setting {{CheckpointConfig.setPreferCheckpointForRecovery(true)}}? I would be sceptical because I couldn't find it documented anywhere. Exposing such an option is in my opinion a recipe for disaster and exposes yet another special case knob for tuning Flink which is already hard enough to operate properly. Especially, with the new sources where it can lead to not reading data it can be problematic.;;;","01/Dec/20 20:05;nkruber;If Flink 1.12 adds additional risks here due to the new sources, that would of course be something to consider, but I'm a bit unclear what the problems are there.

Are you saying that any restore operation for these sources that is not based on the latest snapshot will/may lead to data loss? So including restoring from an arbitrary savepoint (or actually also a retained checkpoint)? If that was the case, that's imho a bug in the new sources...;;;","02/Dec/20 14:58;trohrmann;No, it is only a problem if a regional failover occurs where we don't resume from the latest checkpoint/savepoint. This can, however, easily happen in normal operation mode and, hence, I believe that we should not allow resuming from an earlier checkpoint/savepoint. Global failovers where all tasks are restarted or manual resuming from a savepoint are fine. ;;;","03/Dec/20 09:09;nkruber;Ahh...Flink regional failover. Makes sense now: for these, the sources have the implicit assumption that they recover from the latest snapshot (while for a global failover, they restore from the snapshotted state). It would be good to add a safeguard these to ensure that the source is always recovering from the latest snapshot for regional failovers then (ideally it would then fall back to a global failover if this assumption is not true).
As for removing {{CheckpointConfig.setPreferCheckpointForRecovery}}: Let's evaluate with the user ml whether anyone is relying on this and if not, let's remove it and get rid of one more special case. The only use case I can think of which may benefit from this feature here is a low-latency use case which tolerates duplicates in the sinks but has very strong SLAs even in the failure case: these could work around the slow savepoint-restore via retained checkpoints. Retained checkpoints, however, don't really serve as a backup which you can roll back to in case of bugs - user-triggered checkpoints could be a better solution here, as mentioned, but they don't exist yet.;;;","03/Dec/20 13:26;sewen;We need to do something about this, because the current state is just broken. Like Till mentioned, any regional failover is inconsistent. Exactly-once sinks are by definition broken with this.

I am in favor of deprecating this in 1.12. It would not break programs, but would make users aware that this is not a feature with well-defined semantics.

Then let's clearly define how savepoints, side effects, and durability notifications (checkpoint complete) should interact in all these scenarios. Then we can think about adding this option back, or maybe the option will not be necessary any more.;;;","26/Dec/20 09:40;Liwen Liu;Hi，[~trohrmann]

Could you assign this ticket to me ?

Thanks~;;;","28/Dec/20 09:46;trohrmann;Hi [~Liwen Liu], this ticket is not only about removing this config option but also to define the proper semantics of savepoints and side effects. Hence, we would first need a discussion about how they can be properly defined before removing this setting.;;;","28/Dec/20 09:54;Liwen Liu;[~trohrmann] I got you.Thanks.;;;","17/Feb/21 18:15;rex-remind;We've been relying on this because recovery from savepoints takes hours instead of minutes. Our sources are Kafka and sink is ES. We already expect ES to receive duplicate writes but don't care. However, we can't lose data from Kafka. Is one of the sources impacted by this bug Kafka?;;;","18/Feb/21 12:40;trohrmann;[~rex-remind], the sources are not affected by this problem. You have problems if you require your Flink job to have exactly once processing guarantees wrt to your sink or external systems you are interacting. If your setup tolerates duplicates in ES, then this is not a problem.;;;","19/Feb/21 02:39;rex-remind;Cool, thanks!;;;","07/Apr/21 06:54;dwysakowicz;Is this still targeted at 1.13?;;;","22/Apr/21 12:24;flink-jira-bot;This critical issue is unassigned and itself and all of its Sub-Tasks have not been updated for 7 days. So, it has been labeled ""stale-critical"". If this ticket is indeed critical, please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:49;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/May/21 22:56;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/May/21 08:02;trohrmann;I think this is still a very important issue to sort out and fix because it can lead to unwanted data loss.;;;","27/May/21 23:05;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Jun/21 23:23;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Aug/21 06:26;xtsong;[~trohrmann],
This interface has been deprecated in 1.12.0. Shall we remove it completely in 1.14.0?;;;","18/Aug/21 08:51;trohrmann;I would be in favour because using this feature breaks processing guarantees. However, I am not sure whether [~nkruber] would agree. Stephan wrote a design for defining the checkpoint and savepoint semantics properly. This will most likely be implemented in the next Flink version.

[~nkruber] do we know whether our users actually use this flag or can we remove it after being deprecated for two releases?;;;","20/Aug/21 11:03;nkruber;[~trohrmann] it may be cleaner to remove this along with the new checkpoint/savepoint semantics, but I also don't have any user in mind who relies on this flag.;;;","24/Aug/21 08:27;sewen;I'll start a poll on the mailing list to see if anyone uses this and whether there are big concerns about removing this.;;;","24/Aug/21 09:52;sewen;About the above-mentioned design doc for the improved semantics: I'll try to share this publicly asap, once I have cleaned it up a bit (it currently contains a lot of raw notes still).;;;","24/Aug/21 09:59;sewen;Here is the Mailing List Poll: https://lists.apache.org/x/thread.html/rce4e625f80c1b4014f96b15fb1add9ff7dd13203b871e3f6d3b80f11@%3Cdev.flink.apache.org%3E;;;","03/Sep/21 09:12;trohrmann;Fixed via

1.15.0:
eb8c21c12d60db2e579d88e465df0cdbeccba74b
6b0383719cacdbd2cff14fa9280c2971cd2fa216

1.14.0:
fcb8ccf6856cd43d0faec91e6296255ed619bccd
af85ad47c8461751ca7f3e1aac9a14e88a95d44d;;;",,,,,,,,,,,,,,,,,,,,,,,
Broken links to hadoop.md,FLINK-20426,13343324,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,dwysakowicz,dwysakowicz,30/Nov/20 14:27,01/Dec/20 15:00,13/Jul/23 08:12,01/Dec/20 15:00,1.12.0,1.13.0,,,,1.12.0,1.13.0,,Documentation,,,,,0,pull-request-available,,,,,"In FLINK-20347 we removed the {{hadoop.md}} page, but there are still links in other pages:
* dev/project-configuration.md
* dev/batch/hadoop-compatibility.md
* connectors/hive/index.md",,dian.fu,dwysakowicz,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20347,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 15:00:46 UTC 2020,,,,,,,,,,"0|z0l1vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 14:28;dwysakowicz;cc [~rmetzger];;;","30/Nov/20 14:35;rmetzger;On it!;;;","30/Nov/20 14:53;dwysakowicz;Thank you! BTW I don't know the background for dropping the hadoop.md page. I also don't know exactly all its usages, but I think it was rather useful or even necessary in the context of Hive setup.;;;","30/Nov/20 15:09;rmetzger;I removed it because 
- the ""providing Hadoop classes"" can be summarized in ""export the HADOOP_CLASSPATH"" on the YARN page. For the FileSystem it is not relevant anymore, because of the plugin mechanism
- ""Running a job locally"" is only relevant for FileSystem-stuff, thus can be removed
- ""Using flink-shaded-hadoop2-uber"" is not maintained since 1.11, that's why I thought we can remove it in 1.12. It's still mentioned in the updated YARN page, in an advanced section.

What is really necessary for the Hive setup, from that page?;;;","30/Nov/20 15:36;dwysakowicz;The `export HADOOP_CLASSPATH` ;);;;","01/Dec/20 05:03;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10351&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98;;;","01/Dec/20 14:54;rmetzger;Fixed on master in https://github.com/apache/flink/pull/14272/commits/285c5afea2dd0c2d130af3c175509e73239a1377;;;","01/Dec/20 15:00;rmetzger;Fixed in release-1.12 in https://github.com/apache/flink/commit/8b44dae49bd875408b3e07b878d36e156b5c1665;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert fails due to failure to generate execution plan,FLINK-20419,13343276,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,30/Nov/20 09:44,09/Dec/20 04:28,13/Jul/23 08:12,09/Dec/20 04:27,,,,,,1.12.1,1.13.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Test case to reproduce:
{code}
	@Test
	public void test() throws Exception {
		tableEnv.executeSql(""create table src(x int)"");
		tableEnv.executeSql(""create table dest(x int) partitioned by (p string,q string)"");
		tableEnv.executeSql(""insert into dest select x,'0','0' from src order by x"").await();
	}
{code}",,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 13:28:22 UTC 2020,,,,,,,,,,"0|z0l1l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 09:45;lirui;Detailed failure is:
{noformat}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

FlinkLogicalSink(table=[test-catalog.default.dest], fields=[x, EXPR$1, EXPR$2])
+- FlinkLogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
   +- FlinkLogicalCalc(select=[x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2])
      +- FlinkLogicalTableSourceScan(table=[[test-catalog, default, src]], fields=[x])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.

	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:72)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1267)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:675)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:759)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:665)
	at org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.test(TableEnvHiveConnectorITCase.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=BATCH_PHYSICAL, FlinkRelDistributionTraitDef=any, sort=[].
Missing conversion is FlinkLogicalSort[convention: LOGICAL -> BATCH_PHYSICAL, sort: [0 ASC-nulls-first] -> [1 ASC-nulls-first, 2 ASC-nulls-first]]
There is 1 empty subset: rel#219:RelSubset#6.BATCH_PHYSICAL.any.[1 ASC-nulls-first, 2 ASC-nulls-first], the relevant part of the original plan is as follows
205:FlinkLogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
  203:FlinkLogicalCalc(subset=[rel#204:RelSubset#5.LOGICAL.any.[]], select=[x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2])
    175:FlinkLogicalTableSourceScan(subset=[rel#202:RelSubset#4.LOGICAL.any.[]], table=[[test-catalog, default, src]], fields=[x])

Root: rel#209:RelSubset#7.BATCH_PHYSICAL.any.[]
Original rel:
FlinkLogicalSink(subset=[rel#173:RelSubset#3.LOGICAL.any.[]], table=[test-catalog.default.dest], fields=[x, EXPR$1, EXPR$2]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 186
  FlinkLogicalSort(subset=[rel#185:RelSubset#1.LOGICAL.any.[0 ASC-nulls-first]], sort0=[$0], dir0=[ASC-nulls-first]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 184
    FlinkLogicalCalc(subset=[rel#183:RelSubset#1.LOGICAL.any.[]], select=[x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 187
      FlinkLogicalTableSourceScan(subset=[rel#176:RelSubset#0.LOGICAL.any.[]], table=[[test-catalog, default, src]], fields=[x]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}, id = 175

Sets:
Set#4, type: RecordType(INTEGER x)
	rel#202:RelSubset#4.LOGICAL.any.[], best=rel#175
		rel#175:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[test-catalog, default, src],fields=x), rowcount=1.0E8, cumulative cost={1.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#212:RelSubset#4.BATCH_PHYSICAL.any.[], best=rel#211
		rel#211:BatchExecTableSourceScan.BATCH_PHYSICAL.any.[](table=[test-catalog, default, src],fields=x), rowcount=1.0E8, cumulative cost={1.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
		rel#224:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#212,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#227:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#212,distribution=single), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
	rel#223:RelSubset#4.BATCH_PHYSICAL.single.[], best=rel#227
		rel#224:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#212,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#227:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#212,distribution=single), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
Set#5, type: RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)
	rel#204:RelSubset#5.LOGICAL.any.[], best=rel#203
		rel#203:FlinkLogicalCalc.LOGICAL.any.[](input=RelSubset#202,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#214:RelSubset#5.BATCH_PHYSICAL.any.[], best=rel#213
		rel#213:BatchExecCalc.BATCH_PHYSICAL.any.[](input=RelSubset#212,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2), rowcount=1.0E8, cumulative cost={2.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
		rel#216:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#214,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#222:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#214,distribution=single), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 8.0E8 network, 0.0 memory}
		rel#225:BatchExecCalc.BATCH_PHYSICAL.single.[](input=RelSubset#223,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
	rel#215:RelSubset#5.BATCH_PHYSICAL.single.[], best=rel#225
		rel#216:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#214,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#222:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#214,distribution=single), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 8.0E8 network, 0.0 memory}
		rel#225:BatchExecCalc.BATCH_PHYSICAL.single.[](input=RelSubset#223,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
Set#6, type: RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)
	rel#206:RelSubset#6.LOGICAL.any.[0 ASC-nulls-first], best=rel#205
		rel#205:FlinkLogicalSort.LOGICAL.any.[0 ASC-nulls-first](input=RelSubset#204,sort0=$0,dir0=ASC-nulls-first), rowcount=1.0E8, cumulative cost={3.0E8 rows, 2.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#218:RelSubset#6.BATCH_PHYSICAL.single.[0 ASC-nulls-first], best=rel#217
		rel#217:BatchExecSort.BATCH_PHYSICAL.single.[0 ASC-nulls-first](input=RelSubset#215,orderBy=x ASC), rowcount=1.0E8, cumulative cost={4.0E8 rows, 2.3468272297580948E10 cpu, 4.0E8 io, 4.0E8 network, 6.4E9 memory}
	rel#219:RelSubset#6.BATCH_PHYSICAL.any.[1 ASC-nulls-first, 2 ASC-nulls-first], best=null
Set#7, type: RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)
	rel#208:RelSubset#7.LOGICAL.any.[], best=rel#207
		rel#207:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#206,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2), rowcount=1.0E8, cumulative cost={4.0E8 rows, 3.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#209:RelSubset#7.BATCH_PHYSICAL.any.[], best=null
		rel#210:AbstractConverter.BATCH_PHYSICAL.any.[](input=RelSubset#208,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#220:BatchExecSink.BATCH_PHYSICAL.any.[](input=RelSubset#219,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2), rowcount=1.0E8, cumulative cost={inf}

Graphviz:
digraph G {
	root [style=filled,label=""Root""];
	subgraph cluster4{
		label=""Set 4 RecordType(INTEGER x)"";
		rel175 [label=""rel#175:FlinkLogicalTableSourceScan\ntable=[test-catalog, default, src],fields=x\nrows=1.0E8, cost={1.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel211 [label=""rel#211:BatchExecTableSourceScan\ntable=[test-catalog, default, src],fields=x\nrows=1.0E8, cost={1.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel224 [label=""rel#224:AbstractConverter\ninput=RelSubset#212,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]\nrows=1.0E8, cost={inf}"",shape=box]
		rel227 [label=""rel#227:BatchExecExchange\ninput=RelSubset#212,distribution=single\nrows=1.0E8, cost={2.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}"",color=blue,shape=box]
		subset202 [label=""rel#202:RelSubset#4.LOGICAL.any.[]""]
		subset212 [label=""rel#212:RelSubset#4.BATCH_PHYSICAL.any.[]""]
		subset223 [label=""rel#223:RelSubset#4.BATCH_PHYSICAL.single.[]""]
		subset212 -> subset223;	}
	subgraph cluster5{
		label=""Set 5 RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)"";
		rel203 [label=""rel#203:FlinkLogicalCalc\ninput=RelSubset#202,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2\nrows=1.0E8, cost={2.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel213 [label=""rel#213:BatchExecCalc\ninput=RelSubset#212,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2\nrows=1.0E8, cost={2.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel216 [label=""rel#216:AbstractConverter\ninput=RelSubset#214,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]\nrows=1.0E8, cost={inf}"",shape=box]
		rel222 [label=""rel#222:BatchExecExchange\ninput=RelSubset#214,distribution=single\nrows=1.0E8, cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 8.0E8 network, 0.0 memory}"",shape=box]
		rel225 [label=""rel#225:BatchExecCalc\ninput=RelSubset#223,select=x, _UTF-16LE'0' AS EXPR$1, _UTF-16LE'0' AS EXPR$2\nrows=1.0E8, cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}"",color=blue,shape=box]
		subset204 [label=""rel#204:RelSubset#5.LOGICAL.any.[]""]
		subset214 [label=""rel#214:RelSubset#5.BATCH_PHYSICAL.any.[]""]
		subset215 [label=""rel#215:RelSubset#5.BATCH_PHYSICAL.single.[]""]
		subset214 -> subset215;	}
	subgraph cluster6{
		label=""Set 6 RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)"";
		rel205 [label=""rel#205:FlinkLogicalSort\ninput=RelSubset#204,sort0=$0,dir0=ASC-nulls-first\nrows=1.0E8, cost={3.0E8 rows, 2.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel217 [label=""rel#217:BatchExecSort\ninput=RelSubset#215,orderBy=x ASC\nrows=1.0E8, cost={4.0E8 rows, 2.3468272297580948E10 cpu, 4.0E8 io, 4.0E8 network, 6.4E9 memory}"",color=blue,shape=box]
		subset206 [label=""rel#206:RelSubset#6.LOGICAL.any.[0 ASC-nulls-first]""]
		subset218 [label=""rel#218:RelSubset#6.BATCH_PHYSICAL.single.[0 ASC-nulls-first]""]
		subset219 [label=""rel#219:RelSubset#6.BATCH_PHYSICAL.any.[1 ASC-nulls-first, 2 ASC-nulls-first]"",color=red]
	}
	subgraph cluster7{
		label=""Set 7 RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)"";
		rel207 [label=""rel#207:FlinkLogicalSink\ninput=RelSubset#206,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2\nrows=1.0E8, cost={4.0E8 rows, 3.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel210 [label=""rel#210:AbstractConverter\ninput=RelSubset#208,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]\nrows=1.0E8, cost={inf}"",shape=box]
		rel220 [label=""rel#220:BatchExecSink\ninput=RelSubset#219,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2\nrows=1.0E8, cost={inf}"",shape=box]
		subset208 [label=""rel#208:RelSubset#7.LOGICAL.any.[]""]
		subset209 [label=""rel#209:RelSubset#7.BATCH_PHYSICAL.any.[]""]
	}
	root -> subset209;
	subset202 -> rel175[color=blue];
	subset212 -> rel211[color=blue];
	subset223 -> rel224; rel224 -> subset212;
	subset223 -> rel227[color=blue]; rel227 -> subset212[color=blue];
	subset204 -> rel203[color=blue]; rel203 -> subset202[color=blue];
	subset214 -> rel213[color=blue]; rel213 -> subset212[color=blue];
	subset215 -> rel216; rel216 -> subset214;
	subset215 -> rel222; rel222 -> subset214;
	subset215 -> rel225[color=blue]; rel225 -> subset223[color=blue];
	subset206 -> rel205[color=blue]; rel205 -> subset204[color=blue];
	subset218 -> rel217[color=blue]; rel217 -> subset215[color=blue];
	subset208 -> rel207[color=blue]; rel207 -> subset206[color=blue];
	subset209 -> rel210; rel210 -> subset208;
	subset209 -> rel220; rel220 -> subset219;
}
	at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:742)
	at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:365)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:520)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	... 48 more
{noformat};;;","30/Nov/20 11:31;jark;Hi [~lirui], are you in streaming mode? Order by regular columns is not supported in streaming mode. Do you mean we should improve this error message?;;;","30/Nov/20 11:41;lirui;Hi [~jark], I was running in batch mode. So I guess this is could be a bug, although I haven't dug deep into it.;;;","30/Nov/20 12:06;jark;Got it [~lirui].;;;","02/Dec/20 13:41;lirui;I suspect the issue is related to dynamic partition grouping. {{BatchExecSinkRule}} automatically adds field collations for dynamic partitions by calling {{RelTraitSet::plus}}, which would replace previous collation {{RelTrait}}. This error message seems to imply the issue:
{noformat}
Missing conversion is FlinkLogicalSort[convention: LOGICAL -> BATCH_PHYSICAL, sort: [0 ASC-nulls-first] -> [1 ASC-nulls-first, 2 ASC-nulls-first]]
{noformat}
I tried to disable partition grouping and the query passed.
Perhaps we shouldn't do partition grouping if the input already defined collation trait.;;;","02/Dec/20 13:47;lirui;Hi [~jark], I think you implemented {{BatchExecSinkRule}}, any thoughts about my above proposal?;;;","03/Dec/20 08:02;jark;[~lirui], the logic of {{BatchExecSinkRule}} is copied from {{BatchExecLegacySinkRule}}.;;;","08/Dec/20 11:26;jark;Fixed in 
 - master: c4a4d6290daf2199bfdfb00a92ac464c72c93876, a40b261d0cb77420fca6eed17952677aa49c788b
 - release-1.12: 5a55f0173a26dfc7af250d7aed55f81a00660978, 87d8358e8d2e88a032c851a517eb677709c98234;;;","08/Dec/20 13:28;lirui;[~jark] Thanks for reviewing and merging! I have opened PR for 1.12 here:
https://github.com/apache/flink/pull/14339;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in IteratorSourceReader,FLINK-20418,13343267,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,roman,roman,30/Nov/20 09:11,22/Jun/21 14:05,13/Jul/23 08:12,01/Dec/20 19:36,1.12.0,,,,,1.11.3,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,,"With the following job
{code}
	@Test
	public void testNpe() throws Exception {
		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		env.setParallelism(1);
		env.setRestartStrategy(new NoRestartStrategyConfiguration());
		env.enableCheckpointing(50, CheckpointingMode.EXACTLY_ONCE);
		env
			.fromSequence(0, 100)
			.map(x -> {
				Thread.sleep(10);
				return x;
			})
			.addSink(new DiscardingSink<>());
		env.execute();
	}
{code}

I (always) get  an exception like this:
{code}
...
Caused by: java.lang.Exception: Could not perform checkpoint 1 for operator Source: Sequence Source -> Map -> Sink: Unnamed (1/1)#0.
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:866)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$8(StreamTask.java:831)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator Source: Sequence Source -> Map -> Sink: Unnamed (1/1)#0. Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:226)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:158)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:343)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:924)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:914)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:857)
	... 10 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.snapshotState(IteratorSourceReader.java:132)
	at org.apache.flink.streaming.api.operators.SourceOperator.snapshotState(SourceOperator.java:264)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:197)
	... 20 more
{code}

Adding a null check solves the issue. But if I then change sleep time from 10 to 50 I get
{code}
Caused by: java.lang.IllegalArgumentException: 'from' must be <= 'to'
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:142)
	at org.apache.flink.api.connector.source.lib.NumberSequenceSource$NumberSequenceSplit.<init>(NumberSequenceSource.java:142)
	at org.apache.flink.api.connector.source.lib.NumberSequenceSource$NumberSequenceSplit.getUpdatedSplitForIterator(NumberSequenceSource.java:169)
	at org.apache.flink.api.connector.source.lib.NumberSequenceSource$NumberSequenceSplit.getUpdatedSplitForIterator(NumberSequenceSource.java:135)
	at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.snapshotState(IteratorSourceReader.java:135)
	at org.apache.flink.streaming.api.operators.SourceOperator.snapshotState(SourceOperator.java:264)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:197)
	... 20 more
{code}
",,AHeise,roman,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 19:36:35 UTC 2020,,,,,,,,,,"0|z0l1j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 09:58;ym;with the default setting, I get the same exception:

 
{code:java}
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
 at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996) at akka.dispatch.OnComplete.internal(Future.scala:264) at akka.dispatch.OnComplete.internal(Future.scala:261) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:533) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ... 4 moreCaused by: java.lang.Exception: Could not perform checkpoint 1 for operator Source: Sequence Source -> Map -> Sink: Unnamed (1/1)#0. at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:866) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$8(StreamTask.java:831) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator Source: Sequence Source -> Map -> Sink: Unnamed (1/1)#0. Failure reason: Checkpoint was declined. at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:226) at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:158) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:343) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:924) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:914) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:857) ... 10 moreCaused by: java.lang.NullPointerException at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.snapshotState(IteratorSourceReader.java:132) at org.apache.flink.streaming.api.operators.SourceOperator.snapshotState(SourceOperator.java:264) at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:197) ... 20 more{code}
 ;;;","30/Nov/20 09:59;ym;change the sleep time to 50, get the same error:

 
{code:java}
Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.snapshotState(IteratorSourceReader.java:132) at org.apache.flink.streaming.api.operators.SourceOperator.snapshotState(SourceOperator.java:264) at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:197) ... 20 more{code};;;","01/Dec/20 19:36;arvid;Merged into master as d019ce782164ddf317dd497770e48facf5ccbe2c
Merged into 1.12 as 6f4bc8c957.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
【HBase】FileNotFoundException: File /tmp/hbase-deploy/hbase/lib does not exist,FLINK-20414,13343206,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhisheng,zhisheng,30/Nov/20 02:59,27/Oct/21 10:47,13/Jul/23 08:12,30/Nov/20 03:25,1.12.0,,,,,1.12.0,,,Connectors / HBase,Table SQL / Ecosystem,,,,0,,,,,,"{code:java}
CREATE TABLE yarn_log_datagen_test_hbase_sink (
 appid INT,
 message STRING
) WITH (
 'connector' = 'datagen',
 'rows-per-second'='10',
 'fields.appid.kind'='random',
 'fields.appid.min'='1',
 'fields.appid.max'='1000',
 'fields.message.length'='100'
);

CREATE TABLE hbase_test1 (
 rowkey INT,
 family1 ROW<message STRING>
) WITH (
 'connector' = 'hbase-1.4',
 'table-name' = 'test_flink',
 'zookeeper.quorum' = 'xxx:2181',
 'sink.parallelism' = '2',
 'sink.buffer-flush.interval' = '1',
 'sink.buffer-flush.max-rows' = '1',
 'sink.buffer-flush.max-size' = '1'
);

INSERT INTO hbase_test1 SELECT appid, ROW(message) FROM yarn_log_datagen_test_hbase_sink;

{code}
I run the sql, has exception, and data is not write into hbase, i add the  flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar  in the lib folder

 
{code:java}
2020-11-30 10:49:45,772 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Finding class again: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException2020-11-30 10:49:45,774 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Class org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException not found - using dynamical class loader2020-11-30 10:49:45,774 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Finding class: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException2020-11-30 10:49:45,774 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Loading new jar files, if any2020-11-30 10:49:45,776 WARN  org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Failed to check remote dir status /tmp/hbase-deploy/hbase/libjava.io.FileNotFoundException: File /tmp/hbase-deploy/hbase/lib does not exist.    at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:795) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem.access$700(DistributedFileSystem.java:106) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:853) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:849) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.7.3.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:860) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader.loadNewJars(DynamicClassLoader.java:206) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:168) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:140) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_92]    at java.lang.Class.forName(Class.java:348) [?:1.8.0_92]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.protobuf.ProtobufUtil.toException(ProtobufUtil.java:1753) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.protobuf.ResponseConverter.getResults(ResponseConverter.java:157) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:180) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:53) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:806) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_92]    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_92]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_92]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_92]    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_92]2020-11-30 10:49:45,776 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Finding class again: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException2020-11-30 10:49:47,815 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Class org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException not found - using dynamical class loader2020-11-30 10:49:47,815 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Finding class: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException2020-11-30 10:49:47,815 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Loading new jar files, if any2020-11-30 10:49:47,817 WARN  org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Failed to check remote dir status /tmp/hbase-deploy/hbase/libjava.io.FileNotFoundException: File /tmp/hbase-deploy/hbase/lib does not exist.    at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:795) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem.access$700(DistributedFileSystem.java:106) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:853) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:849) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.7.3.jar:?]    at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:860) ~[hadoop-hdfs-2.7.3.4.jar:?]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader.loadNewJars(DynamicClassLoader.java:206) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader.tryRefreshClass(DynamicClassLoader.java:168) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader.loadClass(DynamicClassLoader.java:140) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_92]    at java.lang.Class.forName(Class.java:348) [?:1.8.0_92]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.protobuf.ProtobufUtil.toException(ProtobufUtil.java:1753) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.protobuf.ResponseConverter.getResults(ResponseConverter.java:157) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:180) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:53) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:806) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:1009) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1342) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1296) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1500(AsyncProcess.java:667) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:814) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.sendMultiAction(AsyncProcess.java:1102) [flink-sql-connector-hbase-1.4_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code}
 ",,Echo Lee,jark,zhisheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 27 10:47:05 UTC 2021,,,,,,,,,,"0|z0l15k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 03:51;jark;Did you find the reason? [~zhisheng];;;","02/Dec/20 05:03;zhisheng;[~jark] yes, i solved it, but the exception is not friendly, if the log level='INFO', I can't find the DEBUG log
{code:java}
2020-11-30 10:49:45,772 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Finding class again: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException2020-11-30 10:49:45,774 DEBUG org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.util.DynamicClassLoader [] - Class org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException not found - using dynamical class loader2020-11-30 10:49:45,774 {code};;;","27/Oct/21 10:47;Echo Lee;Hi [~zhisheng] [~jark] I also encountered this problem, is because the sql hbase connector is shaded. I think this is a bug,  why close it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sources should add splits back in ""resetSubtask()"", rather than in ""subtaskFailed()"".",FLINK-20413,13343193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,29/Nov/20 21:53,12/Dec/20 11:58,13/Jul/23 08:12,30/Nov/20 17:48,,,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,,,,,,"Because ""subtaskFailed()"" has no strong order guarantees with checkpoint completion, we need to return failed splits in ""resetSubtask()"" instead.

See FLINK-20396 for a detailed explanation of the race condition.",,sewen,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 17:48:23 UTC 2020,,,,,,,,,,"0|z0l12o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 17:48;sewen;Fixed in
  - 1.12.0 (release-1.12) via 9cda31ae399883a595e97b39f7ea20d28526e085
  - 1.13.0 (master) via edc198f4ef89e062ec4c27dcd6b85dfad649b78a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeper quorum fails to start due to missing log4j library,FLINK-20404,13343015,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,PedroMrChaves,PedroMrChaves,27/Nov/20 15:14,07/Dec/20 20:15,13/Jul/23 08:12,07/Dec/20 20:15,1.11.2,,,,,1.11.3,1.12.1,1.13.0,Build System,,,,,0,pull-request-available,,,,,"Upon starting a zookeeper quorum using flink's bootstrapped zookeeper, it throws the following exception.

 
{code:java}
2020-11-27 13:13:38,371 ERROR org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer  [] - Error running ZooKeeper quorum peer: org/apache/log4j/jmx/HierarchyDynamicMBean
java.lang.NoClassDefFoundError: org/apache/log4j/jmx/HierarchyDynamicMBean
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.jmx.ManagedUtil.registerLog4jMBeans(ManagedUtil.java:51) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-11.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:125) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-11.0]
        at org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer.runFlinkZkQuorumPeer(FlinkZooKeeperQuorumPeer.java:123) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer.main(FlinkZooKeeperQuorumPeer.java:79) [flink-dist_2.11-1.11.2.jar:1.11.2]
Caused by: java.lang.ClassNotFoundException: org.apache.log4j.jmx.HierarchyDynamicMBean
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_262]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_262]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_262]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_262]
        ... 4 more
{code}
This happens because the new flink version is missing a log4j library. This can be solved by adding log4j-1.2.17.jar to the classpath, nonetheless the bootstrapped zookeepeer version should be compatible with the log4j2 libraries that come with flink's default installation.

 

*Steps to reproduce:*
 # Fresh install of flink version 1.11.2 
 # Change the zookeeper config to start as a quorum
{code:java}
server.1=<hostname>:2888:3888
server.2=<hostname>:2888:3888{code}

 # Start zookeeper
 # <installation_path>/bin/zookeeper.sh start-foreground 1",,PedroMrChaves,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 20:15:43 UTC 2020,,,,,,,,,,"0|z0kzzc:",9223372036854775807,"The Zookeeper scripts in the Flink distribution have been modified to disable the Log4j JMX integration due to an incompatibility between Zookeeper 3.4 and Log4j 2.
To re-enable this feature, remove the line in the ""zookeeper.sh"" file that sets ""zookeeper.jmx.log4j.disable"".",,,,,,,,,,,,,,,,,,,"30/Nov/20 00:20;chesnay;This is due to ZK 3.4 having a hard dependency on log4j 1, which we cannot fix. You can try using ZK 3.5 (presumably ZK was decoupled from log4j1 in ZOOKEEPER-1371), or use Flink with log4j1 as described in the [documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/logging.html#configuring-log4j1].;;;","03/Dec/20 13:42;sewen;[~chesnay] Looks like Flink ships by default ZK 3.4, so the built-in ZK scripts are no effectively not usable with the default build?;;;","03/Dec/20 13:44;sewen;Does that mean we should either
  - remove the utility scripts to start the ZK quorum (not great for the starting experience)
  - Upgrade Flink to use ZK 3.5 by default?
;;;","03/Dec/20 13:56;chesnay;They do work, just not if you use certain log4j features, in this case I suppose some zookeeper JMX integration.
We have e2e tests that ensure Flink and the bundled ZK work.;;;","03/Dec/20 13:59;chesnay;oh, let me clarify. As described in the issue description, if you configure more than 1 quorum peer then it does not work.
If you have only 1 peer, then it does work.;;;","03/Dec/20 14:15;chesnay;We could disable the log4j jmx integration by passing {{-Dzookeeper.jmx.log4j.disable=true}} to the JVM.;;;","03/Dec/20 14:34;sewen;Thanks for clarifying. 

I guess updating to ZK 3.5 is not yet an option, to be able to keep compatibility with ZK 3.4 ensembles?

The suggestion about excluding the log4j jmx integration sounds like a nice fix.;;;","03/Dec/20 14:39;chesnay;I will prepare a PR with the fix. I wouldn't want to switch the default ZK version so soon before a release.;;;","07/Dec/20 15:50;chesnay;FYI, due to the zoo.cfg only having a single clientPort setting it isn't easily possible to start multiple peers anyway, as they all try to bind to the same port.;;;","07/Dec/20 20:15;chesnay;master: 09988136f994e48c16e75cc71bd7779e39fe50fe
1.12: a1b48aecee6b937005d53da5afec6475ea6586e4 
1.11: 741b8e2b9536c0160b43b10b922d11e2af7f588c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""OperatorCoordinator.resetSubtask()"" to fix order problems of ""subtaskFailed()""",FLINK-20396,13343003,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,27/Nov/20 14:14,12/Dec/20 11:58,13/Jul/23 08:12,30/Nov/20 11:58,1.11.2,,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"There are no strong order guarantees between {{OperatorCoordinator.subtaskFailed()}} and {{OperatorCoordinator.notifyCheckpointComplete()}}.

It can happen that a checkpoint completes after the notification for task failure is sent:
  - {{OperatorCoordinator.checkpoint()}}
  - {{OperatorCoordinator.subtaskFailed()}}
  - {{OperatorCoordinator.checkpointComplete()}}

The subtask failure here does not know whether the previous checkpoint completed or not. It cannot decide what state the subtask will be in after recovery.
There is no easy fix right now to strictly guarantee the order of the method calls, so alternatively we need to provide the necessary information to reason about the status of tasks.

We should replace {{OperatorCoordinator.subtaskFailed(int subtask)}} with {{OperatorCoordinator.subtaskRestored(int subtask, long checkpoint)}}. That implementations get the explicit checkpoint ID for the subtask recovery, and can align that with the IDs of checkpoints that were taken.

It is still (in rare cases) possible that for a specific checkpoint C, {{OperatorCoordinator.subtaskRestored(subtaskIndex, C))}} comes before {{OperatorCoordinator.checkpointComplete(C)}}.


h3. Background

The Checkpointing Procedure is partially asynchronous on the {{JobManager}} / {{CheckpointCoordinator}}: After all subtasks acknowledged the checkpoint, the finalization (writing out metadata and registering the checkpoint in ZooKeeper) happens in an I/O thread, and the checkpoint completes after that.

This sequence of events can happen:
  - tasks acks checkpoint
  - checkpoint fully acknowledged, finalization starts
  - task fails
  - task failure notification is dispatched
  - checkpoint completes.

For task failures and checkpoint completion, no order is defined.

However, for task restore and checkpoint completion, the order is well defined: When a task is restored, pending checkpoints are either canceled or complete. None can be within finalization. That is currently guaranteed with a lock in the {{CheckpointCoordinator}}.
(An implication of that being that restores can be blocking operations in the scheduler, which is not ideal from the perspective of making the scheduler async/non-blocking, but it is currently essential for correctness).
",,dian.fu,kezhuw,roman,sewen,stevenz3wu,Thesharing,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20413,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 11:58:57 UTC 2020,,,,,,,,,,"0|z0kzwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/20 16:43;sewen;In the first version, I would add the {{subtaskReset()}} method in addition to the {{subtaskFailed()}} method because of two reasons:
  - {{subtaskFailed()}} can potentially do slightly faster cleanup (for example unregistering readers)
  - It is complex to communicate a failure cause to {{subtaskReset()}}. If we want to make actions dependent on exception types, we need to handle that in the {{subtaskFailed()}} method.

We can decide to remove {{subtaskFailed()}} in the future, if we feel we don't need the slightly faster notification, or the failure reason.

If we want to retain the failure reason, but do not care about the slightly faster notification, we can consolidate the two into a single method {{subtaskReset(int subtask, long checkpointId, Throwable failureCause);}}.
The {{OperatorCoordinatorHolder}} can remember the exceptions per subtask between failure and restore to pass them to the coordinator.
Because that needs potentially noticeably more heap memory (retain many exceptions with stack traces) I would suggest to do that change ""if needed"" and not immediately.;;;","30/Nov/20 11:58;sewen;Fixed in
  - 1.12.0 (release-1.12) via bc870f66c9baf8ed0f36d75c27222a4a929428a0
  - 1.13.0 (master) via 0ea11b854b3eff58e21983c95d06c425f3982ec1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase failure caused by NullPointerException,FLINK-20389,13342937,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,mapohl,mapohl,27/Nov/20 07:42,22/Jun/21 14:05,13/Jul/23 08:12,28/Dec/20 07:06,1.12.0,1.13.0,,,,1.12.1,1.13.0,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"[Build|https://dev.azure.com/mapohl/flink/_build/results?buildId=118&view=results] failed due to {{UnalignedCheckpointITCase}} caused by a {{NullPointerException}}:
{code:java}
Test execute[Parallel cogroup, p = 10](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase) failed with:
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
	at akka.dispatch.OnComplete.internal(Future.scala:264)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:533)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase$LongSource$LongSplit.access$900(UnalignedCheckpointTestBase.java:263)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase$LongSource$LongSourceReader.notifyCheckpointComplete(UnalignedCheckpointTestBase.java:227)
	at org.apache.flink.streaming.api.operators.SourceOperator.notifyCheckpointComplete(SourceOperator.java:282)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:99)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:283)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:990)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$11(StreamTask.java:961)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:977)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
{code}",,AHeise,cxiiiiiii,dian.fu,hxbks2ks,kezhuw,mapohl,rmetzger,stevenz3wu,trohrmann,zhisheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20781,,,,,"30/Nov/20 09:41;mapohl;FLINK-20389-failure.log;https://issues.apache.org/jira/secure/attachment/13016203/FLINK-20389-failure.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 28 07:06:51 UTC 2020,,,,,,,,,,"0|z0kzi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/20 07:45;dian.fu;cc [~AHeise];;;","27/Nov/20 09:03;arvid;Purely a test issue, so downgrading. Most likely introduced with FLINK-20309.;;;","27/Nov/20 10:07;arvid;First failure is related to 

{noformat}
java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_275]
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:227) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_275]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_275]
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:994) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:263) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.OnComplete.internal(Future.scala:261) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[scala-library-2.11.12.jar:?]
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) ~[scala-library-2.11.12.jar:?]
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) ~[scala-library-2.11.12.jar:?]
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601) ~[scala-library-2.11.12.jar:?]
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109) ~[scala-library-2.11.12.jar:?]
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599) ~[scala-library-2.11.12.jar:?]
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_275]
Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_275]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_275]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_275]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_275]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_275]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_275]
	... 1 more
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_63#895545685]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	... 9 more
{noformat}

[~mapohl] is that related to your change?;;;","27/Nov/20 19:32;arvid;Improved UCITCase to not fail with NPE on unexpected exeception, for the actual exception, [~mapohl] is investigating.

Merged improvement into master as f5889bdc0f63ae97859743dc73f5c675dfa7bd2f and into 1.12 as 4744fb9de0.;;;","30/Nov/20 09:39;mapohl;[~trohrmann@apache.org] and I investigated the issue. There's a gap of around 10 seconds in the logs that correlate with Arvid's findings:
{noformat}
17:45:05,847 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task failing-map (7/10)#0 (36c7924edd6e9e16b114d5a350b55d7a), deploy into slot with allocation id 4882d7edecaca6bb93fd8de02e7d64c7.
17:45:14,550 [failing-map (6/10)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - failing-map (6/10)#0 (7461e165ceec401b721854bb88326dfd) switched from CREATED to DEPLOYING.
{noformat}
Considering that there are not many steps between the two log statements in [TaskExecutor|https://github.com/apache/flink/blob/3f4cdb9eadf9050a01adfd466963c4124ad43daf/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java#L650] and [Task|https://github.com/apache/flink/blob/6bf7d77a76afa1c705c86adf46fb8732841b9dd8/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L969] that could actually block the execution, the assumptions is that the execution just got blocked by the VM being low on CPU resources.

I added the logs for the failing testcase to this ticket.;;;","30/Nov/20 09:40;mapohl;The issue is considered to be closed. The findings are documented in the comments above. Thanks for looking into it, [~AHeise].;;;","11/Dec/20 07:04;rmetzger;Reopen or new ticket? 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10779&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185&l=4382

{code}
Caused by: java.lang.NullPointerException
	at org.apache.flink.streaming.api.operators.SourceOperator.notifyCheckpointAborted(SourceOperator.java:299)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:311)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:968)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:977)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
{code};;;","12/Dec/20 02:20;hxbks2ks;Similar Case

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10816&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185];;;","12/Dec/20 02:30;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10818&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc]

 ;;;","12/Dec/20 02:34;hxbks2ks;I'm reopening it due to the failure frequency.;;;","13/Dec/20 08:35;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10824&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","14/Dec/20 02:22;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10833&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185;;;","14/Dec/20 18:24;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10855&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","15/Dec/20 02:45;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10872&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","16/Dec/20 01:55;dian.fu;Hi [~mapohl] It seems that this issue occurs quite frequently, could you help to take a further look at of this issue?;;;","16/Dec/20 03:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10907&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","16/Dec/20 17:59;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10931&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

[~AHeise] could you take a look while [~mapohl] is out of office?;;;","17/Dec/20 01:58;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10944&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","17/Dec/20 03:04;zhuzh;another instance:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10934&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","18/Dec/20 01:42;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10963&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","18/Dec/20 02:22;cxiiiiiii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10993&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","18/Dec/20 03:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10990&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185;;;","18/Dec/20 09:35;mapohl;I'm gonna take a look. Thanks for sharing.;;;","18/Dec/20 10:11;arvid;The last couple of reported issues are caused by an NPE in the SourceOperator itself. [~jqin] and [~sewen], could you take a look?

{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.flink.streaming.api.operators.SourceOperator.notifyCheckpointAborted(SourceOperator.java:299)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:311)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$12(StreamTask.java:968)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$13(StreamTask.java:977)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:91)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:412)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:585)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.afterInvoke(SourceOperatorStreamTask.java:128)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:547)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
;;;","19/Dec/20 01:54;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11028&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","19/Dec/20 13:10;kezhuw;Another case about {{NullPointerException}} from {{SourceOperator.sourceReader}}:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11024&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4050]

Seems that {{SourceOperator.notifyCheckpointAborted}} was called after closed. I see two potential paths:
 * Inside, {{StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator}}, checkpoint abortion mail could interleave between ""step 2"" and ""step 3"".
 * Inside, {{StreamTask.afterFork}}, check point abortion mail could interleave between {{operatorChain.closeOperators(actionExecutor)}} and {{mailboxProcessor.prepareClose()}}, then it will be invoked in {{mailboxProcessor.drain()}}.

 If {{SourceOperator.notifyCheckpointAborted}} is indeed possible be invoked after operator closed, it would be better to document this behavior, though I think it shouldn't.;;;","21/Dec/20 02:35;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11083&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a]

 ;;;","21/Dec/20 09:08;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11090&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","28/Dec/20 07:06;mapohl;I close this issue again after opening FLINK-20781 as it the underlying issue seems to be a different one as [Arvid pointed out|https://issues.apache.org/jira/browse/FLINK-20389?focusedCommentId=17251651&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17251651].;;;",,,,,,,,,,,,,,,,,,,,,,,,
Broken Link in deployment/ha/kubernetes_ha.zh.md,FLINK-20384,13342888,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,27/Nov/20 01:52,28/Nov/20 06:31,13/Jul/23 08:12,28/Nov/20 06:31,1.12.0,1.13.0,,,,1.12.0,,,Deployment / Kubernetes,Documentation,,,,0,pull-request-available,,,,,"When executing the script build_docs.sh, it will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'deployment/resource-providers/standalone/kubernetes.md' in tag 'link'. Make sure the document exists and the path is correct. in deployment/ha/kubernetes_ha.zh.md Could not find document 'deployment/resource-providers/standalone/kubernetes.md' in tag 'link'.
{code}",,dian.fu,hxbks2ks,klion26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 28 06:31:15 UTC 2020,,,,,,,,,,"0|z0kz74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/20 06:31;klion26;merged into master d5b15652fc85fe4b0929e7faf274b46c04b7e924
1.12 2e8d9b9489a1ea33cecdcfc4d84912d5c68c1bf0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSet allround end-to-end test fails with NullPointerException,FLINK-20383,13342878,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,rmetzger,rmetzger,26/Nov/20 20:35,22/Jun/21 13:55,13/Jul/23 08:12,04/May/21 15:56,1.11.3,1.12.0,1.13.0,,,1.11.4,1.12.0,,API / DataSet,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10204&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{code}
2020-11-26T19:45:51.5478354Z Nov 26 19:45:51 Running 'DataSet allround end-to-end test'
2020-11-26T19:45:51.5478763Z Nov 26 19:45:51 ==============================================================================
2020-11-26T19:45:51.5490922Z Nov 26 19:45:51 TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-51548553152
2020-11-26T19:45:51.7135919Z Nov 26 19:45:51 Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-11-26T19:45:51.7232126Z Nov 26 19:45:51 Run DataSet-Allround-Test Program
2020-11-26T19:45:51.7266761Z Nov 26 19:45:51 Setting up SSL with: internal JDK dynamic
2020-11-26T19:45:51.7300820Z Nov 26 19:45:51 Using SAN dns:fv-az679-746.omrnvntobknexcvwb1hiviload.fx.internal.cloudapp.net,ip:10.1.0.4,ip:172.17.0.1,ip:172.18.0.1
2020-11-26T19:45:52.8564809Z Certificate was added to keystore
2020-11-26T19:45:54.9192018Z Certificate was added to keystore
2020-11-26T19:45:55.5263075Z Certificate reply was installed in keystore
2020-11-26T19:45:55.7151814Z Nov 26 19:45:55 Setting up SSL with: rest JDK dynamic
2020-11-26T19:45:55.7186828Z Nov 26 19:45:55 Using SAN dns:fv-az679-746.omrnvntobknexcvwb1hiviload.fx.internal.cloudapp.net,ip:10.1.0.4,ip:172.17.0.1,ip:172.18.0.1
2020-11-26T19:45:56.8610453Z Certificate was added to keystore
2020-11-26T19:45:59.2956135Z Certificate was added to keystore
2020-11-26T19:45:59.9115744Z Certificate reply was installed in keystore
2020-11-26T19:46:00.0800993Z Nov 26 19:46:00 Mutual ssl auth: false
2020-11-26T19:46:00.1688426Z Nov 26 19:46:00 Starting cluster.
2020-11-26T19:46:00.8775396Z Nov 26 19:46:00 Starting standalonesession daemon on host fv-az679-746.
2020-11-26T19:46:02.4016699Z Nov 26 19:46:02 Starting taskexecutor daemon on host fv-az679-746.
2020-11-26T19:46:02.4595285Z Nov 26 19:46:02 Waiting for Dispatcher REST endpoint to come up...
2020-11-26T19:46:03.5161706Z Nov 26 19:46:03 Waiting for Dispatcher REST endpoint to come up...
2020-11-26T19:46:04.6157725Z Nov 26 19:46:04 Waiting for Dispatcher REST endpoint to come up...
2020-11-26T19:46:05.7153907Z Nov 26 19:46:05 Waiting for Dispatcher REST endpoint to come up...
2020-11-26T19:46:07.4507631Z Nov 26 19:46:07 Waiting for Dispatcher REST endpoint to come up...
2020-11-26T19:46:08.6436701Z Nov 26 19:46:08 Dispatcher REST endpoint is up.
2020-11-26T19:46:08.6437477Z Nov 26 19:46:08 Start 3 more task managers
2020-11-26T19:46:09.4234133Z Nov 26 19:46:09 [INFO] 1 instance(s) of taskexecutor are already running on fv-az679-746.
2020-11-26T19:46:09.4242548Z Nov 26 19:46:09 Starting taskexecutor daemon on host fv-az679-746.
2020-11-26T19:46:11.0906754Z Nov 26 19:46:11 [INFO] 2 instance(s) of taskexecutor are already running on fv-az679-746.
2020-11-26T19:46:11.0920876Z Nov 26 19:46:11 Starting taskexecutor daemon on host fv-az679-746.
2020-11-26T19:46:14.2637898Z Nov 26 19:46:14 [INFO] 3 instance(s) of taskexecutor are already running on fv-az679-746.
2020-11-26T19:46:14.2688442Z Nov 26 19:46:14 Starting taskexecutor daemon on host fv-az679-746.
2020-11-26T19:46:25.7685789Z Nov 26 19:46:25 Job has been submitted with JobID fa6775cf8a2ed83f2e4ba98930bc2cb2
2020-11-26T19:47:37.4444212Z 
2020-11-26T19:47:37.4448196Z ------------------------------------------------------------
2020-11-26T19:47:37.4448668Z  The program finished with the following exception:
2020-11-26T19:47:37.4448911Z 
2020-11-26T19:47:37.4453074Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: fa6775cf8a2ed83f2e4ba98930bc2cb2)
2020-11-26T19:47:37.4458530Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:330)
2020-11-26T19:47:37.4459217Z 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
2020-11-26T19:47:37.4459771Z 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
2020-11-26T19:47:37.4460243Z 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:743)
2020-11-26T19:47:37.4460711Z 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:242)
2020-11-26T19:47:37.4461161Z 	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:971)
2020-11-26T19:47:37.4461649Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1047)
2020-11-26T19:47:37.4462191Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
2020-11-26T19:47:37.4462932Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1047)
2020-11-26T19:47:37.4471377Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: fa6775cf8a2ed83f2e4ba98930bc2cb2)
2020-11-26T19:47:37.4489158Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-11-26T19:47:37.4489770Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-26T19:47:37.4490173Z 	at org.apache.flink.client.program.ContextEnvironment.getJobExecutionResult(ContextEnvironment.java:112)
2020-11-26T19:47:37.4490584Z 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:76)
2020-11-26T19:47:37.4490985Z 	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:873)
2020-11-26T19:47:37.4492050Z 	at org.apache.flink.batch.tests.DataSetAllroundTestProgram.main(DataSetAllroundTestProgram.java:181)
2020-11-26T19:47:37.4492407Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-26T19:47:37.4492964Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-26T19:47:37.4493418Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-26T19:47:37.4493799Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-26T19:47:37.4494196Z 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:316)
2020-11-26T19:47:37.4494512Z 	... 8 more
2020-11-26T19:47:37.4494866Z Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: fa6775cf8a2ed83f2e4ba98930bc2cb2)
2020-11-26T19:47:37.4495434Z 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:119)
2020-11-26T19:47:37.4495929Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-26T19:47:37.4496350Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-11-26T19:47:37.4497022Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-26T19:47:37.4497454Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-26T19:47:37.4497924Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$22(RestClusterClient.java:602)
2020-11-26T19:47:37.4498599Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-11-26T19:47:37.4499059Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-11-26T19:47:37.4499496Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-26T19:47:37.4499924Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-26T19:47:37.4500389Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:379)
2020-11-26T19:47:37.4500859Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-11-26T19:47:37.4501395Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-11-26T19:47:37.4501846Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-26T19:47:37.4502258Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-11-26T19:47:37.4502691Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-11-26T19:47:37.4503137Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-11-26T19:47:37.4503557Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-11-26T19:47:37.4503990Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-11-26T19:47:37.4504346Z 	at java.lang.Thread.run(Thread.java:748)
2020-11-26T19:47:37.4504693Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-26T19:47:37.4505141Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-11-26T19:47:37.4505653Z 	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:117)
2020-11-26T19:47:37.4506010Z 	... 19 more
2020-11-26T19:47:37.4506333Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-11-26T19:47:37.4506873Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-26T19:47:37.4507477Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-26T19:47:37.4508045Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-11-26T19:47:37.4508543Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-11-26T19:47:37.4509573Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-11-26T19:47:37.4510110Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:533)
2020-11-26T19:47:37.4510595Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
2020-11-26T19:47:37.4511054Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
2020-11-26T19:47:37.4511451Z 	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
2020-11-26T19:47:37.4511841Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-26T19:47:37.4512223Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-26T19:47:37.4512625Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-26T19:47:37.4513178Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-26T19:47:37.4513651Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-26T19:47:37.4514126Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-26T19:47:37.4514535Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-26T19:47:37.4514891Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-26T19:47:37.4515271Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-26T19:47:37.4515660Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-26T19:47:37.4516037Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-26T19:47:37.4516427Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-26T19:47:37.4516811Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-26T19:47:37.4517210Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-26T19:47:37.4517577Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-26T19:47:37.4520483Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-26T19:47:37.4520859Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-26T19:47:37.4521208Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-26T19:47:37.4521543Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-26T19:47:37.4524388Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-26T19:47:37.4525133Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-26T19:47:37.4525599Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-26T19:47:37.4526099Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-26T19:47:37.4526629Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-26T19:47:37.4527162Z Caused by: java.lang.NullPointerException
2020-11-26T19:47:37.4527553Z 	at org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.notifyDataAvailable(PipelinedSubpartition.java:500)
2020-11-26T19:47:37.4528065Z 	at org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.add(PipelinedSubpartition.java:163)
2020-11-26T19:47:37.4528524Z 	at org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.finish(PipelinedSubpartition.java:133)
2020-11-26T19:47:37.4529000Z 	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.finish(BufferWritingResultPartition.java:215)
2020-11-26T19:47:37.4529403Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:737)
2020-11-26T19:47:37.4529908Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-11-26T19:47:37.4530233Z 	at java.lang.Thread.run(Thread.java:748)
2020-11-26T19:47:37.4656278Z Nov 26 19:47:37 [FAIL] Test script contains errors.
{code}",,AHeise,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 04 15:56:18 UTC 2021,,,,,,,,,,"0|z0kz4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/20 08:34;arvid;Merged as fd23b35d6e2147c28ec6b7cee7c7dd962c82d42b into master.

Merged as 33c19e1703 into release-1.12.;;;","03/May/21 09:26;pnowojski;Missing backport to 1.11.x (FIINK-22332).;;;","04/May/21 15:56;pnowojski;back ported as 3fc8a2e into apache:release-1.11 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception thrown from JobMaster.startScheduling() may be ignored.,FLINK-20382,13342872,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,becket_qin,becket_qin,26/Nov/20 19:19,03/Dec/20 17:28,13/Jul/23 08:12,30/Nov/20 09:31,1.11.2,,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Currently {{JobMaster.resetAndStartScheduler()}} invokes {{startScheduling()}} in a {{thenRun}} clause without {{exceptionally}} or {{handle}} to handle exceptions. The job may hang if an exception is thrown when starting scheduling, e.g. failed to create operator coordinators. ",,becket_qin,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 17:28:18 UTC 2020,,,,,,,,,,"0|z0kz3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/20 08:47;trohrmann;Thanks for reporting this issue [~becket_qin]. This is indeed a good finding. I will fix it.;;;","27/Nov/20 12:02;trohrmann;The solution will be to fail hard because currently it is not expected that exceptions can be thrown from the {{SchedulerNG.startScheduling}} method.;;;","30/Nov/20 09:31;trohrmann;Fixed via

master: f8e56226550f8fa1620b4650d83edc1993e210b8
1.12.0: c5188a1b36953c3cfb889c5866c4835cc3b5dcb5
1.11.3: dfa13d8b75ead9ce60ae982b469f70943ad69d35;;;","30/Nov/20 13:13;rmetzger;Thanks for the quick fix! I agree that failing hard is a good short-term fix.

But from a user's perspective, I would not expect my JobManager to exit if I submit a custom source implementation that throws an exception. [I guess failing the job with the exception|https://github.com/apache/flink/pull/14251#discussion_r531639026] thrown by the operator coordinator would be the expected behavior.
[~sewen] Can you take a look at his?;;;","03/Dec/20 14:05;sewen;For the suggestion from [~rmetzger], a few questions:

As far as I can tell, the ""startScheduling()"" method of the scheduler is called only once. So if the exception there is caught and triggers a global failure, then we will never try to start the coordinator again. So that sounds like it is not a feasible solution.
[~trohrmann] Could you confirm that?

I would say, let keep the ""fatal error is start()"" behavior for now, because the sources should (in the future) no longer do anything in ""start()"" that fails.;;;","03/Dec/20 17:28;trohrmann;Yes, the code is currently not written with the assumption that {{startScheduling}} can fail and, hence, is lacking a retry. If we want to tolerate {{startScheduling}} failures, then we need to change this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect use of yarn-session.sh -id vs -yid in log statements.,FLINK-20381,13342842,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,26/Nov/20 13:49,01/Dec/20 07:44,13/Jul/23 08:12,01/Dec/20 07:44,1.12.0,,,,,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,,,,,"The Yarn per-job modes log about the recommended shutdown of yarn, which doesn't work.


See: https://github.com/apache/flink/pull/10964#issuecomment-734166399",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 07:44:44 UTC 2020,,,,,,,,,,"0|z0kyww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/20 07:44;rmetzger;Resolved in master (1.13): https://github.com/apache/flink/commit/f7765d4beb809f39ccf7b61936558437d470e514
resolved in release-1.12: https://github.com/apache/flink/commit/dedf39f4f89177809be43a4af1951f0c4d92c00c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update KafkaRecordDeserializationSchema to enable reuse of DeserializationSchema and KafkaDeserializationSchema,FLINK-20379,13342832,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sewen,sewen,26/Nov/20 12:51,12/Jan/22 18:51,13/Jul/23 08:12,12/Jan/22 18:51,1.12.0,,,,,1.13.0,,,Connectors / Kafka,,,,,0,auto-deprioritized-critical,auto-unassigned,pull-request-available,,,"The new Kafka Connector defines its own deserialization schema and is incompatible with the existing library of deserializers.

That means that users cannot use all of Flink's Formats (Avro, JSON, Csv, Protobuf, Confluent Schema Registry, ...) with the new Kafka Connector.

I think we should change the new Kafka Connector to use the existing Deserialization classes, so all formats can be used, and users can reuse their deserializer implementations.

It would also be good to use the existing KafkaDeserializationSchema. Otherwise all users need to migrate their sources again.
",,aljoscha,andrew_lin,becket_qin,dian.fu,dwysakowicz,gengmao,guoyangze,lzljs3620320,martijnvisser,renqs,rmetzger,sewen,stevenz3wu,trohrmann,zhisheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 12 18:51:10 UTC 2022,,,,,,,,,,"0|z0kyuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/21 06:54;dwysakowicz;Hey [~jqin]. Has this issue been fixed?;;;","07/Apr/21 07:31;becket_qin;[~dwysakowicz] The issue has been fixed in the master branch as well as 1.13. I left the ticket open because it has not been backported to 1.12 yet. Since it is an API change to release 1.12, we will need a public discussion before doing that. There are a few other tickets that needs to be backported as well. I'll send a discussion email to the mailing list.;;;","07/Apr/21 07:38;dwysakowicz;Thank you! Good to now! Could we post the commit id for the master branch here?;;;","07/Apr/21 07:43;becket_qin;Sure. The commit in master branch is following. It is the same as in release-1.13.
2339616518269a4d94553362a1e9ffeb02557609;;;","07/Apr/21 08:30;dwysakowicz;Thanks, btw. just letting you know that I created the 1.13 branch accidentally when creating the rc0. It is not a proper 1.13 branch yet. There is no need to merge it there yet. We will announce the actual cut off in the ML. ;;;","19/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","29/Apr/21 07:15;trohrmann;[~becket_qin] do we plan to backport this fix to 1.12.4 as well? Is this the reason why the PR is still open? Shall we do it because otherwise we will simply forget it.;;;","19/May/21 22:54;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","27/May/21 23:05;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Jun/21 23:23;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","01/Jul/21 08:55;lzljs3620320;It seems that we need to backport this fix to 1.12.6 as well according to [https://lists.apache.org/thread.html/r8184e5ecee508c8d94bf4102c42c7ecc8a6229aea74b40c3b10635fc%40%3Cdev.flink.apache.org%3E];;;","12/Jan/22 18:51;martijnvisser;This ticket was still open but the PR was merged. I've adjusted the fixVersion to 1.13.0.

Commit: d408241eebc5cc9743823598860ede71026e9306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong result when shuffling changelog stream on non-primary-key columns,FLINK-20374,13342795,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,jark,jark,26/Nov/20 09:19,16/May/22 09:47,13/Jul/23 08:12,30/Sep/21 07:15,,,,,,1.13.3,1.14.0,,Table SQL / Runtime,,,,,2,auto-deprioritized-critical,,,,,"This is reported from user-zh ML: http://apache-flink.147419.n8.nabble.com/flink-1-11-2-cdc-cdc-sql-sink-save-point-job-sink-td8593.html

{code:sql}
CREATE TABLE test (
`id` INT,
`name` VARCHAR(255),
`time` TIMESTAMP(3),
`status` INT,
PRIMARY KEY(id) NOT ENFORCED
) WITH (
  'connector' = 'mysql-cdc',
  'hostname' = 'localhost',
  'port' = '3306',
  'username' = 'root',
  'password' = '1',
  'database-name' = 'ai_audio_lyric_task',
  'table-name' = 'test'
)

CREATE TABLE status (
`id` INT,
`name` VARCHAR(255),
PRIMARY KEY(id) NOT ENFORCED
) WITH (  
  'connector' = 'mysql-cdc',
  'hostname' = 'localhost',
  'port' = '3306',
  'username' = 'root',
  'password' = '1',
  'database-name' = 'ai_audio_lyric_task',
  'table-name' = 'status'
);

-- output
CREATE TABLE test_status (
`id` INT,
`name` VARCHAR(255),
`time` TIMESTAMP(3),
`status` INT,
`status_name` VARCHAR(255)
PRIMARY KEY(id) NOT ENFORCED
) WITH (
  'connector' = 'elasticsearch-7',
  'hosts' = 'xxx',
  'index' = 'xxx',
  'username' = 'xxx',
  'password' = 'xxx',
  'sink.bulk-flush.backoff.max-retries' = '100000',
  'sink.bulk-flush.backoff.strategy' = 'CONSTANT',
  'sink.bulk-flush.max-actions' = '5000',
  'sink.bulk-flush.max-size' = '10mb',
  'sink.bulk-flush.interval' = '1s'
);

INSERT into test_status
SELECT t.*, s.name
FROM test AS t
LEFT JOIN status AS s ON t.status = s.id;
{code}

Data in mysql table:

{code}
test:
0, name0, 2020-07-06 00:00:00 , 0
1, name1, 2020-07-06 00:00:00 , 1
2, name2, 2020-07-06 00:00:00 , 1
.....

status
0, status0
1, status1
2, status2
.....
{code}

Operations: 
1. start job with paralleslim=40, result in test_status sink is correct: 

{code}
    0, name0, 2020-07-06 00:00:00 , 0, status0
    1, name1, 2020-07-06 00:00:00 , 1, status1
    2, name2, 2020-07-06 00:00:00 , 1, status1
{code}

2. Update {{status}} of {{id=2}} record in table {{test}} from {{1}} to {{2}}.
3. Result is not correct because the {{id=2}} record is missing in the result. 



The reason is that it shuffles the changelog {{test}} on {{status}} column which is not the primary key. Therefore, the ordering can't be guaranteed, and the result is wrong. 
The {{-U[2, name2, 2020-07-06 00:00:00 , 1]}} and {{+U[2, name2, 2020-07-06 00:00:00 , 2]}} will possible be shuffled to different join task, so the order of joined results  is not guaranteed when they arrive to the sink task. It is possbile  {{+U[2, name2, 2020-07-06 00:00:00 , status2]}} arrives first, and then {{-U[2, name2, 2020-07-06 00:00:00 , status1]}} , then the {{id=2}} record is missing in Elasticsearch. 

It seems that we need a changelog ordering mechanism in the planner. ",,asolimando,dforciea,fsk119,gaoyunhaii,godfreyhe,hackergin,jark,jingzhang,Jocean,joemoe,kyledong,leonard,libenchao,lirui,lvycc,lzljs3620320,nkruber,okowr,qingyue,sansejin,twalthr,txhsj,yunta,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22826,,,,,,,FLINK-23350,FLINK-22899,FLINK-22901,FLINK-23054,,,FLINK-23835,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 16 09:47:04 UTC 2022,,,,,,,,,,"0|z0kymg:",9223372036854775807,The 1.13.3 release aims to fix various primary key issues that effectively made it impossible to use this feature. The change might affect savepoint backwards compatibility for those incorrect pipelines. Also the resulting changelog stream might be different after these changes. Pipelines that were correct before should be restorable from a savepoint. ,,,,,,,,,,,,,,,,,,,"16/Apr/21 10:18;twalthr;There was another user confused by this behavior: https://lists.apache.org/thread.html/r4d929dc0eee80edebb55436843960fb8289c9830903b98c8bc9532ab%40%3Cuser.flink.apache.org%3E
I increased the priority and added a fixed version. I guess this is not easy. I selected 1.14.0.;;;","28/May/21 02:05;sansejin;I also encountered this problem causing my cdc log stream based data to be incorrect;;;","28/May/21 03:45;lzljs3620320;I'll try to solve this problem;;;","28/May/21 07:53;lzljs3620320;This problem involves a wide range and is complicated. I will write a document.;;;","28/May/21 09:14;leonard;Thanks [~lzljs3620320] for taking this ticket,

[~sansejin]  One tips that we should use English in Jira communication which could ensure all people understand the discussion well.;;;","03/Jun/21 02:04;sansejin;flink1.13.1 Even if I set the parallelism to setParallelism(1), and yarn ys to 1 still cannot avoid data first-U after + U, just started normal one day yesterday, this problem occurred today and my program ran for two days.;;;","03/Jun/21 02:41;lzljs3620320;[~sansejin]

Are you sure that the operator parallelism of your Flink web ui is just 1?;;;","03/Jun/21 03:23;sansejin;Very sure, Do a canal-json-based association, Probably my job has not configured any window and state failure parameters, The first day of submission is normal, Because the first submission task consumed only the day 's data did not record yesterday' s state, If my program runs two bars continuously, For example, the first day of the order status is the order status, However, the program passes continuously for the second day, The next day I query upsert-kafka after join data discovery associated data is-U after + U, But the next day of the canal-json original layer the second day of the data for three + U, But somehow after + U -U.;;;","03/Jun/21 04:11;lzljs3620320;[~sansejin] We should discuss this in FLINK-22826, you description make me confused.;;;","08/Jul/21 13:07;twalthr;[~lzljs3620320] can you give a time estimate for this? It seems this is kind of a fundamental problem but we should plan it accordingly. Do you think 1.14 is still realistic?;;;","09/Jul/21 02:05;lzljs3620320;[~twalthr] Sorry, I forgot to close this. This is solved by FLINK-22899  FLINK-22901 FLINK-23054.

I will close this one. CC [~jark] Please help to confirm.;;;","09/Jul/21 08:58;twalthr;Thanks for working on this tricky issue [~lzljs3620320]. Could we start documenting the changelog behavior of the planner in our docs as part of this issue? Users should get more insights what the planner is doing. It is too much of a black box right now. It is very difficult to interpret why and when +U,-U or special operators are inserted into the pipeline.;;;","12/Jul/21 02:48;lzljs3620320;[~twalthr] Good idea, I will create a related Jira for doc.;;;","08/Nov/21 03:03;leonard;[~lzljs3620320] Do you have plan to cp this fix to release-1.12 ?;;;","16/May/22 09:47;lvycc;Hi，I have a new problem，here is my sql：

CREATE TABLE t_order (
order_id INT,
order_name STRING,
product_id INT,
user_id INT,
PRIMARY KEY(order_id) NOT ENFORCED
) WITH (
'connector' = 'mysql-cdc',
'hostname' = 'localhost',
'port' = '3306',
'username' = 'root',
'password' = 'ycc123',
'database-name' = 'wby_test',
'table-name' = 't_order'
);
CREATE TABLE t_logistics (
logistics_id INT,
logistics_target STRING,
logistics_source STRING,
logistics_time TIMESTAMP(0),
order_id INT,
PRIMARY KEY(logistics_id) NOT ENFORCED
) WITH (
'connector' = 'mysql-cdc',
'hostname' = 'localhost',
'port' = '3306',
'username' = 'root',
'password' = 'ycc123',
'database-name' = 'wby_test',
'table-name' = 't_logistics'
);
CREATE TABLE t_join_sink (
order_id INT,
order_name STRING,
logistics_id INT,
logistics_target STRING,
logistics_source STRING,
logistics_time timestamp,
PRIMARY KEY(order_id) NOT ENFORCED
) WITH (
'connector' = 'jdbc',
'url' = 'jdbc:mysql://localhost:3306/wby_test?characterEncoding=utf8&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai',
'table-name' = 't_join_sink',
'username' = 'root',
'password' = 'ycc123'
);
INSERT INTO t_join_sink
SELECT ord.order_id,
ord.order_name,
logistics.logistics_id,
logistics.logistics_target,
logistics.logistics_source,
now()
FROM t_order AS ord
LEFT JOIN t_logistics AS logistics ON ord.order_id=logistics.order_id;

 

The new data cannot be deleted because I used the now() function in the query .

I found it was due to the SinkUpsertMaterializer ,What should I do to properly process the data;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Kafka SQL connector page to mention properties.* options,FLINK-20372,13342786,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,hiscat,hiscat,26/Nov/20 08:45,18/Dec/20 09:35,13/Jul/23 08:12,08/Dec/20 11:23,1.11.2,,,,,1.12.1,1.13.0,,Connectors / Kafka,Documentation,Table SQL / Ecosystem,,,0,pull-request-available,,,,,"I tried to configure kerberos on flink sql kafka connector, but i found that the connector did not provide options.

eg. security.protocol, {{sasl.kerberos.service.name, sasl.mechanism.}}

{{}}",,fsk119,hiscat,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18768,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 11:23:40 UTC 2020,,,,,,,,,,"0|z0kykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 08:51;jark;You can pass throught all the Kafka configurations by prefix with {{properties.}}
For example: {{'properties.sasl.kerberos.service.name' = 'xyz'}};;;","26/Nov/20 08:52;jark;But we should of course update the docs, it is not mentioned in the Kafka connector page.;;;","26/Nov/20 08:52;jark;Do you want to take this? [~fsk119];;;","26/Nov/20 09:42;fsk119;ok. Please assign it to me.;;;","08/Dec/20 11:23;jark;Fixed in 
 - master: ba0dfbb4b136b0372b7f3e26e027483b8fea2036f59f0
 - release-1.12: c7b89af0bfd5165c82d3e69563d7b2c8cba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Result is wrong when sink primary key is not the same with query,FLINK-20370,13342784,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lincoln.86xy,jark,jark,26/Nov/20 08:43,13/Jan/22 07:52,13/Jul/23 08:12,08/Dec/21 09:04,1.12.0,,,,,1.13.6,1.14.3,1.15.0,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Both sources are upsert-kafka which synchronizes the changes from MySQL tables (source_city, source_customer). The sink is another MySQL table which is in upsert mode with ""city_name"" primary key. The join key is ""city_id"". 

In this case, the result will be wrong when updating {{source_city.city_name}} column in MySQL, as the UPDATE_BEFORE is ignored and the old city_name is retained in the sink table. 

{code}
Sink(table=[default_catalog.default_database.sink_kafka_count_city], fields=[city_name, count_customer, sum_gender], changelogMode=[NONE])
+- Calc(select=[city_name, CAST(count_customer) AS count_customer, CAST(sum_gender) AS sum_gender], changelogMode=[I,UA,D])
   +- Join(joinType=[InnerJoin], where=[=(city_id, id)], select=[city_id, count_customer, sum_gender, id, city_name], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey], changelogMode=[I,UA,D])
      :- Exchange(distribution=[hash[city_id]], changelogMode=[I,UA,D])
      :  +- GlobalGroupAggregate(groupBy=[city_id], select=[city_id, COUNT_RETRACT(count1$0) AS count_customer, SUM_RETRACT((sum$1, count$2)) AS sum_gender], changelogMode=[I,UA,D])
      :     +- Exchange(distribution=[hash[city_id]], changelogMode=[I])
      :        +- LocalGroupAggregate(groupBy=[city_id], select=[city_id, COUNT_RETRACT(*) AS count1$0, SUM_RETRACT(gender) AS (sum$1, count$2)], changelogMode=[I])
      :           +- Calc(select=[city_id, gender], changelogMode=[I,UB,UA,D])
      :              +- ChangelogNormalize(key=[customer_id], changelogMode=[I,UB,UA,D])
      :                 +- Exchange(distribution=[hash[customer_id]], changelogMode=[UA,D])
      :                    +- MiniBatchAssigner(interval=[3000ms], mode=[ProcTime], changelogMode=[UA,D])
      :                       +- TableSourceScan(table=[[default_catalog, default_database, source_customer]], fields=[customer_id, city_id, age, gender, update_time], changelogMode=[UA,D])
      +- Exchange(distribution=[hash[id]], changelogMode=[I,UA,D])
         +- ChangelogNormalize(key=[id], changelogMode=[I,UA,D])
            +- Exchange(distribution=[hash[id]], changelogMode=[UA,D])
               +- MiniBatchAssigner(interval=[3000ms], mode=[ProcTime], changelogMode=[UA,D])
                  +- TableSourceScan(table=[[default_catalog, default_database, source_city]], fields=[id, city_name], changelogMode=[UA,D])
{code}

We have suggested users to use the same key of the query as the primary key on sink in the documentation: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#deduplication. We should make this attention to be more highlight in CREATE TABLE page. 
",,Ashulin,caozhen1937,empcl,fsk119,godfreyhe,hackergin,hailong wang,jark,Jocean,libenchao,lincoln.86xy,lzljs3620320,martijnvisser,qinjunjerry,twalthr,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 08 09:07:42 UTC 2021,,,,,,,,,,"0|z0kyk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 08:47;jark;I think this is not a trivial work. And the problems are not only sit in Join, but also other operations (e.g. TopN, Aggregation). 
We should figure out a comprehensive solution for this. For example, do not ignore UPDATE_BEFORE when PK is not equal ? ;;;","26/Nov/20 09:20;fsk119;I am not sure whether this Jira will take the example below into the consideration.

Example:
{code:java}
CREATE TABLE WordCountSource (
  word STRING,
  cnt INT,
  PRIMARY KEY (word) NOT ENFORCED
) WITH (
  ...
)

CREATE TABLE WordCountSink (
  word STRING,
  cnt INT,
  PRIMARY KEY (cnt) NOT ENFORCED
) WITH (
  ...
)

INSERT INTO WordCountSink SELECT * FROM  WordCountSource;
{code}
If we have two records {{('a', 1), ('b', 1)}}, the rowkind of the record {{('b', 1)}} is insert for souce but it is update for sink.;;;","26/Nov/20 09:32;jark;[~fsk119], no, this will not be considered, because it has the same semantic with batch query. 
You can run your above code in MySQL, and you will get the same result. 
However, running the example of JIRA description in MySQL, will get a different result. ;;;","08/Sep/21 14:37;twalthr;[~lzljs3620320] [~jark] What is the current status of this issue? As far as I understand it, it can still occur that inferred unique key != primary key of the sink, right? Why don't we add a check that can be disabled with a flag?;;;","09/Sep/21 02:51;lzljs3620320;Yes, it seems that the only solution is single parallelism.

A upsert sink can accept inputs:
# primary key = unique key, it is OK, nothing needs to do.
# input is append only, sometimes is OK, We can only assume that users can allow some degree of distributed disorder.
# input is change log, primary key != unique key (unique key can be none), the most problematic situation;;;","09/Sep/21 03:40;jark;[~lzljs3620320] could UpsertMaterialize also solve this problem?;;;","09/Sep/21 03:41;jark;[~lzljs3620320] there is 4th case: input is changelog, but there is no unique key. ;;;","09/Sep/21 03:59;lzljs3620320;[~jark] I updated cases.
I think UpsertMaterialize can works. For the third case, we can add UpsertMaterialize, but sometimes it may be redundant.;;;","09/Sep/21 10:23;twalthr;Even in case 2, if users declare a PRIMARY KEY for Kafka the same keys should end up in the same partition. Distributed disorder should only be allowed if the sink does not declare a PRIMARY KEY. We should aim for correctness and add a keyBy for case 2 and enable UpsertMaterialize for case 3. Of course, this can be disabled if necessary.

I mentioned it in a different issue already, but we should also introduce hints that allow those important settings for sources and sinks fine-grained per INSERT INTO. I will open an issue for that if it doesn't exist yet.;;;","09/Sep/21 11:16;lzljs3620320;hints +1;;;","10/Sep/21 14:17;twalthr;I opened FLINK-24254. Any opinion to my other suggestion above for the 3 use cases?;;;","27/Sep/21 05:50;lzljs3620320;4.primary key contains unique key , in this case, we can just convert UPDATE_BEFORE/UPDATE_AFTER to DELETE/INSERT. This is a optimization for UpsertMaterialize.;;;","05/Nov/21 10:00;lincoln.86xy;After discussed with [~jark] and [~lzljs3620320] , we found that `upsertMaterialization` cannot solve this problem.
The root cause is `FlinkChangelogModeInferenceProgram` didn't consider  the case when sink's primary key differs from input's upsert keys when infer required `ChangeLogMode` from sink node.
I'll update the pr later.;;;","05/Nov/21 12:02;wenlong.lwl;hi, [~twalthr], could you explain more about case 2, what should we guarantee semantically in this case? If the input of sink actually has the same    pk with sink, and is insert-only, I think there is no distribution disorder. If the input of sink doesn't have the same pk with sink, and is insert-only, I have no idea what guarantee can be provided by adding a key-by?;;;","05/Nov/21 16:10;twalthr;[~wenlong.lwl]

bq. If the input of sink actually has the same pk with sink, and is insert-only, I think there is no distribution disorder.

That is correct.

But let's assume the following schema:

{code}
CREATE TABLE A (uid INT, name STRING)

CREATE TABLE B (uid INT, name STRING, PRIMARY KEY(uid))

INSERT INTO B SELECT * FROM A;
{code}

In databases this query is totally fine because partitions don't exist. Please correct me if I'm wrong but in Flink it could cause issues, because we don't shuffle on uid here and different uid could end up in different Kafka partitions in the end.
;;;","08/Nov/21 02:29;lzljs3620320;[~twalthr] Kafka partitioning should be another matter, depending on the user's connector option, for example, using upsert-kafka or `sink.partitioner`.
In my opinion, if the parallelism changes, such as the parallelism of source and sink is different, the final result will be random, which may not meet the user's expectations. So the premise is to change parallelism.;;;","08/Nov/21 02:41;wenlong.lwl;[~twalthr] thanks for the explanation. 
However, I think when the sink is kafka with pk(I would assume that it is a upsert kafka with key format provided), the target Kafka partition of a record is decided by the key generated. event it is written at different subtask. 
In such case, I think adding a key-by can help keep the order of record with the same uid, only when the input is already partitioned by uid. Is this case is what you want to solve?  ;;;","08/Nov/21 15:12;twalthr;[~wenlong.lwl] I guess for Kafka you are right. Kafka is doing the partitioning for us already with the default sink partitioner and given Kafka key. The only missing case is if you do a {{StreamTableEnvironment.toChangelogStream}} with a declared primary key, the user needs to take case of the partitioning in this case. But that might be acceptable. ;;;","10/Nov/21 11:35;lincoln.86xy;Thanks for all your inputs!
Based on Jingson's summary and the discussion, let me try to summarize, please correct me if I'm wrong

An upsert sink can accept such inputs:
1. input is changelog stream 
1.1 primary key = upsert key, it's already ok 
1.2 primary key != upsert key (upsert key can be none), we should add upsertMaterialize
1.3 primary key contains upsert key, upsertMaterialize can be ommitted but sink requires update_before

2. input is append stream
2.1 sink has same parallelism with the upstream operator, it's ok
2.2 sink's parallelism differs from the upstream operator, we should add a 'keyby' for the primary key by default

The current pr already addressed 1.2 & 1.3, so remaining the 2.2 to be done. The fix is simple, but for the sake of be configurable, 
we should introduce another job level config option similar to 'table.exec.sink.upsert-materialize' (since FLINK-24254 fine-grained setting per INSERT INTO not ready now) 
I temporally name it 'table.exec.sink.pk-shuffle',  and updated the pr, welcome your suggestions.

cc [~twalthr] [~lzljs3620320] [~wenlong.lwl] 

 

 ;;;","22/Nov/21 08:10;lincoln.86xy;According to Jingsong's review comments, I've split the change into two. The 1st part solve the 1.2 & 1.3 above, and a following one address the 2.2 which will introduce a config option 'table.exec.sink.pk-shuffle'.;;;","22/Nov/21 12:26;wenlong.lwl;hi, [~lincoln.86xy], what do you think we should provide when the sink is a DataStreamSinkProvider/TransformationSinkProvider? In such cases, we have no idea about the parallelism of sink, I think this is the case missed in the summary.;;;","23/Nov/21 02:23;lincoln.86xy;[~wenlong.lwl]  Good question!  I think it‘s safe adding ‘keyby’ by default for the two kinds of sink if we can't get parallelism (users can choose to turn it off explicitly).

What do you think?  [~twalthr]  [~lzljs3620320] ;;;","29/Nov/21 08:48;lincoln.86xy;[~wenlong.lwl]  After look into the `TransformationSinkProvider` and `DataStreamSinkProvider`, I found it's unnecessary to add `keyby` because
1. `DataStreamSinkProvider` implements the `ParallelismProvider` interface so that its parallelism can be detected.
2. the internal `TransformationSinkProvider` is used for `ExternalDynamicSink`, when convert to `datastream` it will apply the input's parallelism so it always the same as the input.

pls correct me if I'm wrong.;;;","30/Nov/21 06:19;wenlong.lwl;[~lincoln.86xy] Even sink implements ParallelismProvider, parallelism can still be undefined, because it returns an optional value.;;;","30/Nov/21 09:49;lincoln.86xy;After discussed with [~wenlong.lwl]  offline and confirm the code,  it is safe for TransformationSinkProvider.
For the DataStreamSinkProvider which is designed for advanced connector developers, it's necessary to pay attention to how changes are shuffled to not mess up the changelog per parallel subtask.

 ;;;","03/Dec/21 16:18;twalthr;[~lzljs3620320] are we planning to back port the change to 1.13 and 1.14? I know already one user that would benefit from a 1.14 back port.;;;","06/Dec/21 02:45;lzljs3620320;Hi [~twalthr] 

We have evaluated the impact of this fix and it does affect the compatibility of some correct plans. For example the plan:
 * generates changes
 * No upsert keys
 * but it has no problems with the order

This situation generates materialized node resulting in incompatibility.

If we agree with the following assumptions:
 * This incompatibility case is very rare
 * When this incompatibility occurs, the user can maintain compatibility by configuring option

We can consider cherry-pick.

What do you think? [~lincoln.86xy] ;;;","06/Dec/21 11:38;twalthr;Given that many people still wait for 1.14.1 and don't upgrade to a 1.XX.0 release. I think it is still reasonable to perform the change at least for the 1.14 branch. A user just needs to set `upsert-materialize` to `NONE`?;;;","07/Dec/21 01:58;lzljs3620320;> A user just needs to set `upsert-materialize` to `NONE`?

Yes. With the popularity of cdc and hudi, etc. I am +1 to back port.;;;","07/Dec/21 07:57;martijnvisser;Given the impact, I think it makes sense to wait with the 1.14.1 release until this is backported, what do you think [~lzljs3620320] [~twalthr] ? If so, it would be good to get a fix in for 1.14 asap given that we want to start releasing that soon. ;;;","07/Dec/21 08:38;lzljs3620320;[~MartijnVisser] Got it, I will do it asap.;;;","08/Dec/21 01:29;lincoln.86xy;I've created the cherry pick: [https://github.com/apache/flink/pull/18048]  [~lzljs3620320] would you help to check it?

 ;;;","08/Dec/21 09:07;lzljs3620320;Fixed via:

 

part1: Fix wrong results when sink primary key is not the same with query result's changelog upsert key

master: f8f6935adc841529ecdc0636174650cffbf73719

release-1.14: 7c5ddbd201005e55ab68b4db7ee74c7cbeb13400

release-1.13: 54aaffc7d9b2b06726294cf636d1b9acf74c5d49

 

part2: introduce 'table.exec.sink.keyed-shuffle' option to auto keyby on sink's pk if parallelism are not the same for insertOnly input

master: 9e76585f1fa110288604913a73d86ac2f1542777

 

part2 does not cherry-pick to 1.14 because it may affect the normal plan and lead to incompatibility.;;;",,,,,,,,,,,,,,,,,,,,
ColumnIntervalUtil#getColumnIntervalWithFilter does not consider the case when the predicate is a false constant,FLINK-20366,13342758,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,TsReaper,TsReaper,26/Nov/20 06:31,27/Nov/20 09:36,13/Jul/23 08:12,27/Nov/20 09:36,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"To reproduce this bug, add the following test case to {{DeadlockBreakupTest.scala}}

{code:scala}
@Test
def testSubplanReuse_DeadlockCausedByReusingExchangeInAncestor(): Unit = {
  util.tableEnv.getConfig.getConfiguration.setBoolean(
    OptimizerConfigOptions.TABLE_OPTIMIZER_REUSE_SUB_PLAN_ENABLED, true)
  util.tableEnv.getConfig.getConfiguration.setBoolean(
    OptimizerConfigOptions.TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED, false)
  util.tableEnv.getConfig.getConfiguration.setString(
    ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, ""NestedLoopJoin,SortMergeJoin"")
  val sqlQuery =
    """"""
      |WITH T1 AS (SELECT x1.*, x2.a AS k, x2.b AS v FROM x x1 LEFT JOIN x x2 ON x1.a = x2.a WHERE x2.b > 0)
      |SELECT x.a, T1.* FROM x LEFT JOIN T1 ON x.a = T1.k WHERE x.b > 0 AND T1.v = 0
      |"""""".stripMargin
  util.verifyPlan(sqlQuery)
}
{code}

And we'll get the exception stack
{code}
java.lang.RuntimeException: Error while applying rule FlinkLogicalJoinConverter(in:NONE,out:LOGICAL), args [rel#414:LogicalJoin.NONE.any.[](left=RelSubset#406,right=RelSubset#413,condition==($0, $4),joinType=inner)]

	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.getOptimizedPlan(TableTestBase.scala:431)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:348)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)
	at org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.testSubplanReuse_DeadlockCausedByReusingExchangeInAncestor(DeadlockBreakupTest.scala:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.RuntimeException: Error occurred while applying rule FlinkLogicalJoinConverter(in:NONE,out:LOGICAL)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:169)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 54 more
Caused by: java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:74)
	at scala.collection.mutable.ArrayBuffer.reduceLeft(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$.getColumnIntervalWithFilter(ColumnIntervalUtil.scala:219)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:181)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:156)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:309)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:306)
	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)
	at scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:46)
	at scala.collection.mutable.ArrayBuffer.exists(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getEquiInnerJoinRowCount(FlinkRelMdRowCount.scala:306)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:268)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:410)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalJoin.computeSelfCost(FlinkLogicalJoin.scala:64)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCost.getNonCumulativeCost(FlinkRelMdNonCumulativeCost.scala:41)
	at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost_$(Unknown Source)
	at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getNonCumulativeCost(RelMetadataQuery.java:288)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.getCost(VolcanoPlanner.java:705)
	at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements0(RelSubset.java:415)
	at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements(RelSubset.java:398)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1268)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1227)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
	... 58 more

{code}",,godfreyhe,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 09:36:00 UTC 2020,,,,,,,,,,"0|z0kye8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 06:32;TsReaper;[~godfreyhe] please take a look.;;;","26/Nov/20 06:33;godfreyhe;good catch [~TsReaper], I will fix it;;;","27/Nov/20 09:36;godfreyhe;master: 7bf76c0b41a68ace751d4af48efc1edc2ed2d6c7
release-1.12: 52d1adcfd159bf8e952818eb08baddc568d7658a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native k8s cluster could not be unregistered when executing Python DataStream application attachedly.,FLINK-20365,13342753,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,csq,csq,26/Nov/20 05:26,26/Nov/20 13:08,13/Jul/23 08:12,26/Nov/20 13:08,,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,,,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20123,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 13:08:03 UTC 2020,,,,,,,,,,"0|z0kyd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 13:08;dian.fu;Fixed in 
- master via af1b2cf698141334b2bdb66be471c86e27e7a344
- release-1.12 via caaa476a89549959f61e9e1b81694b1a7a4bef57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Batch SQL end-to-end test"" failed during shutdown",FLINK-20363,13342733,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,dian.fu,dian.fu,26/Nov/20 01:50,08/Jun/21 10:12,13/Jul/23 08:12,08/Jun/21 10:12,1.11.2,,,,,,,,Table SQL / Planner,Tests,,,,0,auto-deprioritized-major,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10138&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994

{code}
2020-11-25T23:03:39.0657020Z ==============================================================================
2020-11-25T23:03:39.0658778Z Running 'Batch SQL end-to-end test'
2020-11-25T23:03:39.0659508Z ==============================================================================
2020-11-25T23:03:39.0802908Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-39079497862
2020-11-25T23:03:39.2712316Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-11-25T23:03:39.3940809Z Starting cluster.
2020-11-25T23:03:40.2900610Z Starting standalonesession daemon on host fv-az510-522.
2020-11-25T23:03:41.8889378Z Starting taskexecutor daemon on host fv-az510-522.
2020-11-25T23:03:41.9309757Z Waiting for Dispatcher REST endpoint to come up...
2020-11-25T23:03:42.9869550Z Waiting for Dispatcher REST endpoint to come up...
2020-11-25T23:03:44.0431842Z Waiting for Dispatcher REST endpoint to come up...
2020-11-25T23:03:45.4007523Z Waiting for Dispatcher REST endpoint to come up...
2020-11-25T23:03:46.5129880Z Dispatcher REST endpoint is up.
2020-11-25T23:03:53.3634621Z Job has been submitted with JobID 2abe4546a6de428f2c19d51de93a5280
2020-11-25T23:03:56.1830568Z pass BatchSQL
2020-11-25T23:03:56.5054769Z Stopping taskexecutor daemon (pid: 56867) on host fv-az510-522.
2020-11-25T23:03:56.7417091Z Stopping standalonesession daemon (pid: 56577) on host fv-az510-522.
2020-11-25T23:03:57.1488127Z Skipping taskexecutor daemon (pid: 56806), because it is not running anymore on fv-az510-522.
2020-11-25T23:03:57.1490284Z Skipping taskexecutor daemon (pid: 57177), because it is not running anymore on fv-az510-522.
2020-11-25T23:03:57.1492572Z Skipping taskexecutor daemon (pid: 57526), because it is not running anymore on fv-az510-522.
2020-11-25T23:03:57.1493779Z Stopping taskexecutor daemon (pid: 57905) on host fv-az510-522.
2020-11-25T23:03:57.1495287Z /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/bin/taskmanager.sh: line 99: 57905 Terminated              ""${FLINK_BIN_DIR}""/flink-daemon.sh $STARTSTOP $ENTRYPOINT ""${ARGS[@]}""
2020-11-25T23:03:57.1499330Z [FAIL] Test script contains errors.
{code}",,dian.fu,godfreyhe,joemoe,knaufk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 10:12:11 UTC 2021,,,,,,,,,,"0|z0ky8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:00;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","23/Apr/21 08:19;knaufk;As part of https://issues.apache.org/jira/browse/FLINK-22029 the ""Test"" Issue Type is removed. I migrated this one to a ""Bug"" and added the label ""test-stability"". If you think this should rather be an ""Improvement"", ""New Feature"" or ""Technical Debt"", feel free to change the issue type.;;;","21/May/21 12:17;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jun/21 10:12;joemoe;let's close it for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken Link in dev/table/sourceSinks.zh.md,FLINK-20362,13342732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,hxbks2ks,hxbks2ks,26/Nov/20 01:40,26/Nov/20 07:28,13/Jul/23 08:12,26/Nov/20 07:28,1.12.0,,,,,1.12.0,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"When executing the script build_docs.sh, it will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'dev/table/legacySourceSinks.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/sourceSinks.zh.md Could not find document 'dev/table/legacySourceSinks.md' in tag 'link'.
{code}",,dian.fu,fsk119,hxbks2ks,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 07:28:34 UTC 2020,,,,,,,,,,"0|z0ky8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 02:09;leonard;Thanks [~hxbks2ks] for the report,
I think the right link should be \{% link dev/table/legacySourceSinks.zh.md %}
 [~fsk119] Do you like to fix this?;;;","26/Nov/20 02:13;fsk119;ok. I will fix it.
;;;","26/Nov/20 07:28;dian.fu;Fixed in 
- master via 47d81ce1328fb5979e237048bdfc8bae5d88d283
- release-1.12 via 3ebeb6c8f5ab6791e32a4f98210a4b8fd8e45cd8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execution.transitionState does not properly log slot location,FLINK-20351,13342657,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trohrmann,trohrmann,trohrmann,25/Nov/20 16:49,26/Nov/20 16:22,13/Jul/23 08:12,26/Nov/20 16:22,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,{{Execution.transitionState}} does not properly log the slot location when reporting the state transition. The problem is that we rely on {{LogicalSlot.toString}} for this information. I suggest to explicitly log the location information consisting of hostname and {{ResourceID}} of the machine on which the {{Execution}} is running.,,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 16:22:00 UTC 2020,,,,,,,,,,"0|z0kxs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 16:22;trohrmann;Fixed via

master: 144a9b265defefa280968e771767c33e5faee1a5
1.12.0:  84b6631eee9207c5289df9e185a6c19dd84ec9fd
1.11.3: 65a6b0f16525c906df787870f5a4f25e301a40f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Query fails with ""A conflict is detected. This is unexpected.""",FLINK-20349,13342639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lirui,lirui,25/Nov/20 15:37,27/Nov/20 13:39,13/Jul/23 08:12,27/Nov/20 13:39,1.12.0,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The test case to reproduce:
{code}
	@Test
	public void test() throws Exception {
		tableEnv.executeSql(""create table src(key string,val string)"");
		tableEnv.executeSql(""SELECT sum(char_length(src5.src1_value)) FROM "" +
				""(SELECT src3.*, src4.val as src4_value, src4.key as src4_key FROM src src4 JOIN "" +
				""(SELECT src2.*, src1.key as src1_key, src1.val as src1_value FROM src src1 JOIN src src2 ON src1.key = src2.key) src3 "" +
				""ON src3.src1_key = src4.key) src5"").collect();
	}
{code}",,godfreyhe,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 13:39:49 UTC 2020,,,,,,,,,,"0|z0kxo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/20 15:39;lirui;Stack trace is:
{noformat}
java.lang.IllegalStateException: A conflict is detected. This is unexpected.

	at org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculator.resolveInputPriorityConflict(InputOrderCalculator.java:170)
	at org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.addTopologyEdges(InputPriorityGraphGenerator.java:167)
	at org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.updateTopologyGraph(InputPriorityGraphGenerator.java:150)
	at org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.access$100(InputPriorityGraphGenerator.java:85)
	at org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator$1.visitNode(InputPriorityGraphGenerator.java:124)
	at org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor.visit(AbstractExecNodeExactlyOnceVisitor.java:41)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.accept(ExecNode.scala:102)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecJoinBase.accept(BatchExecJoinBase.scala:42)
	at org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.lambda$createTopologyGraph$1(InputPriorityGraphGenerator.java:127)
	at java.util.Collections$SingletonList.forEach(Collections.java:4822)
	at org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.createTopologyGraph(InputPriorityGraphGenerator.java:127)
	at org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculator.calculate(InputOrderCalculator.java:63)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.createBatchMultipleInputNode(MultipleInputNodeCreationProcessor.java:502)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.createMultipleInputNode(MultipleInputNodeCreationProcessor.java:470)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.getMultipleInputNode(MultipleInputNodeCreationProcessor.java:438)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.getMultipleInputNode(MultipleInputNodeCreationProcessor.java:433)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.getMultipleInputNode(MultipleInputNodeCreationProcessor.java:433)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.getMultipleInputNode(MultipleInputNodeCreationProcessor.java:433)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.getMultipleInputNode(MultipleInputNodeCreationProcessor.java:433)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.getMultipleInputNode(MultipleInputNodeCreationProcessor.java:433)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.createMultipleInputNodes(MultipleInputNodeCreationProcessor.java:420)
	at org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.process(MultipleInputNodeCreationProcessor.java:93)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToExecNodePlan$1.apply(BatchPlanner.scala:77)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToExecNodePlan$1.apply(BatchPlanner.scala:77)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToExecNodePlan(BatchPlanner.scala:76)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)
{noformat};;;","26/Nov/20 02:44;godfreyhe;good catch;;;","27/Nov/20 13:39;godfreyhe;master: 6709a5aa967be80620263a05ef5763c94d96aaf5
release-1.12: 2f655ec254b6a2162fcfddaf76813c07a19d9ca1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the default value of the flink-conf savepoint folder to distinguish the checkpoint folder,FLINK-20344,13342576,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,OpenOpened,OpenOpened,25/Nov/20 10:11,29/Nov/20 13:16,13/Jul/23 08:12,29/Nov/20 13:16,1.11.2,,,,,1.12.0,,,Documentation,flink-contrib,,,,0,pull-request-available,,,,,"Should savepoints in the flink-conf.yml file be specified as the end of flink-savepoint instead of the default and the same as the configuration of flink-checkpoints

 

state.*checkpoints*.dir: hdfs://namenode-host:port/flink-checkpoints

state.*savepoints*.dir: hdfs://namenode-host:port/flink-checkpoints

 

after modification

state.*savepoints*.dir: hdfs://namenode-host:port/flink-*savepoints*

 ",,OpenOpened,sewen,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 29 13:16:43 UTC 2020,,,,,,,,,,"0|z0kxa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/20 13:16;sewen;Fixed in
  - 1.12.0 (release-1.12) via 64ea9d6136e2825f15514797ffe7c1ed8fa64713
  - 1.13..0 (master) via 89f1754b38befd9d65bd621068c0a66cca6b3e31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RequestReplyFunction should not silently ignore UNRECOGNIZED state value mutations types,FLINK-20336,13342515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tzulitai,tzulitai,tzulitai,25/Nov/20 04:29,28/May/21 07:37,13/Jul/23 08:12,28/May/21 07:37,statefun-2.1.0,statefun-2.2.1,,,,statefun-3.1.0,,,Stateful Functions,,,,,0,auto-unassigned,pull-request-available,,,,"If a function's response has a {{PersistedValueMutation}} type that is {{UNRECOGNIZED}}, we currently just silently ignore that mutation:
https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/reqreply/PersistedRemoteFunctionValues.java#L84

This is incorrect. The {{UNRECOGNIZED}} enum constant is a pre-defined constant used by the Protobuf Java SDK, to represent a constant that was unable to be deserialized (because the the serialized constant does not match any enums defined in the protobuf message).

Therefore, it should be handled by throwing an exception, preferably indicating that there is some sort of version mismatch between the function's Protobuf message definitions, and StateFun's Protobuf message definitions (i.e. most likely a mismatch in the invocation protocol versions).",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 28 07:37:36 UTC 2021,,,,,,,,,,"0|z0kwwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 10:49;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:49;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","28/May/21 07:37;tzulitai;flink-statefun/master: e52f1c1f6cec09cec7650057e66c976462ce364c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink standalone cluster throws metaspace OOM after submitting multiple PyFlink UDF jobs.,FLINK-20333,13342506,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhongwei,zhongwei,zhongwei,25/Nov/20 03:53,14/Sep/21 05:46,13/Jul/23 08:12,25/Nov/20 11:21,,,,,,1.11.3,1.12.0,,API / Python,,,,,1,pull-request-available,,,,,"Currently the Flink standalone cluster will throw metaspace OOM after submitting multiple PyFlink UDF jobs. The root cause is that currently the PyFlink classes are running in user classloader and so each job creates a separate user class loader to load PyFlink related classes. There are many soft references and Finalizers in memory (introduced by the underlying Netty), which prevents the garbage collection of the user classloader of already finished PyFlink jobs. 

Due to their existence, it needs multiple full gc to reclaim the classloader of the completed job. If only one full gc is performed before the metaspace space is insufficient, then OOM will occur.

 ",,dian.fu,dianfu,f.pompermaier,pd17,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 14 05:46:06 UTC 2021,,,,,,,,,,"0|z0kwuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/20 11:21;dian.fu;Fixed in
- master via c354f7bd679b9fa8c1e0d75feb3827ccca7f317b
- release-1.11 via 18392118c38dc6a1b3a76a76bc2d0fb3ee2f8d7f;;;","25/Nov/20 13:44;f.pompermaier;Could this problem affect also normal java jobs? I have the same leak in my Flink session cluster...actually this is the suggested leak message that the Eclipse MAT gives to me:

 
{code:java}
5,416 instances of ""java.lang.Class"", loaded by ""<system class loader>"" occupy 2,706,048 (11.04%)bytes.
Biggest instances:

class java.io.ObjectStreamClass$Caches @ 0xe0f52f98 - 402,896 (1.64%) bytes. {code}
After some job resubmission I get the follogin exception:
{code:java}
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
        at java.lang.ClassLoader.defineClass1(Native Method) ~[?:?]
        at java.lang.ClassLoader.defineClass(ClassLoader.java:1017) ~[?:?]
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:174) ~[?:?]
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:550) ~[?:?]
        at java.net.URLClassLoader$1.run(URLClassLoader.java:458) ~[?:?]
        at java.net.URLClassLoader$1.run(URLClassLoader.java:452) ~[?:?]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
        at java.net.URLClassLoader.findClass(URLClassLoader.java:451) ~[?:?]
        at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:71) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:48) [flink-dist_2.12-1.11.0.jar:1.11.0]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:522) [?:?
{code}
];;;","26/Nov/20 02:05;zhongwei;Hi [~f.pompermaier], as the `org.apache.flink.streaming.runtime.tasks.StreamTask` also implements the `finalize()` method, this problem will affect the normal java jobs which load user code into the flink runtime. ;;;","13/Sep/21 15:58;pd17;Hi, Is this issue resolved for java client... Asking this because in prod task manager with 256Mb jvm metaspace... i am getting metaspace OOM by deploying 4-5 jobs on single task manager irrespective of their parallelism. I am using flink 1.12.0 version. [~dian.fu];;;","14/Sep/21 03:55;dianfu;[~pd17] What do you mean by ""java client""?;;;","14/Sep/21 05:15;pd17;[~dianfu] So what i meant was that in my case i am writing flink udf in java (Java Jobs) and using flink-core 1.12.0 maven dependency. And i am facing metaspace OOM on redeployment of jobs on a single task manager. Just wanted to know if this is a common and open issue or has it been resolved;;;","14/Sep/21 05:46;dianfu;I think it also depends on how you handle the jars (not only the UDF jars, but also connector jars, etc), e.g. whether placing them in the lib directory which are loaded by the context class loader or submitted using pipeline.jars/pipeline.classpaths which are loaded by the user class loader. Could you try to place the jars in the lib directory and see if the issue still exists?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnalignedCheckpointITCase.execute failed with ""Sequence number for checkpoint 20 is not known (it was likely been overwritten by a newer checkpoint 21)""",FLINK-20331,13342494,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,dian.fu,dian.fu,25/Nov/20 01:57,22/Jun/21 13:55,13/Jul/23 08:12,26/Nov/20 07:07,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10059&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc

{code}
2020-11-24T22:42:17.6704402Z [ERROR] execute[parallel pipeline with mixed channels, p = 20](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 7.901 s  <<< ERROR!
2020-11-24T22:42:17.6706095Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-24T22:42:17.6707450Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-11-24T22:42:17.6708569Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
2020-11-24T22:42:17.6709626Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-24T22:42:17.6710452Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-11-24T22:42:17.6711271Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-24T22:42:17.6713170Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-24T22:42:17.6713974Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-11-24T22:42:17.6714517Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-11-24T22:42:17.6715372Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-11-24T22:42:17.6715871Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-24T22:42:17.6716514Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-24T22:42:17.6718475Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
2020-11-24T22:42:17.6719322Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-11-24T22:42:17.6719887Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-11-24T22:42:17.6720271Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-11-24T22:42:17.6720645Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-11-24T22:42:17.6721114Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-24T22:42:17.6721585Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-11-24T22:42:17.6722078Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-11-24T22:42:17.6722738Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-11-24T22:42:17.6723183Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-11-24T22:42:17.6723862Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-11-24T22:42:17.6724435Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-11-24T22:42:17.6724914Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-11-24T22:42:17.6725323Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-11-24T22:42:17.6725866Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-24T22:42:17.6726313Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-11-24T22:42:17.6726829Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-11-24T22:42:17.6727376Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-24T22:42:17.6727891Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-24T22:42:17.6728400Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-11-24T22:42:17.6728855Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-11-24T22:42:17.6729390Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-11-24T22:42:17.6729853Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-11-24T22:42:17.6730345Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-24T22:42:17.6730781Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-24T22:42:17.6731212Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-24T22:42:17.6731657Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-24T22:42:17.6732230Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2020-11-24T22:42:17.6733094Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-24T22:42:17.6733759Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-24T22:42:17.6734418Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-11-24T22:42:17.6734964Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-11-24T22:42:17.6735683Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-11-24T22:42:17.6736231Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:533)
2020-11-24T22:42:17.6736756Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
2020-11-24T22:42:17.6737270Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
2020-11-24T22:42:17.6737684Z 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2020-11-24T22:42:17.6738114Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-24T22:42:17.6738542Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-24T22:42:17.6738960Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-24T22:42:17.6739537Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-24T22:42:17.6740059Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-24T22:42:17.6740560Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-24T22:42:17.6741004Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-24T22:42:17.6741489Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-24T22:42:17.6741882Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-24T22:42:17.6742302Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-24T22:42:17.6743029Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-24T22:42:17.6743601Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-24T22:42:17.6744206Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-24T22:42:17.6744642Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-24T22:42:17.6745047Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-24T22:42:17.6745450Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-24T22:42:17.6745807Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-24T22:42:17.6746184Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-24T22:42:17.6746540Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-24T22:42:17.6746880Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-24T22:42:17.6747244Z 	... 4 more
2020-11-24T22:42:17.6747712Z Caused by: java.lang.IllegalStateException: Sequence number for checkpoint 20 is not known (it was likely been overwritten by a newer checkpoint 21)
2020-11-24T22:42:17.6748244Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:220)
2020-11-24T22:42:17.6749007Z 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getInflightBuffersUnsafe(RemoteInputChannel.java:542)
2020-11-24T22:42:17.6750046Z 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkpointStarted(RemoteInputChannel.java:514)
2020-11-24T22:42:17.6750982Z 	at org.apache.flink.runtime.io.network.partition.consumer.IndexedInputGate.checkpointStarted(IndexedInputGate.java:35)
2020-11-24T22:42:17.6751825Z 	at org.apache.flink.streaming.runtime.io.UnalignedController.preProcessFirstBarrier(UnalignedController.java:54)
2020-11-24T22:42:17.6752730Z 	at org.apache.flink.streaming.runtime.io.AlternatingController.preProcessFirstBarrier(AlternatingController.java:57)
2020-11-24T22:42:17.6753372Z 	at org.apache.flink.streaming.runtime.io.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:121)
2020-11-24T22:42:17.6754237Z 	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:174)
2020-11-24T22:42:17.6754995Z 	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:151)
2020-11-24T22:42:17.6755929Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:157)
2020-11-24T22:42:17.6756494Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
2020-11-24T22:42:17.6757294Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)
2020-11-24T22:42:17.6757944Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)
2020-11-24T22:42:17.6758714Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
2020-11-24T22:42:17.6759583Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
2020-11-24T22:42:17.6760205Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-11-24T22:42:17.6760774Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-11-24T22:42:17.6761323Z 	at java.lang.Thread.run(Thread.java:748)
{code}",,AHeise,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 07:07:13 UTC 2020,,,,,,,,,,"0|z0kws0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/20 01:58;dian.fu;Set the priority as ""Blocker"" before we figure out the root cause of this problem. ;;;","25/Nov/20 02:10;dian.fu;cc [~AHeise] [~roman_khachatryan];;;","25/Nov/20 07:53;arvid;This ticket is most likely a real bug. I hope [~roman_khachatryan] can investigate ;) (or else I will do after fixing test instabilities).;;;","26/Nov/20 07:07;arvid;Not a real bug but a too strict assumption.

Merged a fix as [50af0b161b1962d9db5c692aa965b310a4000da9|https://github.com/apache/flink/commit/50af0b161b1962d9db5c692aa965b310a4000da9] in master.

Merged as 2c77c38d55 into release-1.12.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnalignedCheckpointITCase.execute failed with ""Insufficient number of network buffers: required 1, but only 0 available""",FLINK-20328,13342489,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,dian.fu,dian.fu,25/Nov/20 01:33,22/Jun/21 14:05,13/Jul/23 08:12,25/Nov/20 12:15,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10008&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0]

{code}
2020-11-24T08:18:15.0273526Z [ERROR] execute[non-parallel pipeline with local channels](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 4.338 s  <<< ERROR!
2020-11-24T08:18:15.0287436Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-24T08:18:15.0288449Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-11-24T08:18:15.0292752Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
2020-11-24T08:18:15.0293717Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-24T08:18:15.0294586Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-11-24T08:18:15.0295320Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-24T08:18:15.0296301Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-24T08:18:15.0297083Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-11-24T08:18:15.0299675Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-11-24T08:18:15.0300561Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-11-24T08:18:15.0301312Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-24T08:18:15.0302041Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-24T08:18:15.0302911Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
2020-11-24T08:18:15.0303568Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-11-24T08:18:15.0304112Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-11-24T08:18:15.0304823Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-11-24T08:18:15.0305426Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-11-24T08:18:15.0349665Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-24T08:18:15.0350525Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-11-24T08:18:15.0351135Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-11-24T08:18:15.0351615Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-11-24T08:18:15.0352100Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-11-24T08:18:15.0352715Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-11-24T08:18:15.0353389Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-11-24T08:18:15.0353992Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-11-24T08:18:15.0354631Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-11-24T08:18:15.0355099Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-24T08:18:15.0355553Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-11-24T08:18:15.0356110Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-11-24T08:18:15.0356651Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-24T08:18:15.0357188Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-24T08:18:15.0357682Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-11-24T08:18:15.0358133Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-11-24T08:18:15.0358661Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-11-24T08:18:15.0359167Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-11-24T08:18:15.0359662Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-24T08:18:15.0360113Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-24T08:18:15.0360575Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-24T08:18:15.0361019Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-24T08:18:15.0361599Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2020-11-24T08:18:15.0362291Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-24T08:18:15.0363012Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-24T08:18:15.0450920Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-11-24T08:18:15.0451758Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-11-24T08:18:15.0512154Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-11-24T08:18:15.0512781Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534)
2020-11-24T08:18:15.0513330Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
2020-11-24T08:18:15.0513856Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
2020-11-24T08:18:15.0514482Z 	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
2020-11-24T08:18:15.0514986Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-24T08:18:15.0515459Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-24T08:18:15.0516031Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-24T08:18:15.0516566Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-24T08:18:15.0517102Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-24T08:18:15.0517625Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-24T08:18:15.0518095Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-24T08:18:15.0518622Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-24T08:18:15.0519204Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-24T08:18:15.0519830Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-24T08:18:15.0520684Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-24T08:18:15.0521691Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-24T08:18:15.0522688Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-24T08:18:15.0523629Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-24T08:18:15.0524469Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-24T08:18:15.0525164Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-24T08:18:15.0525987Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-24T08:18:15.0526783Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-24T08:18:15.0527490Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-24T08:18:15.0528118Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-24T08:18:15.0528660Z 	... 4 more
2020-11-24T08:18:15.0529196Z Caused by: org.apache.flink.streaming.runtime.tasks.AsynchronousException: Unable to read channel state
2020-11-24T08:18:15.0530090Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask$StreamTaskAsyncExceptionHandler.handleAsyncException(StreamTask.java:1108)
2020-11-24T08:18:15.0530968Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$0(StreamTask.java:513)
2020-11-24T08:18:15.0531720Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-11-24T08:18:15.0532423Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-11-24T08:18:15.0533016Z 	at java.lang.Thread.run(Thread.java:748)
2020-11-24T08:18:15.0535340Z Caused by: java.io.IOException: Insufficient number of network buffers: required 1, but only 0 available. The total number of network buffers is currently set to 15 of 4096 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
2020-11-24T08:18:15.0536721Z 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.tryRedistributeBuffers(NetworkBufferPool.java:428)
2020-11-24T08:18:15.0565177Z 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:166)
2020-11-24T08:18:15.0566283Z 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:60)
2020-11-24T08:18:15.0567288Z 	at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.requestExclusiveBuffers(BufferManager.java:131)
2020-11-24T08:18:15.0568367Z 	at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.requestBufferBlocking(RecoveredInputChannel.java:226)
2020-11-24T08:18:15.0569349Z 	at org.apache.flink.runtime.checkpoint.channel.InputChannelRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:60)
2020-11-24T08:18:15.0570313Z 	at org.apache.flink.runtime.checkpoint.channel.InputChannelRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:50)
2020-11-24T08:18:15.0571266Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateChunkReader.readChunk(SequentialChannelStateReaderImpl.java:147)
2020-11-24T08:18:15.0572257Z 	at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readSequentially(SequentialChannelStateReaderImpl.java:89)
2020-11-24T08:18:15.0573281Z 	at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.read(SequentialChannelStateReaderImpl.java:78)
2020-11-24T08:18:15.0574468Z 	at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readInputData(SequentialChannelStateReaderImpl.java:63)
2020-11-24T08:18:15.0575364Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$0(StreamTask.java:511)
2020-11-24T08:18:15.0575882Z 	... 3 more
{code}",,AHeise,dian.fu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 12:15:05 UTC 2020,,,,,,,,,,"0|z0kwqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/20 01:59;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10059&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185;;;","25/Nov/20 02:00;dian.fu;Upgrade to ""Blocker"" before we figure out the root cause of this problem.

;;;","25/Nov/20 03:33;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10056&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","25/Nov/20 03:34;jark;This seems rather frequent. I upgrade to ""Blocker"".;;;","25/Nov/20 12:15;arvid;Merged into master as 9902c4f22a1bfd734a02dbca21d8ce161436d7a2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Broken links to ""How to define LookupableTableSource""",FLINK-20326,13342418,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,dwysakowicz,dwysakowicz,24/Nov/20 16:46,30/Nov/20 23:01,13/Jul/23 08:12,30/Nov/20 23:01,,,,,,1.12.0,,,Documentation,Table SQL / Ecosystem,,,,0,,,,,,"I found at least two pages that have a link to a non-existing anchor:
* https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/joins.html#join-with-a-temporal-table
* https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/temporal_tables.html#defining-temporal-table",,dwysakowicz,jark,leonard,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 23:01:11 UTC 2020,,,,,,,,,,"0|z0kwb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/20 03:29;jark;[~Leonard Xu] do you want to take this?;;;","25/Nov/20 03:40;leonard;yes, please assign this to me;;;","25/Nov/20 08:52;leonard;I create a commit to fix this in PR [https://github.com/apache/flink/pull/14152/commits/6b09f2fea094314977b8848ffbe235d995050621];;;","30/Nov/20 23:01;sjwiesman;resolved in 1.12 and master 01b4dbee1bbb188bce2e1a315d84f9c63098acff
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get NPE when using AvroDeserializationSchema to deserialize null input,FLINK-20321,13342359,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xwang51,fsk119,fsk119,24/Nov/20 12:48,23/Sep/21 18:02,13/Jul/23 08:12,08/Jan/21 14:32,1.12.0,,,,,1.12.5,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,sprint,starter,,,"You can reproduce the bug by adding the code into the {{AvroDeserializationSchemaTest}}.
The code follows


{code:java}
@Test
	public void testSpecificRecord2() throws Exception {
		DeserializationSchema<Address> deserializer = AvroDeserializationSchema.forSpecific(Address.class);

		Address deserializedAddress = deserializer.deserialize(null);
		assertEquals(null, deserializedAddress);
	}

{code}

Exception stack:

{code:java}
java.lang.NullPointerException
	at org.apache.flink.formats.avro.utils.MutableByteArrayInputStream.setBuffer(MutableByteArrayInputStream.java:43)
	at org.apache.flink.formats.avro.AvroDeserializationSchema.deserialize(AvroDeserializationSchema.java:131)
	at org.apache.flink.formats.avro.AvroDeserializationSchemaTest.testSpecificRecord2(AvroDeserializationSchemaTest.java:69)
{code}

",,caozhen1937,fsk119,hehuiyuan,jark,libenchao,sampadsaha5,xwang51,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20459,,,,,,,FLINK-21889,FLINK-20121,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 12 03:48:10 UTC 2021,,,,,,,,,,"0|z0kvy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 12:53;jark;This problem also exists in {{AvroRowDataDeserializationSchema}} and {{ConfluentRegistryAvroDeserializationSchema}}.
;;;","24/Nov/20 12:54;jark;Do you want to fix this [~danny0405]? ;;;","27/Nov/20 11:54;sampadsaha5;Hi,

This is Sampad here. I can help with fixing this issue. My proposed fix is to check if the message is null before [this|https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroDeserializationSchema.java#L131] line and throw an exception saying ""The message is null"".

Does this sound good? Can you please suggest any better idea? Please don't mind if my idea doesn't match the standard of this community. I am new here.;;;","27/Nov/20 14:26;jark;[~sampadsaha5]. Please do not throw exception because this will fail the job. We should just {{return null}} to skip the null value.

We should also fix {{AvroRowDataDeserializationSchema}} and {{ConfluentRegistryAvroDeserializationSchema}} and {{JsonRowDataDeserializationSchema}} and {{CsvRowDataDeserializationSchema}}. Please also remember to add tests.;;;","27/Nov/20 14:39;sampadsaha5;{quote}We should just {{return null}} to skip the null value.
{quote}
Thanks. This sounds a better idea.
{quote}Please also remember to add tests.
{quote}
Yah. Sure I will do that too.;;;","27/Dec/20 06:55;jark;Hi [~sampadsaha5], are you still working on this? What's the status of this issue?;;;","29/Dec/20 16:55;xwang51;Hi [~jark], I just joined the community and would like to contribute on some starter issues first to familiarize myself with the code contribution process. This issue seems easy and I should be able to raise a PR to fix it in a day or two. Since we haven't heard from the last assignee for over a month, do you mind reassigning this issue to me?;;;","29/Dec/20 17:42;sampadsaha5;Hi [~xwang51], if you are interested, you can work on this issue. This would help you understand how different deserialization schema are actually wotking.

Hi [~jark], I am sorry for my behaviour. I had caught up with some other commitment. Really sorry for the delay. Would you please assign this ticket to [~xwang51]?

 

 ;;;","30/Dec/20 02:37;xwang51;Thank you, [~sampadsaha5]. I've worked with Avro Serdes and Confluent Schema Registry during several projects. I'll definitely take this opportunity to learn more about Flink and contribute to the community. Feel free to get back when you have time, contributions are always welcome. Cheers!;;;","30/Dec/20 04:25;jark;I assigned this issue to you [~xwang51]. 
Welcome contribution anytime [~sampadsaha5]!;;;","31/Dec/20 16:06;xwang51;Thank you, [~jark]. I just submitted the [PR|https://github.com/apache/flink/pull/14539].;;;","08/Jan/21 14:32;jark;Fixed in master: 241185a40118cfe3087eb4f7fc14203afe4073cc;;;","06/Jul/21 04:51;hehuiyuan;Hi [~jark] , is it considered merging to 1.12?;;;","06/Jul/21 06:33;jark;[~hehuiyuan] I can help to merge if there is a pull request for 1.12. ;;;","06/Jul/21 08:44;hehuiyuan;[~jark] , i do it.;;;","12/Jul/21 03:48;jark;Fixed in release-1.12: 2eaabca6075f91617d8c7f2bfd363964e893d65e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix cast question for properies() method in kafka ConnectorDescriptor,FLINK-20318,13342318,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,24/Nov/20 09:42,28/Aug/21 12:16,13/Jul/23 08:12,15/Jun/21 06:06,,,,,,1.14.0,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,,,,," 

This Jira fixes Kafka connector. There is a cast problem when use properties method.
{code:java}
Properties props = new Properties();
 props.put( ""enable.auto.commit"", ""false"");
 props.put( ""fetch.max.wait.ms"", ""3000"");
 props.put(""flink.poll-timeout"", 5000);
 props.put( ""flink.partition-discovery.interval-millis"", false);
kafka = new Kafka()
 .version(""0.11"")
 .topic(topic)
 .properties(props);
{code}
{code:java}
Exception in thread ""main"" java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
Exception in thread ""main"" java.lang.ClassCastException: java.lang.Boolean cannot be cast to java.lang.String
{code}
change :

- *change (String) v ----> String.valueOf() in Kafka.java*

 

 

 ",,hehuiyuan,jark,luoyuxia,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 06:06:06 UTC 2021,,,,,,,,,,"0|z0kvow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/21 13:02;hehuiyuan;Hi [@twalthr|https://github.com/twalthr] , please look at this.;;;","29/Mar/21 13:02;hehuiyuan;Hi [~jark],  who can look this?;;;","15/Jun/21 06:06;lzljs3620320;Fixed via:

master: df92fa95ad28206df1dcb434df2c76b563c5ed4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty Calc is not removed by CalcRemoveRule,FLINK-20314,13342277,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,bugboy,bugboy,24/Nov/20 05:41,26/Nov/20 01:57,13/Jul/23 08:12,26/Nov/20 01:57,1.12.0,,,,,1.12.0,,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,,,"My DDL：
{code:java}
create table if not exists t_order(
id int PRIMARY KEY comment '订单id',
timestamps bigint comment '订单创建时间',
orderInformationId string comment '订单信息ID',
userId string comment '用户ID',
categoryId int comment '商品类别',
productId int comment '商品ID',
price decimal(10,2) comment '单价',
productCount int comment '购买数量',
priceSum decimal(10,2) comment '订单总价',
shipAddress string comment '商家地址',
receiverAddress string comment '收货地址',
ts AS TO_TIMESTAMP(FROM_UNIXTIME(timestamps/1000)),
WATERMARK FOR ts AS ts - INTERVAL '3' SECOND
)with(
'connector' = 'kafka',
'format' = 'debezium-avro-confluent', 
'debezium-avro-confluent.schema-registry.url' = 'http://hostname:8081',
'topic' = 'ods.userAnalysis.order',
'properties.bootstrap.servers' = 'hostname:9092',
'properties.group.id' = 'flink-analysis',
'scan.startup.mode' = 'latest-offset'
)

{code}
 

query is ok  when using the following SQLs：
{code:java}
select * from t_order{code}
{code:java}
select receiverAddress from t_order{code}
{code:java}
select
id,
timestamps,
orderInformationId,
userId,
categoryId,
productId,
price,
productCount,
priceSum,
shipAddress
from t_order{code}
but when I add the *receiveraddress* field to the third sql like:
{code:java}
select
id,
timestamps,
orderInformationId,
userId,
categoryId,
productId,
price,
productCount,
priceSum,
shipAddress,
receiverAddress
from t_order
{code}
it throws an exception：
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: This calc has no useful projection and no filter. It should be removed by CalcRemoveRule.Exception in thread ""main"" org.apache.flink.table.api.TableException: This calc has no useful projection and no filter. It should be removed by CalcRemoveRule. at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:166) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:59) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:84) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:59) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:57) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:59) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:57) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48) at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:66) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:65) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:702) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1065) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664) at com.bugboy.analysis.AnalysisCase$.main(AnalysisCase.scala:161) at com.bugboy.analysis.AnalysisCase.main(AnalysisCase.scala){code}
 

 
 
 ",,bugboy,fsk119,godfreyhe,hailong wang,jark,leonard,libenchao,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 01:57:26 UTC 2020,,,,,,,,,,"0|z0kvfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 08:40;nicholasjiang;[~bugboy], please translate your description of issue to English.;;;","24/Nov/20 13:16;hailong wang;This is for we push watermark acrossing `FlinkLogicalCalc`, and the computed column is also push down to `TableScan`. Then the calc projection list is the same with tableScan output fields.

Maybe we can add a rule to remove `FlinkLogicalCalc` after `PushWatermarkIntoTableSourceScanAcrossCalcRule` like `CalcRemoveRule` ?;;;","24/Nov/20 13:42;jark;Yes. I think so. cc [~fsk119] [~godfreyhe];;;","25/Nov/20 02:56;fsk119;Please assign it to me [~jark] . I will fix it.;;;","26/Nov/20 01:57;godfreyhe;master: 4cc1da96e88ddc6166181a7f83c891761db283a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Liquid Exception: Could not find document 'dev/table/streaming/time_attributes.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/sql/create.zh.md,FLINK-20308,13342239,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,24/Nov/20 01:39,24/Nov/20 07:35,13/Jul/23 08:12,24/Nov/20 07:35,1.12.0,,,,,1.12.0,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"When executing the script build_docs.sh, i will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'dev/table/streaming/time_attributes.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/sql/create.zh.md Could not find document 'dev/table/streaming/time_attributes.md' in tag 'link'.
{code}",,dian.fu,hxbks2ks,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 07:35:35 UTC 2020,,,,,,,,,,"0|z0kv7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 04:33;jark;I can't reproduce this problem in my local machine with the latest master branch. 
Could you check it again [~hxbks2ks]?;;;","24/Nov/20 06:04;hxbks2ks;[~jark] Yes, I just pulled the latest code of master branch, and I still get the error. I will fix it as soon as possible.;;;","24/Nov/20 07:35;jark;Fixed in master (1.12.0): 0c78dfde726bae404a0d52324b7a249683f788a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail hard when trying to run job with broadcast state in BATCH execution mode,FLINK-20304,13342187,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kkl0u,trohrmann,trohrmann,23/Nov/20 18:03,27/Nov/20 09:29,13/Jul/23 08:12,27/Nov/20 09:29,1.12.0,,,,,1.12.0,1.13.0,,API / DataStream,,,,,0,pull-request-available,,,,,"Contrary to the documentation it is currently possible to run a job with broadcast state in {{BATCH}} execution mode. Since accessing the keyed state from the broadcast side fails, we shouldn't allow the submissions of these kind of jobs in the first place. Hence, I would suggest to fail hard if one tries to run a job using the broadcast state pattern in {{BATCH}} execution mode.",,aljoscha,dian.fu,kkl0u,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20115,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 09:29:46 UTC 2020,,,,,,,,,,"0|z0kuw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 18:23;rmetzger;[~aljoscha] can you take a look and potentially adjust the priority of this ticket (Only if you make it a blocker, it'll be considered for delaying the next RC);;;","24/Nov/20 10:18;aljoscha;I think it's not a blocker but still nice to have.;;;","27/Nov/20 09:29;kkl0u;master: ed82aabab760c483a0dda6e3b93bb98a28ac6d48
1.12:  7a594cf856313faebbe0a1c51c12b57ce9f87a54;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Chinese table overview,FLINK-20299,13342155,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,sjwiesman,sjwiesman,23/Nov/20 15:30,08/Oct/21 11:10,13/Jul/23 08:12,26/Aug/21 04:25,,,,,,,,,chinese-translation,Documentation,,,,0,auto-deprioritized-major,pull-request-available,,,,"Sync `dev/table/index.zh.md` with the changes introduced in FLINK-18279

commit: 0fd9dc6f717fc4bb0a4e77b2212f317c8f318e03",,camilesing,jark,sjwiesman,t0ugh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 26 04:23:49 UTC 2021,,,,,,,,,,"0|z0kupc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 02:27;t0ugh;Hi, [~sjwiesman]! I am very interested in this job, can you assign it to me?;;;","27/Nov/20 06:11;t0ugh;Hi, [~sjwiesman]! I'm sorry. I am a newcomer.I just finished the work and pull request without assigned. ;;;","22/Apr/21 11:00;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:02;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","25/Aug/21 12:01;camilesing;Hi [~jark], I find this job hasn't changed for a long time. I think I can do it, can you assign it to me?;;;","26/Aug/21 04:23;jark;Sorry [~camilesing], this page has been translated by other tickets. I will close this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File Source lost data when reading from directories created by FileSystemTableSink with JSON format,FLINK-20295,13342092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,gaoyunhaii,gaoyunhaii,23/Nov/20 10:02,25/Nov/20 10:46,13/Jul/23 08:12,25/Nov/20 06:38,,,,,,1.12.0,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"When testing the compaction functionality of the FileSystemTableSink, I found that when using json format, the produced directories could not be read correctly by the file source, namely only a part of records are read.


By checking the produced directories, the number of the records in it is the same as expected, thus it seems to be the issue of the source side.

 

The issue only exists for JSON format.

The data is produced by [FileCompactionTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionTest.java] and read by  [FileCompactionCheckTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionCheckTest.java] . An example directories tar file of 8000 records are also attached.

 ",,aljoscha,gaoyunhaii,jark,lzljs3620320,maguowei,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20340,,,,,,FLINK-20122,,,,,"23/Nov/20 10:02;gaoyunhaii;compaction.tgz;https://issues.apache.org/jira/secure/attachment/13015842/compaction.tgz",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 10:45:33 UTC 2020,,,,,,,,,,"0|z0kubc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 15:32;sewen;Thanks for reporting this. Is it possible to have a minimal example that reproduces this problem?;;;","24/Nov/20 00:41;gaoyunhaii;Ok, no problem, I'll implement a minimal example.;;;","24/Nov/20 04:20;gaoyunhaii;Hi [~sewen], I submitted the minimal example: [The minimal example|https://github.com/gaoyunhaii/flink/commit/8874b3494dc25bda1859d332a301771ff98238c3] .

I also debugged the case and found that it should be due to that _DeserializationSchemaAdapter_ always returned the same cached iterator for different batches, thus it might be the data get override before it is fully emitted. Returns different iterators should be able to solve this issue. ;;;","24/Nov/20 06:09;lzljs3620320;[~gaoyunhaii] Thanks for your debugging and research, {{HiveBulkFormatAdapter}} also has this problem. I and [~lirui] will fix this.;;;","24/Nov/20 06:25;gaoyunhaii;OK, very thanks [~lzljs3620320] and [~lirui] for attending this issue!;;;","24/Nov/20 13:35;sewen;Thanks for debugging this. So this is a bug in the {{DeserializationFormatAdapter}} implementation?

[~lzljs3620320] A thought on the {{DeserializationFormatAdapter}} implementation:

  - I think it makes sense to use a StreamRecordFormat here. Then you also don't need to worry about batching.
  - We can also use Java's {{BufferedReader(InputStreamReader())}} to parse the lines. That is a bit less performant than out own fast parsing DelimitedInputFormat, but it supports different charset encodings properly. Currently, the DelimitedInputFormat fails on UTF-16 and some other charsets.;;;","25/Nov/20 06:38;lzljs3620320;master (1.12): 07a449e2f68384007cad61c10d0dcbbe0cf89c26;;;","25/Nov/20 06:51;lzljs3620320;[~sewen] Yes, it is a bug in the {{DeserializationFormatAdapter}} implementation.

Yes, I've also thought about using StreamFormat, but we don't have splittable StreamFormat implementation at present. In order to avoid functional regression, I still write DelimitedInputFormat.

I will create a Jira to remove DelimitedInputFormat in {{DeserializationFormatAdapter}};;;","25/Nov/20 09:09;sewen;StreamRecordFormats can be splittable. I think the biggest problem is how to do the line parsing in a way that supports charsets properly. -Even for UTF-8, just searching byte-wise for a '\n' character leads to wrong results due to multi-byte code points.- The current DelimitedInputFormat cannot handle various cases.;;;","25/Nov/20 09:26;lzljs3620320;Let me continue to investigate, I remember that UTF-8 is OK, because the coding will ensure the uniqueness of each byte.

I quoted your words in FLINK-20340.;;;","25/Nov/20 10:45;sewen;[~lzljs3620320] - I think you are right, UTF-8 is actually okay. UTF-16 was a big issue, though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the document about table formats overlap in user fat jar,FLINK-20292,13342082,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,gaoyunhaii,gaoyunhaii,23/Nov/20 09:14,03/Dec/20 08:10,13/Jul/23 08:12,03/Dec/20 08:10,,,,,,1.12.0,,,Documentation,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,,"When testing the Flink 1.12 in a standalone mode cluster, I found that if the user job jar contains both _flink-avro_ and _flink-parquet/flink-orc_, the FileSystemTableSink would not be able to load the corresponding format factory correctly. But if only one format is dependent it works.

The test project located in [here|https://github.com/gaoyunhaii/flink1.12test] and the test class is [FileCompactionTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionTest.java].

The conflict does not seem to affect the local runner, but only has problem when submitted to the standalone cluster.

If the problem does exists, we might need to fix it or give user some tips about the conflicts. ",,gaoyunhaii,leonard,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20122,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 08:10:30 UTC 2020,,,,,,,,,,"0|z0ku94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 18:28;rmetzger;[~lzljs3620320] Can you take a look at this problem? ;;;","24/Nov/20 03:04;lzljs3620320;Hi [~gaoyunhaii], what is the exception message?;;;","24/Nov/20 03:11;lzljs3620320;I think this is because the packaging method.

In table, we use java SPI to load Factory, so if you use jar-with-deps, the factories will overlap each other.

The recommend way is using shaded and ServicesResourceTransformer transformer just like in Flink/pom.xml.;;;","24/Nov/20 03:12;lzljs3620320;This is a known problem and we can improve the documentation to alert users. what do you think? [~gaoyunhaii];;;","24/Nov/20 03:19;lzljs3620320;CC: [~jark];;;","24/Nov/20 03:35;gaoyunhaii;Very thanks [~lzljs3620320] for the explanation! And adding the transformer to the shaded plugin configuration did solve this issue:
{code:java}
<!-- The service transformer is needed to merge META-INF/services files -->
<transformer implementation=""org.apache.maven.plugins.shade.resource.ServicesResourceTransformer""/>
{code}
I agree with that we could improve the document to alert users. Also, I saw that Flink's project architecture _flink-walkthrough-datastream-java_ also do not have this configuration, it might be also better to add the configuration in the architecture. ;;;","24/Nov/20 03:39;lzljs3620320;Thanks for the confirmation. I think _flink-walkthrough-datastream-java_ is for DataStream, so it does not have this configuration. Maybe we can have a _flink-walkthrough-table-java._;;;","03/Dec/20 08:10;lzljs3620320;master (1.13): 25b07f158f48e2db6fc844834176ae285489d1d3

release-1.12: b1eb0d11b0fac8c0292324dce54fa37ce46ecb0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the exception message of FileSystemTableSink when missing format dependencies,FLINK-20291,13342076,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,gaoyunhaii,gaoyunhaii,23/Nov/20 08:56,25/Nov/20 03:58,13/Jul/23 08:12,25/Nov/20 03:58,,,,,,1.12.0,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,1,pull-request-available,,,,,"Current when the format factory failed to load, the following exception would be thrown:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.sink'.

Table options are:

'auto-compaction'='true'
'connector'='filesystem'
'format'='csv'
'path'='file:///tmp/compaction'
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:166)
  at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:362)
  at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:220)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.Iterator$class.foreach(Iterator.scala:891)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:674)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:757)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
  at FileCompactionTest.main(FileCompactionTest.java:147)
Caused by: org.apache.flink.table.api.ValidationException: Please implement at least one of the following formats: BulkWriter.Factory, SerializationSchema, FileSystemFormatFactory.
  at org.apache.flink.table.filesystem.FileSystemTableSink.<init>(FileSystemTableSink.java:124)
  at org.apache.flink.table.filesystem.FileSystemTableFactory.createDynamicTableSink(FileSystemTableFactory.java:83)
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:163)
  ... 18 more
{code}
 

We might directly advice users to check if the format dependency is added or if there are package conflicts so that users would fix this issue faster.

 ",,gaoyunhaii,jark,leonard,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20122,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 03:58:28 UTC 2020,,,,,,,,,,"0|z0ku7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/20 03:58;jark;Fixed in master (1.12.0): 264f2e9fafb5aa692640db3248498cafd62aa7d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated output in FileSource continuous ITCase with TM failover,FLINK-20290,13342066,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,azagrebin,azagrebin,23/Nov/20 08:04,03/Dec/20 09:07,13/Jul/23 08:12,30/Nov/20 17:51,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,,"If FileSourceTextLinesITCase::testContinuousTextFileSource includes TM restarts (after failing TM with TestingMiniCluster::terminateTaskExecutor, see testContinuousTextFileSourceWithTaskManagerFailover in [branch|https://github.com/azagrebin/flink/tree/FLINK-20118-it]) then sometimes I observe duplicated lines in the output after running the whole test suite FileSourceTextLinesITCase 5-10 times in IDE:
{code:java}
Test testContinuousTextFileSourceWithTaskManagerFailover(org.apache.flink.connector.file.src.FileSourceTextLinesITCase) failed with:
java.lang.AssertionError: 
Expected: [""And by opposing end them?--To die,--to sleep,--"", ""And enterprises of great pith and moment,"", ""And lose the name of action.--Soft you now!"", ""And makes us rather bear those ills we have"", ""And thus the native hue of resolution"", ""Be all my sins remember'd."", ""But that the dread of something after death,--"", ""Devoutly to be wish'd. To die,--to sleep;--"", ""For in that sleep of death what dreams may come,"", ""For who would bear the whips and scorns of time,"", ""Is sicklied o'er with the pale cast of thought;"", ""Must give us pause: there's the respect"", ""No more; and by a sleep to say we end"", ""No traveller returns,--puzzles the will,"", ""Or to take arms against a sea of troubles,"", ""Than fly to others that we know not of?"", ""That flesh is heir to,--'tis a consummation"", ""That makes calamity of so long life;"", ""That patient merit of the unworthy takes,"", ""The fair Ophelia!--Nymph, in thy orisons"", ""The heartache, and the thousand natural shocks"", ""The insolence of office, and the spurns"", ""The oppressor's wrong, the proud man's contumely,"", ""The pangs of despis'd love, the law's delay,"", ""The slings and arrows of outrageous fortune"", ""The undiscover'd country, from whose bourn"", ""Thus conscience does make cowards of us all;"", ""To be, or not to be,--that is the question:--"", ""To grunt and sweat under a weary life,"", ""To sleep! perchance to dream:--ay, there's the rub;"", ""When he himself might his quietus make"", ""When we have shuffled off this mortal coil,"", ""Whether 'tis nobler in the mind to suffer"", ""With a bare bodkin? who would these fardels bear,"", ""With this regard, their currents turn awry,""]
     but: was [""And by opposing end them?--To die,--to sleep,--"", ""And enterprises of great pith and moment,"", ""And lose the name of action.--Soft you now!"", ""And makes us rather bear those ills we have"", ""And thus the native hue of resolution"", ""Be all my sins remember'd."", ""But that the dread of something after death,--"", ""Devoutly to be wish'd. To die,--to sleep;--"", ""Devoutly to be wish'd. To die,--to sleep;--"", ""For in that sleep of death what dreams may come,"", ""For who would bear the whips and scorns of time,"", ""Is sicklied o'er with the pale cast of thought;"", ""Must give us pause: there's the respect"", ""No more; and by a sleep to say we end"", ""No more; and by a sleep to say we end"", ""No traveller returns,--puzzles the will,"", ""Or to take arms against a sea of troubles,"", ""Than fly to others that we know not of?"", ""That flesh is heir to,--'tis a consummation"", ""That flesh is heir to,--'tis a consummation"", ""That makes calamity of so long life;"", ""The fair Ophelia!--Nymph, in thy orisons"", ""The heartache, and the thousand natural shocks"", ""The heartache, and the thousand natural shocks"", ""The slings and arrows of outrageous fortune"", ""The undiscover'd country, from whose bourn"", ""Thus conscience does make cowards of us all;"", ""To be, or not to be,--that is the question:--"", ""To grunt and sweat under a weary life,"", ""To sleep! perchance to dream:--ay, there's the rub;"", ""To sleep! perchance to dream:--ay, there's the rub;"", ""When we have shuffled off this mortal coil,"", ""Whether 'tis nobler in the mind to suffer"", ""With a bare bodkin? who would these fardels bear,"", ""With this regard, their currents turn awry,""]
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.verifyResult(FileSourceTextLinesITCase.java:198)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSource(FileSourceTextLinesITCase.java:151)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSourceWithTaskManagerFailover(FileSourceTextLinesITCase.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}",,azagrebin,becket_qin,dian.fu,sewen,stevenz3wu,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20118,,,,,,,,,,,,,,,,FLINK-20309,,,,,,,,FLINK-20465,FLINK-20427,,,,"23/Nov/20 13:04;TsReaper;collect-debug-jm.png;https://issues.apache.org/jira/secure/attachment/13015847/collect-debug-jm.png","23/Nov/20 12:38;TsReaper;collect-debug.png;https://issues.apache.org/jira/secure/attachment/13015845/collect-debug.png","23/Nov/20 12:38;azagrebin;log;https://issues.apache.org/jira/secure/attachment/13015846/log",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 17:51:18 UTC 2020,,,,,,,,,,"0|z0ku5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 09:37;azagrebin;FYI, I am not sure about the semantics or correctness of CollectResultIterator in DataStreamUtils::collectRecordsFromUnboundedStream. I would assume that it should support exactly once (see CollectResultIterator's [PR|https://github.com/apache/flink/pull/12073#issue-415916681]) and it looks like it deals with checkpoints. When I try to introduce JM leadership failovers (FileSourceTextLinesITCase::testContinuousTextFileSourceWithJobManagerFailover, [branch|https://github.com/azagrebin/flink/tree/FLINK-20118-it-jm]), I get the following failure:


{code:java}
Test testContinuousTextFileSourceWithJobManagerFailover(org.apache.flink.connector.file.src.FileSourceTextLinesITCase) failed with:
java.util.NoSuchElementException
	at java.util.LinkedList.removeLast(LinkedList.java:283)
	at org.apache.flink.streaming.api.operators.collect.AbstractCollectResultBuffer.revert(AbstractCollectResultBuffer.java:114)
	at org.apache.flink.streaming.api.operators.collect.CheckpointedCollectResultBuffer.sinkRestarted(CheckpointedCollectResultBuffer.java:37)
	at org.apache.flink.streaming.api.operators.collect.AbstractCollectResultBuffer.dealWithResponse(AbstractCollectResultBuffer.java:87)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:140)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77)
	at org.apache.flink.streaming.api.datastream.DataStreamUtils.collectRecordsFromUnboundedStream(DataStreamUtils.java:142)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSource(FileSourceTextLinesITCase.java:186)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSourceWithJobManagerFailover(FileSourceTextLinesITCase.java:152)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}

As if non-checkpointed stuff gets returned and then this non-checkpointed output cannot be reverted after the sink restart to the latest checkpoint offset, maybe because it has been already returned. I am still looking into this whether it is a real problem (same or another issue) or a test setup issue.

cc [~TsReaper] [~ykt836] [~sjwiesman] It would be nice to confirm what is the intended behaviour. If DataStream::executeAndCollect does not support exactly once then I suggest to document this.;;;","23/Nov/20 12:29;sewen;[~azagrebin] Is this an issue that only occurs at JM failover, or can this also happen during TM failover and explain this test result?;;;","23/Nov/20 12:40;TsReaper;[~azagrebin] [~sewen] Collect iterator should support exactly once if both the checkpoint interval and the checkpoint mode is set. It seems to me that the checkpoint state for sink and source is inconsistent.

{{CollectSinkFunction}} will write what it has received into its state and read it out when it is initialized (that is to say, when the task containing the sink restarts). See {{CollectSinkFunction#initializeState}} and {{CollectSinkFunction#snapshotState}} for details. I've printed out what is stored into the state and what is read from the state and the result is presented as follows. The ""invoke"" log below indicates that the sink receives the record:

 !collect-debug.png! 

I've prepared 10 different strings (from ""aaaaaaa"" to ""jjjjjjj"") for this test. As is shown in the graph, ""bbbbbbb"" is already in the checkpointed state (It appears in the buffer when initializing). However this record is received again (invoke: bbbbbbb). This indicates that this record is sent to the sink even if it should have been checkpointed and should not be sent.

So I suspect the bug lies either in the checkpointing of the collect sink or in that of the file source, or the implementation of the test is incorrect (and, shamelessly, I suspect more on the source / test side as collect sink has a very strong random test for checkpointing and such). Could you check both of the code?

I also notice that this bug will only happen when the checkpointing actually succeeds, but the operators are not notified by the {{notifyCheckpointComplete}} due to the TM restart.;;;","23/Nov/20 12:55;azagrebin;[~sewen] The TM and JM failures always result in different test failures in my experiments as reported in this issue, so they can be actually different issues (then JM failure case needs another ticket). It was just my suspicion that something is wrong with CollectResultIterator/checkpointing leading to duplicates for TM but it is not clear to me yet.;;;","23/Nov/20 13:07;TsReaper;The job manager failure problem seems to me that the state has somehow “lost”.

 !collect-debug-jm.png! 

{{CollectSinkFunction#snapshotState}} will write {{offset}} into a state called {{offsetState}}. This state will be read when the sink is initialized (see {{CollectSinkFunction#initializeState}}). In the image above. {{CPOK: 1}} indicates that {{offsetState}} is filled with number 1. However when the sink is initialized again this number goes back to 0.;;;","23/Nov/20 14:46;azagrebin;Sorry for confusion, I think I misconfigured the JM failure case. Let's focus on the originally reported TM failure case.;;;","23/Nov/20 14:54;azagrebin;I think that the attached log file probably confirms the [~TsReaper]'s observation about duplicated output from source. The attached log file shows that the duplicates come from 'nested1/text.1' file (ID=0000000089) and it is assigned and finished reading two times before the TM failure:
{code:java}
 4063 [SourceCoordinator-Source: file-source] INFO  org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner - Assigning remote split to requesting host 'localhost': Optional[FileSourceSplit: file:/var/folders/67/          v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7737023041992368052/junit2962518782319371379/nested1/text.1 [0, 225) (no host info) ID=0000000089 position=null]
 4065 [Source: file-source (2/4)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase - Adding split(s) to reader: [FileSourceSplit: file:/var/folders/67/v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7737023041992368052/        junit2962518782319371379/nested1/text.1 [0, 225) (no host info) ID=0000000089 position=null]
 4065 [jobmanager-future-thread-5] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 25 for job c2a549b2a4e01f6ab22149b688200de6 (1832 bytes in 13 ms).
 4065 [Source Data Fetcher for Source: file-source (2/4)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Starting split fetcher 0
 4065 [SourceCoordinator-Source: file-source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Marking checkpoint 25 as completed for source Source: file-source.
 4066 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Triggering checkpoint 26 (type=CHECKPOINT) @ 1606134907136 for job c2a549b2a4e01f6ab22149b688200de6.
 4068 [Source Data Fetcher for Source: file-source (2/4)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Finished reading from splits [0000000089]
 4068 [Source: file-source (2/4)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase - Finished reading split(s) [0000000089]
{code}
and after the TM failure:
{code:java}
 5149 [SourceCoordinator-Source: file-source] INFO  org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner - Assigning remote split to requesting host 'localhost': Optional[FileSourceSplit: file:/var/folders/67/          v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7737023041992368052/junit2962518782319371379/nested1/text.1 [0, 225) (no host info) ID=0000000089 position=null]
 5149 [Source Data Fetcher for Source: file-source (4/4)#1] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Split fetcher 0 exited.
 5150 [SourceCoordinator-Source: file-source] INFO  org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner - Assigning remote split to requesting host 'localhost': Optional[FileSourceSplit: file:/var/folders/67/          v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7737023041992368052/junit2962518782319371379/text.1 [0, 219) (no host info) ID=0000000095 position=null]
 5151 [Source: file-source (2/4)#1] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase - Adding split(s) to reader: [FileSourceSplit: file:/var/folders/67/v4yp_42d21j6_n8k1h556h0c0000gn/T/junit7737023041992368052/        junit2962518782319371379/nested1/text.1 [0, 225) (no host info) ID=0000000089 position=null]
 5152 [Source Data Fetcher for Source: file-source (2/4)#1] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Starting split fetcher 1
 5152 [Source Data Fetcher for Source: file-source (2/4)#1] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Finished reading from splits [0000000089]
 5153 [Source: file-source (2/4)#1] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase - Finished reading split(s) [0000000089]
{code};;;","23/Nov/20 17:38;becket_qin;I am not sure if the following sequence was what happened, but it seems possible to me.

Assuming there are 2 TM: TM_1 (SourceReader_1) and TM_2 (SourceReader_2).

The following sequence may cause duplicate split assignment.
 # The split enumerator assigned a split *S* to the TM_1.
 # The `SourceCoordinator` tracks this assignment as a pending assignment which may be returned back to the split enumerator in case of reader failure before checkpoint.
 # Checkpoint 1 is triggered. 
 # TM_1 finishes the checkpoint, which contains *S* in its state.
 # TM_1 failed, then SourceCoordinator.subtaskFailed() is invoked. Because Checkpoint 1 has not completed, the SourceCoordinator adds the split back to the SplitEnumerator.
 # TM_2 finishes the checkpoint, which completes Checkpoint 1. If TM_1 did not fail, *S* would be removed from the pending assignment because TM_1 already checkpointed the assignment. However, in this case, *S* was returned to SplitEnumerator which will cause double assignment in next steps.
 # TM_1 recovers and continue reading from the split *S* from its own checkpoint.
 # After finishes *S*, TM_1 requests for a new split from the SplitEnumerator and got assigned *S* again.

I am actually not sure and curious whether Checkpoint 1 will be considered as successful in this case. Will the JM wait until the ongoing checkpoint to finish before restarting TM_1 so TM_1 will be restored to Checkpoint 1? In the above case, the checkpoint has succeeded for TM_1, but effectively failed for the SourceCoordinator.

[~sewen] Would you help confirm if the above guess is possible?;;;","23/Nov/20 22:11;sewen;I think the sequence like that cannot happen, currently, because checkpoints are eagerly aborted as soon as one task goes into recovery.
So either TM fails before the checkpoint is complete, and the checkpoint never completes, of it fails after the checkpoint completes. The scheduler processes messages single-threaded in order, to they cannot overtake each other.

However, it is possible in the future that we only let a task failure fail a checkpoint if we are still waiting for an ack from that task.
In that case, the situation you described above is possible.

So to be safe, we should do the following:
  - remember splits assigned per checkpoint
  - on checkpoint, mark them as pending for that checkpoint
  - if a pending split is returned, stage it until we know what happened to the checkpoint.
  - staged splits are added back on checkpoint abort, and ignored on checkpoint completion.
;;;","23/Nov/20 22:11;sewen;We still need to look for a different explanation for this bug, though.;;;","29/Nov/20 21:51;sewen;Update: A sequence similar to the one described by [~becket_qin] is possible, where the race is between checkpoint finalization in the Checkpoint Coordinator, and the subtask failure notifications. See FLINK-20396 for details.

 ;;;","30/Nov/20 17:51;sewen;Fixed with the resolution of FLINK-20396 and FLINK-20413;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LazyFromSourcesSchedulingStrategy is possible to schedule non-CREATED vertices,FLINK-20285,13342031,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,zhuzh,zhuzh,23/Nov/20 04:06,24/Nov/20 07:08,13/Jul/23 08:12,24/Nov/20 07:08,1.11.0,,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"LazyFromSourcesSchedulingStrategy is possible to schedule vertices which are not in CREATED state. This will lead result in unexpected check failure and result in fatal error (see attached error).

The reason is that the status of a vertex to schedule was changed in LazyFromSourcesSchedulingStrategy#allocateSlotsAndDeployExecutionVertices() during the invocation of schedulerOperations.allocateSlotsAndDeploy(...) on other vertices.

e.g. ev1 and ev2 are in the same pipelined region and are restarted one by one in the scheduling loop in LazyFromSourcesSchedulingStrategy#allocateSlotsAndDeployExecutionVertices(). They are all CREATED at the moment. ev1 is scheduled first but it immediately fails due to some slot allocation error and ev2 will be canceled as a result. So when ev2 is scheduled, its state would be CANCELED and the state check failed.

More details see FLINK-20220.

{code:java}
2020-11-19 13:34:17,231 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.IllegalStateException: expected vertex aafcbb93173905cec9672e46932d7790_3 to be in CREATED state, was: CANCELED
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:708) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:687) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_222]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11.2.jar:1.11.2]
Caused by: java.lang.IllegalStateException: expected vertex aafcbb93173905cec9672e46932d7790_3 to be in CREATED state, was: CANCELED
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:217) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$validateDeploymentOptions$3(DefaultScheduler.java:326) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_222]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_222]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_222]
        at java.util.Collections$2.tryAdvance(Collections.java:4719) ~[?:1.8.0_222]
        at java.util.Collections$2.forEachRemaining(Collections.java:4727) ~[?:1.8.0_222]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_222]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_222]
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_222]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_222]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_222]
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_222]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.validateDeploymentOptions(DefaultScheduler.java:326) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:297) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.strategy.LazyFromSourcesSchedulingStrategy.allocateSlotsAndDeployExecutionVertices(LazyFromSourcesSchedulingStrategy.java:140) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.strategy.LazyFromSourcesSchedulingStrategy.restartTasks(LazyFromSourcesSchedulingStrategy.java:93) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$restartTasks$2(DefaultScheduler.java:265) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_222]
        ... 24 more
{code}
",,rmetzger,trohrmann,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 07:08:28 UTC 2020,,,,,,,,,,"0|z0ktxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 06:06;xtsong;[~zhuzh]

Thanks for reporting this issue. This indeed sounds should be a release blocker to me.

Any plan how and when this issue can be fixed?;;;","23/Nov/20 06:31;zhuzh;I'd like to fix it by changing LazyFromSourcesSchedulingStrategy#allocateSlotsAndDeployExecutionVertices() to put the vertex filtering and allocateSlotsAndDeploy() invocation in one same loop.
The fix PR will be opened today.;;;","23/Nov/20 06:43;xtsong;Thanks for the update. Sounds good to me.;;;","24/Nov/20 07:08;zhuzh;fixed via:
1.12&master: d4a54673fe73431bbb009185335d6f393ed8f69d
1.11: 3854c303e7910c639c8cfaec8ea8f0c70a61c898;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error happens in TaskExecutor when closing JobMaster connection if there was a python UDF,FLINK-20284,13342030,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,zhuzh,zhuzh,23/Nov/20 03:37,26/Nov/20 02:49,13/Jul/23 08:12,26/Nov/20 01:09,1.11.0,1.12.0,,,,1.11.3,1.12.0,,API / Python,,,,,0,pull-request-available,,,,,"When a TaskExecutor successfully finished running a python UDF task and disconnecting from JobMaster, errors below will happen. This error, however, seems not affect job execution at the moment.

{code:java}
2020-11-20 17:05:21,932 INFO  org.apache.beam.runners.fnexecution.logging.GrpcLoggingService [] - 1 Beam Fn Logging clients still connected during shutdown.
2020-11-20 17:05:21,938 WARN  org.apache.beam.sdk.fn.data.BeamFnDataGrpcMultiplexer        [] - Hanged up for unknown endpoint.
2020-11-20 17:05:22,126 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> select: (f0) -> select: (add_one(f0) AS a) -> to: Tuple2 -> Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51) switched from RUNNING to FINISHED.
2020-11-20 17:05:22,126 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Custom Source -> select: (f0) -> select: (add_one(f0) AS a) -> to: Tuple2 -> Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51).
2020-11-20 17:05:22,128 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source -> select: (f0) -> select: (add_one(f0) AS a) -> to: Tuple2 -> Sink: Streaming select table sink (1/1)#0 b0c2104dd8f87bb1caf0c83586c22a51.
2020-11-20 17:05:22,156 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: b67c3307dcf93757adfb4f0f9f7b8c7b, jobId: d05f32162f38ec3ec813c4621bc106d9).
2020-11-20 17:05:22,157 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job d05f32162f38ec3ec813c4621bc106d9 from job leader monitoring.
2020-11-20 17:05:22,157 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job d05f32162f38ec3ec813c4621bc106d9.
2020-11-20 17:05:23,064 ERROR org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?
java.lang.NoClassDefFoundError: org/apache/beam/vendor/grpc/v1p26p0/io/netty/util/concurrent/GlobalEventExecutor$2
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.startThread(GlobalEventExecutor.java:227) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.execute(GlobalEventExecutor.java:215) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:498) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:96) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1089) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_261]
Caused by: java.lang.ClassNotFoundException: org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor$2
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_261]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_261]
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:72) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:49) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_261]
        ... 11 more
{code}
",,dian.fu,hxbks2ks,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20116,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 02:49:42 UTC 2020,,,,,,,,,,"0|z0ktxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 03:38;zhuzh;cc [~dian.fu];;;","23/Nov/20 03:45;dian.fu;[~hxbks2ks] Could you help to take a look at this issue?;;;","25/Nov/20 09:56;hxbks2ks;[~zhuzh] [~dian.fu]  Thanks a lot for reporting this issue.

This error will not affect the correctness of the result. This is caused by not releasing the EventLoopGroup resource of the netty server correctly.

This problem has been reported by users since Beam release 2.8.0 https://issues.apache.org/jira/browse/BEAM-5397, but it has not been resolved. After investigation, I found that the reason for the problem is that Grpc releases the related resources of Netty Server by schedule tasks started by 1s. https://github.com/grpc/grpc-java/blob/master/core/src/main/java/io/grpc/internal/SharedResourceHolder.java#L150. After 1s, when the schedule Task starts to reclaim resources, the related classes have been unloaded by the UserClassLoader, so ClassNotFoundError is thrown.;;;","26/Nov/20 01:09;dian.fu;Fixed in
- master via a0b2453e49401e49e386f18be67aa9b7e60c3e8d and bf2c13d7f426e4dd768d2dd37411a9d1bfe10d08
- release-1.11 via 3b81ec8094b9ac564b50aa4a14a752bb1bbda04c and 6229c7bda01039db53dfb10f7271b57a678d32ba;;;","26/Nov/20 02:49;zhuzh;Thanks for the investigation and fixing this problem! [~hxbks2ks];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-1.11.2 ContinuousFileMonitoringFunction cannot restore from failure,FLINK-20277,13342017,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,hiscat,hiscat,23/Nov/20 02:07,24/Nov/20 02:37,13/Jul/23 08:12,24/Nov/20 02:37,1.11.2,,,,,1.11.3,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,,"流式消费Hive表，出现失败时，任务无法正常恢复，一直重启。

一直报错：The ContinuousFileMonitoringFunction has already restored from a previous Flink version.

 

{color:#FF0000}java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/rawdata/db/bw_hana/sapecc/hepecc_ekko_cut{color}
 at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1270) ~[hadoop-hdfs-2.6.0-cdh5.16.2.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1262) ~[hadoop-hdfs-2.6.0-cdh5.16.2.jar:?]
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0-cdh5.16.2.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1262) ~[hadoop-hdfs-2.6.0-cdh5.16.2.jar:?]
 at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:85) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:588) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.getInputSplitsSortedByModTime(ContinuousFileMonit
 oringFunction.java:279) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.monitorDirAndForwardSplits(ContinuousFileMonitori
 ngFunction.java:251) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.run(ContinuousFileMonitoringFunction.java:215) ~[
 flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213) ~[flink-dist_2
 .11-1.11.2.jar:1.11.2]

 

 

2020-11-23 05:00:33,313 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Split Reader: HiveFileMonitoringFunction -> S
 ink: Sink(table=[default_catalog.default_database.kafka_hepecc_ekko_cut_json], fields=[mandt, ebeln, bukrs, bstyp, bsart, bsakz, loekz, statu
 , aedat, ernam, pincr, lponr, lifnr, spras, zterm, zbd1t, zbd2t, zbd3t, zbd1p, zbd2p, ekorg, ekgrp, waers, wkurs, kufix, bedat, kdatb, kdate,
 bwbdt, angdt, bnddt, gwldt, ausnr, angnr, ihran, ihrez, verkf, telf1, llief, kunnr, konnr, abgru, autlf, weakt, reswk, lblif, inco1, inco2, 
 ktwrt, submi, knumv, kalsm, stafo, lifre, exnum, unsez, logsy, upinc, stako, frggr, frgsx, frgke, frgzu, frgrl, lands, lphis, adrnr, stceg_l,
 stceg, absgr, addnr, kornr, memory, procstat, rlwrt, revno, scmproc, reason_code, memorytype, rettp, retpc, dptyp, dppct, dpamt, dpdat, msr_
 id, hierarchy_exists, threshold_exists, legal_contract, description, release_date, force_id, force_cnt, reloc_id, reloc_seq_id, source_logsys
 , auflg, yxcort, ysyb, ypsfs, yxqlx, yjplx, ylszj, yxdry, yxdrymc, ylbid1, yqybm, ysxpt_order, yy_write_if, fanpfg, yresfg, yrcofg, yretxt, y
 ...skipping...
 {color:#FF0000}java.lang.IllegalArgumentException: The ContinuousFileMonitoringFunction has already restored from a previous Flink version.{color}
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.initializeState(ContinuousFileMonitoringFunction.java:176) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:185) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:167) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:106) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:258) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:479) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:475) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:528) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_251]

 ",,godfreyhe,hiscat,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 02:37:13 UTC 2020,,,,,,,,,,"0|z0ktuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 02:29;xtsong;[~hiscat]

Thanks for reporting this. I'd like to kindly remind that the Apache Flink community is an international open source community, and it is important to communicate in English.

[~godfreyhe]

Could you help check on this issue?;;;","23/Nov/20 04:13;godfreyhe;[~hiscat] Thanks for reporting this, I would like to take this ticket.;;;","23/Nov/20 06:23;hiscat;Sorry, I'll pay attention next time.

I have a job that needs to read the Hive table once a minute. When the task fails, the job keeps restarting, keeps throwing exceptions, and is unrecoverable. It seems that this exception is thrown when there is no file under the hive table, and after the exception is thrown, the state cannot be recovered.

 ;;;","23/Nov/20 09:27;godfreyhe;In 1.11, the reason is {{HiveTableSource}} uses {{ContinuousFileMonitoringFunction}} with given {{globalModificationTime}}, while in {{ContinuousFileMonitoringFunction}} the state restore logic always checks whether {{globalModificationTime}} is equal to {{Long.MIN_VALUE}}. The correct logic should be checking whether the {{globalModificationTime}} is equal to the initialized {{globalModificationTime}}.

In master, after [FLINK-19888|https://issues.apache.org/jira/browse/FLINK-19888] finished, {{HiveTableSource}} does not depend on {{ContinuousFileMonitoringFunction}} any more, we can revert the changes of [[FLINK-17435]] about the {{ContinuousFileMonitoringFunction}} part;;;","23/Nov/20 09:43;godfreyhe;I create another JIRA to fix the master, https://issues.apache.org/jira/browse/FLINK-20293;;;","24/Nov/20 02:37;godfreyhe;release-1.11: 39aea577c27bbac3c1905ddc944bb606235e33d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transparent DeCompression of streams missing on new File Source,FLINK-20276,13341997,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,22/Nov/20 16:19,22/Nov/20 22:55,13/Jul/23 08:12,22/Nov/20 19:59,,,,,,1.12.0,,,Connectors / FileSystem,,,,,0,,,,,,"The existing {{FileInputFormat}} applies decompression (gzip, xy, ...) automatically on the file input stream, based on the file extension.

We need to add similar functionality for the {{StreamRecordFormat}} of the new FileSource to be on par with this functionality.

This can be easily applied in the {{StreamFormatAdapter}} when opening the file stream.",,gyfora,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 22 22:55:37 UTC 2020,,,,,,,,,,"0|z0ktq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/20 18:38;gyfora;Some compressed formats like Bzip2 are splittable at block boundaries (when using certain codecs like Hadoop's bzip2 codec) but this seems to be fairly tricky to integrate with the current FileInputFormat. The problem is that the InputFormat itself tracks the read number of bytes instead of getting the actual offsets of the compressed file splits.

I wonder if this is something that is worth thinking about at this point (for the new File Source) or we can simply deal with it later. What do you think [~sewen]?;;;","22/Nov/20 19:59;sewen;Fixed in 1.12.0 (master) via fea57e5e0a818b76586db1995b7ceeb6f08fe88f;;;","22/Nov/20 22:55;sewen;[~gyfora] This sounds like a nice one to integrate. But let's make this a separate effort - this ticket here was only trying to get the status quo fixed on the new file source api, so that there is no regression.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment should be ; ",FLINK-20275,13341992,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,csq,csq,22/Nov/20 15:40,23/Nov/20 01:39,13/Jul/23 08:12,23/Nov/20 01:39,,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,"Currently, the path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment is "","", this would cause the rest client fail to upload the specified jars and stuck forever without errors. It should be "";"" instead.",,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20135,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 01:39:25 UTC 2020,,,,,,,,,,"0|z0ktp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 01:39;dian.fu;Fixed in master(1.12.0) via e245eb1e8108d318480e593b006a6eeb5e49a8da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Table api Kafka connector Sink Partitioner Document Error,FLINK-20273,13341970,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,22/Nov/20 08:08,07/Dec/20 15:28,13/Jul/23 08:12,07/Dec/20 15:28,1.12.0,,,,,1.12.1,1.13.0,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"The [doc|https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#sink-partitioning] tells us that the kafka sink uses fixed partitioner by default. However, in my local test, the it uses sticky partitioner to get the record partition id if key is not set.

You can add the test in the {{KafkaTableITCase}}, the code follows
{code:java}
public void testKafkaSourceSinkWithDefaultPartitioner() throws Exception {
		if (isLegacyConnector) {
			return;
		}
		// we always use a different topic name for each parameterized topic,
		// in order to make sure the topic can be created.
		final String topic = ""key_full_value_topic_"" + format;
		createTestTopic(topic, 3, 1);

		// ---------- Produce an event time stream into Kafka -------------------
		String groupId = standardProps.getProperty(""group.id"");
		String bootstraps = standardProps.getProperty(""bootstrap.servers"");

		// compared to the partial value test we cannot support both k_user_id and user_id in a full
		// value due to duplicate names after key prefix stripping,
		// fields are reordered on purpose,
		// fields for keys and values are overlapping
		final String createSourceTable = String.format(
				""CREATE TABLE kafkaSource (\n""
						+ ""  `user_id` BIGINT,\n""
						+ ""  `name` STRING,\n""
						+ ""  `partition` INT METADATA""
						+ "") WITH (\n""
						+ ""  'connector' = 'kafka',\n""
						+ ""  'topic' = '%s',\n""
						+ ""  'properties.bootstrap.servers' = '%s',\n""
						+ ""  'properties.group.id' = '%s',\n""
						+ ""  'scan.startup.mode' = 'earliest-offset',\n""
						+ ""  'format' = '%s'\n""
						+ "")"",
				topic,
				bootstraps,
				groupId,
				format);
		final String createSinkTable = String.format(
				""CREATE TABLE kafkaSink (\n""
						+ ""  `user_id` BIGINT,\n""
						+ ""  `name` STRING\n""
						+ "") WITH (\n""
						+ ""  'connector' = 'kafka',\n""
						+ ""  'topic' = '%s',\n""
						+ ""  'properties.bootstrap.servers' = '%s',\n""
						+ ""  'properties.group.id' = '%s',\n""
						+ ""  'scan.startup.mode' = 'earliest-offset',\n""
						+ ""  'format' = '%s'\n""
						+ "")"",
				topic,
				bootstraps,
				groupId,
				format);

		tEnv.executeSql(createSourceTable);
		tEnv.executeSql(createSinkTable);

		String initialValues = ""INSERT INTO kafkaSink\n""
									+ ""VALUES\n""
									+ "" (1, 'name 1'),\n""
									+ "" (2, 'name 2'),\n""
									+ "" (3, 'name 3')"";
		tEnv.executeSql(initialValues).await();

		initialValues = ""INSERT INTO kafkaSink\n""
				+ ""VALUES\n""
				+ "" (4, 'name 4'),\n""
				+ "" (5, 'name 5'),\n""
				+ "" (6, 'name 6')"";
		tEnv.executeSql(initialValues).await();

		initialValues = ""INSERT INTO kafkaSink\n""
				+ ""VALUES\n""
				+ "" (7, 'name 7'),\n""
				+ "" (8, 'name 8'),\n""
				+ "" (9, 'name 9')"";
		tEnv.executeSql(initialValues).await();


		// ---------- Consume stream from Kafka -------------------

		final List<Row> result = collectRows(tEnv.sqlQuery(""SELECT * FROM kafkaSource""), 9);



		// ------------- cleanup -------------------

		deleteTestTopic(topic);
	}
{code}
The test will use the kafka default partitioner and sends record to kafka topic. After insert, we can read the record with the parititon id. If it uses the fixed partitioner, all records will has the same partition id. I repeat the test 3 times and the results are
{code:java}
// the first result
<1,name 1,1>
<2,name 2,1>
<3,name 3,1>
<7,name 7,1>
<4,name 4,0>
<5,name 5,0>
<6,name 6,0>
<8,name 8,0>
<9,name 9,0>
// the second result
<1,name 1,1>
<2,name 2,1>
<3,name 3,1>
<4,name 4,0>
<5,name 5,0>
<6,name 6,0>
<7,name 7,0>
<8,name 8,0>
<9,name 9,0>
// the third result
<9,name 9,2>
<1,name 1,0>
<2,name 2,0>
<3,name 3,0>
<4,name 4,0>
<5,name 5,0>
<6,name 6,0>
<7,name 7,1>
<8,name 8,1>
{code}
The last column is the partition-id and we have 3 partitions in the test. The results show the default partitioner is sticky paritioner rather than fixed partitioner.

By the way, the sink partitioning section in the doc only works when the key is null. If we set the key fields, the {{round-robin}} strategy will not work.",,fsk119,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 15:28:44 UTC 2020,,,,,,,,,,"0|z0ktk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/20 14:05;jark;Thanks [~fsk119] for the investigation!;;;","07/Dec/20 15:28;jark;Fixed in 
 - master: d59e9f9748234e4dc4749f45863fe6e0a34b8961
 - release-1.12: 033b230a155ed188ca8051c5defcda016427591a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong result for TopN when removing records out of TopN,FLINK-20272,13341963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,fsk119,fsk119,22/Nov/20 06:10,26/Nov/20 02:10,13/Jul/23 08:12,24/Nov/20 11:36,1.11.0,1.12.0,,,,1.12.0,,,Table SQL / Planner,,,,,0,,,,,,"Add the following test in the {{RetractableTopNFunctionTest}}.

{code:java}
@Test
public void testCornerCase2() throws Exception {
	AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER, new ConstantRankRange(1, 2), false,
				false);
	OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
	testHarness.open();
	testHarness.processElement(insertRecord(""a"", 1L, 1));
	testHarness.processElement(insertRecord(""a"", 2L, 2));
	testHarness.processElement(insertRecord(""a"", 3L, 2));
	testHarness.processElement(insertRecord(""a"", 4L, 4));
	testHarness.processElement(insertRecord(""a"", 5L, 4));

	testHarness.processElement(deleteRecord(""a"", 4L, 4));
	testHarness.processElement(deleteRecord(""a"", 1L, 1));
	testHarness.processElement(deleteRecord(""a"", 2L, 2));
	testHarness.close();

	List<Object> expectedOutput = new ArrayList<>();
	expectedOutput.add(insertRecord(""a"", 1L, 1));
	expectedOutput.add(insertRecord(""a"", 2L, 2));
	expectedOutput.add(deleteRecord(""a"", 1L, 1));
	expectedOutput.add(insertRecord(""a"", 3L, 2));
	expectedOutput.add(deleteRecord(""a"", 2L, 2));
	expectedOutput.add(insertRecord(""a"", 5L, 4));
	assertorWithRowNumber.assertOutputEquals(""output wrong."", expectedOutput, testHarness.getOutput());
}
{code}

When the operator gets delete message, it will only delete the record whose current rank is in the range. If we keep deleting the message in the range, the operator will send the undeleted message to the sink.
",,fsk119,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20121,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 11:36:32 UTC 2020,,,,,,,,,,"0|z0ktio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 11:36;jark;Fixed in master (1.12.0): 2fd73c140e58bf9a38bf583d06b32843d299a78a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ArrayIndexOutOfBounds for RetractableTopNFunction,FLINK-20271,13341962,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,fsk119,fsk119,22/Nov/20 05:30,10/Dec/20 14:31,13/Jul/23 08:12,24/Nov/20 11:36,1.11.0,1.12.0,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"We can get the bug by adding the case into the UnitTest {{RetractableTopNFunctionTest}}


{code:java}
// Some comments here
	@Test
	public void testCornerCase() throws Exception {
		AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER, new ConstantRankRange(1, 3), false,
				false);
		OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
		testHarness.open();
		testHarness.processElement(insertRecord(""a"", 1L, 1));
		testHarness.processElement(insertRecord(""a"", 2L, 2));
		testHarness.processElement(insertRecord(""a"", 3L, 2));
		testHarness.processElement(insertRecord(""a"", 4L, 2));
		testHarness.processElement(insertRecord(""a"", 5L, 3));
		testHarness.processElement(insertRecord(""a"", 6L, 4));
		testHarness.processElement(updateBeforeRecord(""a"", 2L, 2));
		testHarness.close();
	}
{code}
",,fsk119,jark,jingzhang,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19633,,,,,,FLINK-18376,,,,,,,FLINK-20121,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 11:36:08 UTC 2020,,,,,,,,,,"0|z0ktig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 11:36;jark;Fixed in master (1.12.0): ebf5e08a03fcc7e8ba87d806e3eaad1e4e47d61d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the regression of missing ExternallyInducedSource support in FLIP-27 Source.,FLINK-20270,13341961,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,becket_qin,becket_qin,22/Nov/20 05:25,12/Dec/20 11:59,13/Jul/23 08:12,24/Nov/20 10:14,1.11.3,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,,,,,The current FLIP-27 design has a regression of missing the support of {{ExternallyInducedSource}} support. This ticket is created to fix that.,,becket_qin,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 10:14:26 UTC 2020,,,,,,,,,,"0|z0kti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 10:14;sewen;Fixed in 1.12.0 via
  - 4c5fd598d01b7843d9d44439480274a1ca31e87d
  - 0de189cb6e2de63d15b55876e0384f285b9c87d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JaasModule prevents Flink from starting if working directory is a symbolic link,FLINK-20267,13341829,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,trohrmann,trohrmann,20/Nov/20 17:47,20/Oct/22 04:30,13/Jul/23 08:12,24/Nov/20 15:02,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"[~AHeise] reported that starting Flink on EMR fails with

{code}
java.lang.RuntimeException: unable to generate a JAAS configuration file
        at org.apache.flink.runtime.security.modules.JaasModule.generateDefaultConfigFile(JaasModule.java:170)
        at org.apache.flink.runtime.security.modules.JaasModule.install(JaasModule.java:94)
        at org.apache.flink.runtime.security.SecurityUtils.installModules(SecurityUtils.java:78)
        at org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:59)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1045)
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:727)
        at org.apache.flink.runtime.security.modules.JaasModule.generateDefaultConfigFile(JaasModule.java:162)
        ... 4 more
{code}

The problem is that on EMR {{/tmp}} is a symbolic link. Due to FLINK-19252 where we introduced the [creation of the working directory|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/JaasModule.java#L162] in order to create the default Jaas config file, the start up process fails if the path for the working directory is not a directory (apparently {{Files.createDirectories}} cannot deal with symbolic links).",,AHeise,dian.fu,gvauvert,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19252,,,,,,,,,,FLINK-29698,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 02 23:17:43 UTC 2021,,,,,,,,,,"0|z0ksp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 18:21;arvid;There is a workaround for my setting by disabling Jaas with
{noformat}
security.module.factory.classes: ""org.apache.flink.runtime.security.modules.HadoopModuleFactory"";""org.apache.flink.runtime.security.modules.ZookeeperModuleFactory"" {noformat}
But of course, if you need Jaas, it's not an option.;;;","23/Nov/20 07:26;dian.fu;[~guoyangze] Could you help to take a look at this issue?;;;","23/Nov/20 08:31;dian.fu;Confirmed with [~guoyangze] that he is out of office today and so could not take this issue. Here is analysis from him (Hope it could help on fixing this issue):
{quote}
IIUC, check if the dir already exist before creating it could solve this issue, just like TaskManagerServices#checkTempDirs. Right?

The tmp dir will be created if not exist in TaskManagerServices#checkTempDirs . FLINK-19252 just move this logic forward to the installation progress of JaasModule.
{quote};;;","24/Nov/20 12:58;mapohl;The fix of PR #14171 was tested on an AWS EMR:
 * Pre-fix version:

{code:java}
[hadoop@ip-172-31-36-74 ~]$ ./flink-1.12-pre/bin/flink run -m yarn-cluster -p 4 -yjm 1024m -ytm 4096m flink-1.12-pre/examples/streaming/StateMachineExample.jar 
Setting HBASE_CONF_DIR=/etc/hbase/conf because no HBASE_CONF_DIR was set. 
SLF4J: Class path contains multiple SLF4J bindings. 
SLF4J: Found binding in [jar:file:/home/hadoop/flink-1.12-pre/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] java.lang.RuntimeException: unable to generate a JAAS configuration file at org.apache.flink.runtime.security.modules.JaasModule.generateDefaultConfigFile(JaasModule.java:170) 
  at org.apache.flink.runtime.security.modules.JaasModule.install(JaasModule.java:94) 
  at org.apache.flink.runtime.security.SecurityUtils.installModules(SecurityUtils.java:78) 
  at org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:59) 
  at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1045) Caused by: java.nio.file.FileAlreadyExistsException: /tmp 
  at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) 
  at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 
  at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 
  at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) 
  at java.nio.file.Files.createDirectory(Files.java:674) 
  at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) 
  at java.nio.file.Files.createDirectories(Files.java:727) 
  at org.apache.flink.runtime.security.modules.JaasModule.generateDefaultConfigFile(JaasModule.java:162) ... 4 more{code}
 * Post-fix version:

{code:java}
[hadoop@ip-172-31-36-74 ~]$ ./flink-1.12-SNAPSHOT/bin/flink run -m yarn-cluster -p 4 -yjm 1024m -ytm 4096m flink-1.12-SNAPSHOT/examples/streaming /StateMachineExample.jar 
Setting HBASE_CONF_DIR=/etc/hbase/conf because no HBASE_CONF_DIR was set. SLF4J: Class path contains multiple SLF4J bindings. 
SLF4J: Found binding in [jar:file:/home/hadoop/flink-1.12-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] 
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 
Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>] Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>] Options for both the above setups: [--backend <file|rocks>] [--checkpoint-dir <filepath>] [--async-checkpoints <true|false>] [--incremental-checkpoints <true|false>] [--output <filepath> OR null for stdout]Using standalone source with error rate 0.000000 and sleep delay 1 millis
2020-11-24 12:53:38,616 WARN org.apache.flink.yarn.configuration.YarnLogConfigUtil [] - The configuration directory ('/home/hadoop/flink-1.12-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file. 
2020-11-24 12:53:38,767 INFO org.apache.hadoop.yarn.client.RMProxy [] - Connecting to ResourceManager at ip-172-31-36-74.eu-central-1.compute.internal/172.31.36.74:8032 
2020-11-24 12:53:38,892 INFO org.apache.hadoop.yarn.client.AHSProxy [] - Connecting to Application History server at ip-172-31-36-74.eu-central-1.compute.internal/172.31.36.74:10200 
2020-11-24 12:53:38,902 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar 
2020-11-24 12:53:39,044 INFO org.apache.hadoop.conf.Configuration [] - resource-types.xml not found 
2020-11-24 12:53:39,045 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils [] - Unable to find 'resource-types.xml'. 
2020-11-24 12:53:39,050 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils [] - Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE 
2020-11-24 12:53:39,050 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils [] - Adding resource type - name = vcores, units = , type = COUNTABLE 
2020-11-24 12:53:39,052 WARN org.apache.flink.yarn.YarnClusterDescriptor [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN. 
2020-11-24 12:53:39,107 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=4096, slotsPerTaskManager=1} 
2020-11-24 12:53:40,475 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - Submitting application master application_1606220091660_0001 
2020-11-24 12:53:40,806 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl [] - Submitted application application_1606220091660_0001 
2020-11-24 12:53:40,806 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - Waiting for the cluster to be allocated 
2020-11-24 12:53:40,809 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - Deploying cluster, current state ACCEPTED 
2020-11-24 12:53:46,104 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - YARN application has been deployed successfully. 
2020-11-24 12:53:46,105 INFO org.apache.flink.yarn.YarnClusterDescriptor [] - Found Web Interface ip-172-31-38-181.eu-central-1.compute.internal:43067 of application 'application_1606220091660_0001'. 
Job has been submitted with JobID 41d1df99e30414d581183c58aed615d9{code};;;","24/Nov/20 15:02;trohrmann;Fixed via cdcc57c6541a1511822a69714daba823986f9ed7;;;","02/Mar/21 23:17;gvauvert;This issue occurs also in Flink 1.11.3.

 

As a workaround, the temp directory can be configured to use the linked directory directly.

In flink-conf.yaml, add:
{noformat}
io.tmp.dirs: /mnt/tmp{noformat}
 

Used to upgrade EMR 6.2.0 Flink 1.11.2 to Flink 1.11.3 using an EMR Step.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Sources prevent JVM shutdown when running a job,FLINK-20266,13341826,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,trohrmann,trohrmann,20/Nov/20 17:37,12/Dec/20 11:55,13/Jul/23 08:12,23/Nov/20 00:14,1.12.0,,,,,1.11.3,1.12.0,,Connectors / FileSystem,,,,,0,,,,,,"When trying out the new {{FileSource}} I noticed that the jobs which I started from my IDE won't properly terminate. To be more precise the spawned JVM for the jobs wouldn't properly terminate. I cannot really tell what the {{FileSource}} does differently, but when not using this source, the JVM terminates properly.

The stack trace of the hanging JVM is

{code}
2020-11-20 18:20:02
Full thread dump OpenJDK 64-Bit Server VM (11.0.2+9 mixed mode):

Threads class SMR info:
_java_thread_list=0x00007fb5bc15f1b0, length=19, elements={
0x00007fb60d807000, 0x00007fb60d80c000, 0x00007fb60d80f000, 0x00007fb60d809000,
0x00007fb60d81a000, 0x00007fb61f00b000, 0x00007fb63d80e000, 0x00007fb61d826800,
0x00007fb61d829800, 0x00007fb61e800000, 0x00007fb63d95d800, 0x00007fb63e2f8800,
0x00007fb5ba37a800, 0x00007fb5afe1a800, 0x00007fb61dff6800, 0x00007fb63da49800,
0x00007fb63e8d0800, 0x00007fb5be001000, 0x00007fb5bb8a4000
}

""Reference Handler"" #2 daemon prio=10 os_prio=31 cpu=10.05ms elapsed=86.35s tid=0x00007fb60d807000 nid=0x4b03 waiting on condition  [0x00007000036e9000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.ref.Reference.waitForReferencePendingList(java.base@11.0.2/Native Method)
	at java.lang.ref.Reference.processPendingReferences(java.base@11.0.2/Reference.java:241)
	at java.lang.ref.Reference$ReferenceHandler.run(java.base@11.0.2/Reference.java:213)

""Finalizer"" #3 daemon prio=8 os_prio=31 cpu=0.90ms elapsed=86.35s tid=0x00007fb60d80c000 nid=0x3803 in Object.wait()  [0x00007000037ec000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <0x0000000600204780> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:155)
	- waiting to re-lock in wait() <0x0000000600204780> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:176)
	at java.lang.ref.Finalizer$FinalizerThread.run(java.base@11.0.2/Finalizer.java:170)

""Signal Dispatcher"" #4 daemon prio=9 os_prio=31 cpu=0.31ms elapsed=86.34s tid=0x00007fb60d80f000 nid=0x4203 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=31 cpu=2479.36ms elapsed=86.34s tid=0x00007fb60d809000 nid=0x3f03 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

""C1 CompilerThread0"" #8 daemon prio=9 os_prio=31 cpu=1412.88ms elapsed=86.34s tid=0x00007fb60d81a000 nid=0x3d03 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

""Sweeper thread"" #9 daemon prio=9 os_prio=31 cpu=42.82ms elapsed=86.34s tid=0x00007fb61f00b000 nid=0xa803 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Common-Cleaner"" #10 daemon prio=8 os_prio=31 cpu=3.25ms elapsed=86.29s tid=0x00007fb63d80e000 nid=0x5703 in Object.wait()  [0x0000700003cfb000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <0x0000000600205aa0> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:155)
	- waiting to re-lock in wait() <0x0000000600205aa0> (a java.lang.ref.ReferenceQueue$Lock)
	at jdk.internal.ref.CleanerImpl.run(java.base@11.0.2/CleanerImpl.java:148)
	at java.lang.Thread.run(java.base@11.0.2/Thread.java:834)
	at jdk.internal.misc.InnocuousThread.run(java.base@11.0.2/InnocuousThread.java:134)

""JDWP Transport Listener: dt_socket"" #11 daemon prio=10 os_prio=31 cpu=43.46ms elapsed=86.27s tid=0x00007fb61d826800 nid=0xa603 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""JDWP Event Helper Thread"" #12 daemon prio=10 os_prio=31 cpu=220.06ms elapsed=86.27s tid=0x00007fb61d829800 nid=0x5e03 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""JDWP Command Reader"" #13 daemon prio=10 os_prio=31 cpu=27.26ms elapsed=86.27s tid=0x00007fb61e800000 nid=0x6103 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Service Thread"" #14 daemon prio=9 os_prio=31 cpu=0.06ms elapsed=86.19s tid=0x00007fb63d95d800 nid=0xa203 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""ForkJoinPool.commonPool-worker-19"" #25 daemon prio=1 os_prio=31 cpu=2.00ms elapsed=84.76s tid=0x00007fb63e2f8800 nid=0x8003 waiting on condition  [0x000070000584c000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x0000000600a81188> (a java.util.concurrent.ForkJoinPool)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194)
	at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1628)
	at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)

""ForkJoinPool.commonPool-worker-23"" #58 daemon prio=5 os_prio=31 cpu=0.19ms elapsed=84.12s tid=0x00007fb5ba37a800 nid=0xcc03 waiting on condition  [0x0000700007197000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x0000000600a81188> (a java.util.concurrent.ForkJoinPool)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194)
	at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1628)
	at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)

""ForkJoinPool.commonPool-worker-9"" #59 daemon prio=5 os_prio=31 cpu=0.17ms elapsed=84.07s tid=0x00007fb5afe1a800 nid=0xcf03 waiting on condition  [0x000070000729a000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x0000000600a81188> (a java.util.concurrent.ForkJoinPool)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194)
	at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1628)
	at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)

""ForkJoinPool.commonPool-worker-27"" #63 daemon prio=5 os_prio=31 cpu=0.72ms elapsed=83.97s tid=0x00007fb61dff6800 nid=0xd503 waiting on condition  [0x00007000076a6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x0000000600a81188> (a java.util.concurrent.ForkJoinPool)
	at java.util.concurrent.locks.LockSupport.parkUntil(java.base@11.0.2/LockSupport.java:275)
	at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1619)
	at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)

""Cleaner-0"" #185 daemon prio=8 os_prio=31 cpu=10.55ms elapsed=83.39s tid=0x00007fb63da49800 nid=0x1570b in Object.wait()  [0x00007000093fd000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.2/Native Method)
	- waiting on <0x0000000600c5bd78> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:155)
	- waiting to re-lock in wait() <0x0000000600c5bd78> (a java.lang.ref.ReferenceQueue$Lock)
	at jdk.internal.ref.CleanerImpl.run(java.base@11.0.2/CleanerImpl.java:148)
	at java.lang.Thread.run(java.base@11.0.2/Thread.java:834)
	at jdk.internal.misc.InnocuousThread.run(java.base@11.0.2/InnocuousThread.java:134)

""ComponentClosingUtil"" #232 prio=1 os_prio=31 cpu=0.39ms elapsed=81.78s tid=0x00007fb63e8d0800 nid=0x13107 waiting on condition  [0x0000700004f31000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method)
	- parking to wait for  <0x00000006008fb930> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@11.0.2/AbstractQueuedSynchronizer.java:2081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.2/ScheduledThreadPoolExecutor.java:1170)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.2/ScheduledThreadPoolExecutor.java:899)
	at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@11.0.2/ThreadPoolExecutor.java:1054)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.2/ThreadPoolExecutor.java:1114)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.2/ThreadPoolExecutor.java:628)
	at java.lang.Thread.run(java.base@11.0.2/Thread.java:834)

""DestroyJavaVM"" #282 prio=5 os_prio=31 cpu=2125.94ms elapsed=81.19s tid=0x00007fb5be001000 nid=0x1d03 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Attach Listener"" #291 daemon prio=9 os_prio=31 cpu=0.57ms elapsed=0.09s tid=0x00007fb5bb8a4000 nid=0x14207 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""VM Thread"" os_prio=31 cpu=74.59ms elapsed=86.36s tid=0x00007fb63f807000 nid=0x3603 runnable

""GC Thread#0"" os_prio=31 cpu=51.88ms elapsed=86.38s tid=0x00007fb62d803800 nid=0x2e03 runnable

""GC Thread#1"" os_prio=31 cpu=50.32ms elapsed=85.97s tid=0x00007fb61d836800 nid=0x9e03 runnable

""GC Thread#2"" os_prio=31 cpu=45.79ms elapsed=85.97s tid=0x00007fb61d8b8000 nid=0x9d03 runnable

""GC Thread#3"" os_prio=31 cpu=46.08ms elapsed=85.97s tid=0x00007fb61d83a800 nid=0x9c03 runnable

""GC Thread#4"" os_prio=31 cpu=48.88ms elapsed=85.97s tid=0x00007fb61f01c000 nid=0x9a03 runnable

""GC Thread#5"" os_prio=31 cpu=44.32ms elapsed=85.97s tid=0x00007fb63d8e5000 nid=0x9803 runnable

""GC Thread#6"" os_prio=31 cpu=49.81ms elapsed=85.97s tid=0x00007fb63d8e6000 nid=0x9703 runnable

""GC Thread#7"" os_prio=31 cpu=51.55ms elapsed=85.97s tid=0x00007fb61d83b800 nid=0x6503 runnable

""GC Thread#8"" os_prio=31 cpu=46.05ms elapsed=85.97s tid=0x00007fb62d928000 nid=0x9403 runnable

""GC Thread#9"" os_prio=31 cpu=50.43ms elapsed=85.97s tid=0x00007fb62d929000 nid=0x9203 runnable

""G1 Main Marker"" os_prio=31 cpu=0.65ms elapsed=86.37s tid=0x00007fb62d83b000 nid=0x2f03 runnable

""G1 Conc#0"" os_prio=31 cpu=11.46ms elapsed=86.37s tid=0x00007fb62d83b800 nid=0x3203 runnable

""G1 Conc#1"" os_prio=31 cpu=11.43ms elapsed=85.39s tid=0x00007fb63da4e800 nid=0x6703 runnable

""G1 Conc#2"" os_prio=31 cpu=10.03ms elapsed=85.39s tid=0x00007fb63da4f800 nid=0x6803 runnable

""G1 Refine#0"" os_prio=31 cpu=8.22ms elapsed=86.37s tid=0x00007fb63e102000 nid=0x4e03 runnable

""G1 Refine#1"" os_prio=31 cpu=0.06ms elapsed=81.10s tid=0x00007fb5aeff4000 nid=0x1d50f runnable

""G1 Young RemSet Sampling"" os_prio=31 cpu=12.94ms elapsed=86.37s tid=0x00007fb63e102800 nid=0x3403 runnable
""VM Periodic Task Thread"" os_prio=31 cpu=57.93ms elapsed=86.19s tid=0x00007fb61e833800 nid=0xa003 waiting on condition

JNI global refs: 55, weak refs: 15453
{code}

My environment is and I used the openjdk version ""1.8.0_262"" to run the job:

IntelliJ IDEA 2020.2.3 (Community Edition)
Build #IC-202.7660.26, built on October 6, 2020
Runtime version: 11.0.8+10-b944.34 x86_64
VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o.
macOS 10.14.6
GC: ParNew, ConcurrentMarkSweep
Memory: 1979M
Cores: 12
Non-Bundled Plugins: CheckStyle-IDEA, com.jetbrains.performancePlugin, org.jetbrains.kotlin, org.intellij.scala

cc [~sewen]",,becket_qin,dian.fu,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20115,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 00:14:07 UTC 2020,,,,,,,,,,"0|z0ksog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/20 14:55;sewen;Looks like this is an issue in the FLIP-27 sources in general, not specific to the FileSource.

If I read this correctly, the ""ComponentClosingUtil"" is keeping the JVM alive. The problem is that this is a static executor pool that created non-daemon threads. By definition, the JVM stays alive as long as any non-daemon thread exists (or until someone calls System.exit()).

This pattern is a problem, having a thread pool as a static utility, because it means the pool will never be shut down (static utils don't have an owner and defined life cycle).

I would suggest to do the following:
  - Drop the executor / thread pool. just spawn a thread when this is uses. The use is rare enough that the thread pooling seems not necessary. And the thread pooling opens up the ""ownership"" question (who shuts it down).
  - We can also think about moving this from ""flink-core"" to ""flink-runtime"", in which case we can simply reuse the {{FutureUtils.Delayer}}. Then we don't need any extra thread. The delayer does not have this problem because it uses the proper factories to instantiate the threads in a safe way. The delayer is a bit of a hack itself, but it is already better to have one hack (Delayer) than two hacks (Delayer and ComponentClosingUtil).

I think only making the threads daemon threads is a quick-fix, but kind of ignores the main problem (introducing new static contexts with threads).

As a side note: To my understanding, the way that the coordinator closes and re-creates the SplitEnumerator means that users/connectors should not need to worry about closing with timeout themselves. This is handled in the SourceCoordinator.;;;","21/Nov/20 14:56;sewen;[~jqin] If you don't object, I would fix this using the above suggestion (moving utils, consolidating with Delayer).;;;","21/Nov/20 15:56;becket_qin;[~sewen] The suggested fix sounds good to me. Given that the implementation probably won't use the ComponentClosingUtil, having that only in runtime should be fine.;;;","23/Nov/20 00:14;sewen;Fixed in 1.12.0 (master) via d986d3018c0cc913604ffab5801123318436d7a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building flink-dist docker image does not work without python2,FLINK-20262,13341804,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,20/Nov/20 14:39,23/Nov/20 14:01,13/Jul/23 08:12,23/Nov/20 14:01,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Build System,Test Infrastructure,,,,0,pull-request-available,,,,,"The script {{common_docker.sh}} in function {{start_file_server}} tests existence of {{python3}}, but executes command using {{python}}:

{code}
    command -v python3 >/dev/null 2>&1
    if [[ $? -eq 0 ]]; then
      python ${TEST_INFRA_DIR}/python3_fileserver.py &
      return
    fi
{code}

The script {{python3_fileserver.py}} uses python2 {{SocketServer}} which does not exist in python3. It should use {{socketserver}}.",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 14:01:51 UTC 2020,,,,,,,,,,"0|z0ksjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 14:01;dwysakowicz;Fixed in:
* master
** a4cc8c22b0bc9d50d9eda456ba5f6ce636282fcd
* 1.11.3
** 4b74c060557642945a28a91d2273da141508c4aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exception in ExecutorNotifier due to split assignment broken by failed task,FLINK-20261,13341795,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,azagrebin,azagrebin,20/Nov/20 13:34,22/Nov/20 22:56,13/Jul/23 08:12,22/Nov/20 22:56,1.12.0,,,,,1.12.0,,,Connectors / Common,Connectors / FileSystem,,,,0,,,,,,"While trying to extend {{FileSourceTextLinesITCase::testContinuousTextFileSourceWithTaskManagerFailover}} with recovery test after TM failure ({{TestingMiniCluster::terminateTaskExecutor}}, [branch|https://github.com/azagrebin/flink/tree/FLINK-20118-it]) in FLINK-20118, I encountered the following case:
* {{SourceCoordinatorContext::assignSplits}} schedules async assignment (all reader tasks alive)
* call {{TestingMiniCluster::terminateTaskExecutor}} while doing writeFile in a loop of testContinuousTextFileSource
* causes graceful {{TaskExecutor::onStop}} shutdown
* causes TM/RM disconnect and failing slot allocations in JM by RM
* eventually causes {{SourceCoordinatorContext::unregisterSourceReader}}
* actual assignment starts ({{SourceCoordinatorContext::assignSplits: callInCoordinatorThread}})
* {{registeredReaders.containsKey(subtaskId)}} check fails (due to failed task) with {{IllegalArgumentException}} which is uncaught in single thread executor
* forces ThreadPool to recreate the single thread
* calls {{CoordinatorExecutorThreadFactory::newThread}}
* fails expected condition of single thread creation with {{IllegalStateException}} which is uncaught
* calls {{FatalExitExceptionHandler}} and exits JVM abruptly


{code:java}
[SourceCoordinator-Source: file-source] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler - FATAL: Thread 'SourceCoordinator-Source: file-source' produced an uncaught exception. Stopping the process...
java.lang.IllegalStateException: Should never happen. This factory should only be used by a SingleThreadExecutor.
	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider$CoordinatorExecutorThreadFactory.newThread(SourceCoordinatorProvider.java:94) ~[classes/:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619) ~[?:1.8.0_172]
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932) ~[?:1.8.0_172]
	at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) ~[?:1.8.0_172]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) ~[?:1.8.0_172]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_172]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_172]

Process finished with exit code 239
{code}
",,azagrebin,becket_qin,rmetzger,sewen,stevenz3wu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20118,,,,,,,,,,,,,,,,,,FLINK-20081,FLINK-20157,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 22 22:56:18 UTC 2020,,,,,,,,,,"0|z0kshk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 13:53;trohrmann;This looks a bit like a race condition/not proper handling of concurrent events in the {{SourceCoordinator}}. Given that a disconnecting {{TaskManager}} can bring down a {{JobManager}}, I think this qualifies as a release blocker. Very good finding [~azagrebin]!

cc [~sewen], [~jqin];;;","20/Nov/20 14:12;rmetzger;This is related: FLINK-20157 (FLINK-20157 root cause is that the new KafkaSource is not using the usercode classloader on the JobManager). ;;;","21/Nov/20 11:49;becket_qin;It seems the root cause here is a little different. Because the SourceCoordinator is single threaded, so actually there should not be any race condition with its state. In the reported case, the calling sequence in the `SplitEnumerator` side was:
 # {{SplitEnumerator.subtaskFailed()}} is called, the failed subtask is removed from the {{SplitEnumeratorContext}}.
 # {{FileSplitEnumerator.handleSplitRequest()}} is invoked to let the split enumerator handle the split request from the failed task. At this point the the failed subtask should have already been unregistered.

The problem here is that at step 2, both {{ContinuousFileSplitEnumerator}} and {{StaticFileSplitEnumerator}} naively call `SplitEnumeratorContext.assignSplits()` to assign split to the failed subtask without checking the liveness of the subtask in question. This will trigger IllegalArgumentException in the {{SplitEnumeratorContext}}.

So I think the fix here should be just ignoring the split request when the subtask does not exist in the {{SplitEnumeratorContext}}.

That being said, the exception handling in the ExecutorNotifier still needs to be improved. But that seems orthogonal to the issue reported here.

CC: [~sewen] Can you also help check if this is the case?;;;","22/Nov/20 20:43;sewen;[~becket_qin] You are right, that is what is happening here. I'll fix the behavior of the file source enumerator under this ticket, and we fix the enumerator uncaught exception handling in the linked issues.;;;","22/Nov/20 22:56;sewen;Fixed in 1.12.0 via 75599bca3ecff89c0abf9d9027d1280e1ac812b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDAF type inference will fail if accumulator contains MapView with Pojo value type,FLINK-20256,13341736,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,TsReaper,TsReaper,20/Nov/20 08:28,24/Nov/20 16:02,13/Jul/23 08:12,24/Nov/20 16:02,,,,,,1.12.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"To reproduce this bug, add the following test to {{FunctionITCase.java}}.

{code:java}
public static class MyPojo implements Serializable {
	public String a;
	public int b;

	public MyPojo(String s) {
		this.a = s;
		this.b = s.length();
	}
}

public static class MyAcc implements Serializable {

	public MapView<String, MyPojo> view = new MapView<>();

	public MyAcc() {}

	public void add(String a, String b) {
		try {
			view.put(a, new MyPojo(b));
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}
}

public static class TestUDAF extends AggregateFunction<String, MyAcc> {

	@Override
	public MyAcc createAccumulator() {
		return new MyAcc();
	}

	public void accumulate(MyAcc acc, String value) {
		if (value != null) {
			acc.add(value, value);
		}
	}

	@Override
	public String getValue(MyAcc acc) {
		return ""test"";
	}
}

@Test
public void myTest() throws Exception {
	String ddl = ""create function MyACC as '"" + TestUDAF.class.getName() + ""'"";
	tEnv().executeSql(ddl).await();
	try (CloseableIterator<Row> it = tEnv().executeSql(""SELECT MyACC('123')"").collect()) {
		while (it.hasNext()) {
			System.out.println(it.next());
		}
	}
}
{code}

And we'll get the following exception stack
{code}
java.lang.ClassCastException: org.apache.flink.table.types.AtomicDataType cannot be cast to org.apache.flink.table.types.KeyValueDataType

	at org.apache.flink.table.planner.typeutils.DataViewUtils$MapViewSpec.getKeyDataType(DataViewUtils.java:257)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1$$anonfun$22.apply(AggsHandlerCodeGenerator.scala:1231)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1$$anonfun$22.apply(AggsHandlerCodeGenerator.scala:1231)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$.org$apache$flink$table$planner$codegen$agg$AggsHandlerCodeGenerator$$addReusableDataViewSerializer(AggsHandlerCodeGenerator.scala:1294)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1.apply(AggsHandlerCodeGenerator.scala:1228)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1.apply(AggsHandlerCodeGenerator.scala:1211)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$.addReusableStateDataViews(AggsHandlerCodeGenerator.scala:1211)
	at org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.<init>(ImperativeAggCodeGen.scala:112)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$3.apply(AggsHandlerCodeGenerator.scala:233)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$3.apply(AggsHandlerCodeGenerator.scala:214)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.initialAggregateInformation(AggsHandlerCodeGenerator.scala:214)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.generateAggsHandler(AggsHandlerCodeGenerator.scala:325)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.translateToPlanInternal(StreamExecGroupAggregate.scala:143)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.translateToPlanInternal(StreamExecGroupAggregate.scala:52)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.translateToPlan(StreamExecGroupAggregate.scala:52)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:163)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:83)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:49)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:78)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:77)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translate(StreamPlanner.scala:69)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1065)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
	at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.myTest(FunctionITCase.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}

However {{MapView<String, String>}} will be alright.",,jark,libenchao,lincoln.86xy,lzljs3620320,rmetzger,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 16:02:35 UTC 2020,,,,,,,,,,"0|z0ks4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 08:29;TsReaper;[~twalthr] please take a look.;;;","20/Nov/20 11:04;twalthr;Thanks for reporting this [~TsReaper] this is definitely a bug.;;;","20/Nov/20 14:24;rmetzger;Is this something easy to fix? I'm trying to find out if we can create a new RC by Monday morning.;;;","20/Nov/20 17:00;twalthr;[~rmetzger] I will investigate that now and come back shortly. But map views are more of an internal feature anyway. We recently updated the docs to mention them in one sentence but without examples.;;;","20/Nov/20 17:16;twalthr;[~TsReaper] the reason is that {{MyPojo}} is not a valid structured type. It has neither a default constructor nor fully assigning constructor. However, this issue is valid because the exception is swallowed due to the special {{MapView}} path in the logic.;;;","23/Nov/20 02:43;lzljs3620320;Hi [~twalthr], Even if it's not a POJO, it should work as a generic type info. This can be supported by older Flink versions.;;;","23/Nov/20 06:19;TsReaper;Indeed, if I make the test more complex, for example if I'm using a user-defined interface in a user-defined class then the test still fails.

{code:java}
public interface MyInterface {}

public static class MyClass implements Serializable {
	public String a;
	public int b;
	public MyInterface[] c;

	public MyClass() {}

	public MyClass(String a) {
		this.a = a;
		this.b = a.length();
		this.c = new MyInterface[b];
	}
}

public static class MyAcc implements Serializable {

	public MapView<String, MyClass> view = new MapView<>();

	public MyAcc() {}

	public void add(String a, String b) {
		try {
			view.put(a, new MyClass(b));
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}
}

public static class TestUDAF extends AggregateFunction<String, MyAcc> {

	@Override
	public MyAcc createAccumulator() {
		return new MyAcc();
	}

	public void accumulate(MyAcc acc, String value) {
		if (value != null) {
			acc.add(value, value);
		}
	}

	@Override
	public String getValue(MyAcc acc) {
		return ""test"";
	}
}

@Test
public void myTest() throws Exception {
	String ddl = ""create function MyACC as '"" + TestUDAF.class.getName() + ""'"";
	tEnv().executeSql(ddl).await();
	try (CloseableIterator<Row> it = tEnv().executeSql(""SELECT MyACC('123')"").collect()) {
		while (it.hasNext()) {
			System.out.println(it.next());
		}
	}
}
{code}

{code}
java.lang.ClassCastException: org.apache.flink.table.types.AtomicDataType cannot be cast to org.apache.flink.table.types.KeyValueDataType

	at org.apache.flink.table.planner.typeutils.DataViewUtils$MapViewSpec.getKeyDataType(DataViewUtils.java:257)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1$$anonfun$22.apply(AggsHandlerCodeGenerator.scala:1231)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1$$anonfun$22.apply(AggsHandlerCodeGenerator.scala:1231)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$.org$apache$flink$table$planner$codegen$agg$AggsHandlerCodeGenerator$$addReusableDataViewSerializer(AggsHandlerCodeGenerator.scala:1294)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1.apply(AggsHandlerCodeGenerator.scala:1228)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$addReusableStateDataViews$1.apply(AggsHandlerCodeGenerator.scala:1211)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$.addReusableStateDataViews(AggsHandlerCodeGenerator.scala:1211)
	at org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.<init>(ImperativeAggCodeGen.scala:112)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$3.apply(AggsHandlerCodeGenerator.scala:233)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$3.apply(AggsHandlerCodeGenerator.scala:214)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.initialAggregateInformation(AggsHandlerCodeGenerator.scala:214)
	at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.generateAggsHandler(AggsHandlerCodeGenerator.scala:325)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.translateToPlanInternal(StreamExecGroupAggregate.scala:143)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.translateToPlanInternal(StreamExecGroupAggregate.scala:52)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.translateToPlan(StreamExecGroupAggregate.scala:52)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:163)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:83)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:59)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:49)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:78)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:77)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translate(StreamPlanner.scala:69)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1065)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
	at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.myTest(FunctionITCase.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code};;;","23/Nov/20 08:27;twalthr;[~lzljs3620320] I haven't tested it yet but if people would like to use the invalid POJO as a generic RAW type they should be able to enable it via annotation. I will investigate this again and propose a PR.;;;","24/Nov/20 06:49;lzljs3620320;Thanks [~twalthr];;;","24/Nov/20 16:02;twalthr;Fixed in 1.12.0: 4ea1faf6d2c586711d2ca0f6fac655231f53d35d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when invoking AsyncLookupJoinRunner#close method,FLINK-20250,13341597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,hailong wang,hailong wang,19/Nov/20 14:27,08/Dec/20 03:04,13/Jul/23 08:12,08/Dec/20 03:04,1.12.0,,,,,1.13.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"NPE when invoking AsyncLookupJoinRunner#close method,
{code:java}
java.lang.NullPointerException
    at org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinRunner.close(AsyncLookupJoinRunner.java:156)
    at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:670)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:581)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:483)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:709)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
    at java.lang.Thread.run(Thread.java:745)
{code}
This is because the job failed before it was initialized.

So We can add a null condition for allResultFutures object.

 ",,hailong wang,jark,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 03:04:19 UTC 2020,,,,,,,,,,"0|z0kr9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 14:50;jark;Yes. We can do that. ;;;","24/Nov/20 13:29;nicholasjiang;[~hailong wang], are you working for this issue? If you have no time, I would like to fix this problem.;;;","24/Nov/20 15:44;hailong wang;Sure [~nicholasjiang], We should also add a test by invoking close method directly without invoking open method.;;;","24/Nov/20 15:45;hailong wang;Hi [~jark], Could you help to assign this to [~nicholasjiang].;;;","08/Dec/20 03:04;jark;Fixed in master: 5043ca9ef6405bb6980b5d6bad0935749de6d8ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove useless words in documents,FLINK-20243,13341570,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaozilong,xiaozilong,xiaozilong,19/Nov/20 12:05,20/Nov/20 04:16,13/Jul/23 08:12,20/Nov/20 04:16,1.11.0,,,,,1.11.3,1.12.0,,Documentation,,,,,0,pull-request-available,,,,,!image-2020-11-19-20-05-05-752.png!,,jark,xiaozilong,zouyunhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/20 12:05;xiaozilong;image-2020-11-19-20-05-05-752.png;https://issues.apache.org/jira/secure/attachment/13015623/image-2020-11-19-20-05-05-752.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 04:16:22 UTC 2020,,,,,,,,,,"0|z0kr3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 04:16;jark;Fixed in
 - master (1.12.0): 5c7ed8e1c87b985ea80bb47ce8de36c234b13828
 - 1.11.3: 13940b606fcf5aa4f26d172aaa86d82054ecd301;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve exception message when hive deps are missing on JM/TM,FLINK-20241,13341558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,dwysakowicz,dwysakowicz,19/Nov/20 11:06,01/Dec/20 02:54,13/Jul/23 08:12,27/Nov/20 02:44,1.12.0,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"I followed the setup here: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/#dependencies
I put the flink-sql-connector-hive-2.3.6 in the \lib directory

I tried running queries against an ORC table in hive from sql-client:
{code}
SET table.sql-dialect=hive;
 CREATE TABLE hive_table_orc (
   user_id STRING,
   order_amount DOUBLE
 ) PARTITIONED BY (dt STRING, hr STRING) STORED AS orc TBLPROPERTIES (
   'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',
   'sink.partition-commit.trigger'='partition-time',
   'sink.partition-commit.delay'='1 s',
   'sink.partition-commit.policy.kind'='metastore,success-file',
   'streaming-source.enable'='true');

SET table.sql-dialect=default;
insert into hive_table_orc VALUES ('1', 123.0, '2020-11-11', '12');
// or 
SELECT * FROM hive_table_orc;
{code}

but I am getting:

{code}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function.
	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:325)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:146)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.StreamCorruptedException: unexpected block data
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1549)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2125)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2125)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550)
	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511)
	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:310)
	... 6 more
{code}",,dian.fu,dwysakowicz,gengmao,jark,lirui,lzljs3620320,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20151,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 02:44:25 UTC 2020,,,,,,,,,,"0|z0kr0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 11:48;dwysakowicz;I checked that I get the same behaviour with {{TEXTFILE}};;;","24/Nov/20 01:30;dian.fu;cc [~lzljs3620320] [~lirui];;;","24/Nov/20 07:04;lzljs3620320;Thanks [~dwysakowicz] for reporting.

works fine in my cluster, [~dwysakowicz]

are Hadoop related dependencies ready?;;;","24/Nov/20 07:31;lirui;Hi [~dwysakowicz] thanks for testing and reporting the issue. I tried both standalone and yarn-session mode on my laptop and didn't reproduce the issue. Could you let me know which kind of cluster you're using?;;;","24/Nov/20 08:43;dwysakowicz;I've tried it again. In the end it was my fault as I did not copy over the {{hive-exec}}. I assumed it is on the classpath as returned by
{code}
export HADOOP_CLASSPATH=`hadoop classpath`
{code}

After copying over the {{hive-exec}} it works. Sorry again for the confusion.;;;","24/Nov/20 08:58;lirui;[~dwysakowicz] No problem at all. However, if you already put {{flink-sql-connector-hive-2.3.6}} under the lib folder, you shouldn't be needing to copy hive-exec because it's included in that uber jar.

The purpose of the {{flink-sql-connector-hive-x.x.x}} jars is to make it easier for users to add dependencies (although the name is admittedly a little confusing). Each {{flink-sql-connector-hive-x.x.x}} can be considered as ""{{flink-connector-hive}} + {{hive-exec/hive-metastore}} + {{orc/parquet}}"".;;;","24/Nov/20 09:05;rmetzger;Great to hear that this is not a real issue. I'm wondering if we can improve the error reporting a bit? Failing with a serialization issue because a dependency is missing doesn't sound like a great user experience.;;;","24/Nov/20 09:50;sewen;Yes, I think the problem is the pattern where data is directly written to the ObjectOutputStream. That always creates hard to understand error messages:

  - This results in bad messages: https://github.com/apache/flink/blob/master/flink-formats/flink-sequence-file/src/main/java/org/apache/flink/formats/sequencefile/SerializableHadoopConfiguration.java#L47

   - This results in good messages: https://github.com/apache/flink/blob/master/flink-formats/flink-orc/src/main/java/org/apache/flink/orc/util/SerializableHadoopConfigWrapper.java#L52

I think we should change all parts where config / code is serialized to the second pattern, to make errors better.;;;","24/Nov/20 09:51;sewen;I reopened this as a blocker, because it is impossible for users to understand and debug this error.;;;","27/Nov/20 02:44;lzljs3620320;master (1.13): fad84798d9b66ae3b43fb3d4940a1e5c3b26600e

release-1.12: 598386763115cb9fa3de29a5e70ec9fd0e0faf57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Hive dependencies,FLINK-20235,13341523,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,dwysakowicz,dwysakowicz,19/Nov/20 08:26,24/Nov/20 09:35,13/Jul/23 08:12,24/Nov/20 09:35,1.12.0,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"I tried following the setup here: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/#dependencies

I put the flink-sql-connector-hive-2.3.6 in the {{\lib}} directory and tried running queries (as described in https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_streaming.html) via {{sql-client}}.

{code}
SET table.sql-dialect=hive;
CREATE TABLE hive_table (
  user_id STRING,
  order_amount DOUBLE
) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (
  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',
  'sink.partition-commit.trigger'='partition-time',
  'sink.partition-commit.delay'='1 s',
  'sink.partition-commit.policy.kind'='metastore,success-file'
);

SET table.sql-dialect=default;
SELECT * FROM hive_table;
{code}

It fails with:
{code}
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.hive.shaded.parquet.format.converter.ParquetMetadataConverter
	at org.apache.flink.hive.shaded.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:112)
	at org.apache.flink.hive.shaded.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:73)
	at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.createReader(HiveBulkFormatAdapter.java:99)
	at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.createReader(HiveBulkFormatAdapter.java:62)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:110)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:68)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:136)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:100)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
{code}","hive 2.3.4
hadoop 2.7.4",dian.fu,dwysakowicz,jark,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19564,,,,,,,FLINK-20151,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 09:35:32 UTC 2020,,,,,,,,,,"0|z0kqt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 09:00;lzljs3620320;This may be the same as FLINK-19564 ;;;","19/Nov/20 09:01;dwysakowicz;Yes, I think it is the same [~lzljs3620320];;;","19/Nov/20 09:03;dwysakowicz;I closed the other one, as this one has user facing steps to reproduce the issue.;;;","23/Nov/20 07:51;rmetzger;Shouldn't this ticket be a blocker?

I would like to create the first voting RC soon, I guess we want to include this fix in there? If so, please make sure to merge a fix soon. If you want me to track it, upgrade it to blocker.;;;","24/Nov/20 01:28;dian.fu;[~lzljs3620320] [~lirui] Could you help to take a look at this issue?;;;","24/Nov/20 02:56;lzljs3620320;[~rmetzger] [~dian.fu] I will create a PR soon;;;","24/Nov/20 02:57;dian.fu;[~lzljs3620320] Thanks a lot~;;;","24/Nov/20 04:29;lzljs3620320;Verified [https://github.com/apache/flink/pull/14189] , passed in my env.;;;","24/Nov/20 09:35;lzljs3620320;master (1.12): 7e7dbc27b02fa2d339f0ddb6fd3810745a2e3763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle object reuse properly in the operators of Python DataStream API,FLINK-20232,13341501,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,dian.fu,dian.fu,19/Nov/20 05:39,19/Nov/20 12:13,13/Jul/23 08:12,19/Nov/20 12:13,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,"Currently, it doesn't consider object reuse in PythonTimestampsAndWatermarksOperator. We should put a copy of the input element into bufferedInputs if object reuse of enabled.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20135,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 12:13:54 UTC 2020,,,,,,,,,,"0|z0kqo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 12:13;dian.fu;Fixed in master(1.12.0) via bd28f529a7a0470ccd25c65f0a019c298d132719;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"INSERT INTO  EMPTY VALUES, THROW FileNotFoundException",FLINK-20230,13341473,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,jinxi,jinxi,19/Nov/20 02:13,28/May/21 09:03,13/Jul/23 08:12,12/Apr/21 06:49,1.11.2,,,,,1.13.0,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,1,pull-request-available,,,,,"table  USE SQL ""INSERT INTO VALUES"", WHEN THE VALUES IS NULL, AND IT WILL THROW FileNotFoundException;

TABLE DDL:

{code:sql}
CREATE TABLE IF NOT EXISTS test1 (
 uid string,
 truename string,
 dt string
 )
 ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ''
 STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS test2(
 uid string COMMENT 'ID',
 truename string
 )
 PARTITIONED BY ( `dt` string )
 ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ''
 STORED AS TEXTFILE;
 # CODE IS:
 EnvironmentSettings settings = EnvironmentSettings.newInstance()
 .useBlinkPlanner()
 .inBatchMode()
 .build();
 TableEnvironment tableEnv = TableEnvironment.create(settings);

String nowDt = ""202011111"";
 Table table = tableEnv.sqlQuery(String.format(""select \n"" +
 ""uid,\n"" +
 ""truename,\n""
 ""'%s' as dt\n"" +
 ""from test1 "" +
 ""where dt = '%s'"", nowDt, nowDt
 ));
 table.executeInsert(""test2"");
{code}
 

*Exception:*


{code:java}
Caused by: java.lang.Exception: Failed to finalize execution on masterCaused by: java.lang.Exception: Failed to finalize execution on master ... 34 moreCaused by: org.apache.flink.table.api.TableException: Exception in finalizeGlobal at org.apache.flink.table.filesystem.FileSystemOutputFormat.finalizeGlobal(FileSystemOutputFormat.java:97) at org.apache.flink.runtime.jobgraph.InputOutputFormatVertex.finalizeOnMaster(InputOutputFormatVertex.java:132) at org.apache.flink.runtime.executiongraph.ExecutionGraph.vertexFinished(ExecutionGraph.java:1286) ... 33 moreCaused by: java.io.FileNotFoundException: File hdfs://nameservice1/user/hive/warehouse/test.db/test2/.staging_1605749732741 does not exist. at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:901) at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112) at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961) at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958) at org.apache.flink.hive.shaded.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:157) at org.apache.flink.table.filesystem.PartitionTempFileManager.headCheckpoints(PartitionTempFileManager.java:140) at org.apache.flink.table.filesystem.FileSystemCommitter.commitUpToCheckpoint(FileSystemCommitter.java:98) at org.apache.flink.table.filesystem.FileSystemOutputFormat.finalizeGlobal(FileSystemOutputFormat.java:95)
{code}
",,jark,jinxi,leonard,lzljs3620320,macdoor615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21891,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21891,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 06:49:46 UTC 2021,,,,,,,,,,"0|z0kqi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 03:41;jark;Could you help to have a look at this? [~lzljs3620320];;;","20/Nov/20 03:55;lzljs3620320;Looks like this is a bug in Filesystem **batch** sink.;;;","24/Nov/20 03:55;lzljs3620320;I think this case is not very common. Change this to 1.13.;;;","12/Apr/21 06:49;lzljs3620320;master (1.13): 562f31a3ebe53d6827591dfa84501e2babda3693;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The RecreateOnResetOperatorCoordinator and SourceCoordinator executor thread should use the user class loader.,FLINK-20223,13341346,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,18/Nov/20 13:54,12/Dec/20 11:56,13/Jul/23 08:12,23/Nov/20 11:38,,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,,,,,The {{RecreateOnResetOperatorCoordinator}} and the {{SourceCoordinator}} executor thread should both use the user class loader. Otherwise {{ClassNotFoundException}} may be thrown from the interaction with the {{SplitEnumerator}}s.,,becket_qin,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 11:38:20 UTC 2020,,,,,,,,,,"0|z0kpq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 11:38;sewen;Fixed in 1.12.0 via
  - 0132fa2bafd81bc55ff10f0306534a163296d6da
  - 40bbc174411b1502842ed3cdb9c421d51aa5fc0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The CheckpointCoordinator should reset the OperatorCoordinators when fail before the first checkpoint.,FLINK-20222,13341341,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,becket_qin,becket_qin,18/Nov/20 13:37,12/Dec/20 11:57,13/Jul/23 08:12,24/Nov/20 13:13,,,,,,1.11.3,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"Right now, if a job failed before the first successful checkpoint, the CheckpointCoordinator will not reset the OperatorCoordinator state. This may leave the OperatorCoordinators in inconsistent state.

The CheckpointCoordinator should also reset the OperatorCoordinator state in this case, just like it does for the master hooks. It essentially means ""reset to no checkpoint"". There are two options for the fix:
 # Add a reset() method to the OperatorCoordinator.
 # Call resetToCheckpoint(null) on the OperatorCoordinator.",,becket_qin,klion26,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 13:13:03 UTC 2020,,,,,,,,,,"0|z0kpow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 13:13;sewen;Fixed in 1.12.0 (master) via 7a0419781e715e5b818f572c61225ba5e155f7ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelimitedInputFormat does not restore compressed filesplits correctly leading to dataloss,FLINK-20221,13341337,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,18/Nov/20 13:32,08/Dec/20 11:36,13/Jul/23 08:12,08/Dec/20 11:36,1.10.2,1.11.2,1.12.0,,,1.11.3,1.12.1,1.13.0,Connectors / FileSystem,,,,,0,pull-request-available,,,,,"It seems that the delimited input format cannot correctly restore input splits if they belong to compressed files. Basically when a compressed filesplit is restored in the middle, it won't read it anymore leading to dataloss.

The cause of the problem is that for compressed splits that use an inflater stream, the splitlength is set to the magic number -1 which is ignored in the reopen method and causes the split to go to `end` state immediately.

The problem and the fix is shown in this commit:
[https://github.com/gyfora/flink/commit/4adc8ba8d1989fff2db43881c9cb3799848c6e0d]",,dian.fu,gyfora,klion26,rmetzger,twalthr,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 11:36:10 UTC 2020,,,,,,,,,,"0|z0kpo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 02:38;xtsong;[~gyfora], thanks for reporting the issue.

I'm not familiar with the problem, and may have not fully understand the problem. However, I think any issue that may lead to data loss should be considered as a release blocker.

Tagged the fix versions so that this issue shows in the release kanban boards.;;;","20/Nov/20 07:30;rmetzger;[~gyfora] What's the plan with merging this? do you want to create a PR for the change?

it would be nice to include this into the next release candidate for 1.12.0 which I would like to create on Monday;;;","20/Nov/20 08:25;gyfora;[~rmetzger] , I would like to open a PR but first I think I need some feedback from someone who understands the internals of these input formats more in detail. 

Turns out this is not a super straightforward change as the current behaviour of `reopen` calls `open` which causes some problems as the open method already start reading the stream which would trigger a backwards seek in the reopen in many cases. Backwards seek is however not supported on the wrapper inflater input streams used for compressed files.

So maybe we have to change reopen so as to not call open (or maybe only call it if we checkpointed the state before we started reading the split, not even sure if this is possible). 

In any case changing the reopen this way would cause problems in the hierarchy as other subclasses like CsvInputFormat rely on the open method being always called (even on restore). 

I am not super eager to push some last minute change that will break this completely :D  ;;;","20/Nov/20 11:30;rmetzger;Thanks a lot for your comment. I agree that we should not rush things last minute.
Let's include this fix on a best effort basis into the release. Since the issue has been around for a while, we can also include a fix in the next bugfix release (which usually comes quite soon after the .0 release).;;;","23/Nov/20 02:50;xtsong;Given that the fix for this issue requires quite many changes and may come up with a risk breaking other things, I'm also good with fixing it on a best effort basis and deferring to the next bugfix release if necessary.

Then shall we downgrading it to `Critical`? ;;;","23/Nov/20 07:24;dian.fu;I'm also in favor of downgrading the priority to Critical and trying to fix this issue on a best effort basis.;;;","23/Nov/20 07:50;dian.fu;I have downgraded this issue to Critical. Feel free to set it back if anyone feels that this should be a blocker issue.;;;","02/Dec/20 12:16;gyfora;master: 02f16958346483d91cd59cb902e0cce63fa5875f;;;","02/Dec/20 17:22;gyfora;release-1.12: b82a3dc3939bcf4fa5e6f398eafb7d0e92bf62c7
release-1.11: edf59d73631b3108149c3a97172253f6706c32fe;;;","08/Dec/20 11:15;twalthr;[~gyfora] can we close this issue or are you planning to also fix it for 1.10?;;;","08/Dec/20 11:23;gyfora;Do you think we should also backport this to 1.10? I feel that there is not much use for that;;;","08/Dec/20 11:36;twalthr;We can still do it later, if there is a need for it. I will close this issue now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AttributeError: module 'urllib' has no attribute 'parse',FLINK-20218,13341316,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Nov/20 12:03,19/Nov/20 13:50,13/Jul/23 08:12,19/Nov/20 03:39,1.10.3,,,,,1.10.3,,,API / Python,,,,,0,pull-request-available,,,,,"When executing python udf related examples, it will throw the following exception in constructing python environment:
{code:java}
AttributeError: module 'urllib' has no attribute 'parse'{code}",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BEAM-11307,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 03:39:34 UTC 2020,,,,,,,,,,"0|z0kpjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 01:58;hxbks2ks;The reason for the problem through investigation is:
1. apache-beam's `dataflow_runner.py` use `urllib.parse` before importing `urllib.parse`. 
https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/dataflow/dataflow_runner.py

2. There was no problem before, because apache-beam will import google's `protobuf` package at the beginning, and the protobuf package will import `urllib.parse`, but this behavior has been removed in the newly released `protobuf` version `3.14.0`, so indirectly caused The mistake on our side.
https://github.com/protocolbuffers/protobuf/commit/66e3562aafce093d30473555248f22b3fdc88aad#diff-e0c1278207c48e008bf5970ca223d400e2df9d0cdc648af58efeedd10bcdae5e

3. The cleanest way is to modify its incorrect use in the beam community. The current appropriate approach on the pyflink side is that we import `urllib.parse` before import beam to bypass the error behavior of beam.;;;","19/Nov/20 02:34;dian.fu;[~hxbks2ks] Thanks a lot for the investigation. +1 to import `usrlib.parse` at PyFlink side to avoid this problem as we could not change the dependent Beam version for PyFlink 1.10 and so fixing this problem at Beam side could not help to address the problem in PyFlink 1.10.;;;","19/Nov/20 03:39;dian.fu;Fixed in release-1.10 via 2fbef47bed73d3847fd88a292802551fcef64050;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition commit is delayed when records keep coming,FLINK-20213,13341284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,18/Nov/20 09:46,28/Jan/21 08:21,13/Jul/23 08:12,22/Dec/20 02:15,1.11.2,,,,,1.11.3,1.12.1,,Connectors / FileSystem,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"When set partition-commit.delay=0, Users expect partitions to be committed immediately.

However, if the record of this partition continues to flow in, the bucket for the partition will be activated, and no inactive bucket will appear.

We need to consider listening to bucket created.",,jark,lzljs3620320,Paul Lin,wind_ljy,xiaozilong,ZhuShang,zouyunhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20771,,,,,,,,,,,,,,,,,,,FLINK-20671,,,,,"26/Nov/20 04:00;zouyunhe;image-2020-11-26-12-00-23-542.png;https://issues.apache.org/jira/secure/attachment/13016054/image-2020-11-26-12-00-23-542.png","26/Nov/20 04:00;zouyunhe;image-2020-11-26-12-00-55-829.png;https://issues.apache.org/jira/secure/attachment/13016055/image-2020-11-26-12-00-55-829.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 02:15:29 UTC 2020,,,,,,,,,,"0|z0kpc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 02:35;zouyunhe;[~lzljs3620320] Hello, we get your PR  and  test it in Partition-Time tigger ,  and we find that the hive partition is commited before the data is written finished .

perhaps it has some other problems.;;;","26/Nov/20 03:43;lzljs3620320;[~zouyunhe] Thanks for you feedback, Can you check the watermark is correct?;;;","26/Nov/20 04:01;zouyunhe;[~lzljs3620320]  We found the watermark as below, the time is about 2020-11-25 21:53:02

!image-2020-11-26-12-00-23-542.png!

 

And we see the data write in hive partition , the _SUCCESS file has been writeen finished.

!image-2020-11-26-12-00-55-829.png!

But Some in-progress file was not finished, which mean this paritition is not finished.;;;","26/Nov/20 05:59;lzljs3620320;[~zouyunhe] What is the delay?;;;","26/Nov/20 06:02;zouyunhe;[~lzljs3620320] the commit delay is 1 min, the tblproperties 

'partition.time-extractor.timestamp-pattern'='$day $hour:00:00', 
 'sink.partition-commit.delay'='1 min', 
 'sink.partition-commit.policy.kind'='metastore,success-file', 
 'sink.partition-commit.trigger'='partition-time',;;;","26/Nov/20 06:10;lzljs3620320;Hi [~zouyunhe], I think your correct commit delay should be 1 hour plus 1min;;;","26/Nov/20 06:59;zouyunhe;[~lzljs3620320] OK;;;","22/Dec/20 02:15;lzljs3620320;release-1.11: 75b759d6a3a7e193521ab7f0c738b9faf908601c

release-1.12: 8a9b632dfc41e20b35c85b357f9ae5cfc2f70fff

master (1.13): a046ee8f2d1944d3c1f45f24bb14eb53682a3b09;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC source shouldn't send UPDATE_BEFORE messages if the downstream doesn't need it,FLINK-20205,13341239,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,jark,jark,18/Nov/20 06:56,27/Nov/20 14:22,13/Jul/23 08:12,27/Nov/20 14:22,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,,,,,,"Currently, the CDC source will always generate UPDATE_BEFORE and UPDATE_AFTER messages for update. However, many downstream operators don't need the UDPATE_BEFORE, e.g. temporal join and upsert sink. Currently, {{debezium-cdc-source => upsert-kafka}} will generate lots of tombstone messages, which is quite not efficient.  ",,fsk119,jark,leonard,libenchao,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 14:22:40 UTC 2020,,,,,,,,,,"0|z0kp28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/20 16:19;jark;I created a pull request which can also fix this problem: https://github.com/apache/flink/pull/14160;;;","27/Nov/20 14:22;jark;This has been fixed by FLINK-19795.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Liquid Exception: Could not find document 'dev/table/connectors/formats/index.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/upsert-kafka.zh.md,FLINK-20198,13341182,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Nov/20 01:34,18/Nov/20 02:03,13/Jul/23 08:12,18/Nov/20 02:03,1.12.0,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"When executing the script build_docs.sh, it will throw the following exception:
{code:java}
 Liquid Exception: Could not find document 'dev/table/connectors/formats/index.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/upsert-kafka.zh.md
{code}",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 02:03:00 UTC 2020,,,,,,,,,,"0|z0kopk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/20 02:03;dian.fu;Fixed in master(1.12.0) via 3f67e0655756bf3ea4d0e30a1cad8de2bd615327;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink fails with UnsupportedOperationException when using default values,FLINK-20197,13341128,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,trohrmann,trohrmann,17/Nov/20 17:35,18/Nov/20 12:14,13/Jul/23 08:12,18/Nov/20 11:48,1.12.0,,,,,1.12.0,,,API / DataStream,Connectors / FileSystem,,,,0,pull-request-available,,,,,"When using the default values for the new {{FileSink}}, it fails with 

{code}
Caused by: java.lang.UnsupportedOperationException: not supported
	at org.apache.flink.connector.file.sink.writer.FileWriter$BucketerContext.currentProcessingTime(FileWriter.java:296) ~[flink-connector-files-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssigner.getBucketId(DateTimeBucketAssigner.java:111) ~[flink-file-sink-common-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssigner.getBucketId(DateTimeBucketAssigner.java:55) ~[flink-file-sink-common-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.connector.file.sink.writer.FileWriter.write(FileWriter.java:189) ~[flink-connector-files-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.operators.sink.AbstractSinkWriterOperator.processElement(AbstractSinkWriterOperator.java:80) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamGroupedReduceOperator.processElement(StreamGroupedReduceOperator.java:68) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:193) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:179) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:152) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:574) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:538) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
{code}

The problem seems that the {{FileWriter.BucketerContext.currentProcessingTime}} throws an {{UnsupportedOperationException}}. This feels not correct. A job for reproducing the problem can be found [here|https://github.com/tillrohrmann/flink-streaming-batch-execution/tree/file-sink-fails-with-default-bucket-assigner].",,gaoyunhaii,kkl0u,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20115,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 11:48:02 UTC 2020,,,,,,,,,,"0|z0kodk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/20 11:48;kkl0u;master: ac94befe5992ce34d9c3e5050a485e8aa2681cd3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs endpoint returns duplicated jobs,FLINK-20195,13341116,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,airblader,airblader,17/Nov/20 16:52,10/Feb/22 08:31,13/Jul/23 08:12,12/Dec/21 11:21,1.11.2,,,,,1.13.6,1.14.3,1.15.0,Runtime / Coordination,Runtime / REST,,,,0,pull-request-available,,,,,"The GET /jobs endpoint can, for a split second, return a duplicated job after it has been cancelled. This occurred in Ververica Platform after canceling a job (using PATCH /jobs/\{jobId}) and calling GET /jobs.

I've reproduced this and queried the endpoint in a relatively tight loop (~ every 0.5s) to log the responses of GET /jobs and got this:

 

 
{code:java}
…

{""jobs"":[{""id"":""e110531c08dd4e3dbbfcf7afc1629c3d"",""status"":""RUNNING""},{""id"":""53fd11db25394308862c997dce9ef990"",""status"":""CANCELLING""}]}

{""jobs"":[{""id"":""e110531c08dd4e3dbbfcf7afc1629c3d"",""status"":""RUNNING""},{""id"":""53fd11db25394308862c997dce9ef990"",""status"":""CANCELLING""}]}

{""jobs"":[{""id"":""e110531c08dd4e3dbbfcf7afc1629c3d"",""status"":""FAILED""},{""id"":""53fd11db25394308862c997dce9ef990"",""status"":""CANCELED""},{""id"":""53fd11db25394308862c997dce9ef990"",""status"":""CANCELED""}]}

{""jobs"":[{""id"":""53fd11db25394308862c997dce9ef990"",""status"":""CANCELED""},{""id"":""e110531c08dd4e3dbbfcf7afc1629c3d"",""status"":""FAILED""}]}

{""jobs"":[{""id"":""53fd11db25394308862c997dce9ef990"",""status"":""CANCELED""},{""id"":""e110531c08dd4e3dbbfcf7afc1629c3d"",""status"":""FAILED""}]}

…{code}
 

You can see in in between that for just a moment, the endpoint returned the same Job ID twice.

 ",,airblader,gvauvert,keatspeeks,klion26,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24594,,,,,,,,,FLINK-25732,,FLINK-24232,,,,,,,,,,,"09/Dec/21 17:36;keatspeeks;logs (2).csv;https://issues.apache.org/jira/secure/attachment/13037203/logs+%282%29.csv",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 10 08:31:27 UTC 2022,,,,,,,,,,"0|z0koaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/20 00:34;chesnay;Am I understanding you correctly that this happens when the job transitioned to CANCELED? Or can it be reproduced for any job, regardless of state transitions?

If it just happens during a transition, then this likely occurs due to {{Dispatcher#requestMultipleJobDetails}} not having a fully consistent view over all jobs. It first queries all job masters for it's respective job, and then the execution graph store where completed jobs reside in. It is conceivable that a JM can return a job, the job is then archived to the store, and then we retrieve the same job from the store.
An easy fix would be to de-duplicate entries based on the Job ID.;;;","30/Nov/20 07:24;airblader;[~chesnay] Yes, we only observed this during a transition to CANCELED.;;;","30/Nov/20 08:50;trohrmann;I think in the current master, it should not be possible that a job is archived and there is still a {{DispatcherJob}} from which we can get a duplicated result since we remove the {{DispatcherJob}} when we are archiving the {{ExecutionGraph}}.;;;","30/Nov/20 11:14;chesnay;Is there not a small window where the job is in a terminal state but the Dispatcher logic for terminal jobs have not been executed yet?
In {{JobMaster#jobStatusChanged}} the notification to the dispatcher is executed by the {{scheduledExecutorService}}, so it should be possible for a request from the REST API to be processed.

- job terminates but dispatcher does not know it yet
- REST API queries DispatcherJobs, and receives the terminated yet
- Dispatcher is notified, archives job and cleans up DispatcherJob
- REST API queries ExecutionGraphStore, receives the same terminated job;;;","30/Nov/20 13:05;trohrmann;I think the part where the {{Dispatcher}} asks the {{DispatcherJobs}} for their state and then the {{ArchivedExecutionGraphStore}} happens in the same method call. Hence, there should not be anything else which is executed in between.;;;","30/Nov/20 13:12;chesnay;true, I forgot that both queries are issued by the Dispatcher and not the handler thread.;;;","30/Nov/20 14:28;trohrmann;But somewhere this duplication must come from. It could also be that it is a problem of the {{1.11.x}} version.;;;","30/Nov/20 14:51;chesnay;I only briefly looked at the 1.11 code, but it seems the Dispatcher follows a similar pattern there. I'm currently trying to reproduce the issue on 1.12.;;;","30/Nov/20 16:24;chesnay;I have not been able to reproduce the issue with 1.11.2 or 1.12 so far. That said, chances are that this is requires very specific timing.

Ideally we'll spend more time reproducing the issue; if this does occur it would be good to figure out why, if anything to ensure it is truly no longer present in 1.12 .;;;","09/Sep/21 14:12;chesnay;When a job is suspended (or terminates in any other way) then information about the job is stored in the Dispatchers executionGraphStore.

This leads to duplication if the same JM is re-elected as the leader; it restarts the job and then we have one entry for the suspended job in the executionGraphStore, and an active JobMaster for the running job.

If another JM is elected then this issue is not visible because the executionGraphStore is not persisted across Dispatchers.


I'm not sure if there is a clean solution to the problem (that doesn't require a lot of work). We do want suspended jobs to be added to the store so that they are still accessible while the job is suspended. Removing that entry when the job restarts makes somewhat sense and would fix the issue.;;;","01/Oct/21 11:05;trohrmann;[~chesnay] are you sure that we store {{SUSPENDED}} jobs in the {{executionGraphStore}}? To me it looks as if the {{DefaultJobMasterServiceProcess}} will only complete the result future in case of a globally terminal state. If we are indeed storing {{SUSPENDED}} jobs and also archive them, then this is not correct and should be fixed imo.;;;","19/Oct/21 15:07;chesnay;Maybe not in 1.11, but definitely in more recent versions due to FLINK-22434.;;;","08/Dec/21 11:02;keatspeeks;This issue is very problematic for us. It happens very frequently, and the duplicated ""suspending"" job never disappears from the ""running jobs"". Worse : it eventually leads to a situation where the JM tries to restore two checkpoints from ZK instead of one, and fails because only one exists.

 

We're considering forking flink just to get rid of this FLINK-22434 ticket that introduced the bug. Which is not ideal.;;;","08/Dec/21 16:13;trohrmann;Which version of Flink are you using [~keatspeeks]?;;;","08/Dec/21 19:39;keatspeeks;[~trohrmann] We're on 1.13.2.

 

More details : We have ~30 JMs, with 1 job/JM. The duplicated jobs (same id, with one of them ""suspending"") appear at the same time than ZK ""lost connection"" events, which trigger re-elections of the same JMs, so we believe it is exactly this issue. It appears with relatively low probabilities (just a few JMs at a time), but it can pile up if we don't clean them.

*That's the first issue (job duplication in the UI)*

 

After some time, if we don't clean them (we clean them by stopping and redeploying the job btw), some JMs fail with this :
org.apache.flink.util.FlinkException: Could not retrieve checkpoint XXXXXX from state handle under /0000000000000XXXXXX. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.
Caused by: java.io.FileNotFoundException: File does not exist: /XXX/YYY/completedCheckpoint94b37b00554d
*That's the second issue.* This ZK key should not exist, it lives along another one, like this :

/default/<jobid>/0000000000000XXXXXX  <= ""ghost"" key, referencing a non-existent checkpoint

/default/<jobid>/0000000000000YYYYYY <= good key, referencing the last checkpoint of the running job

So it seems the job duplication has an impact on ZK (and not only the REST API/ UI), preventing the JMs to restart. We still haven't understood why. It can be fixed (temporarily) by removing the ""ghost"" key : then the JM restart successfully and the job is restored properly.;;;","09/Dec/21 09:55;trohrmann;Could it be the case that you are running into FLINK-24543 with the second issue? This problem should be fixed with the upcoming 1.13.4 and 1.14.1 releases.

Do you have by chance the logs of the problematic runs for the first problem (ideally on DEBUG level)? This could help us pinpointing the problem.;;;","09/Dec/21 17:36;keatspeeks;Yes, it could be FLINK-24543. We'll update to the next version as soon as possible. We really thought these two issues were strongly related.

Thank you !

 

About the logs for the first issue, I've attached them, but they're mostly on WARN level unfortunately. I'll see what i can do for mort detailed ones but it's hard to reproduce (sometimes it goes without issues for days). 

 

[^logs (2).csv];;;","10/Dec/21 09:26;trohrmann;Thanks for the logs [~keatspeeks]. [~chesnay] found the problem which is double accounting in the {{Dispatcher.requestMultipleJobDetails}} method w/o deduplicating the jobs based on their {{JobID}}. The fix should be done very soon.;;;","12/Dec/21 11:21;chesnay;master: 75452cfab0e2d099896ab7f9da721a91427005e8
1.14: d3977d2912ed284aa1f7cd7c216232f9bcff73e5
1.13: 11ac7c223812c47251fcc4279d50bd2e1da747fb;;;","10/Feb/22 08:31;gvauvert;Please note that this fix triggers https://issues.apache.org/jira/browse/FLINK-25732 that will be/is fixed in 1.14.4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceFetcherManager.commitOffsets() should handle the case when there is no split fetcher.,FLINK-20194,13341089,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,17/Nov/20 15:25,24/Nov/20 13:13,13/Jul/23 08:12,24/Nov/20 07:43,,,,,,1.11.3,1.12.0,,Connectors / Kafka,,,,,0,pull-request-available,,,,,"The {{KafkaSourceFetcherManager.commitOffsets()}} may throw NPE when there is no split fetcher. Given that such cases should usually be rare and only happens when the source is shutting down or just started. We can create a new {{KafkaPartitionSplitReader}} to commit offset and close it immediately. The main thread maybe blocked for some time, but given that there is no event to process at that point, there should be no performance concerns.",,becket_qin,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20114,FLINK-20157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 07:43:48 UTC 2020,,,,,,,,,,"0|z0ko4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 07:43;becket_qin;merged to master
4bde1c9c10c0db2c44e25ea81ba04983b870b518.

The fix for SingleThreadFetcherManager has been cherry-picked back to 1.11.
78236cb12a78ef128a55ce14a0eb5eefb73cc735;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinator should catch exception thrown from SplitEnumerator.start(),FLINK-20193,13341085,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,17/Nov/20 15:11,12/Dec/20 11:57,13/Jul/23 08:12,23/Nov/20 20:49,,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,,,,,"This is a bug introduced in FLINK-18323 where we put {{SplitEnumrator.start()}} into the CoordinatorExecutor without catching exception. The fix is catching exception thrown from {{SplitEnumerator.start()}} and fail the job accordingly, just like the other method delegated to the {{CoordinatorExecutor.}}",,becket_qin,dian.fu,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20114,FLINK-20157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 20:49:03 UTC 2020,,,,,,,,,,"0|z0ko40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 20:49;sewen;Fixed in 1.12.0 (master) via 57e41f98024f950adf8cbc02622435d3d2f695bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FactoryUtil#createTableSource will be confused by a table source and a table sink factory with same identifier,FLINK-20187,13341006,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,TsReaper,TsReaper,17/Nov/20 08:43,19/Nov/20 03:42,13/Jul/23 08:12,19/Nov/20 03:42,1.12.0,,,,,1.12.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"When creating a table source I'm faced with the following exception:
{code:java}
 Caused by: org.apache.flink.table.api.ValidationException: Multiple factories for identifier 'odps' that implement 'org.apache.flink.table.factories.DynamicTableFactory' found in the classpath.

Ambiguous factory classes are:
{code}
However there is only one table source factory with this identifier, and another table sink factory with this identifier. {{FactoryUtil#createTableSource}} shouldn't be confused.

This is caused by {{FactoryUtil.java}} line 370, where {{factory = discoverFactory(context.getClassLoader(), DynamicTableFactory.class, connectorOption);}} should be {{factory = discoverFactory(context.getClassLoader(), factoryClass, connectorOption);}}.

I understand that this change aims to display a better exception message. We might need to change the logic of selecting a proper exception message a little bit.",,jark,liufangliang,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18938,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 03:42:42 UTC 2020,,,,,,,,,,"0|z0knmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 08:43;TsReaper;[~liufangliang] please take a look.;;;","17/Nov/20 09:30;liufangliang;[~TsReaper] I will take a look, thanks for your feedback.

CC :[~jark];;;","17/Nov/20 14:07;twalthr;[~jark] is this related to the commit that you merged recently? Let's revert the commit for now if this is the case.;;;","17/Nov/20 14:46;jark;I will take this issue, but I think we don't need to revert the commit. It only affects very few connectors. 

I guess the ""odps"" connector has two factory implementation, each implements {{DynamicTableSinkFactory}} and {{DynamicTableSourceFactory}}. 
Usually, a connector should implement them both in one factory class. So I think ""odps"" is a corner case. 
But I will fix this. ;;;","19/Nov/20 03:42;jark;Fixed in master (1.12.0): b4d885dc9cfc28d7a6cb2f58925cf072dc916b0b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FactoryUtil will give an incorrect error message when multiple factories fit the connector identifier,FLINK-20186,13341001,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,TsReaper,TsReaper,17/Nov/20 08:32,24/Nov/20 08:53,13/Jul/23 08:12,24/Nov/20 08:53,1.11.2,1.12.0,,,,1.12.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"I was playing with user-defined connectors when I found the following error message:

{code}
Caused by: org.apache.flink.table.api.ValidationException: Multiple factories for identifier 'odps' that implement 'org.apache.flink.table.factories.DynamicTableFactory' found in the classpath.

Ambiguous factory classes are:

java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:258)
	at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:370)
	... 71 more
{code}

This is caused by {{FactoryUtil.java}} line 265, where {{.map(f -> factories.getClass().getName())}} should be {{.map(f -> f.getClass().getName())}}",,jark,libenchao,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 08:53:58 UTC 2020,,,,,,,,,,"0|z0knlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 08:33;TsReaper;[~twalthr] please take a look.;;;","17/Nov/20 10:36;twalthr;Thanks for reporting this [~TsReaper]. I will assign this ticket to me.;;;","24/Nov/20 08:53;twalthr;Fixed in 1.12.0: e31320de9453f64a51a853a3bf6f3f3d6b83f4d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the default PYTHONPATH is overwritten in client side,FLINK-20183,13340973,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,17/Nov/20 06:11,18/Nov/20 08:35,13/Jul/23 08:12,18/Nov/20 08:35,1.10.0,1.11.2,1.12.0,,,1.10.3,1.11.3,1.12.0,API / Python,,,,,0,pull-request-available,,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 08:35:44 UTC 2020,,,,,,,,,,"0|z0knf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/20 08:35;dian.fu;Fixed in
- master(1.12.0) via e7caf34a799bfbb4d7e634c1f8216920082d68b2
- release-1.11 via 61f4148524931e47761c52c39e571e5f7bad7885
- release-1.10 via 2eaba7499dbdd3447425e54bea024fc09142a517;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remote file does not exist -- broken link!!! ,FLINK-20178,13340951,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,dian.fu,dian.fu,17/Nov/20 02:02,18/Nov/20 02:02,13/Jul/23 08:12,18/Nov/20 02:02,1.12.0,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9660&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98

{code}
[2020-11-16 21:08:03] ERROR `/zh/dev/table/connectors/downloads.html' not found.
[2020-11-16 21:08:06] ERROR `/zh/ops/memory/ops/config.zh.md %}' not found.
http://localhost:4000/zh/dev/table/connectors/downloads.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/ops/memory/ops/config.zh.md%20%25%7D:
Remote file does not exist -- broken link!!!
---------------------------------------------------------------------------
Found 2 broken links.
{code}",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 02:02:23 UTC 2020,,,,,,,,,,"0|z0kna8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 02:23;hxbks2ks;[~dian.fu] Thanks a lot for creating this issue. If you have't fixed it, could you assign it to me;;;","17/Nov/20 02:25;dian.fu;[~hxbks2ks] Thanks a lot. I have assigned it to you~;;;","18/Nov/20 02:02;dian.fu;Fixed in master(1.12.0) via 0e5749c315631d92e614bae0a8e394c39984b42d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Liquid Exception: Could not find document 'dev/table/connectors/kinesis.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/index.zh.md,FLINK-20177,13340949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,17/Nov/20 01:26,17/Nov/20 02:28,13/Jul/23 08:12,17/Nov/20 02:28,1.12.0,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"When executing the script build_docs.sh, it will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'dev/table/connectors/kinesis.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/index.zh.md
{code}",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 02:28:39 UTC 2020,,,,,,,,,,"0|z0kn9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 01:38;hxbks2ks;The problem is that index.zh.md points to the wrong link dev/table/connectors/kinesis.md, the correct one should be dev/table/connectors/kinesis.zh.md. I will submit a PR to fix it.;;;","17/Nov/20 02:28;dian.fu;Fixed in master(1.12.0) via 6caee2ba76ea3edf247d6800d5f2d24efa77e94e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro Confluent Registry SQL format does not support adding nullable columns,FLINK-20175,13340904,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,16/Nov/20 19:25,29/Nov/20 19:47,13/Jul/23 08:12,29/Nov/20 19:47,1.11.2,1.12.0,,,,1.11.3,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"The {{AvroSchemaConverter#convertToSchema}} generates a union with ""null"" for nullable logical types, but it does not set the default value to null. In turn it makes it impossible to generate a backwards compatible schema from a DDL statement.

Example:
1. Create a table: {{CREATE TABLE t (id INT NOT NULL) WITH (/* avro confluent format*/)}}
2. Create a new table over the same topic or alter the old table with {{CREATE TABLE newT(id INT NOT NULL, optionalDescription STRING) WITH (/* avro confluent format */)}}
3. When reading from {{newT}} records inserted into {{t}} it will fail, because the {{optionalDescription}} has no default value.",,dwysakowicz,jark,lingyaKK,rmetzger,swhelan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 29 19:47:10 UTC 2020,,,,,,,,,,"0|z0kmzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 14:31;dwysakowicz;I included a fix as part of this PR: https://github.com/apache/flink/pull/14085;;;","25/Nov/20 18:04;dwysakowicz;Fixed in master: efc12ca13743c2562d297a511d82cbdccc718cd0;;;","26/Nov/20 14:17;rmetzger;Are we planing to backport this fix to 1.11.3? (Q from the user list: https://lists.apache.org/thread.html/r131d7dc3f4f68bf0ad2c28c28416b2a086446fb56ab64b68b68b0303%40%3Cuser.flink.apache.org%3E );;;","26/Nov/20 14:23;dwysakowicz;I am currently checking if it is feasible. I am running a backported version on my azure: https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=578&view=results

Unfortunately a backport is not as easy as simply cherry picking.;;;","29/Nov/20 19:47;dwysakowicz;Backported to 1.11.3: d2f24e8177f05fd41c8804f2bb962a1a7500b095;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatedSourceITCase.testEnumeratorReaderCommunication fails with IllegalStateException,FLINK-20173,13340859,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,rmetzger,rmetzger,16/Nov/20 15:54,25/Dec/20 01:23,13/Jul/23 08:12,25/Dec/20 01:23,1.11.3,,,,,,,,Connectors / Common,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9633&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa

{code}
2020-11-16T13:03:16.9279059Z [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.612 s <<< FAILURE! - in org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase
2020-11-16T13:03:16.9279932Z [ERROR] testEnumeratorReaderCommunication(org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase)  Time elapsed: 0.067 s  <<< ERROR!
2020-11-16T13:03:16.9280589Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-16T13:03:16.9281391Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-11-16T13:03:16.9282173Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:677)
2020-11-16T13:03:16.9282802Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-11-16T13:03:16.9283356Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1697)
2020-11-16T13:03:16.9283953Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1679)
2020-11-16T13:03:16.9284600Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.executeAndVerify(CoordinatedSourceITCase.java:84)
2020-11-16T13:03:16.9285253Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.testEnumeratorReaderCommunication(CoordinatedSourceITCase.java:52)
2020-11-16T13:03:16.9285749Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-16T13:03:16.9286151Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-16T13:03:16.9286627Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-16T13:03:16.9287041Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-16T13:03:16.9287456Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-11-16T13:03:16.9287932Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-11-16T13:03:16.9288389Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-11-16T13:03:16.9288851Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-11-16T13:03:16.9289280Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-11-16T13:03:16.9289638Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-11-16T13:03:16.9290071Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-11-16T13:03:16.9290510Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-11-16T13:03:16.9291178Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-11-16T13:03:16.9291665Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-16T13:03:16.9292059Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-16T13:03:16.9292466Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-16T13:03:16.9292856Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-16T13:03:16.9293258Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-16T13:03:16.9293655Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-11-16T13:03:16.9294131Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-11-16T13:03:16.9294517Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-11-16T13:03:16.9294875Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-16T13:03:16.9295298Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-11-16T13:03:16.9295788Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-11-16T13:03:16.9296265Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-11-16T13:03:16.9296739Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-11-16T13:03:16.9297241Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-11-16T13:03:16.9297740Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-11-16T13:03:16.9298207Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-11-16T13:03:16.9298652Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-11-16T13:03:16.9299212Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-11-16T13:03:16.9299907Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-16T13:03:16.9301093Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-16T13:03:16.9302068Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-11-16T13:03:16.9302897Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-11-16T13:03:16.9303594Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-11-16T13:03:16.9304414Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-11-16T13:03:16.9305221Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:384)
2020-11-16T13:03:16.9305908Z 	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
2020-11-16T13:03:16.9306567Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-16T13:03:16.9307220Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-16T13:03:16.9307909Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-16T13:03:16.9308708Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-16T13:03:16.9309544Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-16T13:03:16.9310390Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-16T13:03:16.9311404Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-16T13:03:16.9312075Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-16T13:03:16.9312712Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-16T13:03:16.9313331Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-16T13:03:16.9313899Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-16T13:03:16.9314396Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-16T13:03:16.9314796Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-16T13:03:16.9315188Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-16T13:03:16.9315584Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-16T13:03:16.9315967Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-16T13:03:16.9316332Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-16T13:03:16.9316713Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-16T13:03:16.9317063Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-16T13:03:16.9317395Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-16T13:03:16.9317768Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-16T13:03:16.9318184Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-16T13:03:16.9318620Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-16T13:03:16.9319055Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-16T13:03:16.9320025Z Caused by: java.lang.IllegalStateException: Called 'finishedOrAvailableLater()' with shut-down fetchers but non-empty queue
2020-11-16T13:03:16.9320611Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.finishedOrAvailableLater(SourceReaderBase.java:271)
2020-11-16T13:03:16.9321449Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:118)
2020-11-16T13:03:16.9322205Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:142)
2020-11-16T13:03:16.9322732Z 	at org.apache.flink.streaming.api.operators.SourceOperator.pollNextRecord(SourceOperator.java:219)
2020-11-16T13:03:16.9323321Z 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:209)
2020-11-16T13:03:16.9323819Z 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:47)
2020-11-16T13:03:16.9324421Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
2020-11-16T13:03:16.9324949Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:351)
2020-11-16T13:03:16.9325456Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191)
2020-11-16T13:03:16.9326016Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181)
2020-11-16T13:03:16.9326546Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:564)
2020-11-16T13:03:16.9327008Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:534)
2020-11-16T13:03:16.9327444Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
2020-11-16T13:03:16.9327835Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
2020-11-16T13:03:16.9328163Z 	at java.lang.Thread.run(Thread.java:748)
{code}",,kezhuw,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19448,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 25 01:23:09 UTC 2020,,,,,,,,,,"0|z0kmps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/20 16:02;trohrmann;Is this also relevant for {{1.12.0}} [~rmetzger]?;;;","16/Nov/20 16:07;rmetzger;[~becket_qin] told me offline that this has been fixed already in 1.12.0, and that we might miss a backport only to fix this issue.;;;","18/Nov/20 03:03;xtsong;I'm deferring the fix version to 1.11.4 for now. Will change it back if it makes the 1.11.3 release in the end.;;;","24/Dec/20 13:34;kezhuw;This has been solved in 1.11.3, I checked it by download source jar of {{flink-connector-base:1.11.3}}. One could also check it online:
 * 1.11.3 source code for [SourceReaderBase.finishedOrAvailableLater |https://github.com/apache/flink/blob/release-1.11.3/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java#L273].
 * Diff in commit [fe2b269b8bcab8a2344841c09d643cf2ebdcb5be|https://github.com/apache/flink/commit/fe2b269b8bcab8a2344841c09d643cf2ebdcb5be] show that the throw statement was replaced with return statement.;;;","25/Dec/20 01:23;xtsong;Thanks [~kezhuw].

Closing this ticket. Fixed by FLINK-19448.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FlinkKafkaProducerITCase.testRunOutOfProducersInThePool failed with ""CheckpointException: Could not complete snapshot 1 for operator MockTask (1/1)#0. Failure reason: Checkpoint was declined.""",FLINK-20166,13340733,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,dian.fu,dian.fu,16/Nov/20 01:50,17/Nov/20 19:22,13/Jul/23 08:12,17/Nov/20 16:37,1.12.0,,,,,1.12.0,,,Connectors / Kafka,Runtime / Checkpointing,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9596&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

{code}
2020-11-15T22:54:53.4151222Z [ERROR] testRunOutOfProducersInThePool(org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase)  Time elapsed: 61.224 s  <<< ERROR!
2020-11-15T22:54:53.4152487Z org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator MockTask (1/1)#0. Failure reason: Checkpoint was declined.
2020-11-15T22:54:53.4153577Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:226)
2020-11-15T22:54:53.4154747Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:158)
2020-11-15T22:54:53.4155760Z 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:343)
2020-11-15T22:54:53.4156772Z 	at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.snapshotWithLocalState(AbstractStreamOperatorTestHarness.java:638)
2020-11-15T22:54:53.4157865Z 	at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.snapshot(AbstractStreamOperatorTestHarness.java:630)
2020-11-15T22:54:53.4159065Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.testRunOutOfProducersInThePool(FlinkKafkaProducerITCase.java:527)
{code}",,dian.fu,rmetzger,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18634,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 19:22:46 UTC 2020,,,,,,,,,,"0|z0klxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 10:40;rmetzger;[~roman_khachatryan] can you take a look?;;;","17/Nov/20 16:36;roman;I think this is the same problem as FLINK-18634. The test is different, but the exception reported from kafka is the same:  
{code:java}
Timeout expired after 60000milliseconds while awaiting InitProducerId 
{code}
 

There are also InterruptedExceptions followed by
{code:java}
Detected producer leak. Thread name: kafka-producer-network-thread | producer-MockTask-00000000000002a000000000000002c-0
{code}
Which also seem to be the same issue (InterruptedExceptions are also present in FLINK-18634 log, and thread leak follows from it).;;;","17/Nov/20 16:37;roman;Closing as duplicate.;;;","17/Nov/20 19:22;rmetzger;Thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOITCase.checkForProhibitedLogContents: Error occurred during initialization of boot layer java.lang.IllegalStateException: Module system already initialized,FLINK-20165,13340732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,dian.fu,dian.fu,16/Nov/20 01:32,26/Mar/21 16:34,13/Jul/23 08:12,26/Mar/21 16:34,1.11.3,1.13.0,,,,1.11.3,1.12.0,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9597&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=8560c56f-9ec1-5c40-4ff5-9d3eaaaa882d

{code}
2020-11-15T22:42:03.3053212Z 22:42:03,303 [   Time-limited test] INFO  org.apache.flink.yarn.YARNSessionFIFOITCase                  [] - Finished testDetachedMode()
2020-11-15T22:42:37.9020133Z [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 2, Time elapsed: 67.485 s <<< FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOSecuredITCase
2020-11-15T22:42:37.9022015Z [ERROR] testDetachedMode(org.apache.flink.yarn.YARNSessionFIFOSecuredITCase)  Time elapsed: 12.841 s  <<< FAILURE!
2020-11-15T22:42:37.9023701Z java.lang.AssertionError: 
2020-11-15T22:42:37.9025649Z Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-1_0/application_1605480087188_0002/container_1605480087188_0002_01_000002/taskmanager.out with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2020-11-15T22:42:37.9026730Z [
2020-11-15T22:42:37.9027080Z Error occurred during initialization of boot layer
2020-11-15T22:42:37.9027623Z java.lang.IllegalStateException: Module system already initialized
2020-11-15T22:42:37.9033278Z java.lang.IllegalStateException: Module system already initialized
2020-11-15T22:42:37.9033825Z ]
2020-11-15T22:42:37.9034291Z 	at org.junit.Assert.fail(Assert.java:88)
2020-11-15T22:42:37.9034971Z 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:479)
2020-11-15T22:42:37.9035814Z 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:83)
{code}",,dian.fu,dwysakowicz,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 19:00:29 UTC 2020,,,,,,,,,,"0|z0klxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/20 09:08;trohrmann;cc [~rmetzger] could you take a look at this problem?;;;","16/Nov/20 09:18;rmetzger;Yes;;;","18/Nov/20 03:02;xtsong;I'm deferring the fix version to 1.11.4 for now. Will change it back if it makes the 1.11.3 release in the end.;;;","18/Nov/20 07:48;rmetzger;I believe this is a JVM bug. Searching for ""Module system already initialized"" reveals very few search results. Basically just the JVM source code, and this ticket. We might be the first to hit this issue.
As a mitigation, I propose to update the JDK we are using in our CI docker image. If the issue occurs in the latest JDK as well, I'll reach out to them.;;;","20/Nov/20 19:00;rmetzger;update & improve logging in release-1.11: https://github.com/apache/flink/commit/ffb14e41a4b251d7c8f3644faca8f3b9877b243e

same for master: https://github.com/apache/flink/commit/6e3870513ae49e7b960edf71779b6df6d227194c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix time zone problems for some time related functions,FLINK-20162,13340629,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jark,jark,14/Nov/20 10:17,29/Apr/21 15:22,13/Jul/23 08:12,29/Apr/21 15:22,,,,,,,,,Table SQL / API,,,,,0,auto-unassigned,,,,,"Currently, lots of time related functions are returning {{TIMESTAMP}} type, including {{PROCTIME()}}, {{NOW()}}, {{LOCALTIMESTAMP}},etc. However, they should return {{TIMESTAMP WIHT LOCAL TIME ZONE}} type. The session time zone will be used when converting {{TIMESTAMP WITH LOCAL TIME ZONE}} to string, but not for {{TIMESTAMP}} type.

That's why many users report the result is incorrect with 8 hours when converting these functions to string.",,fsk119,hailong wang,jark,leonard,libenchao,twalthr,wei.wei,xiaodao,zhisheng,zhoujira86,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19757,FLINK-18206,FLINK-16889,FLINK-16938,FLINK-20387,FLINK-11223,FLINK-21617,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 15:22:27 UTC 2021,,,,,,,,,,"0|z0klao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/20 09:41;twalthr;[~jark] I'm about to open a PR to make adding built-in functions easier. Once we have an easy and consistent function architecture, it should be straight forward to add and fix functions.;;;","17/Nov/20 03:25;jark;Sounds greate [~twalthr].;;;","16/Apr/21 10:50;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:50;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","29/Apr/21 15:22;jark;This has been fixed in 1.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSource does not implement ResultTypeQueryable,FLINK-20158,13340579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zzodoo,rmetzger,rmetzger,13/Nov/20 18:51,23/Nov/20 09:20,13/Jul/23 08:12,23/Nov/20 09:20,1.12.0,,,,,1.12.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,"As a user of the new Kafka Source introduced in (FLINK-18323), I always have to specify the return type:

{code}
DataStream<Event> events = env.fromSource(source, WatermarkStrategy.noWatermarks(),
			""Kafka Source"").returns(TypeInformation.of(Event.class));
{code}

The old Kafka source implementation implements {{ResultTypeQueryable}}, which allows the DataStream API to get the return type from the deserializer.
The new Kafka Source also should have access to the produced type from the deserializer to forward it.",,becket_qin,rmetzger,zzodoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 09:20:04 UTC 2020,,,,,,,,,,"0|z0kkzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 18:52;rmetzger;[~becket_qin] I guess this would be a nice usability extension. I haven't checked if this works with the new ""env.fromSource()"", but I see no reason why it shouldn't.;;;","16/Nov/20 12:01;zzodoo;[~rmetzger] I'm interested to pick this task. Could you assign it to me?;;;","16/Nov/20 14:01;becket_qin;[~rmetzger] Thanks for the suggestion. That sounds a good improvement. [~zzodoo] I just assigned the ticket to you.;;;","17/Nov/20 07:50;rmetzger;[~zzodoo] What's the rough timeline for you to open the ticket? We are planning to create the next release candidate soon (end of this week), and I would like to include this fix into it.;;;","17/Nov/20 12:07;zzodoo;[~rmetzger] I think I will open PR in two days, most likely earlier;;;","17/Nov/20 19:35;rmetzger;Awesome, thanks!;;;","23/Nov/20 09:20;rmetzger;Resolved on master in https://github.com/apache/flink/commit/ea6d42be514df9956c36ba8713897117eec0de67.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinatorProvider kills JobManager with IllegalStateException on job submission,FLINK-20157,13340553,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,rmetzger,rmetzger,13/Nov/20 16:10,14/Jan/21 09:20,13/Jul/23 08:12,24/Nov/20 13:14,1.12.0,,,,,1.12.0,,,Connectors / Kafka,,,,,0,,,,,,"While setting up a test job using the new Kafka source for testing the RC1 of Flink 1.12, my JobManager died with a fatal exception:

{code}
2020-11-13 17:05:53,947 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Flat Map -> Sink: Print to Std. Out (1/1) (attempt #0) with attempt id fc36327d85e775204e82fc8507bf4264 to 192.168.1.25:57387-78ca68 @ robertsbabamac2.localdomain (dataPort=57390) with allocation id a8d918c0cfb57305908ce5a4f4787034
2020-11-13 17:05:53,988 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'SourceCoordinator-Source: Kafka Source' produced an uncaught exception. Stopping the process...
java.lang.IllegalStateException: Should never happen. This factory should only be used by a SingleThreadExecutor.
        at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider$CoordinatorExecutorThreadFactory.newThread(SourceCoordinatorProvider.java:94) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
        at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619) ~[?:1.8.0_222]
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932) ~[?:1.8.0_222]
        at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) ~[?:1.8.0_222]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) ~[?:1.8.0_222]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_222]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
{code}

I'm using the KafkaSource as documented, with a single partition topic:
{code:java}
		KafkaSource<Event> source = KafkaSource
			 .<Event>builder()
			 .setBootstrapServers(brokers)
			 .setGroupId(""myGroup"")
			 .setTopics(Arrays.asList(kafkaTopic))
			 .setDeserializer(new NewEventDeserializer())
			 .build();
{code}",,becket_qin,dian.fu,leonard,rmetzger,sewen,stevenz3wu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20193,FLINK-20223,FLINK-20194,FLINK-20222,FLINK-20081,,,,,,,,,,,,,,,,,,,FLINK-20081,,,,,,FLINK-20261,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 13:14:52 UTC 2020,,,,,,,,,,"0|z0kkts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 16:19;rmetzger;[~becket_qin] Can you take a look?;;;","20/Nov/20 15:53;becket_qin;[~rmetzger] Thanks for reporting the issue. It seems that there are a few bugs related to this issue.

I have fired a few tickets, including FLINK-20193, FLINK-20194, FLINK-20222, FLINK-20223 and FLINK-20081. With the patches from those tickets, the {{StateMachineExample}} runs fine.;;;","23/Nov/20 07:38;dian.fu;It seems that this issue depends on several other tickets. If we see this issue as a blocker issue, then I guess we need also mark all the dependent issues as blockers.;;;","24/Nov/20 13:14;sewen;Fixed for 1.12.0 by fixing all the associated issues. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLClientKafkaITCase.testKafka failed with ""Did not get expected results before timeout.""",FLINK-20149,13340526,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,dian.fu,dian.fu,13/Nov/20 14:18,16/Nov/20 09:58,13/Jul/23 08:12,16/Nov/20 07:28,1.12.0,,,,,1.12.0,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9558&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-11-13T13:47:21.6002762Z Nov 13 13:47:21 [ERROR] testKafka[0: kafka-version:2.4.1 kafka-sql-version:universal](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 183.015 s <<< FAILURE! 2020-11-13T13:47:21.6003744Z Nov 13 13:47:21 java.lang.AssertionError: Did not get expected results before timeout. 2020-11-13T13:47:21.6004745Z Nov 13 13:47:21 at org.junit.Assert.fail(Assert.java:88) 2020-11-13T13:47:21.6005325Z Nov 13 13:47:21 at org.junit.Assert.assertTrue(Assert.java:41) 2020-11-13T13:47:21.6006007Z Nov 13 13:47:21 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.checkCsvResultFile(SQLClientKafkaITCase.java:226) 2020-11-13T13:47:21.6007091Z Nov 13 13:47:21 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:166)
{code}",,aljoscha,dian.fu,leonard,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20098,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 16 09:58:52 UTC 2020,,,,,,,,,,"0|z0kkns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/20 12:50;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9570&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","14/Nov/20 18:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9581&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","14/Nov/20 18:58;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9576&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","14/Nov/20 19:05;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9576&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","14/Nov/20 19:05;rmetzger;Upgrading to blocker, all e2e tests are failing;;;","15/Nov/20 01:13;dian.fu;Found the following log:
{code}
13:45:17,751 [ Thread-23] INFO org.apache.flink.tests.util.flink.FlinkDistribution [] - [31;1m[ERROR] Could not execute SQL statement. Reason:
13:45:17,752 [ Thread-23] INFO org.apache.flink.tests.util.flink.FlinkDistribution [] - java.lang.ClassNotFoundException: org.apache.flink.connector.file.src.reader.BulkFormat[0m
{code}

It should be introduced by FLINK-20098.  cc [~aljoscha];;;","16/Nov/20 01:52;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9596&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","16/Nov/20 07:24;rmetzger;Thanks for looking into it. I will revert FLINK-20098.;;;","16/Nov/20 07:28;rmetzger;I reverted https://issues.apache.org/jira/browse/FLINK-20098 and reopened the ticket.;;;","16/Nov/20 09:58;aljoscha;Sorry about this one, I saw the ""test instability"" but was assuming that it was just because ""SQLClientKafkaITCase"" was unstable because it doesn't have anything to do with the file system connector, I assumed wrongly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Invalid lambda deserialization"" when trying to read metadata from Kafka in SQL",FLINK-20147,13340520,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,rstephen,rstephen,13/Nov/20 13:32,18/Nov/20 01:05,13/Jul/23 08:12,17/Nov/20 12:07,1.12.0,,,,,1.12.0,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"When creating a table to read from Kafka like so using the recently added support for kafka metadata in 1.12:

{noformat}
CREATE TABLE test_table (
    `foo` STRING,
     `bar` STRING,
     `record` STRING,
     `offset` BIGINT METADATA VIRTUAL
) WITH (
    'connector' = 'kafka',
    'topic' = 'kewltest.r-test.dummy',
    'key.fields' = 'foo;bar',
    'key.format' = 'json',
    'value.format' = 'json',
    'value.fields-include' = 'EXCEPT_KEY',
    'scan.startup.mode' = 'earliest-offset',
    'properties.bootstrap.servers' = '<HOST:PORT>',
    'properties.group.id' = '<CONSUMER-GROUP>'
)
{noformat}

the ""offset"" column causes queries of the table to fail with an exception: 
Caused by: java.lang.IllegalArgumentException: Invalid lambda deserialization

Full stack trace is attached.
Left priority as default as I wasn't sure what to set it to.",,jackylau,jark,knaufk,rstephen,twalthr,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/20 13:31;rstephen;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/13015225/stacktrace.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 01:05:33 UTC 2020,,,,,,,,,,"0|z0kkmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 14:25;jark;This is a very annoying problem. We encountered a similar problem before FLINK-18006. 
The reason is that the shade plugin doesn't work correctly with lambda which contains the shaded class. 

I'm a little afraid to use lambda in Flink now...;;;","13/Nov/20 16:47;twalthr;Yes, this is very annoying. It also exists for Debezium and probably Kinesis.;;;","17/Nov/20 12:07;twalthr;Fixed in 1.12.0: 29bce28f6f590c08d994b76afda3aa95185e1ffc & 488bc0ad7c7d29a37c4765a9fa6d08969b9fd5d2
;;;","18/Nov/20 01:05;jackylau;Yes, I met this problem in cdc too, and also solved by replace lambda with anonymous classes. It will only exist in distribuited environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming job fails with IllegalStateException: Should only poll priority events,FLINK-20145,13340458,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,rmetzger,rmetzger,13/Nov/20 08:44,22/Jun/21 14:05,13/Jul/23 08:12,23/Nov/20 13:00,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,,,,,"While testing the 1.12 release, I came across the following failure cause:

{code}
2020-11-13 09:41:52,110 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - dynamic filter (3/4)#0 (b977944851531f96e5324e786f055eb7) switched from RUNNING to FAILED.
java.lang.IllegalStateException: Should only poll priority events
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:198) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:116) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:577) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:541) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
{code}

I have unaligned checkpointing enabled, the failing operator is a CoFlatMapFunction. The error happend on all four TaskManagers, very soon after job submission. The error doesn't happen when unaligned checkpointing is disabled.",,AHeise,aljoscha,dian.fu,kevin.cyj,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 12:59:11 UTC 2020,,,,,,,,,,"0|z0kk8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 08:55;rmetzger;When trying to reproduce the error, I also got this exception:

{code}
2020-11-13 09:54:44,634 INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - dynamic filter (2/4)#0 discarding 0 drained requests
2020-11-13 09:54:44,674 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - dynamic filter (2/4)#0 (290b30403ae7bc304c40cd5af2ec1b11) switched from RUNNING to FAILED.
java.lang.NullPointerException: null
	at java.util.ArrayDeque.addFirst(ArrayDeque.java:233) ~[?:1.8.0_222]
	at org.apache.flink.runtime.io.network.partition.PrioritizedDeque.addPriorityElement(PrioritizedDeque.java:67) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.PrioritizedDeque.add(PrioritizedDeque.java:101) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.processBufferOrEvent(UnionInputGate.java:219) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.waitAndGetNextData(UnionInputGate.java:207) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:176) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.pollNext(UnionInputGate.java:168) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:142) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:157) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:92) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:577) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:541) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
{code};;;","14/Nov/20 23:10;roman;I came across this issue while debugging another case.

I think the problem is that PrioritizedDeque exposes deque as iterator which is then modified without updating numPriorityElements.

I opened a [PR|https://github.com/apache/flink/pull/14073] with a simple fix that helped in my case.

[~rmetzger] can you please verify it?
  ;;;","16/Nov/20 07:30;rmetzger;Thanks a lot for addressing this so quickly! 
I will verify the fix.

Another question: We have found this issue through manual testing only, and the PR also doesn't contain any new tests. I wonder if we need additional test coverage to verify this fix, and ensure that it won't be broken again in the future.;;;","16/Nov/20 08:27;roman;Sure, I just wanted to see whether it indeed solves the problem

(if not, I'd just add a unit test for the fix).;;;","16/Nov/20 09:30;rmetzger;Running on this commit: https://github.com/apache/flink/commit/f3f5e316622255b86f416ba5eb1c283562732823
II still get the same exception:

{code}
2020-11-16 10:26:48,339 INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - dynamic filter (1/4)#0 discarding 0 drained requests
2020-11-16 10:26:48,361 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - dynamic filter (1/4)#0 (ccfcf8c742c10df96d99f53025761a79) switched from RUNNING to FAILED.
java.lang.IllegalStateException: Should only poll priority events
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:198) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:116) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:574) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:538) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:832) [?:?]
{code};;;","23/Nov/20 12:59;arvid;Merged into master as d878cacc22cf9504d07a1065da746c211ff6d6a4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use `yarn.provided.lib.dirs` config deploy job failed in yarn per job mode,FLINK-20143,13340454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,zhisheng,zhisheng,13/Nov/20 08:24,18/Nov/20 10:06,13/Jul/23 08:12,18/Nov/20 10:06,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Client / Job Submission,Deployment / YARN,,,,0,pull-request-available,,,,,"use follow command deploy flink job to yarn failed 
{code:java}
./bin/flink run -m yarn-cluster -d -ynm flink-1.12-test -ytm 3g -yjm 3g -yD yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib    ./examples/streaming/StateMachineExample.jar
{code}
log:
{code:java}
$ ./bin/flink run -m yarn-cluster -d -ynm flink-1.12-test -ytm 3g -yjm 3g -yD yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib    ./examples/streaming/StateMachineExample.jar$ ./bin/flink run -m yarn-cluster -d -ynm flink-1.12-test -ytm 3g -yjm 3g -yD yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib    ./examples/streaming/StateMachineExample.jarSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/data1/app/flink-1.12-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/data1/app/hadoop-2.7.3-snappy-32core12disk/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/data1/app/hadoop-2.7.3-snappy-32core12disk/share/hadoop/tools/lib/hadoop-aliyun-2.9.2-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2020-11-13 16:14:30,347 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Dynamic Property set: yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib2020-11-13 16:14:30,347 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Dynamic Property set: yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/libUsage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]Options for both the above setups: [--backend <file|rocks>] [--checkpoint-dir <filepath>] [--async-checkpoints <true|false>] [--incremental-checkpoints <true|false>] [--output <filepath> OR null for stdout]
Using standalone source with error rate 0.000000 and sleep delay 1 millis
2020-11-13 16:14:30,706 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/data1/app/flink-1.12-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.2020-11-13 16:14:30,947 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at FAT-hadoopuat-69117.vm.dc01.tech/10.69.1.17:102002020-11-13 16:14:30,958 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2020-11-13 16:14:31,065 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] - Failing over to rm22020-11-13 16:14:31,130 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured JobManager memory is 3072 MB. YARN will allocate 4096 MB to make up an integer multiple of its minimum allocation memory (2048 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 1024 MB may not be used by Flink.2020-11-13 16:14:31,130 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured TaskManager memory is 3072 MB. YARN will allocate 4096 MB to make up an integer multiple of its minimum allocation memory (2048 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 1024 MB may not be used by Flink.2020-11-13 16:14:31,130 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=3072, taskManagerMemoryMB=3072, slotsPerTaskManager=2}2020-11-13 16:14:31,681 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory      [] - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2020-11-13 16:14:33,417 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1599741232083_219902020-11-13 16:14:33,446 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1599741232083_219902020-11-13 16:14:33,446 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated2020-11-13 16:14:33,448 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED
------------------------------------------------------------ The program finished with the following exception:
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Could not deploy Yarn job cluster. at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:330) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:743) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:242) at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:971) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1047) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1047)Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster. at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:460) at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:70) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1916) at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128) at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1798) at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:316) ... 11 moreCaused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment.Diagnostics from YARN: Application application_1599741232083_21990 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1599741232083_21990_000002 exited with  exitCode: -1Failing this attempt.Diagnostics: [2020-11-13 16:14:38.244]Destination must be relativeFor more detailed output, check the application tracking page: http://FAT-hadoopuat-69117.vm.dc01.tech:8188/applicationhistory/app/application_1599741232083_21990 Then click on links to logs of each attempt.. Failing the application.If log aggregation is enabled on your cluster, use this command to further investigate the issue:yarn logs -applicationId application_1599741232083_21990 at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1078) at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:558) at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:453) ... 22 more2020-11-13 16:14:38,492 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cancelling deployment from Deployment Failure Hook2020-11-13 16:14:38,494 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at FAT-hadoopuat-69117.vm.dc01.tech/10.69.1.17:102002020-11-13 16:14:38,495 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Killing YARN application2020-11-13 16:14:38,499 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] - Failing over to rm22020-11-13 16:14:38,503 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1599741232083_219902020-11-13 16:14:38,503 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deleting files in hdfs://flashHadoopUAT/user/deploy/.flink/application_1599741232083_21990.
{code}
but if i set `execution.target: yarn-per-job` in flink-conf.yaml, it runs ok

if i run in application mode, it runs ok too
{code:java}
./bin/flink run-application -p 2 -d -t yarn-application -ytm 3g -yjm 3g -yD yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib ./examples/streaming/StateMachineExample.jar
{code}
but the jobid is 00000000000000000000000000000000

 

 

 ",,kkl0u,wangyang0918,zhisheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/20 09:21;zhisheng;image-2020-11-13-17-21-47-751.png;https://issues.apache.org/jira/secure/attachment/13015203/image-2020-11-13-17-21-47-751.png","13/Nov/20 09:22;zhisheng;image-2020-11-13-17-22-06-111.png;https://issues.apache.org/jira/secure/attachment/13015204/image-2020-11-13-17-22-06-111.png","13/Nov/20 10:43;zhisheng;image-2020-11-13-18-43-55-188.png;https://issues.apache.org/jira/secure/attachment/13015211/image-2020-11-13-18-43-55-188.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 10:06:56 UTC 2020,,,,,,,,,,"0|z0kk7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 09:10;kkl0u;Can you share with us the job manager and task manager logs [~zhisheng]? This may help figuring out what is happening. ;;;","13/Nov/20 09:16;kkl0u;Also I think that your second command is not correct. You are using {{-t}} which activates the {{GenericCLI}} but then you specify parameters using the {{YarnSessionCLI}} convention of putting a {{-y}} as a prefix. Can you verify if the memory specifications you put are picked up and the shared.lib.dir is actually. used?;;;","13/Nov/20 09:19;zhisheng;[~kkl0u] it does not have jobmanager log and taskmanager log;;;","13/Nov/20 09:22;zhisheng;!image-2020-11-13-17-21-47-751.png!

 

!image-2020-11-13-17-22-06-111.png!;;;","13/Nov/20 09:24;zhisheng;[~kkl0u] yes, -ytm and -yjm does not take effect，i create a issue some days ago https://issues.apache.org/jira/browse/FLINK-19973;;;","13/Nov/20 09:25;kkl0u;I am not sure if I can figure out what is happening from what is here. ;;;","13/Nov/20 09:27;kkl0u;As discussed in the issue, you have to specify the full config option name prefixed by {{-D}} when using the {{GenericCLI}}. This means for example that {{-ytm}} should become {{-Dtaskmanager.memory.process.size=...}}.;;;","13/Nov/20 09:32;zhisheng; 
{code:java}
./bin/flink run -m yarn-cluster -d  -Dyarn.provided.lib.dirs=""hdfs:///flink/flink-1.12-SNAPSHOT/lib"" ./examples/streaming/StateMachineExample.jar
{code}
 i use this command(remove the -ynm flink-1.12-test -ytm 3g -yjm ), it runs ok

 ;;;","13/Nov/20 09:35;zhisheng;in our production environment，has many flink job，every job have the -ytm and -yjm -ynm config，if we upgrade to 1.12，It could change a lot [~kkl0u];;;","13/Nov/20 09:36;zhisheng;Are there any other methods to make job config compatibility？[~kkl0u];;;","13/Nov/20 09:40;kkl0u;Also the above command seems to be problematic.
{code:java}
./bin/flink run -m yarn-cluster -d  -Dyarn.provided.lib.dirs=""hdfs:///flink/flink-1.12-SNAPSHOT/lib"" ./examples/streaming/StateMachineExample.jar
{code}

What if you use?

{code:java}
./bin/flink run -t yarn-per-job -Dexecution.attached=false -Dyarn.provided.lib.dirs=""hdfs:///flink/flink-1.12-SNAPSHOT/lib"" ./examples/streaming/StateMachineExample.jar
{code}

and please check the logs to see if the shared dir is picked up or you are shipping everything from the client.
;;;","13/Nov/20 10:06;zhisheng;{code:java}
$ ./bin/flink run -t yarn-per-job -Dexecution.attached=false -Dyarn.provided.lib.dirs=""hdfs:///flink/flink-1.12-SNAPSHOT/lib"" ./examples/streaming/StateMachineExample.jar

SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/data1/app/flink-1.12-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/data1/app/hadoop-2.7.3-snappy-32core12disk/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/data1/app/hadoop-2.7.3-snappy-32core12disk/share/hadoop/tools/lib/hadoop-aliyun-2.9.2-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]Options for both the above setups: [--backend <file|rocks>] [--checkpoint-dir <filepath>] [--async-checkpoints <true|false>] [--incremental-checkpoints <true|false>] [--output <filepath> OR null for stdout]
Using standalone source with error rate 0.000000 and sleep delay 1 millis
2020-11-13 18:05:51,974 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/data1/app/flink-1.12-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.2020-11-13 18:05:52,202 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at FAT-hadoopuat-69117.vm.dc01.tech/10.69.1.17:102002020-11-13 18:05:52,213 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2020-11-13 18:05:52,324 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] - Failing over to rm22020-11-13 18:05:52,387 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured JobManager memory is 1600 MB. YARN will allocate 2048 MB to make up an integer multiple of its minimum allocation memory (2048 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 448 MB may not be used by Flink.2020-11-13 18:05:52,388 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured TaskManager memory is 1728 MB. YARN will allocate 2048 MB to make up an integer multiple of its minimum allocation memory (2048 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 320 MB may not be used by Flink.2020-11-13 18:05:52,388 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=1728, slotsPerTaskManager=2}2020-11-13 18:05:52,932 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory      [] - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2020-11-13 18:05:55,076 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1599741232083_220112020-11-13 18:05:55,307 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1599741232083_220112020-11-13 18:05:55,308 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated2020-11-13 18:05:55,310 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED
------------------------------------------------------------ The program finished with the following exception:
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Could not deploy Yarn job cluster. at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:330) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:743) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:242) at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:971) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1047) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1047)Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster. at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:460) at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:70) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1916) at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128) at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1798) at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:316) ... 11 moreCaused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment.Diagnostics from YARN: Application application_1599741232083_22011 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1599741232083_22011_000002 exited with  exitCode: -1Failing this attempt.Diagnostics: [2020-11-13 18:06:01.081]Destination must be relativeFor more detailed output, check the application tracking page: http://FAT-hadoopuat-69117.vm.dc01.tech:8188/applicationhistory/app/application_1599741232083_22011 Then click on links to logs of each attempt.. Failing the application.If log aggregation is enabled on your cluster, use this command to further investigate the issue:yarn logs -applicationId application_1599741232083_22011 at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1078) at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:558) at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:453) ... 22 more2020-11-13 18:06:01,111 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cancelling deployment from Deployment Failure Hook2020-11-13 18:06:01,113 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at FAT-hadoopuat-69117.vm.dc01.tech/10.69.1.17:102002020-11-13 18:06:01,114 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Killing YARN application2020-11-13 18:06:01,120 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] - Failing over to rm22020-11-13 18:06:01,124 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1599741232083_220112020-11-13 18:06:01,124 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deleting files in hdfs://flashHadoopUAT/user/deploy/.flink/application_1599741232083_22011.
{code}
[~kkl0u];;;","13/Nov/20 10:33;kkl0u;Can't you use {{yarn logs -applicationId application_1599741232083_22011}} to get the logs as the message says?;;;","13/Nov/20 10:35;kkl0u;Also could you run the same command with DEBUG logging enabled?;;;","13/Nov/20 10:44;zhisheng;!image-2020-11-13-18-43-55-188.png!

 

does not has any log, i had say just now;);;;","13/Nov/20 10:50;zhisheng;{code:java}
22020-11-13 18:46:43,014 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------2020-11-13 18:46:43,014 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------2020-11-13 18:46:43,019 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Starting Command Line Client (Version: 1.12-SNAPSHOT, Scala: 2.11, Rev:c55420b, Date:2020-11-05T05:29:49+01:00)2020-11-13 18:46:43,019 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  OS current user: deploy2020-11-13 18:46:43,415 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Current Hadoop/Kerberos user: deploy2020-11-13 18:46:43,416 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JVM: Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.92-b142020-11-13 18:46:43,416 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Maximum heap size: 7136 MiBytes2020-11-13 18:46:43,416 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JAVA_HOME: /app/jdk/2020-11-13 18:46:43,418 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Hadoop version: 2.7.32020-11-13 18:46:43,418 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  JVM Options:2020-11-13 18:46:43,418 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog.file=/data1/app/flink-1.12-SNAPSHOT/log/flink-deploy-client-FAT-hadoopuat-69120.vm.dc01. .tech.log2020-11-13 18:46:43,418 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog4j.configuration=file:/data1/app/flink-1.12-SNAPSHOT/conf/log4j-cli.properties2020-11-13 18:46:43,418 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlog4j.configurationFile=file:/data1/app/flink-1.12-SNAPSHOT/conf/log4j-cli.properties2020-11-13 18:46:43,418 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dlogback.configurationFile=file:/data1/app/flink-1.12-SNAPSHOT/conf/logback.xml2020-11-13 18:46:43,419 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Program Arguments:2020-11-13 18:46:43,420 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     run2020-11-13 18:46:43,420 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -t2020-11-13 18:46:43,421 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     yarn-per-job2020-11-13 18:46:43,421 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dexecution.attached=false2020-11-13 18:46:43,421 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     -Dyarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib2020-11-13 18:46:43,421 INFO  org.apache.flink.client.cli.CliFrontend                      [] -     ./examples/streaming/StateMachineExample.jar2020-11-13 18:46:43,421 INFO  org.apache.flink.client.cli.CliFrontend                      [] -  Classpath: /data1/app/flink-1.12-SNAPSHOT/lib/flink-connector-jdbc_2.11-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-csv-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-json-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-shaded-zookeeper-3.4.14.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-sql-connector-elasticsearch7_2.11-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-sql-connector-kafka_2.11-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-table_2.11-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-table-blink_2.11-1.12-SNAPSHOT.jar:/data1/app/flink-1.12-SNAPSHOT/lib/log4j-1.2-api-2.12.1.jar:/data1/app/flink-1.12-SNAPSHOT/lib/log4j-api-2.12.1.jar:/data1/app/flink-1.12-SNAPSHOT/lib/log4j-core-2.12.1.jar:/data1/app/flink-1.12-SNAPSHOT/lib/log4j-slf4j-impl-2.12.1.jar:/data1/app/flink-1.12-SNAPSHOT/lib/flink-dist_2.11-1.12-SNAPSHOT.jar:/app/hadoop/etc/hadoop:/app/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/app/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/app/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/app/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/app/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/app/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/app/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/app/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/app/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/app/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/app/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/app/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/app/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/app/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/app/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/app/hadoop/share/hadoop/common/lib/xz-1.0.jar:/app/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/app/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/app/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/app/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/app/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/app/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/app/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/app/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/app/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/app/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/app/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/app/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/app/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/app/hadoop/share/hadoop/common/lib/-rack-awareness-policy-1.0.jar:/app/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/app/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/app/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/app/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/app/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/app/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/app/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/app/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/app/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/app/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/app/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/app/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/app/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/app/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/app/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/app/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/app/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/app/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/app/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/app/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/app/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/app/hadoop/share/hadoop/common/lib/junit-4.11.jar:/app/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/app/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/app/hadoop/share/hadoop/common/lib/activation-1.1.jar:/app/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/app/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/app/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/app/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/app/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/app/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/app/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/app/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/app/hadoop/share/hadoop/common/lib/asm-3.2.jar:/app/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/app/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/app/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/app/hadoop/share/hadoop/hdfs:/app/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/app/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/app/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/app/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/app/hadoop/share/hadoop/hdfs/lib/ranger-plugin-classloader-1.1.0.jar:/app/hadoop/share/hadoop/hdfs/lib/ranger-hdfs-plugin-shim-1.1.0.jar:/app/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/app/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/app/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/app/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/app/hadoop/share/hadoop/hdfs/lib/-block-placement-policy-1.0.jar:/app/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/app/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/app/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/app/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/app/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/app/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/app/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/app/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/app/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/app/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/app/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/app/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/app/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/app/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/app/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/app/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/app/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/app/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/app/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/app/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/app/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/app/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/app/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/app/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/app/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/app/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/app/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/app/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/app/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/app/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/app/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/app/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/app/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/app/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/app/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/app/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/app/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/app/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/app/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/app/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/app/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/app/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/app/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/app/hadoop/share/hadoop/yarn/lib/spark-2.3.1-yarn-shuffle.jar:/app/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/app/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/app/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/app/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/app/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/app/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/app/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/app/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/app/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/app/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/app/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/app/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/app/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/app/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/app/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/app/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/app/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/app/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/app/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/app/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/app/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/app/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/app/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/app/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/app/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/app/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/app/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/app/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/app/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/app/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/app/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/app/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/app/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/app/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/app/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/app/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/app/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/app/hadoop/share/hadoop/tools/lib/hadoop-aliyun-2.9.2-jar-with-dependencies.jar:/app/hadoop/contrib/capacity-scheduler/*.jar:/app/hadoop/etc/hadoop:/app/hadoop/etc/hadoop:/app/hbase/conf2020-11-13 18:46:43,422 INFO  org.apache.flink.client.cli.CliFrontend                      [] - --------------------------------------------------------------------------------2020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost2020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 61232020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600m2020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m2020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 22020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 12020-11-13 18:46:43,426 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability, zookeeper2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.storageDir, hdfs:///flink/ha/2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.zookeeper.quorum, 10.69.1.15:2181,10.69.1.16:2181,10.69.1.17:21812020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend, rocksdb2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.dir, hdfs:///flink/checkpoints2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.savepoints.dir, hdfs:///flink/savepoints2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.incremental, true2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.num-retained, 22020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region2020-11-13 18:46:43,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.externalized-checkpoint-retention, RETAIN_ON_CANCELLATION2020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.interval, 120s2020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.mode, AT_LEAST_ONCE2020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.timeout, 20 min2020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.min-pause, 1 s2020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.web.address, fat-hadoopuat-69120.vm.dc01. .tech2020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.web.port, 80822020-11-13 18:46:43,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.archive.fs.refresh-interval, 100002020-11-13 18:46:43,429 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.archive.fs.dir, hdfs:///flink/history-log2020-11-13 18:46:43,429 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.archive.fs.dir, hdfs:///flink/history-log2020-11-13 18:46:43,611 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to deploy (auth:SIMPLE)2020-11-13 18:46:43,618 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-264858002832469329.conf.2020-11-13 18:46:43,627 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Running 'run' command.2020-11-13 18:46:43,648 INFO  org.apache.flink.client.cli.CliFrontend                      [] - Building program from JAR file2020-11-13 18:46:43,666 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: false)2020-11-13 18:46:43,717 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using predefined options: DEFAULT.2020-11-13 18:46:43,718 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using default options factory: DefaultConfigurableOptionsFactory{configuredOptions={}}.2020-11-13 18:46:43,768 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.streaming.examples.statemachine.event.Event does not contain a setter for field type2020-11-13 18:46:43,768 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.streaming.examples.statemachine.event.Event cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.2020-11-13 18:46:43,883 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.streaming.examples.statemachine.event.Alert does not contain a setter for field address2020-11-13 18:46:43,883 INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.streaming.examples.statemachine.event.Alert cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.2020-11-13 18:46:44,015 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/data1/app/flink-1.12-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.2020-11-13 18:46:44,228 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at FAT-hadoopuat-69117.vm.dc01. .tech/10.69.1.17:102002020-11-13 18:46:44,238 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2020-11-13 18:46:44,255 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead2020-11-13 18:46:44,259 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead2020-11-13 18:46:44,352 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] - Failing over to rm22020-11-13 18:46:44,412 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured JobManager memory is 1600 MB. YARN will allocate 2048 MB to make up an integer multiple of its minimum allocation memory (2048 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 448 MB may not be used by Flink.2020-11-13 18:46:44,413 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured TaskManager memory is 1728 MB. YARN will allocate 2048 MB to make up an integer multiple of its minimum allocation memory (2048 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 320 MB may not be used by Flink.2020-11-13 18:46:44,413 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=1728, slotsPerTaskManager=2}2020-11-13 18:46:44,960 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory      [] - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2020-11-13 18:46:47,129 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead2020-11-13 18:46:47,146 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1599741232083_220232020-11-13 18:46:47,378 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1599741232083_220232020-11-13 18:46:47,378 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated2020-11-13 18:46:47,380 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED2020-11-13 18:46:52,167 ERROR org.apache.flink.client.cli.CliFrontend                      [] - Error while running the command.org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Could not deploy Yarn job cluster. at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:330) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:743) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:242) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:971) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1047) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_92] at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_92] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) [hadoop-common-2.7.3.jar:?] at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1047) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster. at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:460) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:70) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1916) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1798) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_92] at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:316) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] ... 11 moreCaused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment.Diagnostics from YARN: Application application_1599741232083_22023 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1599741232083_22023_000002 exited with  exitCode: -1Failing this attempt.Diagnostics: [2020-11-13 18:46:51.947]Destination must be relativeFor more detailed output, check the application tracking page: http://FAT-hadoopuat-69117.vm.dc01. .tech:8188/applicationhistory/app/application_1599741232083_22023 Then click on links to logs of each attempt.. Failing the application.If log aggregation is enabled on your cluster, use this command to further investigate the issue:yarn logs -applicationId application_1599741232083_22023 at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1078) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:558) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:453) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:70) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1916) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1798) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_92] at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:316) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] ... 11 more2020-11-13 18:46:52,177 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cancelling deployment from Deployment Failure Hook2020-11-13 18:46:52,179 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at FAT-hadoopuat-69117.vm.dc01. .tech/10.69.1.17:102002020-11-13 18:46:52,180 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Killing YARN application2020-11-13 18:46:52,184 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] - Failing over to rm22020-11-13 18:46:52,188 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1599741232083_220232020-11-13 18:46:52,188 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deleting files in hdfs://flashHadoopUAT/user/deploy/.flink/application_1599741232083_22023.
{code};;;","13/Nov/20 12:00;kkl0u;Hi [~zhisheng], I run the command that I wrote in my previous comment on a local yarn cluster, and it seems to be working. I do not have your config.yaml though, as I start with the default one.;;;","13/Nov/20 14:28;kkl0u;[~fly_in_gis] do you have an idea about this issue?;;;","16/Nov/20 03:41;wangyang0918;[~zhisheng] Could you add the hdfs schema in the {{yarn.provided.lib.dirs}} and have a try again.

For example, {{-yD yarn.provided.lib.dirs=hdfs://hdpdev/flink/flink-1.12-SNAPSHOT/lib}};;;","16/Nov/20 06:11;wangyang0918;Hmm. I think maybe I find the root cause. When the {{yarn.provided.lib.dirs}} is set to the non-qualified path(e.g. hdfs:///path/of/sharedLib), the {{URI#relativize}} in {{YarnApplicationFileUploader#getAllFilesInProvidedLibDirs}} could not work as expected.

 

[~zhisheng] So for your situation, I guess all the deployment(e.g. yarn-per-job, yarn-application, yarn-session) could not work effectively if you are using non-qualified path.

 

[~kkl0u] Even though we have a work around, specify a qualified path(e.g. hdfs://hdpdev/path/of/sharedLib), I think it is better to fix this issue. I will attach a PR for this ticket.;;;","16/Nov/20 07:38;kkl0u;Thanks [~fly_in_gis], feel free to ping me for a review.;;;","16/Nov/20 11:13;wangyang0918;[~zhisheng] I have attached a PR to fix this issue. Also I verified your command in a Yarn cluster after this change, it works well. Please have a try.
{code:java}
./bin/flink run -m yarn-cluster -d -ynm flink-1.12-test -ytm 3g -yjm 3g -yD yarn.provided.lib.dirs=hdfs:///flink/flink-1.12-SNAPSHOT/lib    ./examples/streaming/StateMachineExample.jar
{code};;;","17/Nov/20 12:00;zhisheng;thanks [~fly_in_gis] , it works well now, thanks [~kkl0u] too, it will push to 1.12.0 ?;;;","17/Nov/20 13:43;kkl0u;Yes, I will try to merge it today and hopefully it will make it in 1.12 [~zhisheng].;;;","18/Nov/20 10:06;kkl0u;master: 424d41d124871b0a82d514f1ce15bc87b52169c6
1.11: f6f6271e8c17f7873ccb4cfe649d0ad35dfde445;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle the timestamp of element properly in Python DataStream API,FLINK-20137,13340403,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,csq,csq,13/Nov/20 02:04,19/Nov/20 05:49,13/Jul/23 08:12,19/Nov/20 05:49,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,"Currently, the timestamp of the element isn't handled correctly and will be dropped after by the Python operators.",,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20135,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 05:49:15 UTC 2020,,,,,,,,,,"0|z0kjwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/20 05:49;dian.fu;Fixed in master(1.12.0) via 492965cb09f3be7432c88194920a0e54a340af02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a few KafkaSource-related bugs,FLINK-20114,13340249,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,rmetzger,rmetzger,12/Nov/20 14:16,27/Jun/22 16:06,13/Jul/23 08:12,07/May/21 00:22,1.12.0,,,,,1.12.4,1.13.0,,Connectors / Kafka,,,,,0,pull-request-available,release-testing,,,,"Feature introduced in https://issues.apache.org/jira/browse/FLINK-18323

-------- 
[General Information about the Flink 1.12 release testing|https://cwiki.apache.org/confluence/display/FLINK/1.12+Release+-+Community+Testing]

When testing a feature, consider the following aspects:
- Is the documentation easy to understand
- Are the error messages, log messages, APIs etc. easy to understand
- Is the feature working as expected under normal conditions
- Is the feature working / failing as expected with invalid input, induced errors etc.

If you find a problem during testing, please file a ticket (Priority=Critical; Fix Version = 1.12.0), and link it in this testing ticket.
During the testing, and once you are finished, please write a short summary of all things you have tested.",,becket_qin,JohnTeslaa,lindong,renqs,rmetzger,stevenz3wu,thw,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20193,FLINK-20194,FLINK-20081,,,,,,,,,,,,,,,,,,,,,FLINK-28266,,,,,,,,,,,"24/Nov/20 14:15;rmetzger;Screenshot 2020-11-24 at 15.14.35.png;https://issues.apache.org/jira/secure/attachment/13015938/Screenshot+2020-11-24+at+15.14.35.png","24/Nov/20 14:28;rmetzger;first-run.tgz;https://issues.apache.org/jira/secure/attachment/13015940/first-run.tgz","24/Nov/20 14:28;rmetzger;second-run.tgz;https://issues.apache.org/jira/secure/attachment/13015939/second-run.tgz",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 05 15:50:45 UTC 2021,,,,,,,,,,"0|z0kiy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 16:19;rmetzger;[~stevenz3wu] would you be interested in helping with the testing of the new Kafka source?
Since it is such a critical component, I believe we would benefit from a lot of testing coverage. I'm pinging you already, even though there are a few known issues (see linked blockers) that we need to resolve first, but I thought it might be helpful for you to schedule this work some time later this week.;;;","17/Nov/20 17:30;stevenz3wu;[~rmetzger] sorry. it might be difficult. We were having some production issues last week that we need to address in the next 2 weeks. We do plan to test 1.12 as soon as we can though.;;;","17/Nov/20 19:23;rmetzger;Allright, no problem. Good luck with resolving the issues!;;;","24/Nov/20 14:11;rmetzger;Testing Issue 1:
I started a job with the KafkaSource on a setup where no Kafka broker was reachable. The job was running without an errors for 15 minutes.
I would expect the job to fail after a timeout of a few minutes if no broker could be reached?



Testing Issue 2:
While testing the new Kafka source on current master, I tried stopping the job with a savepoint. I came across this unexpected exception:

{code}
2020-11-24 14:37:56,517 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering cancel-with-savepoint for job fae21d0ce1804445dd4cc904fcdfbf43.
2020-11-24 14:37:56,522 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 488 (type=SAVEPOINT) @ 1606225076519 for job fae21d0ce1804445dd4cc904fcdfbf43.
2020-11-24 14:37:56,554 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Savepoint stored in file:/tmp/sm-savepoint/savepoint-fae21d-9723b187d107. Now cancelling fae21d0ce1804445dd4cc904fcdfbf43.
2020-11-24 14:37:56,555 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 488 for job fae21d0ce1804445dd4cc904fcdfbf43 (7183 bytes in 35 ms).
2020-11-24 14:37:56,555 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job State machine job (fae21d0ce1804445dd4cc904fcdfbf43) switched from state RUNNING to CANCELLING.
2020-11-24 14:37:56,556 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (1/5) (b23b8a46bee25317d2bf1822bc043c31) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,557 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (2/5) (e40a16e61b64c8e71e873383a18c0a45) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,557 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (3/5) (d5b299c2e3f7a9dd9cfc5579c55845ba) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,557 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (4/5) (06bb492f8fe20534c9c3e9f5adf16d3c) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,557 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (5/5) (70c4d09343b647ee4f64b784b88145b6) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,558 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (1/5) (5b288ff9d47a2528783fe04d30844ac7) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,558 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (2/5) (e8b2cd2fcd7b5d240d7ce83693ad88b6) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,558 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (3/5) (a5ff2af6ae3c00104e0430eff149c2e8) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,558 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (4/5) (26ff08ea4128b9fa16bfe4251f390c11) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,558 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (5/5) (658b5ccf92f07ac5a3b2ea9b648c4ba9) switched from RUNNING to CANCELING.
2020-11-24 14:37:56,560 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 488 as completed for source Source: Kafka Source.
2020-11-24 14:37:56,575 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (1/5) (b23b8a46bee25317d2bf1822bc043c31) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,576 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (3/5) (d5b299c2e3f7a9dd9cfc5579c55845ba) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,577 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (2/5) (e40a16e61b64c8e71e873383a18c0a45) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,578 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (5/5) (70c4d09343b647ee4f64b784b88145b6) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,579 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (2/5) (e8b2cd2fcd7b5d240d7ce83693ad88b6) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,580 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (5/5) (658b5ccf92f07ac5a3b2ea9b648c4ba9) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,582 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (4/5) (06bb492f8fe20534c9c3e9f5adf16d3c) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,583 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (4/5) (26ff08ea4128b9fa16bfe4251f390c11) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,589 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (1/5) (5b288ff9d47a2528783fe04d30844ac7) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,591 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Print to Std. Out (3/5) (a5ff2af6ae3c00104e0430eff149c2e8) switched from CANCELING to CANCELED.
2020-11-24 14:37:56,592 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job State machine job (fae21d0ce1804445dd4cc904fcdfbf43) switched from state CANCELLING to CANCELED.
2020-11-24 14:37:56,592 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job fae21d0ce1804445dd4cc904fcdfbf43.
2020-11-24 14:37:56,593 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2020-11-24 14:37:56,593 INFO  org.apache.flink.runtime.checkpoint.CompletedCheckpoint      [] - Checkpoint with ID 488 at 'file:/tmp/sm-savepoint/savepoint-fae21d-9723b187d107' not discarded.
2020-11-24 14:37:56,596 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job fae21d0ce1804445dd4cc904fcdfbf43 reached globally terminal state CANCELED.
2020-11-24 14:37:56,630 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job State machine job(fae21d0ce1804445dd4cc904fcdfbf43).
2020-11-24 14:37:56,631 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: Kafka Source.
2020-11-24 14:37:56,631 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext [] - Exception while handling result from async call in SourceCoordinator-Source: Kafka Source. Triggering job failover.
org.apache.flink.util.FlinkRuntimeException: Failed to handle partition splits change due to 
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:199) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$1(ExecutorNotifier.java:86) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:832) [?:?]
Caused by: java.lang.RuntimeException: Failed to get topic metadata.
	at org.apache.flink.connector.kafka.source.enumerator.subscriber.TopicListSubscriber.getPartitionChanges(TopicListSubscriber.java:60) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.discoverAndInitializePartitionSplit(KafkaSourceEnumerator.java:176) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:83) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	... 3 more
Caused by: java.lang.InterruptedException
	at java.lang.Object.wait(Native Method) ~[?:?]
	at java.lang.Object.wait(Object.java:321) ~[?:?]
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:92) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.flink.connector.kafka.source.enumerator.subscriber.TopicListSubscriber.getPartitionChanges(TopicListSubscriber.java:58) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.discoverAndInitializePartitionSplit(KafkaSourceEnumerator.java:176) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-075706c79af3170480b19252d2647a17:?]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:83) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	... 3 more
2020-11-24 14:37:56,636 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Suspending SlotPool.
2020-11-24 14:37:56,637 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 1dc7c4d8b846d99ade33fbd7bfb620cf: Stopping JobMaster for job State machine job(fae21d0ce1804445dd4cc904fcdfbf43)..
2020-11-24 14:37:56,637 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping SlotPool.
2020-11-24 14:37:56,638 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job fae21d0ce1804445dd4cc904fcdfbf43 from the resource manager.
{code}
It does not seem to cause any issues, except that the logged exception might confuse users.

Testing Issue 3:
I also noticed that the {{NetworkClient}} kept logging every few seconds for ~ 1.5 minutes after the job has finished. It eventually stopped with a timeout. Maybe we can proactively stop the NetworkClient on the JobManager to avoid resource leaks in case the timeout is configured differently on a user setup:
{code}
2020-11-24 14:37:56,637 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping SlotPool.
2020-11-24 14:37:56,638 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job fae21d0ce1804445dd4cc904fcdfbf43 from the resource manager.
2020-11-24 14:37:57,370 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:37:58,642 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:37:59,806 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:38:00,973 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.

[ ... ]

2020-11-24 14:38:56,002 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:38:56,639 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: Kafka Source closed.
2020-11-24 14:38:56,959 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.

[ ... ]

2020-11-24 14:39:37,927 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:39:38,890 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:39:39,944 WARN  org.apache.kafka.clients.NetworkClient                       [] - [AdminClient clientId=null-enumerator-admin-client] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
2020-11-24 14:39:40,050 INFO  org.apache.kafka.clients.admin.internals.AdminMetadataManager [] - [AdminClient clientId=null-enumerator-admin-client] Metadata update failed
org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.
2020-11-24 14:39:40,158 INFO  org.apache.kafka.clients.admin.internals.AdminMetadataManager [] - [AdminClient clientId=null-enumerator-admin-client] Metadata update failed
org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited.
{code}

Testing Issue 4:

When cancelling my job, I saw this WARNING in the logs. I have not added any jars into my lib folder.

{code}
2020-11-24 15:09:42,325 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_5 for job b5bb1c9bb705d9d771b63e33dc6e877f from the resource manager.
2020-11-24 15:09:42,329 WARN  org.apache.kafka.common.utils.Utils                          [] - Failed to close KafkaClient with type org.apache.kafka.clients.NetworkClient
java.lang.NoClassDefFoundError: org/apache/kafka/common/network/Selector$CloseMode
	at org.apache.kafka.common.network.Selector.close(Selector.java:806) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-1eba36fb91ea0b66651554912ceaca10:?]
	at org.apache.kafka.common.network.Selector.close(Selector.java:365) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-1eba36fb91ea0b66651554912ceaca10:?]
	at org.apache.kafka.clients.NetworkClient.close(NetworkClient.java:639) ~[blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-1eba36fb91ea0b66651554912ceaca10:?]
	at org.apache.kafka.common.utils.Utils.closeQuietly(Utils.java:834) [blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-1eba36fb91ea0b66651554912ceaca10:?]
	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1219) [blob_p-38777177cae883549cd5d28a76d1854e5087cc9f-1eba36fb91ea0b66651554912ceaca10:?]
	at java.lang.Thread.run(Thread.java:832) [?:?]
Caused by: java.lang.ClassNotFoundException: org.apache.kafka.common.network.Selector$CloseMode
	at java.net.URLClassLoader.findClass(URLClassLoader.java:435) ~[?:?]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:589) ~[?:?]
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:72) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:49) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:522) ~[?:?]
	... 6 more
2020-11-24 15:09:42,330 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: Kafka Source closed.
{code};;;","24/Nov/20 14:16;rmetzger;Testing Issue 5:
It seems that metrics are only reported for the ""bytes send"", but not for ""records send"". The receiver properly reports both metrics. I added a screenshot showing the problem to the ticket.;;;","24/Nov/20 14:26;rmetzger;Testing Issue 6:

I created a topic with 64 partitions, and started the job, which started failing with:

{code}
2020-11-24 15:25:00,530 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Kafka Source (5/6) (74a3b791c148ea9ce9d17ae59f59becc) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@10db45bb.
java.lang.Exception: Could not perform checkpoint 11 for operator Source: Kafka Source (5/6)#10.
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:866) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$8(StreamTask.java:831) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:302) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:832) ~[?:?]
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 11 for operator Source: Kafka Source (5/6)#10. Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:226) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:158) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:343) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:924) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:914) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:857) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 10 more
Caused by: org.apache.flink.util.SerializedThrowable: Invalid negative offset
	at org.apache.kafka.clients.consumer.OffsetAndMetadata.<init>(OffsetAndMetadata.java:50) ~[?:?]
	at org.apache.kafka.clients.consumer.OffsetAndMetadata.<init>(OffsetAndMetadata.java:69) ~[?:?]
	at org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.snapshotState(KafkaSourceReader.java:96) ~[?:?]
	at org.apache.flink.streaming.api.operators.SourceOperator.snapshotState(SourceOperator.java:264) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:197) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:158) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:343) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:924) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:914) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:857) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 10 more
{code};;;","24/Nov/20 14:28;rmetzger;I attached the logs for these runs as well: Issues 1 - 5 are in the ""first-run.tgz"", issue 6 is in second-run.tgz;;;","25/Nov/20 19:57;rmetzger;I will continue with the testing once the issues reported so far are resolved. At least Issue 6, which breaks checkpointing.;;;","30/Mar/21 06:24;lindong;Hi [~rmetzger], thank you for doing the test! Here is the status update.

The following patches have been developed to fix KafkaSource related bugs:

1) https://github.com/apache/flink/pull/15161 has been merged with multiple commits to fix the issues mentioned in this bug.

2) Jiangjie is working on https://github.com/becketqin/flink/commit/1bb2be0935d48153c67981476fc5046afa4b172f which can fix a bug that is known to prevent savepoint from completion.

After applying the patch mentioned below, I have run StateMachineExample and believe the issues 1-5 has been fixed. Please see below for details.

Note that for Issue 6, I could not reproduce the original issue and it is not clear which bug caused the original issue. So it may be useful to double check it with the test you used earlier.

1) Testing Issue 1:
This behavior is expected. If no Kafka broker was reachable, KafkaConsumer (and thus KafkaSource) is expected to keep polling without throwing error.

This behavior is reasonable because, when Kafka service goes offline due to either maintenance or failure, it is not helpful for KafkaConsumer to fail fast.

2) Testing Issue 2:
I could start StateMachineExample and then stop it with savepoint successfully.

3) Testing Issue 3:
The issue should be fixed by the commit https://github.com/apache/flink/commit/4c6c42392e7d116dee572fefb2e4e0e02abacefb

4) Testing Issue 4:
I could start StateMachineExample and then cancel it successfully. There is no ClassNotFoundException in log files.

5) Testing Issue 5:
The issue should be fixed by the commit https://github.com/apache/flink/commit/b23f65f17b4de16cf0fd91b225c3c3c61c849450

6) Testing Issue 6:
I could create the flink-demo-topic-1 topic with 64 partitions and then start StateMachineExample successfully. 

;;;","22/Apr/21 11:23;rmetzger;Thanks a lot for your detailed follow up! I won't have time continuing the testing effort for this feature at the moment. ;;;","29/Apr/21 07:14;trohrmann;I am a bit confused [~jqin]. There is a PR which has been merged but it is not listed here. Moreover, the ticket is still open. Do we intend to do other improvements and that's why it is not closed?;;;","29/Apr/21 07:56;becket_qin;[~trohrmann] Sorry for the confusion. The ticket was left open because we were planning to backport the PR to release 1.12. But there are API changes so I started a discussion thread in the mailing list. The ticket can be closed once we reach consensus on whether it should be backported or not. I'll follow up on the mailing list thread.;;;","05/May/21 15:50;thw;[~becket_qin] while testing with 1.12 I run into issues that were already fixed. I will back port these changes and open a PR against 1.12 as otherwise the new KafkaSource isn't very useful even for experimentation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingleThreadFetcherManager may add splits to a shutting down SplitFetcher,FLINK-20108,13340205,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,becket_qin,becket_qin,12/Nov/20 12:10,29/Apr/21 00:44,13/Jul/23 08:12,29/Apr/21 00:44,1.11.2,,,,,1.11.3,,,Connectors / Common,,,,,0,auto-unassigned,,,,,"Currently the split fetchers are only removed from the {{SplitFetcherManager.fetchers}} when the thread exit. The may cause problem because when {{SplitFetcherManager.addSplits()}} is called, it may see a shutting down split fetcher and adds splits to it. These splits will then just be lost.

This issue is actually already fixed in FLINK-18128. The fix needs to cherry-picked to 1.11.3",,becket_qin,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 00:44:29 UTC 2021,,,,,,,,,,"0|z0kiog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 10:50;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:50;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","29/Apr/21 00:44;becket_qin;FLINK-18128 has been backported to 1.11.3.

e72e48533902fe6a7271310736584e77b64d05b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues with setting InputChannel.statePersister ,FLINK-20107,13340195,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,12/Nov/20 11:27,22/Jun/21 13:55,13/Jul/23 08:12,18/Nov/20 07:27,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,,,,,"ChannelStatePersister is a non-final field in InputChannel.

The need for injection is caused by different places, where network- and state- related objects are created (task executor vs stream task).

It is set on remote and local channels upon requesting partitions, after converting unknown or recovered channel.

 Issues
 # Not set on RemoteInputChannel when converting from UnknownChannel
 # No visibility guarantee: written by task thread without any (explicit) synchronization, read by network thread (checkForBarrier)

I see that ""final"" channels (both remote and local) are created only when the writer is known (partitions requested).

So we can just make it final in ""final"" channels and pass from recovered/unknown. For that,
 * need to add to UnknownChannel
 * no need to make it volatile/guarded because in ""non-final"" channels it's accessed only by the task thread; and in ""final"" channels it will be final",,AHeise,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 07:26:50 UTC 2020,,,,,,,,,,"0|z0kim8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/20 07:26;arvid;Merged into master as c7b2e12af37905e664685e838a6ad1a92a6b4ada.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race conditions in InputChannel.ChannelStatePersister,FLINK-20097,13340030,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,roman,roman,11/Nov/20 17:52,22/Jun/21 13:55,13/Jul/23 08:12,12/Nov/20 13:47,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,,,,,"In InputChannel.ChannelStatePersister, stopPersisting() and checkForBarrier() always update pendingCheckpointBarrierId, potentially overwriting newer id (or BARRIER_RECEIVED value) with an old one.


For stopPersisting(), consider a case:
 # Two consecutive UC barriers arrive at the same channel (1st being stale at some point)
 # In RemoteInputChannel.onBuffer, netty thread updates pendingCheckpointBarrierId to BARRIER_RECEIVED
 # Task thread processes the 1st barrier and triggers a checkpoint
Task thread processes the 2nd barrier and aborts 1st checkpoint, calling stopPersisting() from UC controller and setting pendingCheckpointBarrierId to CHECKPOINT_COMPLETED
 # Task thread starts 2nd checkpoint and calls startPersisting() setting pendingCheckpointBarrierId to 2
 # now new buffers have a chance to be included in the 2nd checkpoint (though they belong to the next one)

 

For pendingCheckpointBarrierId(), consider an input gate with two channels A and B and two barriers 1 and 2:
 # Channel A receives both barriers, channel B receives nothing yet
 # Task thread processes both barriers on A, eventually triggering 2nd checkpoint
 # Channel A state is now BARRIER_RECEIVED, channel B - pending (with id=2)
 # Channel B receives the 1st barrier and becomes BARRIER_RECEIVED
 # No buffers in B between barriers 1 and 2 will be included in the checkpoint 
 # Channel B receives the 2nd barrier which will eventually conclude the checkpoint

 

I see a solution in doing an action only if passed checkpointId >= pendingCheckpointId. For that, a separate field will be needed to hold the status (RECEIVED/COMPLETED/PENDING). The class isn't thread-safe so it shouldn't be a problem.
 ",,AHeise,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19681,FLINK-20103,FLINK-22232,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 13:46:41 UTC 2020,,,,,,,,,,"0|z0khlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/20 18:08;roman;I've [added|https://github.com/apache/flink/pull/14040/commits/a2495da394a27d4bd503bdd0c5cfd8b7a8e92c43] a unit test for the 1st part. 

 

[~AHeise], can you confirm this?

If this is an issue, I'm wondering why it wasn't detected by the existing tests.;;;","11/Nov/20 19:41;roman;OK, the tests fail with the increased checkpointing frequency plus some other settings updated.
 I don't know whether the root cause is what I described above.

Changes:
{code:java}
long minCheckpoints = 100;

env.enableCheckpointing(10);
env.getCheckpointConfig().setAlignmentTimeout(0);
env.getCheckpointConfig().setCheckpointTimeout(10); // minimum allowed
env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
env.getCheckpointConfig().setMinPauseBetweenCheckpoints(0);
env.getCheckpointConfig().setTolerableCheckpointFailureNumber(Integer.MAX_VALUE);

// can be lower because of the high TolerableCheckpointFailureNumber
// collector.checkThat(result.<Integer>getAccumulatorResult(NUM_FAILURES), equalTo(EXPECTED_FAILURES));

sink:
    if (random.nextInt(100) == 42) {
        Thread.sleep(7);
    }
{code}
Failures:
 shouldPerformUnalignedCheckpointOnParallelRemoteChannel: NUM_DUPLICATES > 0 (2 out of 70 runs)

With CheckpointTimeout 5, interval 1, sleep 1 
I also get: two out-of-order and one corrupted state on recovery:
{code}
Caused by: java.io.EOFException
	at org.apache.flink.core.memory.DataInputDeserializer.readByte(DataInputDeserializer.java:134)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:199)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46)
	at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55)
	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:92)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:145)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:574)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:538)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
{code}
Corrupted state is very similar to what I observed in FLINK-19681.;;;","11/Nov/20 19:55;roman;And with
{code}
env.enableCheckpointing(1);
env.getCheckpointConfig().setCheckpointTimeout(2); // had to disable check
Thread.sleep(1); // in sink 42/100
{code}

I got one
{code}
Caused by: java.util.NoSuchElementException
	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:642)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getInflightBuffers(RemoteInputChannel.java:537)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkpointStarted(RemoteInputChannel.java:509)
{code}

This is probably FLINK-20030?;;;","11/Nov/20 20:09;roman;Just to clarify, 
 I used shouldPerformUnalignedCheckpointOnParallelRemoteChannel (parallelism 5, slotsPerTaskManager 1, slotSharing false).

In AC mode, it seems to always pass (didn't check all the combinations).;;;","11/Nov/20 20:26;roman;Fixing the original issues with ChannelStatePersister (as described) seems to resolve the failures.;;;","11/Nov/20 21:22;arvid;You are right that the code currently is not behaving well when you have two concurrent UC. I actually wanted to make it possible to process concurrent checkpoint in all 1.12 code, but it didn't make it through the review. I'm assuming that as long as we don't know if we want to support concurrent UC, the code was not needed.

I'm unsure why stuff is failing with one checkpoint. It could be related to cancellation, such that actually two UC barriers arrive at some task instead.;;;","11/Nov/20 21:25;arvid;{noformat}
Caused by: java.util.NoSuchElementException
	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:642)
{noformat}

This looks very much like FLINK-20030.;;;","11/Nov/20 21:27;arvid;> In AC mode, it seems to always pass (didn't check all the combinations).

AC mode never touches ChannelPersister, so I'm not suprised. Especially {{stopPersisting}} is never called by {{AlignedController}}.;;;","11/Nov/20 21:58;roman;Please note, that even with a single checkpoint in flight, there can be a lot of older outdated barrier in the stream.

This can happen if high backpressure and/or low timeout (that's why I setCheckpointTimeout to 10, plus minPause, plus interval).;;;","12/Nov/20 13:46;arvid;Merged into master as 5256c210f5e0a27164c1f1e7c0916f0d6bbd5bb7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up PyFlink documentation,FLINK-20096,13340029,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,11/Nov/20 17:49,17/Dec/20 05:41,13/Jul/23 08:12,12/Nov/20 18:13,,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,,,dian.fu,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 18:13:23 UTC 2020,,,,,,,,,,"0|z0khlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/20 18:13;sjwiesman;fixed in master 92df49e909408d3a063fe4fb1be5c03136473cfe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kinesis][Polling] Issue using Polling consumer at timestamp with empty shard,FLINK-20088,13339986,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny.cranmer,danny.cranmer,danny.cranmer,11/Nov/20 12:54,22/Sep/22 13:21,13/Jul/23 08:12,14/Nov/20 01:34,,,,,,1.12.0,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"*Background*

The consumer fails when a Polling record publisher uses a timestamp sentinel starting position and the first record batch is empty. This is because the consumer tries to recalculate the start position from the timestamp sentinel, this operation is not supported.

*Reproduction Steps*

Setup an application consuming from Kinesis with following properties and consume from an empty shard:
{code:java}
String format = ""yyyy-MM-dd'T'HH:mm:ss"";
String date = new SimpleDateFormat(format).format(new Date());

consumerConfig.setProperty(ConsumerConfigConstants.STREAM_INITIAL_TIMESTAMP, date);
consumerConfig.setProperty(ConsumerConfigConstants.STREAM_TIMESTAMP_DATE_FORMAT, format);
consumerConfig.setProperty(ConsumerConfigConstants.STREAM_INITIAL_POSITION, ""AT_TIMESTAMP""); {code}
*Error*
{code:java}
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
	at akka.dispatch.OnComplete.internal(Future.scala:264)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:36)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:36)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive$$$capture(Actor.scala:517)
	at akka.actor.Actor$class.aroundReceive(Actor.scala)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: java.lang.IllegalArgumentException: Unexpected sentinel type: AT_TIMESTAMP_SEQUENCE_NUM
	at software.amazon.kinesis.connectors.flink.model.StartingPosition.fromSentinelSequenceNumber(StartingPosition.java:107)
	at software.amazon.kinesis.connectors.flink.model.StartingPosition.fromSequenceNumber(StartingPosition.java:90)
	at software.amazon.kinesis.connectors.flink.model.StartingPosition.continueFromSequenceNumber(StartingPosition.java:73)
	at software.amazon.kinesis.connectors.flink.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:113)
	at software.amazon.kinesis.connectors.flink.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:98)
	at software.amazon.kinesis.connectors.flink.internals.ShardConsumer.run(ShardConsumer.java:108)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
	at java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
 

*Solution*

This is fixed by reusing the existing timestamp starting position in this condition.",,danny.cranmer,dian.fu,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 14 01:34:19 UTC 2020,,,,,,,,,,"0|z0khbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/20 01:34;tzulitai;flink/master: d332ff45467c7eb0d9b87e9a6e79bb99e8617ddd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE when generating watermark for record whose rowtime field is null after watermark push down,FLINK-20084,13339904,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,11/Nov/20 07:07,26/Nov/20 02:11,13/Jul/23 08:12,12/Nov/20 10:20,1.12.0,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The problem is from the class {{PushWatermarkIntoTableSourceScanRuleBase$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator#onEvent}}. It doesn't test whether the calculated watermark is null before set.  

 ",,dian.fu,fsk119,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20121,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 10:20:34 UTC 2020,,,,,,,,,,"0|z0kgtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/20 10:20;jark;Fixed in master (1.12.0): fb0ee3a2efd50ac8a68cb5e505f4882fc344c4e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrcFsStreamingSinkITCase times out,FLINK-20083,13339903,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,maguowei,rmetzger,rmetzger,11/Nov/20 06:59,20/Nov/20 00:05,13/Jul/23 08:12,20/Nov/20 00:05,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,test-stability,,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8661&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa

{code}
[ERROR]   OrcFsStreamingSinkITCase>FsStreamingSinkITCaseBase.testPart:84->FsStreamingSinkITCaseBase.test:120->FsStreamingSinkITCaseBase.check:133 » TestTimedOut

[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 26.871 s <<< FAILURE! - in org.apache.flink.orc.OrcFsStreamingSinkITCase
[ERROR] testPart(org.apache.flink.orc.OrcFsStreamingSinkITCase)  Time elapsed: 20.052 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 20 seconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:231)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:119)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77)
	at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:114)
	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.check(FsStreamingSinkITCaseBase.scala:133)
	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:120)
	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPart(FsStreamingSinkITCaseBase.scala:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

{code}",,maguowei,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19253,,,,,,,,,,,,,,,,,,,,,,"20/Nov/20 00:03;maguowei;image-2020-11-20-08-03-43-073.png;https://issues.apache.org/jira/secure/attachment/13015671/image-2020-11-20-08-03-43-073.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 00:03:48 UTC 2020,,,,,,,,,,"0|z0kgtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 00:03;maguowei;I found that `SplitFetcher` exits too early(line:387) that leads some split are not handled(line:388 389).

!image-2020-11-20-08-03-43-073.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorNotifier should run handler in the main thread when receive an exception from the callable.,FLINK-20081,13339891,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,11/Nov/20 05:42,06/Dec/20 12:30,13/Jul/23 08:12,22/Nov/20 22:53,1.11.2,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,,,,,"Currently the {{ExecutorNotifier}} runs the {{handler}} in the worker thread if there is an exception thrown from the {{callable}}. This breaks the threading model and prevents an exception from bubbling up to fail the job.

Another issue is that right now, when an exception bubbles up from the {{SourceCoordinator}}, the UncaughtExceptionHandler will call System.exit(-17) and kill the JM. This is too much. Instead, we should just fail the job to trigger a failover.",,becket_qin,dian.fu,sewen,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20114,FLINK-20157,,,,,,,,,,,,,,,,,,,,,,,FLINK-20157,FLINK-20261,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 22 22:53:43 UTC 2020,,,,,,,,,,"0|z0kgqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/20 22:53;sewen;Fixed for 1.12.0 (master) via
  - d6ac4654fd4e031abe233c6b9978210b536851fe
  - 7940afd96e9f4441d4a18dbb57bbb1e3721f0e2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceeded checkpoint tolerable failure threshold,FLINK-20080,13339867,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,AlexZax,AlexZax,11/Nov/20 01:45,11/Nov/20 08:02,13/Jul/23 08:12,11/Nov/20 08:02,1.11.2,,,,,,,,Runtime / Checkpointing,,,,,0,,,,,,"2020-11-10 15:36:17,051 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint 4 of job fd6d1445eec79363c3318c5aa419e846 expired before completing.
2020-11-10 15:36:17,058 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Trying to recover from a global failure.
org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleJobLevelCheckpointException(CheckpointFailureManager.java:66) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1673) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1650) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java:91) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:1783) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_221]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]",,AlexZax,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-11 01:45:18.0,,,,,,,,,,"0|z0kglc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modified UnalignedCheckpointITCase...MassivelyParallel fails,FLINK-20079,13339836,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,10/Nov/20 20:07,22/Jun/21 13:55,13/Jul/23 08:12,11/Nov/20 16:50,1.11.3,,,,,1.11.3,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,,"In FLINK-19681, UnalignedCheckpointITCase was updated to put more backpressure.

This revealed a bug introduced in FLINK-19907: resultPartitions can be requested before the operator chain is initialized.

A proper fix would be to initialize the chain after the outputs but before the inputs.",,AHeise,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20103,FLINK-22232,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 16:50:18 UTC 2020,,,,,,,,,,"0|z0kgeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/20 16:50;arvid;Merged into master as bf16a7dd2a2786847abd440c69ab8ade59853a1d.
Merged into 1.11 as 57845ce2bf26562667e8e26d800e1b8340e9b4b3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot register a view with MATCH_RECOGNIZE clause,FLINK-20077,13339738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,10/Nov/20 12:30,12/Nov/20 12:53,13/Jul/23 08:12,12/Nov/20 12:53,1.11.0,1.12.0,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"{code}
		TableEnvironment env = TableEnvironment.create(EnvironmentSettings.newInstance()
			.useBlinkPlanner()
			.build());

		env.executeSql("""" +
			""CREATE TEMPORARY TABLE data (\n"" +
			""    id INT,\n"" +
			""    ts AS PROCTIME()\n"" +
			"") WITH (\n"" +
			""    'connector' = 'datagen',\n"" +
			""    'rows-per-second' = '3',\n"" +
			""    'fields.id.kind' = 'sequence',\n"" +
			""    'fields.id.start' = '1000000',\n"" +
			""    'fields.id.end' = '2000000'\n"" +
			"")"");

		env.executeSql("""" +
			""CREATE TEMPORARY VIEW events AS \n"" +
			""SELECT 1 AS key, id, MOD(id, 10) AS measurement, ts \n"" +
			""FROM data"");

		env.executeSql("""" +
			""CREATE TEMPORARY VIEW foo AS \n"" +
			""SELECT * \n"" +
			""FROM events MATCH_RECOGNIZE (\n"" +
			""    PARTITION BY key \n"" +
			""    ORDER BY ts ASC \n"" +
			""    MEASURES \n"" +
			""      this_step.id as startId,\n"" +
			""      next_step.id as nextId,\n"" +
			""      this_step.ts AS ts1,\n"" +
			""      next_step.ts AS ts2,\n"" +
			""      next_step.measurement - this_step.measurement AS diff \n"" +
			""    AFTER MATCH SKIP TO NEXT ROW \n"" +
			""    PATTERN (this_step next_step)\n"" +
			""    DEFINE this_step AS TRUE,\n"" +
			""    next_step AS TRUE\n"" +
			"")"");

		env.executeSql(""SELECT * FROM foo"");
{code}

fails with: 
{code}
java.lang.AssertionError
	at org.apache.calcite.sql.SqlMatchRecognize$SqlMatchRecognizeOperator.createCall(SqlMatchRecognize.java:274)
	at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.result(SqlShuttle.java:117)
	at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.result(SqlShuttle.java:101)
	at org.apache.calcite.sql.util.SqlShuttle.visit(SqlShuttle.java:67)
	at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:153)
	at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:130)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.visitChild(SqlShuttle.java:134)
	at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.visitChild(SqlShuttle.java:101)
	at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:879)
	at org.apache.calcite.sql.SqlSelectOperator.acceptCall(SqlSelectOperator.java:133)
	at org.apache.calcite.sql.util.SqlShuttle.visit(SqlShuttle.java:66)
	at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:153)
	at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:130)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.flink.table.planner.utils.Expander$Expanded.substitute(Expander.java:168)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertViewQuery(SqlToOperationConverter.java:728)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateView(SqlToOperationConverter.java:699)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:226)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:658)
	at org.apache.flink.table.planner.runtime.stream.table.FunctionITCase.test(FunctionITCase.java:165)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
{code}
",,dwysakowicz,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-4390,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 12:53:56 UTC 2020,,,,,,,,,,"0|z0kfso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/20 12:53;dwysakowicz;Fixed in:
* master
** 530c1f0b73306a49abd40ada15ac34b9d92fa30c
* 1.11.3
** 4c91f8af4dac7c6411c414d84d387dd45405b72f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherTest.testOnRemovedJobGraphDoesNotCleanUpHAFiles does not test the desired functionality,FLINK-20076,13339733,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,10/Nov/20 12:04,11/Nov/20 09:47,13/Jul/23 08:12,11/Nov/20 09:47,1.10.2,1.11.2,,,,1.10.3,1.11.3,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"{{DispatcherTest.testOnRemovedJobGraphDoesNotCleanUpHAFiles}} succeeds but due to different reasons: The used {{TestingJobGraphStore}} is not started. An {{IllegalStateException}} prevents the code from reaching the set `removeJobGraphFuture` to get triggered. Hence, the test succeeds but not for the reason the test was implemented for.",,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 09:47:07 UTC 2020,,,,,,,,,,"0|z0kfrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/20 09:47;trohrmann;Fixed via 

1.12.0: 91404f435f20c5cd6714ee18bf4ccf95c81fb73e
1.11.3: 5f1e6654330a29013ef9c940ef1b799e82d3c228
1.10.3: fd624ce133fea2f30781fd855a8ab3a2b1840627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix can't generate plan when joining on changelog source without updates,FLINK-20074,13339708,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,lzljs3620320,lzljs3620320,10/Nov/20 10:40,12/Nov/20 15:23,13/Jul/23 08:12,12/Nov/20 15:23,1.12.0,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,,INSTANCE: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9379&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=e25d5e7e-2a9c-5589-4940-0b638d75a414,,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 15:23:52 UTC 2020,,,,,,,,,,"0|z0kfm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/20 15:23;jark;Fixed in master (1.12.0): 8f43e2ac5318cbbd3c1d88010921d0e6da90637d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_404_check doesn't work properly,FLINK-20069,13339664,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,10/Nov/20 06:58,10/Nov/20 16:10,13/Jul/23 08:12,10/Nov/20 16:09,1.11.0,1.12.0,,,,1.11.3,1.12.0,,Build System,,,,,0,pull-request-available,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43

{code}
Starting: CmdLine
==============================================================================
Task         : Command line
Description  : Run a command line script using Bash on Linux and macOS and cmd.exe on Windows
Version      : 2.177.3
Author       : Microsoft Corporation
Help         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/command-line
==============================================================================
Generating script.
Script contents:
exec ./tools/ci/docs.sh
========================== Starting Command Output ===========================
/bin/bash --noprofile --norc /home/vsts/work/_temp/71629bf0-181a-4981-a18d-44e3c94229f1.sh
Waiting for server...
[DEPRECATED] The `--path` flag is deprecated because it relies on being remembered across bundler invocations, which bundler will no longer do in future versions. Instead please use `bundle config set path '/home/vsts/gem_cache'`, and stop using this flag
Fetching gem metadata from https://rubygems.org/.........
jekyll-4.0.1 requires rubygems version >= 2.7.0, which is incompatible with the
current version, 2.6.14.4
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
Waiting for server...
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 16:10:47 UTC 2020,,,,,,,,,,"0|z0kfc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/20 07:30;rmetzger;[~dian.fu] can you look into it, or should I?;;;","10/Nov/20 07:32;dian.fu;[~rmetzger] I will take a look at this issue.;;;","10/Nov/20 07:36;rmetzger;Thanks a lot!;;;","10/Nov/20 11:31;dian.fu;The root cause is that gem version isn't as expected. I have verified that upgrading the version of gem could solve this problem:
{code}
jekyll-4.0.1 requires rubygems version >= 2.7.0, which is incompatible with the current version, 2.6.14.4
{code};;;","10/Nov/20 16:09;rmetzger;Fixed in https://github.com/apache/flink/commit/3add52d7ddb22fd6f689ef41372edfddf23a7057.;;;","10/Nov/20 16:10;rmetzger;Merged to release-1.11 in 75ef1245b61ab7924a14b0ed8676cdd9045f7b40.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSubscriberTest.testTopicPatternSubscriber failed with unexpected results,FLINK-20068,13339659,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,dian.fu,dian.fu,10/Nov/20 05:18,11/Nov/20 07:17,13/Jul/23 08:12,11/Nov/20 07:16,1.12.0,,,,,1.11.3,1.12.0,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9365&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

{code}
2020-11-10T00:14:22.7658242Z [ERROR] testTopicPatternSubscriber(org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest)  Time elapsed: 0.012 s  <<< FAILURE!
2020-11-10T00:14:22.7659838Z java.lang.AssertionError: expected:<[pattern-topic-5, pattern-topic-4, pattern-topic-7, pattern-topic-6, pattern-topic-9, pattern-topic-8, pattern-topic-1, pattern-topic-0, pattern-topic-3]> but was:<[]>
2020-11-10T00:14:22.7660740Z 	at org.junit.Assert.fail(Assert.java:88)
2020-11-10T00:14:22.7661245Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-11-10T00:14:22.7661788Z 	at org.junit.Assert.assertEquals(Assert.java:118)
2020-11-10T00:14:22.7662312Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-11-10T00:14:22.7663051Z 	at org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest.testTopicPatternSubscriber(KafkaSubscriberTest.java:94)
{code}",,becket_qin,dian.fu,rmetzger,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 07:17:11 UTC 2020,,,,,,,,,,"0|z0kfb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/20 09:16;xuannan;Another instance
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9388&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","10/Nov/20 09:54;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9371&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","10/Nov/20 11:26;rmetzger;Can you take a look [~becket_qin];;;","10/Nov/20 13:13;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9384&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","10/Nov/20 14:19;becket_qin;[~rmetzger] Checking...;;;","11/Nov/20 01:45;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9422&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","11/Nov/20 01:54;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9425&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19;;;","11/Nov/20 06:09;becket_qin;This is a legacy issue caused by asynchronous topic creation in Kafka.

The problems is that we have 3 Kafka brokers. When a topic is created, the {{CreateTopicsRequest}} will be sent to one of the brokers and block waiting until that broker receives a metadata cache update. However, this does not guarantee that the other two brokers have also received the metadata cache update. When a subsequent TopicMetadataRequest goes to the other two brokers, it is still possible that the topic being created is not returned in the topic metadata.

The patch fixes this by letting the {{KafkaTestEnvironmentImpl}} to look into each of the brokers and wait until all the brokers have got the newly created topic in their metadata cache before it returns from {{createTestTopic()}} call.

This should help fix a few other intermittent test failures.;;;","11/Nov/20 07:16;becket_qin;Merged to master.
cacb4c1fb1d6123d0aeb93d550ffcafa6ad4a8fb

Cherry-picked to 1.11：
18e4c1ba5b66c704b3ea7e5ba8fb7f3499207903;;;","11/Nov/20 07:17;rmetzger;Thanks a lot for the explanation and fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchPandasUDAFITTests.test_group_aggregate_with_aux_group unstable,FLINK-20066,13339645,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,dian.fu,dian.fu,10/Nov/20 02:31,11/Nov/20 04:48,13/Jul/23 08:12,11/Nov/20 04:48,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e

{code}
2020-11-09T23:41:41.8853547Z =================================== FAILURES ===================================
2020-11-09T23:41:41.8854000Z __________ BatchPandasUDAFITTests.test_group_aggregate_with_aux_group __________
2020-11-09T23:41:41.8854324Z 
2020-11-09T23:41:41.8854647Z self = <pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests testMethod=test_group_aggregate_with_aux_group>
2020-11-09T23:41:41.8854956Z 
2020-11-09T23:41:41.8855205Z     def test_group_aggregate_with_aux_group(self):
2020-11-09T23:41:41.8855521Z         t = self.t_env.from_elements(
2020-11-09T23:41:41.8858372Z             [(1, 2, 3), (3, 2, 3), (2, 1, 3), (1, 5, 4), (1, 8, 6), (2, 3, 4)],
2020-11-09T23:41:41.8858807Z             DataTypes.ROW(
2020-11-09T23:41:41.8859091Z                 [DataTypes.FIELD(""a"", DataTypes.TINYINT()),
2020-11-09T23:41:41.8859453Z                  DataTypes.FIELD(""b"", DataTypes.SMALLINT()),
2020-11-09T23:41:41.8859783Z                  DataTypes.FIELD(""c"", DataTypes.INT())]))
2020-11-09T23:41:41.8860012Z     
2020-11-09T23:41:41.8860255Z         table_sink = source_sink_utils.TestAppendSink(
2020-11-09T23:41:41.8863051Z             ['a', 'b', 'c', 'd'],
2020-11-09T23:41:41.8863523Z             [DataTypes.TINYINT(), DataTypes.INT(), DataTypes.FLOAT(), DataTypes.INT()])
2020-11-09T23:41:41.8864048Z         self.t_env.register_table_sink(""Results"", table_sink)
2020-11-09T23:41:41.8864715Z         self.t_env.get_config().get_configuration().set_string('python.metric.enabled', 'true')
2020-11-09T23:41:41.8865161Z         self.t_env.register_function(""max_add"", udaf(MaxAdd(),
2020-11-09T23:41:41.8865573Z                                                      result_type=DataTypes.INT(),
2020-11-09T23:41:41.8865999Z                                                      func_type=""pandas""))
2020-11-09T23:41:41.8866426Z         self.t_env.create_temporary_system_function(""mean_udaf"", mean_udaf)
2020-11-09T23:41:41.8866759Z         t.group_by(""a"") \
2020-11-09T23:41:41.8867052Z             .select(""a, a + 1 as b, a + 2 as c"") \
2020-11-09T23:41:41.8867352Z             .group_by(""a, b"") \
2020-11-09T23:41:41.8867660Z             .select(""a, b, mean_udaf(b), max_add(b, c, 1)"") \
2020-11-09T23:41:41.8868026Z >           .execute_insert(""Results"") \
2020-11-09T23:41:41.8868293Z             .wait()
2020-11-09T23:41:41.8868446Z 
2020-11-09T23:41:41.8868704Z pyflink/table/tests/test_pandas_udaf.py:95: 
2020-11-09T23:41:41.8869077Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-11-09T23:41:41.8869464Z pyflink/table/table_result.py:76: in wait
2020-11-09T23:41:41.8870150Z     get_method(self._j_table_result, ""await"")()
2020-11-09T23:41:41.8870850Z .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1286: in __call__
2020-11-09T23:41:41.8871415Z     answer, self.gateway_client, self.target_id, self.name)
2020-11-09T23:41:41.8871768Z pyflink/util/exceptions.py:147: in deco
2020-11-09T23:41:41.8872032Z     return f(*a, **kw)
2020-11-09T23:41:41.8872378Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-11-09T23:41:41.8872629Z 
2020-11-09T23:41:41.8872978Z answer = 'xro24238'
2020-11-09T23:41:41.8873296Z gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f0be26f2358>
2020-11-09T23:41:41.8873792Z target_id = 'o24237', name = 'await'
2020-11-09T23:41:41.8874097Z 
2020-11-09T23:41:41.8874433Z     def get_return_value(answer, gateway_client, target_id=None, name=None):
2020-11-09T23:41:41.8874893Z         """"""Converts an answer received from the Java gateway into a Python object.
2020-11-09T23:41:41.8875212Z     
2020-11-09T23:41:41.8875515Z         For example, string representation of integers are converted to Python
2020-11-09T23:41:41.8875922Z         integer, string representation of objects are converted to JavaObject
2020-11-09T23:41:41.8876255Z         instances, etc.
2020-11-09T23:41:41.8876584Z     
2020-11-09T23:41:41.8876873Z         :param answer: the string returned by the Java gateway
2020-11-09T23:41:41.8877307Z         :param gateway_client: the gateway client used to communicate with the Java
2020-11-09T23:41:41.8877749Z             Gateway. Only necessary if the answer is a reference (e.g., object,
2020-11-09T23:41:41.8878126Z             list, map)
2020-11-09T23:41:41.8878646Z         :param target_id: the name of the object from which the answer comes from
2020-11-09T23:41:41.8879154Z             (e.g., *object1* in `object1.hello()`). Optional.
2020-11-09T23:41:41.8879679Z         :param name: the name of the member from which the answer comes from
2020-11-09T23:41:41.8880097Z             (e.g., *hello* in `object1.hello()`). Optional.
2020-11-09T23:41:41.8880371Z         """"""
2020-11-09T23:41:41.8880621Z         if is_error(answer)[0]:
2020-11-09T23:41:41.8881088Z             if len(answer) > 1:
2020-11-09T23:41:41.8881457Z                 type = answer[1]
2020-11-09T23:41:41.8881967Z                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2020-11-09T23:41:41.8882417Z                 if answer[1] == REFERENCE_TYPE:
2020-11-09T23:41:41.8882754Z                     raise Py4JJavaError(
2020-11-09T23:41:41.8883113Z                         ""An error occurred while calling {0}{1}{2}.\n"".
2020-11-09T23:41:41.8883490Z >                       format(target_id, ""."", name), value)
2020-11-09T23:41:41.8883935Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling o24237.await.
2020-11-09T23:41:41.8884552Z E                   : java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
2020-11-09T23:41:41.8885193Z E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-11-09T23:41:41.8885796Z E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-09T23:41:41.8886595Z E                   	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:119)
2020-11-09T23:41:41.8887337Z E                   	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:86)
2020-11-09T23:41:41.8887976Z E                   	at sun.reflect.GeneratedMethodAccessor225.invoke(Unknown Source)
2020-11-09T23:41:41.8888520Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-09T23:41:41.8889053Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-09T23:41:41.8889688Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-11-09T23:41:41.8890493Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-11-09T23:41:41.8891500Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-11-09T23:41:41.8892134Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-11-09T23:41:41.8892797Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-11-09T23:41:41.8893443Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-11-09T23:41:41.8893959Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-11-09T23:41:41.8894452Z E                   Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
2020-11-09T23:41:41.8895065Z E                   	at org.apache.flink.table.api.internal.InsertResultIterator.hasNext(InsertResultIterator.java:59)
2020-11-09T23:41:41.8895748Z E                   	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355)
2020-11-09T23:41:41.8896497Z E                   	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.isFirstRowReady(TableResultImpl.java:368)
2020-11-09T23:41:41.8897328Z E                   	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:107)
2020-11-09T23:41:41.8897954Z E                   	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2020-11-09T23:41:41.8898554Z E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-11-09T23:41:41.8899153Z E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-11-09T23:41:41.8899678Z E                   	... 1 more
2020-11-09T23:41:41.8900197Z E                   Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-09T23:41:41.8900946Z E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-11-09T23:41:41.8901597Z E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-09T23:41:41.8902192Z E                   	at org.apache.flink.table.api.internal.InsertResultIterator.hasNext(InsertResultIterator.java:57)
2020-11-09T23:41:41.8902668Z E                   	... 7 more
2020-11-09T23:41:41.8903107Z E                   Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-09T23:41:41.8903704Z E                   	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-11-09T23:41:41.8904393Z E                   	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
2020-11-09T23:41:41.8905066Z E                   	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-09T23:41:41.8905647Z E                   	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-11-09T23:41:41.8906253Z E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-09T23:41:41.8906842Z E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-09T23:41:41.8907475Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-11-09T23:41:41.8908136Z E                   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-11-09T23:41:41.8908757Z E                   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-11-09T23:41:41.8909434Z E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-09T23:41:41.8910095Z E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-09T23:41:41.8910691Z E                   	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
2020-11-09T23:41:41.8911343Z E                   	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-11-09T23:41:41.8911826Z E                   	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-11-09T23:41:41.8912326Z E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-11-09T23:41:41.8912822Z E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-11-09T23:41:41.8913336Z E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-09T23:41:41.8914205Z E                   	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-11-09T23:41:41.8914918Z E                   	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-11-09T23:41:41.8915503Z E                   	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-11-09T23:41:41.8916158Z E                   	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-11-09T23:41:41.8916723Z E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-11-09T23:41:41.8917388Z E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-11-09T23:41:41.8917992Z E                   	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-11-09T23:41:41.8918539Z E                   	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-11-09T23:41:41.8919041Z E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-09T23:41:41.8919696Z E                   	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-11-09T23:41:41.8920328Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-11-09T23:41:41.8921129Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-09T23:41:41.8921790Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-09T23:41:41.8922396Z E                   	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-11-09T23:41:41.8922951Z E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-11-09T23:41:41.8923502Z E                   	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-11-09T23:41:41.8924115Z E                   	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-11-09T23:41:41.8924715Z E                   	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-09T23:41:41.8925275Z E                   	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-09T23:41:41.8925845Z E                   	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-09T23:41:41.8926402Z E                   	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-09T23:41:41.8927017Z E                   Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-11-09T23:41:41.8927737Z E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-09T23:41:41.8928609Z E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-09T23:41:41.8929366Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-11-09T23:41:41.8930124Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-11-09T23:41:41.8930893Z E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-11-09T23:41:41.8931631Z E                   	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534)
2020-11-09T23:41:41.8932264Z E                   	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
2020-11-09T23:41:41.8932883Z E                   	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
2020-11-09T23:41:41.8933389Z E                   	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
2020-11-09T23:41:41.8933910Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-09T23:41:41.8934575Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-09T23:41:41.8935103Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-09T23:41:41.8935719Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-09T23:41:41.8936349Z E                   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-09T23:41:41.8936952Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-09T23:41:41.8937525Z E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-09T23:41:41.8938053Z E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-09T23:41:41.8938571Z E                   	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-09T23:41:41.8939116Z E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-09T23:41:41.8939747Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-09T23:41:41.8940259Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-09T23:41:41.8940844Z E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-09T23:41:41.8941682Z E                   	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-09T23:41:41.8942198Z E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-09T23:41:41.8942739Z E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-09T23:41:41.8943237Z E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-09T23:41:41.8943713Z E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-09T23:41:41.8944207Z E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-09T23:41:41.8944660Z E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-09T23:41:41.8945007Z E                   	... 4 more
2020-11-09T23:41:41.8945402Z E                   Caused by: java.lang.RuntimeException: Failed to close remote bundle
2020-11-09T23:41:41.8946046Z E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:368)
2020-11-09T23:41:41.8946794Z E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:322)
2020-11-09T23:41:41.8947691Z E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:283)
2020-11-09T23:41:41.8948546Z E                   	at org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator.endInput(AbstractOneInputPythonFunctionOperator.java:42)
2020-11-09T23:41:41.8949613Z E                   	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperator.endInput(AbstractBatchArrowPythonAggregateFunctionOperator.java:95)
2020-11-09T23:41:41.8950643Z E                   	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperator.endInput(BatchArrowPythonGroupAggregateFunctionOperator.java:33)
2020-11-09T23:41:41.8951667Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:91)
2020-11-09T23:41:41.8952406Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$close$0(StreamOperatorWrapper.java:127)
2020-11-09T23:41:41.8953139Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
2020-11-09T23:41:41.8954086Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:127)
2020-11-09T23:41:41.8954847Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
2020-11-09T23:41:41.8955495Z E                   	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:412)
2020-11-09T23:41:41.8956082Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:587)
2020-11-09T23:41:41.8956660Z E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:549)
2020-11-09T23:41:41.8957218Z E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-11-09T23:41:41.8957747Z E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-11-09T23:41:41.8958311Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-11-09T23:41:41.8959085Z E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 2: Traceback (most recent call last):
2020-11-09T23:41:41.8960839Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 253, in _execute
2020-11-09T23:41:41.8961406Z E                       response = task()
2020-11-09T23:41:41.8962160Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 310, in <lambda>
2020-11-09T23:41:41.8962733Z E                       lambda: self.create_worker().do_instruction(request), request)
2020-11-09T23:41:41.8963624Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 480, in do_instruction
2020-11-09T23:41:41.8964156Z E                       getattr(request, request_type), request.instruction_id)
2020-11-09T23:41:41.8964947Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle
2020-11-09T23:41:41.8965477Z E                       bundle_processor.process_bundle(instruction_id))
2020-11-09T23:41:41.8966276Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 978, in process_bundle
2020-11-09T23:41:41.8966767Z E                       element.data)
2020-11-09T23:41:41.8967636Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded
2020-11-09T23:41:41.8968489Z E                       self.output(decoded_value)
2020-11-09T23:41:41.8968975Z E                     File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output
2020-11-09T23:41:41.8969676Z E                     File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output
2020-11-09T23:41:41.8970276Z E                     File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2020-11-09T23:41:41.8971101Z E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 71, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
2020-11-09T23:41:41.8971649Z E                       with self.scoped_process_state:
2020-11-09T23:41:41.8972182Z E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 75, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
2020-11-09T23:41:41.8972711Z E                       self.func(value), self.consumer.output_stream, True)
2020-11-09T23:41:41.8973182Z E                     File ""<string>"", line 1, in <lambda>
2020-11-09T23:41:41.8973840Z E                     File ""/__w/2/s/flink-python/pyflink/table/udf.py"", line 269, in eval
2020-11-09T23:41:41.8974254Z E                       return self.func.get_value(accumulator)
2020-11-09T23:41:41.8974892Z E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_pandas_udaf.py"", line 709, in get_value
2020-11-09T23:41:41.8975341Z E                       self.assertEqual(self.counter_sum, self.counter.get_count())
2020-11-09T23:41:41.8976040Z E                     File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/unittest/case.py"", line 839, in assertEqual
2020-11-09T23:41:41.8976492Z E                       assertion_func(first, second, msg=msg)
2020-11-09T23:41:41.8977147Z E                     File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/unittest/case.py"", line 832, in _baseAssertEqual
2020-11-09T23:41:41.8977616Z E                       raise self.failureException(msg)
2020-11-09T23:41:41.8977964Z E                   AssertionError: 30 != 10
2020-11-09T23:41:41.8978238Z E                   
2020-11-09T23:41:41.8978633Z E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-11-09T23:41:41.8979293Z E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-09T23:41:41.8979986Z E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:57)
2020-11-09T23:41:41.8980619Z E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:458)
2020-11-09T23:41:41.8981558Z E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:547)
2020-11-09T23:41:41.8982319Z E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:366)
2020-11-09T23:41:41.8982840Z E                   	... 16 more
2020-11-09T23:41:41.8983356Z E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 2: Traceback (most recent call last):
2020-11-09T23:41:41.8984329Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 253, in _execute
2020-11-09T23:41:41.8984816Z E                       response = task()
2020-11-09T23:41:41.8985619Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 310, in <lambda>
2020-11-09T23:41:41.8986330Z E                       lambda: self.create_worker().do_instruction(request), request)
2020-11-09T23:41:41.8987155Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 480, in do_instruction
2020-11-09T23:41:41.8987713Z E                       getattr(request, request_type), request.instruction_id)
2020-11-09T23:41:41.8988500Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle
2020-11-09T23:41:41.8989029Z E                       bundle_processor.process_bundle(instruction_id))
2020-11-09T23:41:41.8989883Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 978, in process_bundle
2020-11-09T23:41:41.8990359Z E                       element.data)
2020-11-09T23:41:41.8991201Z E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded
2020-11-09T23:41:41.8991707Z E                       self.output(decoded_value)
2020-11-09T23:41:41.8992425Z E                     File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output
2020-11-09T23:41:41.8993023Z E                     File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output
2020-11-09T23:41:41.8993656Z E                     File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2020-11-09T23:41:41.8994447Z E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 71, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
2020-11-09T23:41:41.8995170Z E                       with self.scoped_process_state:
2020-11-09T23:41:41.8995838Z E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 75, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
2020-11-09T23:41:41.8996449Z E                       self.func(value), self.consumer.output_stream, True)
2020-11-09T23:41:41.8996844Z E                     File ""<string>"", line 1, in <lambda>
2020-11-09T23:41:41.8997535Z E                     File ""/__w/2/s/flink-python/pyflink/table/udf.py"", line 269, in eval
2020-11-09T23:41:41.8997963Z E                       return self.func.get_value(accumulator)
2020-11-09T23:41:41.8998633Z E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_pandas_udaf.py"", line 709, in get_value
2020-11-09T23:41:41.8999200Z E                       self.assertEqual(self.counter_sum, self.counter.get_count())
2020-11-09T23:41:41.9000317Z E                     File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/unittest/case.py"", line 839, in assertEqual
2020-11-09T23:41:41.9001165Z E                       assertion_func(first, second, msg=msg)
2020-11-09T23:41:41.9001865Z E                     File ""/__w/2/s/flink-python/dev/.conda/lib/python3.7/unittest/case.py"", line 832, in _baseAssertEqual
2020-11-09T23:41:41.9002396Z E                       raise self.failureException(msg)
2020-11-09T23:41:41.9002865Z E                   AssertionError: 30 != 10
2020-11-09T23:41:41.9003240Z E                   
2020-11-09T23:41:41.9003822Z E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:177)
2020-11-09T23:41:41.9004735Z E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:157)
2020-11-09T23:41:41.9005621Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
2020-11-09T23:41:41.9006612Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
2020-11-09T23:41:41.9007519Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
2020-11-09T23:41:41.9008389Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
2020-11-09T23:41:41.9009224Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
2020-11-09T23:41:41.9010178Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
2020-11-09T23:41:41.9011130Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2020-11-09T23:41:41.9012068Z E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2020-11-09T23:41:41.9012702Z E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-11-09T23:41:41.9013380Z E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-11-09T23:41:41.9013805Z E                   	... 1 more
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19842,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 04:48:01 UTC 2020,,,,,,,,,,"0|z0kf80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/20 02:32;dian.fu;cc [~hxbks2ks];;;","11/Nov/20 04:48;dian.fu;This should have been address in FLINK-19842;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointCompatibilityITCase.test failed with AskTimeoutException,FLINK-20065,13339642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,dian.fu,dian.fu,10/Nov/20 02:01,22/Jun/21 13:55,13/Jul/23 08:12,12/Nov/20 13:57,1.11.3,1.12.0,,,,1.10.3,1.11.3,1.12.0,Runtime / Coordination,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9362&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

{code}
2020-11-09T22:19:47.2714024Z [ERROR] test[type: SAVEPOINT, startAligned: true](org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase)  Time elapsed: 1.293 s  <<< ERROR!
2020-11-09T22:19:47.2715260Z java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2020-11-09T22:19:47.2716743Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-11-09T22:19:47.2718213Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-09T22:19:47.2719166Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.runAndTakeSavepoint(UnalignedCheckpointCompatibilityITCase.java:113)
2020-11-09T22:19:47.2720278Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.test(UnalignedCheckpointCompatibilityITCase.java:97)
2020-11-09T22:19:47.2721126Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-09T22:19:47.2721771Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-09T22:19:47.2722773Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-09T22:19:47.2723479Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-09T22:19:47.2724187Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-11-09T22:19:47.2725026Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-11-09T22:19:47.2725817Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-11-09T22:19:47.2726595Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-11-09T22:19:47.2727515Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-11-09T22:19:47.2728192Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-11-09T22:19:47.2744089Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-11-09T22:19:47.2744907Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-11-09T22:19:47.2745573Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-11-09T22:19:47.2746037Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-09T22:19:47.2746445Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-09T22:19:47.2746868Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-09T22:19:47.2747443Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-09T22:19:47.2747876Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-09T22:19:47.2748297Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-09T22:19:47.2748694Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-11-09T22:19:47.2749054Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-11-09T22:19:47.2749414Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-09T22:19:47.2749819Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-09T22:19:47.2750373Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-09T22:19:47.2750923Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-09T22:19:47.2751555Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-09T22:19:47.2752148Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-09T22:19:47.2752938Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-11-09T22:19:47.3085383Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-11-09T22:19:47.3086377Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-11-09T22:19:47.3087146Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-11-09T22:19:47.3088051Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-11-09T22:19:47.3088815Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-11-09T22:19:47.3089472Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-11-09T22:19:47.3090109Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-11-09T22:19:47.3091501Z Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2020-11-09T22:19:47.3092730Z 	at com.sun.proxy.$Proxy33.stopWithSavepoint(Unknown Source)
2020-11-09T22:19:47.3093348Z 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$stopWithSavepoint$9(MiniCluster.java:599)
2020-11-09T22:19:47.3094052Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-09T22:19:47.3094682Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2020-11-09T22:19:47.3095359Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2020-11-09T22:19:47.3096070Z 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:621)
2020-11-09T22:19:47.3096926Z 	at org.apache.flink.runtime.minicluster.MiniCluster.stopWithSavepoint(MiniCluster.java:599)
2020-11-09T22:19:47.3097996Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.stopWithSavepoint(PerJobMiniClusterFactory.java:169)
2020-11-09T22:19:47.3098985Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.runAndTakeSavepoint(UnalignedCheckpointCompatibilityITCase.java:112)
2020-11-09T22:19:47.3099640Z 	... 36 more
2020-11-09T22:19:47.3101706Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_18#-978053777]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-11-09T22:19:47.3103053Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-11-09T22:19:47.3103640Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-11-09T22:19:47.3104114Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-11-09T22:19:47.3104521Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-11-09T22:19:47.3104994Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.run(LightArrayRevolverScheduler.scala:337)
2020-11-09T22:19:47.3105546Z 	at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(LightArrayRevolverScheduler.scala:141)
2020-11-09T22:19:47.3106140Z 	at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(LightArrayRevolverScheduler.scala:140)
2020-11-09T22:19:47.3106646Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-11-09T22:19:47.3107072Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-11-09T22:19:47.3107770Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-11-09T22:19:47.3108203Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-11-09T22:19:47.3108659Z 	at akka.actor.LightArrayRevolverScheduler.close(LightArrayRevolverScheduler.scala:139)
2020-11-09T22:19:47.3109107Z 	at akka.actor.ActorSystemImpl.stopScheduler(ActorSystem.scala:937)
2020-11-09T22:19:47.3109575Z 	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply$mcV$sp(ActorSystem.scala:872)
2020-11-09T22:19:47.3110253Z 	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply(ActorSystem.scala:872)
2020-11-09T22:19:47.3110724Z 	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply(ActorSystem.scala:872)
2020-11-09T22:19:47.3111176Z 	at akka.actor.ActorSystemImpl$$anon$3.run(ActorSystem.scala:892)
2020-11-09T22:19:47.3111666Z 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1068)
2020-11-09T22:19:47.3112220Z 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1068)
2020-11-09T22:19:47.3112970Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-11-09T22:19:47.3113403Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-11-09T22:19:47.3113915Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-09T22:19:47.3114373Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-11-09T22:19:47.3114909Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-11-09T22:19:47.3115439Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-09T22:19:47.3115961Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-09T22:19:47.3116507Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-11-09T22:19:47.3116953Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-11-09T22:19:47.3117501Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-11-09T22:19:47.3117989Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-11-09T22:19:47.3118536Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-09T22:19:47.3118985Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-09T22:19:47.3119440Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-09T22:19:47.3119872Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,AHeise,dian.fu,jark,kezhuw,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20033,,,,FLINK-19585,FLINK-15152,,,,,,,,,,"11/Nov/20 12:39;AHeise;debug.log;https://issues.apache.org/jira/secure/attachment/13015064/debug.log","11/Nov/20 14:12;AHeise;thread.dump;https://issues.apache.org/jira/secure/attachment/13015068/thread.dump",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 13:57:57 UTC 2020,,,,,,,,,,"0|z0kf7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/20 02:05;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9362&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=d6363642-ea4a-5c73-7edb-c00d4548b58e;;;","10/Nov/20 02:08;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9362&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=981eced9-6683-5752-3201-62faf56c149b;;;","10/Nov/20 02:33;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc;;;","10/Nov/20 07:18;xtsong;[~AHeise], could you help take a look at this?;;;","10/Nov/20 11:20;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9393&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","10/Nov/20 19:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9405&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","10/Nov/20 19:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9403&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","11/Nov/20 01:41;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9416&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","11/Nov/20 01:46;dian.fu;Instances on release-1.11:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9426&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9426&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=ec103906-d047-5b8a-680e-05fc000dfca9

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9426&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=d6363642-ea4a-5c73-7edb-c00d4548b58e

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9426&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=981eced9-6683-5752-3201-62faf56c149b;;;","11/Nov/20 01:55;dian.fu;Instance on master:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9425&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185;;;","11/Nov/20 01:56;dian.fu;Upgrade to Blocker due to the failure frequency.;;;","11/Nov/20 03:00;jark;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9391&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","11/Nov/20 05:40;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9431&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","11/Nov/20 11:01;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9446&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323;;;","11/Nov/20 11:04;rmetzger;[~AHeise] what's the progress on the fix? Does it make sense to push a hotfix disabling the test for the time being?;;;","11/Nov/20 12:38;arvid;After some investigation, I'm rather convinced that this is not an issue of the test, but a bug that has been introduced by 03047a78b6ad8017eb240548a009424d68ffc971 of FLINK-20033. Before this commit, I can run 1000/1000 the test without issues, afterwards it fails 10-20% of the time locally. It also matches the behavior of release-1.11, where it was backported.;;;","11/Nov/20 12:39;arvid;It could still be a rather specific test setup that leads to such failure. I'm leaving a debug log here.;;;","11/Nov/20 17:06;trohrmann;I did a bit of investigation and I think I have found the problem. First of all the change FLINK-20033 introduced was that the {{JobMaster.jobStatusChanged}} is now called synchronously when the {{ExecutionGraph}} job status changes. Before we send an asynchronous message which was executed in the {{JobMaster's}} main thread once the state transition operation in the {{ExecutionGraph}} has completely finished.

Now here is the problem this change has introduced: When calling stop with savepoint we only return the savepoint path after the {{ExecutionGraph}} has reached a terminal state. Moreover, we want to restart the checkpoint coordinator if the savepoint has failed. Since the savepoint future can be completed from a different thread than the main thread, we have to splice it back via calling {{handleAsync}} with the main thread executor: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L921. Before the {{ExecutionGraph's}} {{terminationFuture}} is completed, we notify the {{JobStatusListener}} about the job status change. Since the listener reacts to this message directly (change introduced with FLINK-20033), {{JobMaster.jobStatusChanged}} will be called immediately. In this method, we trigger the {{OnCompletionActions.jobReachedGloballyTerminalState}} using the {{scheduledExecutorService}}. This call back will trigger the shutdown of the {{JobMaster}}. Now depending how fast the {{scheduledExecutorService}} executes the callback it can happen that the {{JobMaster}} receives the stop message before receiving the {{RunAsync}} message responsible for running the {{handleAsync}}. As a consequence we will never run the {{handleAsync}} and, hence, never return the savepoint path.

I think the underlying problem is that there is no notion for tasks which need to be executed before shutting down. One idea could be to make the {{Scheduler}} responsible for tracking ongoing operations and only to shutdown after either cancelling them or after they are completed. A slightly less invasive solution could be to not let the savepoint depend on the {{handleAsync}} block which has been introduced with FLINK-15152. That way we could return the savepoint path as soon as it is ready.

For the time being I would suggest to revert FLINK-20033 for the {{1.12.0}} release and to fix it properly with the next release.;;;","11/Nov/20 17:11;trohrmann;Here is a branch with a potential fix for the problem.;;;","12/Nov/20 10:18;rmetzger;[~AHeise] since you are assigned to the ticket: Are you going to revert FLINK-20033 (or get the branch from [~trohrmann]) ?;;;","12/Nov/20 12:39;arvid;I'd rather hand over the ticket to someone with knowledge of the JobMaster.;;;","12/Nov/20 12:47;rmetzger;I'm assigning [~trohrmann] to the ticket, he told me offline that he's preparing PRs for reverting the change.;;;","12/Nov/20 13:57;trohrmann;The problem should be fixed by reverting FLINK-20033:

1.12.0:
dea47d6b17fdd26920c1bfbb5aea24a7003a16be
89262012a86647ebc3658027e5690a242b2f0782

1.11.3:
dcbf474343a43370ae08665e71dad708c65369ca
68d16f0f04b7ddf0ed5f7aadb14092c101a811f5

1.10.3:
d8fa676412a85d5e369e6367978ccfa8a7e07194
fef13fda361e6dfc566553316fcc8ca4943af787;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links in the documentation,FLINK-20064,13339636,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,sjwiesman,sjwiesman,10/Nov/20 00:55,10/Nov/20 12:37,13/Jul/23 08:12,10/Nov/20 12:37,1.11.3,1.12.0,,,,1.11.3,1.12.0,,Documentation,,,,,0,pull-request-available,,,,,"http://localhost:4000/api/java/:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/streaming:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/streaming/time_attributes.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/streaming/query_configuration.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/streaming/temporal_tables.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/common.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/tableApi.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/python/table-api-users-guide/types.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/connectors/filesystem_sink.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/api/java/org/apache/flink/types/RowKind.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/streaming:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/streaming/time_attributes.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/streaming/query_configuration.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/streaming/temporal_tables.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/common.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/tableApi.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/python/table-api-users-guide/types.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/connectors/filesystem_sink.html:
Remote file does not exist -- broken link!!!
---------------------------------------------------------------------------
Found 18 broken links.
Search for page containing broken link using 'grep -R BROKEN_PATH DOCS_DIR'",,dian.fu,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 12:37:39 UTC 2020,,,,,,,,,,"0|z0kf60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/20 01:51;dian.fu;Thanks [~sjwiesman] for reporting this issue. I will take a look at it.;;;","10/Nov/20 12:37;dian.fu;Fixed in
- master(1.12) via 3fed93d62d8f79627d4ded2dd1fae6fae91b36e8
- release-1.11 via e55dd455337ccd4e6207414dd9546b42ab77477d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File Source requests an additional split on every restore.,FLINK-20063,13339591,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,09/Nov/20 19:42,12/Dec/20 11:54,13/Jul/23 08:12,10/Nov/20 14:06,,,,,,1.11.3,1.12.0,,Connectors / FileSystem,,,,,0,pull-request-available,,,,,"Currently, the {{FileSourceReader}} requests a new split when started. That includes cases when it was restored from a checkpoint.

So with every restore, the reader increases its split backlog size by one, causing problems for balanced split assignments.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 14:06:12 UTC 2020,,,,,,,,,,"0|z0kew0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/20 14:06;sewen;Fixed in 1.12.0 (master) via d00941c77170b233c9fe599c7fb0003778eb3299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outdated SQL docs on aggregate functions' merge,FLINK-20059,13339476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,nkruber,nkruber,09/Nov/20 09:28,23/Nov/20 03:38,13/Jul/23 08:12,23/Nov/20 03:38,1.11.2,1.12.0,,,,1.12.0,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"In the java docs as well as the user docs, the {{merge}} method of an aggregation UDF is described as optional, e.g.
{quote}Merges a group of accumulator instances into one accumulator instance. This function must be implemented for data stream session window grouping aggregates and data set grouping aggregates.{quote}

However, it seems that nowadays this method is required in more cases (I stumbled on this for a HOP window in streaming):
{code}
StreamExecGlobalGroupAggregate.scala
      .needMerge(mergedAccOffset, mergedAccOnHeap, mergedAccExternalTypes)
StreamExecGroupWindowAggregateBase.scala
      generator.needMerge(mergedAccOffset = 0, mergedAccOnHeap = false)
StreamExecIncrementalGroupAggregate.scala
      .needMerge(mergedAccOffset, mergedAccOnHeap = true, mergedAccExternalTypes)
StreamExecLocalGroupAggregate.scala
      .needMerge(mergedAccOffset = 0, mergedAccOnHeap = true)
{code}",,jark,leonard,nkruber,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 03:38:10 UTC 2020,,,,,,,,,,"0|z0ke6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/20 11:58;jark;Yes. We should update the documentation and recommend users to implement it. Also the local-global optimization requires merge() method. ;;;","20/Nov/20 14:52;twalthr;[~jark] can you perform the same changes also to the docs of `TableAggregateFunction`?;;;","20/Nov/20 15:49;jark;Sure [~twalthr], AFAIK, `TableAggregateFunction` also requires ""merge"" in hopping windows. But `TableAggregateFunction` doesn't support local-global optimization yet. ;;;","23/Nov/20 03:38;jark;Fixed in master (1.12.0): 
 - 3a066d6ec3fe46716b12e8d2756ba5f0e463fe43
 - d6a1fd049058cb38008289f8563a828930bd55a2

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3 Level List is not supported in ParquetInputFormat,FLINK-20054,13339447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,09/Nov/20 07:23,28/May/21 07:01,13/Jul/23 08:12,02/Dec/20 08:31,1.11.2,,,,,1.13.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"In flink-parquet module doesn't support
reading a 3-level list type in parquet though it is able to process a
2-level list type.

3-level

optional group my_list (LIST) {
  repeated group element {
    required binary str (UTF8);
  };
}


 2-level

optional group my_list (LIST) {
  repeated int32 element;
}
",,Erbureth,lzljs3620320,ZhenqiuHuang,zouyunhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 17:53:11 UTC 2021,,,,,,,,,,"0|z0ke00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/20 08:31;lzljs3620320;master (1.13): c77e6f9e987f344f1bad2deac4793d78d472ac30;;;","11/Mar/21 17:53;Erbureth;{{Hi,}}

3 level lists with struct elements are still yielding errors even with the patch, e. g.:

{{message spark_schema {}}
 {{ required group foo (LIST) {}}
 {{   repeated group list {}}
 {{      required group element {}}
 {{        optional int32 barId;}}
 {{        optional int32 bazId;}}
 {{     }}}
    }
 {{ }}}
{{}}}

results in: 

{{java.lang.UnsupportedOperationException: List field [optional int32 barId] in List [foo] has to be required.}}

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceReaderTestBase.testAddSplitToExistingFetcher failed with NullPointerException,FLINK-20051,13339408,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,dian.fu,dian.fu,09/Nov/20 01:41,09/Nov/20 23:43,13/Jul/23 08:12,09/Nov/20 23:42,1.12.0,,,,,1.12.0,,,Connectors / Common,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9322&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518

{code}
2020-11-08T21:49:29.6792941Z [ERROR] testAddSplitToExistingFetcher(org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest)  Time elapsed: 0.632 s  <<< ERROR!
2020-11-08T21:49:29.6793408Z java.lang.NullPointerException
2020-11-08T21:49:29.6793998Z 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader$KafkaPartitionSplitRecords.nextSplit(KafkaPartitionSplitReader.java:363)
2020-11-08T21:49:29.6795970Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.moveToNextSplit(SourceReaderBase.java:187)
2020-11-08T21:49:29.6796596Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:159)
2020-11-08T21:49:29.6797317Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
2020-11-08T21:49:29.6797942Z 	at org.apache.flink.connector.testutils.source.reader.SourceReaderTestBase.testAddSplitToExistingFetcher(SourceReaderTestBase.java:98)
{code}",,dian.fu,rmetzger,sewen,trohrmann,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 23:42:57 UTC 2020,,,,,,,,,,"0|z0kdrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/20 06:03;rmetzger;[~becket_qin] can you take a look at this failure?;;;","09/Nov/20 16:43;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9356&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","09/Nov/20 20:10;sewen;The problem is that an in-initialized record batch is returned when the reader is woken up to handle the split change.

This line is the problem: https://github.com/apache/flink/blob/0b3f15ee598aa2b55706a4c3a30e2e529ac8f651/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java#L99

We need to add {{recordsBySplits.prepareForRead();}}.

I will push this fix with my next set of commits.;;;","09/Nov/20 23:42;sewen;Fixed in 1.12.0 (master) via a475f571777017805bac8872f19ebe8160ad4cd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinatorProviderTest.testCheckpointAndReset failed with NullPointerException,FLINK-20050,13339407,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,dian.fu,dian.fu,09/Nov/20 01:38,12/Dec/20 11:55,13/Jul/23 08:12,10/Nov/20 06:50,1.12.0,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9322&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392

{code}
2020-11-08T22:24:39.5642544Z [ERROR] testCheckpointAndReset(org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest)  Time elapsed: 0.954 s  <<< ERROR!
2020-11-08T22:24:39.5643055Z java.lang.NullPointerException
2020-11-08T22:24:39.5643578Z 	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest.testCheckpointAndReset(SourceCoordinatorProviderTest.java:94)
{code}",,becket_qin,dian.fu,rmetzger,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20070,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 06:50:30 UTC 2020,,,,,,,,,,"0|z0kdr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/20 09:45;xuannan;I think it is introduced by this commit [d5b5103c|https://github.com/apache/flink/commit/d5b5103c1ed855b45fd08b1738f044b01864de58]. cc [~becket_qin] ;;;","09/Nov/20 15:06;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8644&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374;;;","09/Nov/20 17:41;becket_qin;[~xuannan] Yes, you are right. The methods visible for testing should also be changed to adapt to the asynchronous {{RecreateOnResetOperatorCoordinator}}. I'll put a fix shortly.;;;","10/Nov/20 02:25;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392;;;","10/Nov/20 06:50;becket_qin;Merged to master:  ae09f9be438736763db129937bb7cc70e37fd429;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultLeaderRetrievalService should only notify the LeaderRetrievalListener when leader truly changed,FLINK-20047,13339365,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,wangyang0918,wangyang0918,08/Nov/20 06:52,09/Nov/20 09:49,13/Jul/23 08:12,09/Nov/20 09:49,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Currently, in {{DefaultLeaderRetrievalService}} we are notifying the {{LeaderRetrievalListener}} even though the leader is not truly changed. In Kubernetes HA service, we could get lots of leader information notification events with same leader. Because the ConfigMap is updated periodically. We should filter out these useless events and do not notify the {{LeaderRetrievalListener}}.",,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 09:49:31 UTC 2020,,,,,,,,,,"0|z0kdhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/20 09:49;trohrmann;Fixed via 8bc6a7bae1bc63a44d9d8a5ad2bd629d561dc166;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTableAggregateTests.test_map_view_iterate is instable,FLINK-20046,13339354,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhongwei,dian.fu,dian.fu,08/Nov/20 01:50,12/Nov/20 11:58,13/Jul/23 08:12,12/Nov/20 11:58,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9279&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490

{code}
2020-11-07T22:50:57.4180758Z _______________ StreamTableAggregateTests.test_map_view_iterate ________________
2020-11-07T22:50:57.4181301Z 
2020-11-07T22:50:57.4181965Z self = <pyflink.table.tests.test_aggregate.StreamTableAggregateTests testMethod=test_map_view_iterate>
2020-11-07T22:50:57.4182348Z 
2020-11-07T22:50:57.4182535Z     def test_map_view_iterate(self):
2020-11-07T22:50:57.4182812Z         test_iterate = udaf(TestIterateAggregateFunction())
2020-11-07T22:50:57.4183320Z         self.t_env.get_config().set_idle_state_retention(datetime.timedelta(days=1))
2020-11-07T22:50:57.4183763Z         self.t_env.get_config().get_configuration().set_string(
2020-11-07T22:50:57.4297555Z             ""python.fn-execution.bundle.size"", ""2"")
2020-11-07T22:50:57.4297922Z         # trigger the cache eviction in a bundle.
2020-11-07T22:50:57.4308028Z         self.t_env.get_config().get_configuration().set_string(
2020-11-07T22:50:57.4308653Z             ""python.state.cache-size"", ""2"")
2020-11-07T22:50:57.4308945Z         self.t_env.get_config().get_configuration().set_string(
2020-11-07T22:50:57.4309382Z             ""python.map-state.read-cache-size"", ""2"")
2020-11-07T22:50:57.4309676Z         self.t_env.get_config().get_configuration().set_string(
2020-11-07T22:50:57.4310428Z             ""python.map-state.write-cache-size"", ""2"")
2020-11-07T22:50:57.4310701Z         self.t_env.get_config().get_configuration().set_string(
2020-11-07T22:50:57.4311130Z             ""python.map-state.iterate-response-batch-size"", ""2"")
2020-11-07T22:50:57.4311361Z         t = self.t_env.from_elements(
2020-11-07T22:50:57.4311691Z             [(1, 'Hi_', 'hi'),
2020-11-07T22:50:57.4312004Z              (1, 'Hi', 'hi'),
2020-11-07T22:50:57.4312316Z              (2, 'hello', 'hello'),
2020-11-07T22:50:57.4312639Z              (3, 'Hi_', 'hi'),
2020-11-07T22:50:57.4312975Z              (3, 'Hi', 'hi'),
2020-11-07T22:50:57.4313285Z              (4, 'hello', 'hello'),
2020-11-07T22:50:57.4313609Z              (5, 'Hi2_', 'hi'),
2020-11-07T22:50:57.4313908Z              (5, 'Hi2', 'hi'),
2020-11-07T22:50:57.4314238Z              (6, 'hello2', 'hello'),
2020-11-07T22:50:57.4314558Z              (7, 'Hi', 'hi'),
2020-11-07T22:50:57.4315053Z              (8, 'hello', 'hello'),
2020-11-07T22:50:57.4315396Z              (9, 'Hi2', 'hi'),
2020-11-07T22:50:57.4315773Z              (13, 'Hi3', 'hi')], ['a', 'b', 'c'])
2020-11-07T22:50:57.4316023Z         self.t_env.create_temporary_view(""source"", t)
2020-11-07T22:50:57.4316299Z         table_with_retract_message = self.t_env.sql_query(
2020-11-07T22:50:57.4316615Z             ""select LAST_VALUE(b) as b, LAST_VALUE(c) as c from source group by a"")
2020-11-07T22:50:57.4316919Z         result = table_with_retract_message.group_by(t.c) \
2020-11-07T22:50:57.4317197Z             .select(test_iterate(t.b).alias(""a""), t.c) \
2020-11-07T22:50:57.4317619Z             .select(col(""a"").get(0).alias(""a""),
2020-11-07T22:50:57.4318111Z                     col(""a"").get(1).alias(""b""),
2020-11-07T22:50:57.4318357Z                     col(""a"").get(2).alias(""c""),
2020-11-07T22:50:57.4318586Z                     col(""a"").get(3).alias(""d""),
2020-11-07T22:50:57.4318814Z                     t.c.alias(""e""))
2020-11-07T22:50:57.4319023Z         assert_frame_equal(
2020-11-07T22:50:57.4319208Z >           result.to_pandas(),
2020-11-07T22:50:57.4319408Z             pd.DataFrame([
2020-11-07T22:50:57.4319872Z                 [""hello,hello2"", ""1,3"", 'hello:3,hello2:1', 2, ""hello""],
2020-11-07T22:50:57.4320398Z                 [""Hi,Hi2,Hi3"", ""1,2,3"", ""Hi:3,Hi2:2,Hi3:1"", 3, ""hi""]],
2020-11-07T22:50:57.4321047Z                 columns=['a', 'b', 'c', 'd', 'e']))
2020-11-07T22:50:57.4321198Z 
2020-11-07T22:50:57.4321385Z pyflink/table/tests/test_aggregate.py:468: 
2020-11-07T22:50:57.4321648Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-11-07T22:50:57.4322040Z pyflink/table/table.py:807: in to_pandas
2020-11-07T22:50:57.4322299Z     .collectAsPandasDataFrame(self._j_table, max_arrow_batch_size)
2020-11-07T22:50:57.4322794Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__
2020-11-07T22:50:57.4323103Z     answer, self.gateway_client, self.target_id, self.name)
2020-11-07T22:50:57.4323351Z pyflink/util/exceptions.py:147: in deco
2020-11-07T22:50:57.4323537Z     return f(*a, **kw)
2020-11-07T22:50:57.4323783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-11-07T22:50:57.4323963Z 
2020-11-07T22:50:57.4324225Z answer = 'xro8653'
2020-11-07T22:50:57.4324496Z gateway_client = <py4j.java_gateway.GatewayClient object at 0x7fe5c619db70>
2020-11-07T22:50:57.4324943Z target_id = 'z:org.apache.flink.table.runtime.arrow.ArrowUtils'
2020-11-07T22:50:57.4325312Z name = 'collectAsPandasDataFrame'
2020-11-07T22:50:57.4325439Z 
2020-11-07T22:50:57.4325839Z     def get_return_value(answer, gateway_client, target_id=None, name=None):
2020-11-07T22:50:57.4326420Z         """"""Converts an answer received from the Java gateway into a Python object.
2020-11-07T22:50:57.4326648Z     
2020-11-07T22:50:57.4326881Z         For example, string representation of integers are converted to Python
2020-11-07T22:50:57.4327193Z         integer, string representation of objects are converted to JavaObject
2020-11-07T22:50:57.4327451Z         instances, etc.
2020-11-07T22:50:57.4327614Z     
2020-11-07T22:50:57.4327819Z         :param answer: the string returned by the Java gateway
2020-11-07T22:50:57.4328157Z         :param gateway_client: the gateway client used to communicate with the Java
2020-11-07T22:50:57.4329738Z             Gateway. Only necessary if the answer is a reference (e.g., object,
2020-11-07T22:50:57.4330018Z             list, map)
2020-11-07T22:50:57.4330273Z         :param target_id: the name of the object from which the answer comes from
2020-11-07T22:50:57.4330588Z             (e.g., *object1* in `object1.hello()`). Optional.
2020-11-07T22:50:57.4330873Z         :param name: the name of the member from which the answer comes from
2020-11-07T22:50:57.4331170Z             (e.g., *hello* in `object1.hello()`). Optional.
2020-11-07T22:50:57.4331375Z         """"""
2020-11-07T22:50:57.4331542Z         if is_error(answer)[0]:
2020-11-07T22:50:57.4331761Z             if len(answer) > 1:
2020-11-07T22:50:57.4331954Z                 type = answer[1]
2020-11-07T22:50:57.4332222Z                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2020-11-07T22:50:57.4332531Z                 if answer[1] == REFERENCE_TYPE:
2020-11-07T22:50:57.4332757Z                     raise Py4JJavaError(
2020-11-07T22:50:57.4333016Z                         ""An error occurred while calling {0}{1}{2}.\n"".
2020-11-07T22:50:57.4333303Z >                       format(target_id, ""."", name), value)
2020-11-07T22:50:57.4333700Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
2020-11-07T22:50:57.4334558Z E                   : java.lang.RuntimeException: Could not remove element ',,,1,hi', should never happen.
2020-11-07T22:50:57.4335019Z E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.filterOutRetractRows(ArrowUtils.java:708)
2020-11-07T22:50:57.4335479Z E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:635)
2020-11-07T22:50:57.4336238Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-07T22:50:57.4336645Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-07T22:50:57.4337099Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-07T22:50:57.4337485Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-07T22:50:57.4337911Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-11-07T22:50:57.4338410Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-11-07T22:50:57.4338859Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-11-07T22:50:57.4339324Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-11-07T22:50:57.4339810Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-11-07T22:50:57.4340260Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-11-07T22:50:57.4340651Z E                   	at java.lang.Thread.run(Thread.java:748)
{code}",,dian.fu,rmetzger,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 11:58:33 UTC 2020,,,,,,,,,,"0|z0kdfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/20 01:51;dian.fu;cc [~zhongwei];;;","10/Nov/20 02:23;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3;;;","10/Nov/20 02:26;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","11/Nov/20 01:53;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9425&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e;;;","11/Nov/20 03:05;zhongwei;[~dian.fu] Thanks for your notification, I'll take a look.;;;","11/Nov/20 04:34;dian.fu;[~zhongwei] Thanks a lot~;;;","11/Nov/20 06:56;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8661&view=logs&j=584fa981-f71a-5840-1c49-f800c954fe4b&t=532bf1f8-8c75-59c3-eaad-8c773769bc3a;;;","12/Nov/20 11:58;dian.fu;Fixed in master(1.12.0) via b9b9ff3e88b1de42895229099ec149473de1a055;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ZooKeeperLeaderElectionTest.testZooKeeperLeaderElectionRetrieval failed with ""TimeoutException: Contender was not elected as the leader within 200000ms""",FLINK-20045,13339353,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,dian.fu,dian.fu,08/Nov/20 01:20,10/Nov/20 10:31,13/Jul/23 08:12,10/Nov/20 10:31,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9251&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
2020-11-07T10:34:07.5063203Z [ERROR] testZooKeeperLeaderElectionRetrieval(org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest)  Time elapsed: 202.445 s  <<< ERROR!
2020-11-07T10:34:07.5064331Z java.util.concurrent.TimeoutException: Contender was not elected as the leader within 200000ms
2020-11-07T10:34:07.5064946Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:153)
2020-11-07T10:34:07.5065762Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:139)
2020-11-07T10:34:07.5066565Z 	at org.apache.flink.runtime.leaderelection.TestingLeaderBase.waitForLeader(TestingLeaderBase.java:48)
2020-11-07T10:34:07.5067185Z 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperLeaderElectionRetrieval(ZooKeeperLeaderElectionTest.java:144)
{code}",,dian.fu,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 10:31:18 UTC 2020,,,,,,,,,,"0|z0kdf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/20 06:55;wangyang0918;I am having a look on this failed test.;;;","09/Nov/20 03:09;wangyang0918;I think the root cause is the leader was granted too fast and the {{TestingLeaderElectionEventHandler#init()}} has not been called. So we could find the following exception in the maven log. This could not happen in the production code since we have a {{lock}} in {{DefaultLeaderElectionService}}.

 

How to fix the unstable tests?

I suggest to add a ""wait-with-timeout"" in the {{TestingLeaderElectionEventHandler#onGrantLeadership, #onRevokeLeadership, #onLeaderInformationChange}} so that we have enough time for creating {{LeaderElectionDriver}} and then {{init}} the {{TestingLeaderElectionEventHandler}}.

 
{code:java}
10:30:37,419 [Curator-LeaderLatch-0] WARN  org.apache.flink.shaded.curator4.org.apache.curator.utils.ZKPaths [] - The version of ZooKeeper being used doesn't support Container nodes. CreateMode.PERSISTENT will be used instead.10:30:37,419 [Curator-LeaderLatch-0] WARN  org.apache.flink.shaded.curator4.org.apache.curator.utils.ZKPaths [] - The version of ZooKeeper being used doesn't support Container nodes. CreateMode.PERSISTENT will be used instead.10:30:37,468 [    main-EventThread] ERROR org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer [] - Listener (ZooKeeperLeaderElectionDriver{leaderPath='/leader'}) threw an exceptionorg.apache.flink.util.FlinkRuntimeException: init() should be called first. at org.apache.flink.runtime.leaderelection.TestingLeaderElectionEventHandler.onGrantLeadership(TestingLeaderElectionEventHandler.java:46) ~[test-classes/:?] at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:158) ~[classes/:?] at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:693) ~[flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:689) ~[flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:688) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:567) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:618) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:883) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:653) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:601) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0] at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508) [flink-shaded-zookeeper-3-3.4.14-12.0.jar:3.4.14-12.0]10:33:58,379 [                main] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Closing ZooKeeperLeaderElectionDriver{leaderPath='/leader'}10:33:58,383 [                main] INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Closing ZookeeperLeaderRetrievalDriver{retrievalPath='/leader'}.10:33:58,385 [ Curator-Framework-0] INFO  org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - backgroundOperationsLoop exiting10:33:58,388 [                main] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Session: 0x102de7dc7fc0000 closed10:33:58,389 [    main-EventThread] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - EventThread shut down for session: 0x102de7dc7fc000010:33:58,392 [                main] ERROR org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest [] - 
{code};;;","09/Nov/20 09:27;trohrmann;I think introducing sleeps is not a reliable way of fixing this problem.;;;","09/Nov/20 10:04;trohrmann;I think the underlying problem is that the {{TestingLeaderElectionEventHandler}} tries to do too much. Concretely, writing the leader information via the {{LeaderElectionDriver}}.

I see two ways to solve the problem. Either, we use the {{DefaultLeaderElectionService}} whose task it is to write the leader information back or we introduce a {{start}} method which we can use to start the {{LeaderElectionDriver}}. That way, could initialize the {{TestingLeaderElectionEventHandler}} before starting the driver.;;;","09/Nov/20 12:09;wangyang0918;I agree with you that {{TestingLeaderElectionEventHandler}} could not be responsible for writing the leader information back to external storage(e.g. ZooKeeper, K8s ConfigMap). Then in some driver tests, we need to manually call the {{LeaderElectionDriver#writeLeaderInformation}} so that the {{LeaderRetrievalDriver}} could get the leader information.

 

Does it make sense to you? Or you still believe that we need to use the {{DefaultLeaderElectionService}} which could write back the leader information to external storage automatically.;;;","09/Nov/20 13:38;trohrmann;Maybe it is also fine to simply block the {{onGrantLeadership}} and {{revokeLeadership}} until {{init}} has been called.;;;","09/Nov/20 15:02;wangyang0918;Make sense to me.;;;","10/Nov/20 02:27;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9361&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6;;;","10/Nov/20 10:31;trohrmann;Fixed via 8a63e642911031d8cead43841d6af227dd54d1e8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink end-to-end test is unstable on Azure,FLINK-20039,13339272,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,jark,jark,07/Nov/20 03:53,09/Nov/20 08:44,13/Jul/23 08:12,09/Nov/20 08:43,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9211&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529


{code}
2020-11-06T19:56:33.5300892Z Nov 06 19:56:33 ==============================================================================
2020-11-06T19:56:33.5301454Z Nov 06 19:56:33 Running 'Streaming File Sink end-to-end test'
2020-11-06T19:56:33.5301877Z Nov 06 19:56:33 ==============================================================================
2020-11-06T19:56:33.5315153Z Nov 06 19:56:33 TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-33530977067
2020-11-06T19:56:33.6838741Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test-runner-common.sh: line 57: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_streaming_file_sink.sh: No such file or directory
2020-11-06T19:56:33.6839644Z Nov 06 19:56:33 [FAIL] Test script contains errors.
2020-11-06T19:56:33.6847199Z Nov 06 19:56:33 Checking of logs skipped.
2020-11-06T19:56:33.6847743Z Nov 06 19:56:33 
2020-11-06T19:56:33.6848433Z Nov 06 19:56:33 [FAIL] 'Streaming File Sink end-to-end test' failed after 0 minutes and 0 seconds! Test exited with exit code 1
2020-11-06T19:56:33.6848756Z Nov 06 19:56:33 
2020-11-06T19:56:33.6849046Z Nov 06 19:56:33 ##[group]Environment Information
2020-11-06T19:56:33.6849260Z Nov 06 19:56:33 Jps
2020-11-06T19:56:33.7825775Z Nov 06 19:56:33 77536 Jps
2020-11-06T19:56:33.7946109Z Nov 06 19:56:33 Disk information
2020-11-06T19:56:33.7958895Z Nov 06 19:56:33 Filesystem      Size  Used Avail Use% Mounted on
2020-11-06T19:56:33.7959315Z Nov 06 19:56:33 udev            3.7G     0  3.7G   0% /dev
2020-11-06T19:56:33.7959726Z Nov 06 19:56:33 tmpfs           729M   18M  712M   3% /run
2020-11-06T19:56:33.7960079Z Nov 06 19:56:33 /dev/sda1        90G   63G   28G  70% /
2020-11-06T19:56:33.7960411Z Nov 06 19:56:33 tmpfs           3.7G  8.2k  3.7G   1% /dev/shm
2020-11-06T19:56:33.7960765Z Nov 06 19:56:33 tmpfs           5.3M     0  5.3M   0% /run/lock
2020-11-06T19:56:33.7961111Z Nov 06 19:56:33 tmpfs           3.7G     0  3.7G   0% /sys/fs/cgroup
2020-11-06T19:56:33.7961479Z Nov 06 19:56:33 /dev/sda15      110M  3.8M  106M   4% /boot/efi
2020-11-06T19:56:33.7961811Z Nov 06 19:56:33 /dev/sdb1        15G  4.4G  9.6G  32% /mnt
2020-11-06T19:56:33.7962124Z Nov 06 19:56:33 Allocated ports
2020-11-06T19:56:33.8060627Z Nov 06 19:56:33 Active Internet connections (only servers)
2020-11-06T19:56:33.8061440Z Nov 06 19:56:33 Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
2020-11-06T19:56:33.8062056Z Nov 06 19:56:33 tcp        0      0 127.0.0.1:42609         0.0.0.0:*               LISTEN      1574/containerd 
2020-11-06T19:56:33.8062589Z Nov 06 19:56:33 tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1777/sshd       
2020-11-06T19:56:33.8063102Z Nov 06 19:56:33 tcp6       0      0 :::22                   :::*                    LISTEN      1777/sshd       
2020-11-06T19:56:33.8063617Z Nov 06 19:56:33 udp        0      0 0.0.0.0:68              0.0.0.0:*                           1099/dhclient   
2020-11-06T19:56:33.8069826Z Nov 06 19:56:33 Running docker containers
2020-11-06T19:56:33.8449153Z Nov 06 19:56:33 CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS                         PORTS               NAMES
2020-11-06T19:56:33.8451289Z Nov 06 19:56:33 7360a044662a        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_flink-native-k8s-pyflink-application-1-taskmanager-1-1_default_b9fe79cc-84ff-4bae-95a0-6c105d8c6c61_0
2020-11-06T19:56:33.8452835Z Nov 06 19:56:33 691869b9d84e        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_flink-native-k8s-pyflink-application-1-86985776d9-hc7gb_default_446bf0cc-f01c-4b50-97c6-f27ff3ccfc76_0
2020-11-06T19:56:33.8453885Z Nov 06 19:56:33 9f06de0387f8        70f311871ae1           ""/coredns -conf /etcâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_coredns_coredns-6955765f44-zxft7_kube-system_d7bdc27c-2e87-4949-9574-57b80e3b63cc_5
2020-11-06T19:56:33.8454878Z Nov 06 19:56:33 95d3c8c15deb        70f311871ae1           ""/coredns -conf /etcâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_coredns_coredns-6955765f44-c6wlp_kube-system_f7827ddc-2d88-4418-ba7f-aee6802f6528_5
2020-11-06T19:56:33.8455846Z Nov 06 19:56:33 b0e62a8e0c90        ae853e93800d           ""/usr/local/bin/kubeâ€¦""   About an hour ago   Exited (2) About an hour ago                       k8s_kube-proxy_kube-proxy-t9q7s_kube-system_c2dc345d-1e68-4470-8f16-879f3fbf5283_5
2020-11-06T19:56:33.8456802Z Nov 06 19:56:33 1a7d4a61653e        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-proxy-t9q7s_kube-system_c2dc345d-1e68-4470-8f16-879f3fbf5283_5
2020-11-06T19:56:33.8457759Z Nov 06 19:56:33 5295ad0ee4f4        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_coredns-6955765f44-zxft7_kube-system_d7bdc27c-2e87-4949-9574-57b80e3b63cc_5
2020-11-06T19:56:33.8458715Z Nov 06 19:56:33 c7776fef7357        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_coredns-6955765f44-c6wlp_kube-system_f7827ddc-2d88-4418-ba7f-aee6802f6528_5
2020-11-06T19:56:33.8459678Z Nov 06 19:56:33 5355d9553f26        4689081edb10           ""/storage-provisioner""   About an hour ago   Exited (2) About an hour ago                       k8s_storage-provisioner_storage-provisioner_kube-system_837ab635-4282-4752-a4fa-7901524b86b3_5
2020-11-06T19:56:33.8460651Z Nov 06 19:56:33 b907e7ba51ef        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_storage-provisioner_kube-system_837ab635-4282-4752-a4fa-7901524b86b3_5
2020-11-06T19:56:33.8461628Z Nov 06 19:56:33 a7031f3fd43e        d109c0821a2b           ""kube-scheduler --auâ€¦""   About an hour ago   Exited (2) About an hour ago                       k8s_kube-scheduler_kube-scheduler-fv-az592-353_kube-system_e3025acd90e7465e66fa19c71b916366_5
2020-11-06T19:56:33.8462600Z Nov 06 19:56:33 a18a08c1d08c        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-scheduler-fv-az592-353_kube-system_e3025acd90e7465e66fa19c71b916366_5
2020-11-06T19:56:33.8463569Z Nov 06 19:56:33 8be8abb14306        90d27391b780           ""kube-apiserver --adâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_kube-apiserver_kube-apiserver-fv-az592-353_kube-system_c0e3f2140f3b15f51d2ef9eb87fb4a1a_5
2020-11-06T19:56:33.8464613Z Nov 06 19:56:33 d4e83c2d976e        b0f1517c1f4b           ""kube-controller-manâ€¦""   About an hour ago   Exited (2) About an hour ago                       k8s_kube-controller-manager_kube-controller-manager-fv-az592-353_kube-system_9a4c117280e4dc00b50f14fa58ce4d1f_5
2020-11-06T19:56:33.8465592Z Nov 06 19:56:33 dacb94c2a315        303ce5db0e90           ""etcd --advertise-clâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_etcd_etcd-fv-az592-353_kube-system_d459940a27038deee23ff9b7f8ec5cc7_5
2020-11-06T19:56:33.8466561Z Nov 06 19:56:33 d730fc56d8cf        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-controller-manager-fv-az592-353_kube-system_9a4c117280e4dc00b50f14fa58ce4d1f_5
2020-11-06T19:56:33.8467681Z Nov 06 19:56:33 57c53acd6339        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-apiserver-fv-az592-353_kube-system_c0e3f2140f3b15f51d2ef9eb87fb4a1a_5
2020-11-06T19:56:33.8468624Z Nov 06 19:56:33 6252fcbaa447        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_etcd-fv-az592-353_kube-system_d459940a27038deee23ff9b7f8ec5cc7_5
2020-11-06T19:56:33.8469560Z Nov 06 19:56:33 124869bbc5c7        70f311871ae1           ""/coredns -conf /etcâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_coredns_coredns-6955765f44-zxft7_kube-system_d7bdc27c-2e87-4949-9574-57b80e3b63cc_4
2020-11-06T19:56:33.8470535Z Nov 06 19:56:33 0e1b496a027d        70f311871ae1           ""/coredns -conf /etcâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_coredns_coredns-6955765f44-c6wlp_kube-system_f7827ddc-2d88-4418-ba7f-aee6802f6528_4
2020-11-06T19:56:33.8471511Z Nov 06 19:56:33 4f6bdd81ace2        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_coredns-6955765f44-zxft7_kube-system_d7bdc27c-2e87-4949-9574-57b80e3b63cc_4
2020-11-06T19:56:33.8472489Z Nov 06 19:56:33 660581e63ba9        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_coredns-6955765f44-c6wlp_kube-system_f7827ddc-2d88-4418-ba7f-aee6802f6528_4
2020-11-06T19:56:33.8473461Z Nov 06 19:56:33 18f380410438        ae853e93800d           ""/usr/local/bin/kubeâ€¦""   About an hour ago   Exited (2) About an hour ago                       k8s_kube-proxy_kube-proxy-t9q7s_kube-system_c2dc345d-1e68-4470-8f16-879f3fbf5283_4
2020-11-06T19:56:33.8474402Z Nov 06 19:56:33 abebbf1b58d7        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-proxy-t9q7s_kube-system_c2dc345d-1e68-4470-8f16-879f3fbf5283_4
2020-11-06T19:56:33.8475366Z Nov 06 19:56:33 00d1e1d5493f        4689081edb10           ""/storage-provisioner""   About an hour ago   Exited (2) About an hour ago                       k8s_storage-provisioner_storage-provisioner_kube-system_837ab635-4282-4752-a4fa-7901524b86b3_4
2020-11-06T19:56:33.8476328Z Nov 06 19:56:33 f8daf7dc979b        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_storage-provisioner_kube-system_837ab635-4282-4752-a4fa-7901524b86b3_4
2020-11-06T19:56:33.8477263Z Nov 06 19:56:33 abcd2edb4c51        303ce5db0e90           ""etcd --advertise-clâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_etcd_etcd-fv-az592-353_kube-system_d459940a27038deee23ff9b7f8ec5cc7_4
2020-11-06T19:56:33.8478243Z Nov 06 19:56:33 4c7d2d45eb3f        b0f1517c1f4b           ""kube-controller-manâ€¦""   About an hour ago   Exited (2) About an hour ago                       k8s_kube-controller-manager_kube-controller-manager-fv-az592-353_kube-system_9a4c117280e4dc00b50f14fa58ce4d1f_4
2020-11-06T19:56:33.8479267Z Nov 06 19:56:33 ac261a730a51        d109c0821a2b           ""kube-scheduler --auâ€¦""   About an hour ago   Exited (2) About an hour ago                       k8s_kube-scheduler_kube-scheduler-fv-az592-353_kube-system_e3025acd90e7465e66fa19c71b916366_4
2020-11-06T19:56:33.8480265Z Nov 06 19:56:33 fa6769418d3a        90d27391b780           ""kube-apiserver --adâ€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_kube-apiserver_kube-apiserver-fv-az592-353_kube-system_c0e3f2140f3b15f51d2ef9eb87fb4a1a_4
2020-11-06T19:56:33.8481196Z Nov 06 19:56:33 882b5b6d1759        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_etcd-fv-az592-353_kube-system_d459940a27038deee23ff9b7f8ec5cc7_4
2020-11-06T19:56:33.8482299Z Nov 06 19:56:33 bda98df17be9        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-scheduler-fv-az592-353_kube-system_e3025acd90e7465e66fa19c71b916366_4
2020-11-06T19:56:33.8504731Z Nov 06 19:56:33 cbe4ff12165e        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-controller-manager-fv-az592-353_kube-system_9a4c117280e4dc00b50f14fa58ce4d1f_4
2020-11-06T19:56:33.8505745Z Nov 06 19:56:33 5889ab63349d        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_kube-apiserver-fv-az592-353_kube-system_c0e3f2140f3b15f51d2ef9eb87fb4a1a_4
2020-11-06T19:56:33.8506740Z Nov 06 19:56:33 34b21e724ea2        72b20d9ad76a           ""/docker-entrypoint.â€¦""   About an hour ago   Exited (0) About an hour ago                       k8s_flink-job-cluster_flink-job-cluster-2m9zk_default_5f26e4d8-5d20-44b0-b748-b89e1ba7e96b_0
2020-11-06T19:56:33.8507723Z Nov 06 19:56:33 9ff3a8f4996c        k8s.gcr.io/pause:3.1   ""/pause""                 About an hour ago   Exited (0) About an hour ago                       k8s_POD_flink-job-cluster-2m9zk_default_5f26e4d8-5d20-44b0-b748-b89e1ba7e96b_0
2020-11-06T19:56:33.8508147Z Nov 06 19:56:33 ##[endgroup]
2020-11-06T19:56:33.8539998Z cp: cannot stat '/home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/*': No such file or directory
2020-11-06T19:56:33.8544748Z Nov 06 19:56:33 Published e2e logs into debug logs artifact:
2020-11-06T19:56:33.8572467Z Nov 06 19:56:33 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
2020-11-06T19:56:39.4150274Z Nov 06 19:56:39 COMPRESSING build artifacts.
2020-11-06T19:56:39.4180878Z Nov 06 19:56:39 ./
2020-11-06T19:56:39.4183742Z Nov 06 19:56:39 ./e2e-flink-logs/
2020-11-06T19:56:39.4185971Z Nov 06 19:56:39 ./dmesg.out
2020-11-06T19:56:39.6330306Z Nov 06 19:56:39 No taskexecutor daemon to stop on host fv-az592-353.
2020-11-06T19:56:39.7702995Z Nov 06 19:56:39 No standalonesession daemon to stop on host fv-az592-353.
2020-11-06T19:57:03.0771182Z The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
2020-11-06T19:57:03.0858228Z ##[error]Bash exited with code '1'.
2020-11-06T19:57:08.0909170Z ##[section]Finishing: Run e2e tests
{code}
",,gaoyunhaii,jark,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 08:44:59 UTC 2020,,,,,,,,,,"0|z0kcx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/20 15:16;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9213&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","07/Nov/20 15:21;gaoyunhaii;Very sorry for the inconvenience brought :( The original PR causing the issue has been reverted on the main branch, could you rebase the latest master and try again ?;;;","09/Nov/20 08:40;trohrmann;[~gaoyunhaii] can we close this ticket then if the causing problem has been reverted?;;;","09/Nov/20 08:42;gaoyunhaii;Yes, the issue has been fixed now.;;;","09/Nov/20 08:43;gaoyunhaii;I'll then close it. ;;;","09/Nov/20 08:44;gaoyunhaii;Close this issue, the original bugful PR is reverted and new PR merged as a6bea224ed012e5594ee755526f54ae7f3b0d22f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BlockingShuffleITCase unstable with ""Could not start rest endpoint on any port in port range 8081""",FLINK-20035,13339229,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,rmetzger,rmetzger,06/Nov/20 19:08,24/Nov/20 06:45,13/Jul/23 08:12,24/Nov/20 06:45,1.12.0,,,,,1.12.0,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9178&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0
{code}
2020-11-06T13:52:56.6369221Z [ERROR] testBoundedBlockingShuffle(org.apache.flink.test.runtime.BlockingShuffleITCase)  Time elapsed: 3.522 s  <<< ERROR!
2020-11-06T13:52:56.6370005Z org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
2020-11-06T13:52:56.6370649Z 	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:257)
2020-11-06T13:52:56.6371371Z 	at org.apache.flink.runtime.minicluster.MiniCluster.createDispatcherResourceManagerComponents(MiniCluster.java:412)
2020-11-06T13:52:56.6372258Z 	at org.apache.flink.runtime.minicluster.MiniCluster.setupDispatcherResourceManagerComponents(MiniCluster.java:378)
2020-11-06T13:52:56.6373276Z 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:334)
2020-11-06T13:52:56.6374182Z 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:50)
2020-11-06T13:52:56.6375055Z 	at org.apache.flink.test.runtime.BlockingShuffleITCase.testBoundedBlockingShuffle(BlockingShuffleITCase.java:53)
2020-11-06T13:52:56.6375787Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-06T13:52:56.6376546Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-06T13:52:56.6377514Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-06T13:52:56.6378008Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-06T13:52:56.6378774Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-11-06T13:52:56.6379350Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-11-06T13:52:56.6458094Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-11-06T13:52:56.6459047Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-11-06T13:52:56.6459678Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-11-06T13:52:56.6460182Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-11-06T13:52:56.6460770Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-11-06T13:52:56.6461210Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-06T13:52:56.6461649Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-06T13:52:56.6462089Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-06T13:52:56.6462736Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-06T13:52:56.6463286Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-06T13:52:56.6463728Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-06T13:52:56.6464344Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-11-06T13:52:56.6464918Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-11-06T13:52:56.6465428Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-11-06T13:52:56.6465915Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-11-06T13:52:56.6466405Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-11-06T13:52:56.6467050Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-11-06T13:52:56.6468341Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-11-06T13:52:56.6468794Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-11-06T13:52:56.6469256Z Caused by: java.net.BindException: Could not start rest endpoint on any port in port range 8081
2020-11-06T13:52:56.6469728Z 	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:222)
2020-11-06T13:52:56.6470336Z 	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:162)
2020-11-06T13:52:56.6470824Z 	... 30 more
2020-11-06T13:52:56.6471033Z 
{code}",,dian.fu,kevin.cyj,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 06:45:05 UTC 2020,,,,,,,,,,"0|z0kcnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/20 05:23;kevin.cyj;This failure is caused by port bind exception. I have submitted a PR to use random port for BlockingShuffleITCase and ShuffleCompressionITCase.;;;","09/Nov/20 05:50;rmetzger;Thanks a lot for looking into it. I assigned you to the ticket.;;;","11/Nov/20 01:58;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9425&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc;;;","11/Nov/20 07:32;rmetzger;Fixed in https://github.com/apache/flink/commit/0c36c842c3cb0c3aa75e7e94c56dce217ee548de.;;;","23/Nov/20 02:41;kevin.cyj;[~rmetzger] I made a mistake in the previous fix and I opened a new fix for it. Sorry for that, could you please take a look? Here is the PR: https://github.com/apache/flink/pull/14149.;;;","24/Nov/20 04:05;kevin.cyj;[~rmetzger] I reopened this ticket.;;;","24/Nov/20 06:45;rmetzger;No problem!
Resolved in https://github.com/apache/flink/commit/3295879b69f6aababef484983438ffd61b7d6b13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job fails when stopping JobMaster,FLINK-20033,13339202,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,06/Nov/20 15:32,12/Nov/20 13:56,13/Jul/23 08:12,12/Nov/20 13:56,1.10.2,1.11.2,,,,1.10.3,1.11.3,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"When a {{JobMaster}} is stopped, we first disconnect all {{TaskExecutors}}. This disconnection causes potentially running {{Executions}} to fail. This in turn can cause a restart of the job or in the worst case a transition into {{FAILED}} state if the restarts are depleted. This again can cause the clean up of HA data.

Instead of failing the job, the job should be suspended if the {{JobMaster}} gets stopped because this happens if the {{Dispatcher}} loses its leadership. The problem has been fixed unintentionally by FLINK-19237 in the master branch.",,alpinegizmo,kezhuw,stevenz3wu,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20065,,FLINK-19816,FLINK-19237,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 13:56:29 UTC 2020,,,,,,,,,,"0|z0kchk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/20 16:49;trohrmann;FLINK-19237 also fixes this problem.;;;","07/Nov/20 17:09;trohrmann;Backport PRs are:

1.11: https://github.com/apache/flink/pull/13979
1.10: https://github.com/apache/flink/pull/13980;;;","09/Nov/20 09:11;trohrmann;Fixed via

1.12.0:
03047a78b6ad8017eb240548a009424d68ffc971
86a96c7909a1c1db35157ebec4164529c0290c7d

1.11.3:
90fb07c311aa6c91ad544705ee96a14ab9302122
32e23052af547bb1b99274778c666f3d814201a8

1.10.3:
9840907ec8c16d4d50d92388b883b959cc10f793
10bfd2971dd960ac799ba02156683fc7a740383d;;;","12/Nov/20 13:56;trohrmann;Reverted the test and the change which makes the {{JobMaster.jobStatusChanged}} call synchronous because of FLINK-20065. The actual fix introduced with FLINK-19237 has not been reverted.

1.12.0:
dea47d6b17fdd26920c1bfbb5aea24a7003a16be
89262012a86647ebc3658027e5690a242b2f0782

1.11.3:
dcbf474343a43370ae08665e71dad708c65369ca
68d16f0f04b7ddf0ed5f7aadb14092c101a811f5

1.10.3:
d8fa676412a85d5e369e6367978ccfa8a7e07194
fef13fda361e6dfc566553316fcc8ca4943af787;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Barrier announcement causes outdated RemoteInputChannel#numBuffersOvertaken,FLINK-20030,13339178,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,arvid,AHeise,06/Nov/20 13:38,22/Jun/21 14:06,13/Jul/23 08:12,20/Nov/20 07:15,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,,,,,"{{numBuffersOvertaken}} is set when the announcement is enqueued, but it can take quite a while until the checkpoint is actually started with quite a non-priority buffers being drained.

We should move away from {{numBuffersOvertaken}} and use sequence numbers.",,AHeise,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 07:15:34 UTC 2020,,,,,,,,,,"0|z0kcc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/20 07:15;arvid;Merged into master as a97fe1f3f24993ff8cce2dd008281f162022fa27.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileCompactionITCase is unstable,FLINK-20028,13339169,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,sewen,sewen,06/Nov/20 12:43,22/Dec/20 02:34,13/Jul/23 08:12,22/Dec/20 02:34,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"The {{ParquetFileCompactionITCase}} hangs and times out.

Log: 
https://dev.azure.com/sewen0794/Flink/_build/results?buildId=178&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa

Exception:
{code}
org.junit.runners.model.TestTimedOutException: test timed out after 60 seconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:119)
	at org.apache.flink.table.pi.internal.TableResultImpl.await(TableResultImpl.java:86)
	at org.apache.flink.table.planner.runtime.stream.sql.FileCompactionITCaseBase.testNonPartition(FileCompactionITCaseBase.java:91)

{code}",,aljoscha,dian.fu,hxbks2ks,leonard,lzljs3620320,mapohl,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20176,,,,,,,,,,,,,FLINK-20703,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 02:31:22 UTC 2020,,,,,,,,,,"0|z0kca8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 12:45;rmetzger;[~lzljs3620320] Can you take a look?;;;","06/Nov/20 14:29;lzljs3620320;Thanks [~sewen] for reporting, and thanks [~rmetzger] for reminding, I'll take a look tomorrow~;;;","07/Nov/20 04:30;lzljs3620320;Looks like hang in the source, let's keep looking.;;;","11/Nov/20 03:35;dian.fu;[~sewen] [~lzljs3620320] Do you mean it makes sense to downgrade this issue to ""CRITICAL""?;;;","15/Nov/20 08:42;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9586&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9;;;","17/Nov/20 08:43;mapohl;FLINK-20176 was created but actually seems to cover the same problem. 

The test {{CompactionITCaseBase.testNonPartition}} failed with {{TestTimeOut}} [in this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=108&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=777327ab-6d4e-582e-3e76-4a9391c57e59&s=ab6e269b-88b2-5ded-2544-4aa5b1124530].;;;","17/Nov/20 09:45;lzljs3620320;I think I found the reason, a {{ParallelFiniteTestSource}} task wait for next checkpoint, but maybe some source tasks have ended, so there will never be another checkpoint.

I will create a PR to fix soon.;;;","17/Nov/20 10:49;rmetzger;Thanks a lot!;;;","18/Nov/20 02:06;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9719&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c;;;","18/Nov/20 08:15;lzljs3620320;master (1.12): 2465b7150301a114c36ff07b098e78c744f9bf62

Feel free to re-open this if there is a recurrence.;;;","21/Dec/20 11:18;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11096&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf]

I'm reopening it since there is a recurrence;;;","22/Dec/20 02:09;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11136&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","22/Dec/20 02:31;lzljs3620320;Hi [~hxbks2ks], It should not be the same reason, the previous problems have not been repeated for so long.

The previous phenomenon was that a test timeout occurred occasionally, but now the phenomenon is:
 * All failure are related to the hive tests.
 * All tests timeout when fail.

I'd like to create a new one to track it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testSlotRequestTimeoutWhenNoSlotOffering unstable,FLINK-20027,13339168,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,rmetzger,rmetzger,06/Nov/20 12:31,06/Nov/20 12:55,13/Jul/23 08:12,06/Nov/20 12:55,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,test-stability,,,,,"
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9162&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
[ERROR] Failures: 
[ERROR]   JobMasterTest.testSlotRequestTimeoutWhenNoSlotOffering:963 
Expected: a value equal to or greater than <10L>
     but: <3L> was less than <10L>
[INFO] 
{code}
",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19902,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 12:55:34 UTC 2020,,,,,,,,,,"0|z0kca0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 12:32;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9155&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","06/Nov/20 12:55;chesnay;A fixed version of FLINK-19902 has been pushed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pipeline.cached-files option cannot escape ':' in path,FLINK-20018,13339133,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,xuannan,xuannan,06/Nov/20 09:16,11/Nov/20 09:10,13/Jul/23 08:12,11/Nov/20 09:10,1.10.2,1.11.2,,,,1.10.3,1.11.3,1.12.0,Runtime / Configuration,,,,,0,pull-request-available,,,,,"pipeline.cached-files option cannot escape ':' in path
e.g. When setting it to `name:file1,path:oss://bucket/file1` The path of file1 is parsed as oss. And there are no way to escape the ':' in path.",,dwysakowicz,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 09:10:22 UTC 2020,,,,,,,,,,"0|z0kc28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/20 09:10;dwysakowicz;Fixed:
* master
** 5af1d007e26e0fb437028d5882d79fe09baf937a
* 1.11.3
** 0ec1c4d6ccf7c811b0328cf00e0833ae588aa112
* 1.10.3
** e3beef341bbe72d040a2fdabfc30350da56017af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message for join two windowed stream with time attributes,FLINK-20017,13339131,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,danny0405,danny0405,06/Nov/20 09:01,23/Nov/20 08:41,13/Jul/23 08:12,23/Nov/20 08:41,1.11.2,,,,,1.12.0,,,Table SQL / API,,,,,0,,,,,,"Reported from USER mailing list by Satyam ~
Current for table:

{code:sql}
CREATE TABLE T0 (
  amount BIGINT,
  ts TIMESTAMP(3),
  watermark for ts as ts - INTERVAL '5' SECOND
) WITH (
  'connector' = 'values',
  'data-id' = '$mDataId',
  'bounded' = 'false'
)
{code}

and query:
{code:sql}
WITH A AS (
  SELECT COUNT(*) AS ct, tumble_rowtime(ts, INTERVAL '1' MINUTE) as tm
    FROM T0 GROUP BY tumble(ts, INTERVAL '1' MINUTE))
select L.ct, R.ct, L.tm, R.tm from A as L left join A as R on L.tm = R.tm
{code}


throws stacktrace:

{code:noformat}
java.lang.RuntimeException: Error while applying rule StreamExecIntervalJoinRule(in:LOGICAL,out:STREAM_PHYSICAL), args [rel#320:FlinkLogicalJoin.LOGICAL.any.None: 0.[NONE].[NONE](left=RelSubset#315,right=RelSubset#315,condition==($1, $3),joinType=left)]

	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1065)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
	at org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.testXXX(TableSourceITCase.scala:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
Caused by: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIntervalJoin.<init>(StreamExecIntervalJoin.scala:72)
	at org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecIntervalJoinRule.convert(StreamExecIntervalJoinRule.scala:122)
	at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:167)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 56 more
{code}",,danny0405,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20015,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 08:41:34 UTC 2020,,,,,,,,,,"0|z0kc1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/20 08:41;danny0405;The test case below works ok in master code now, so it is actually fixed, i will just close this issue.


{code:java}
@Test
  def testStreamWindowJoin(): Unit = {
    // data set
    val dataA = List(
      rowOf(2L, LocalDateTime.parse(""1970-01-01T00:00:01"")),
      rowOf(3L, LocalDateTime.parse(""1970-01-01T00:00:04"")),
      rowOf(4L, LocalDateTime.parse(""1970-01-01T00:00:08"")),
      rowOf(5L, LocalDateTime.parse(""1970-01-01T00:00:16"")),
      rowOf(6L, LocalDateTime.parse(""1970-01-01T00:00:32"")))
    val dataID = TestValuesTableFactory.registerData(dataA)

    val createTable =
      s""""""
         |CREATE TABLE T0 (
         |  amount BIGINT,
         |  ts TIMESTAMP(3),
         |  watermark for ts as ts - INTERVAL '5' SECOND
         |) WITH (
         |  'connector' = 'values',
         |  'data-id' = '$dataID',
         |  'bounded' = 'false'
         |)
         |"""""".stripMargin
    tEnv.executeSql(createTable)
    val query =
      s""""""
         |WITH A AS (
         |  SELECT COUNT(*) AS ct, tumble_rowtime(ts, INTERVAL '1' MINUTE) as tm
         |    FROM T0 GROUP BY tumble(ts, INTERVAL '1' MINUTE))
         |select L.ct, R.ct, L.tm from A as L left join A as R on L.tm = R.tm
         |"""""".stripMargin
    val result = CollectionUtil.iteratorToList(tEnv.executeSql(query).collect())
    assertEquals(Seq(""abc""), result)
  }
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedBlockingSubpartition may leak network buffer if task is failed or canceled,FLINK-20013,13339108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,kevin.cyj,kevin.cyj,06/Nov/20 04:18,22/Jun/21 13:55,13/Jul/23 08:12,12/Nov/20 20:10,,,,,,1.10.3,1.11.3,1.12.0,Runtime / Network,,,,,0,pull-request-available,,,,,BoundedBlockingSubpartition may leak network buffer if task is failed or canceled. We need to recycle the current BufferConsumer when task is failed or canceled.,,AHeise,kevin.cyj,nicholasjiang,pnowojski,wind_ljy,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 20:10:33 UTC 2020,,,,,,,,,,"0|z0kbwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 08:16;nicholasjiang;[~zhuzh], I have already offline discussed with [~kevin.cyj]. Could you please assign this issue to me? I would like to work for the issue through closing the BufferConsumer.;;;","06/Nov/20 08:22;zhuzh;[~nicholasjiang] Sorry but I do not know much about this issue.
[~zjwang] [~pnowojski] would you take a look?;;;","06/Nov/20 09:20;nicholasjiang;[~zhuzh], with failed or canceled task, the current BufferConsumer in BoundedBlockingSubpartition isn't closed which causes network buffer leak. Therefore the current BufferConsumer should be closed when task is failed or canceled.
cc [~zjwang] [~pnowojski];;;","09/Nov/20 07:48;zhuzh;Hi [~nicholasjiang], I can see that [~pnowojski] has assigned this ticket to you.
Feel free to open a fix for it.;;;","10/Nov/20 19:16;arvid;Merged into master as c46ae271f11075a837bf5d8a66e2ffd2d73ffb28.;;;","10/Nov/20 19:17;arvid;Shouldn't that be backported to 1.11 (and 1.10)?;;;","11/Nov/20 02:02;nicholasjiang;[~AHeise], IMO, I think it should be backported to 1.11 (and 1.10). 
cc [~kevin.cyj];;;","12/Nov/20 07:51;arvid;Could you please create PRs for the backports, such that we can see with CI that no side-effects are happening?;;;","12/Nov/20 20:10;arvid;Merged into 1.11 as 6626d396acbe14bbec1a9cc2ff1539ff94119046.

Merged into 1.10 as 10f155e3458161b74da7bf45691247b50175da4b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkITCase.writerAndCommitterAndGlobalCommitterExecuteInStreamingMode fails on Azure Pipeline,FLINK-20010,13339100,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kevin.cyj,kevin.cyj,06/Nov/20 01:57,06/Nov/20 02:42,13/Jul/23 08:12,06/Nov/20 02:37,,,,,,1.12.0,,,Tests,,,,,0,,,,,,"SinkITCase.writerAndCommitterAndGlobalCommitterExecuteInStreamingMode fails on Azure Pipeline
{code:java}
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,kevin.cyj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 02:37:35 UTC 2020,,,,,,,,,,"0|z0kbuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 02:28;kevin.cyj;It seems the exception stack is incomplete, here is the log: [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/9065/logs/79];;;","06/Nov/20 02:37;kevin.cyj;It is confirmed by [~maguowei] that the issue has been fixed, so will close the Jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java Deadlock in ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement(),FLINK-20008,13339070,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,sewen,sewen,05/Nov/20 18:02,07/Nov/20 15:21,13/Jul/23 08:12,07/Nov/20 15:21,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"The stack trace detects a deadlock between the testing thread and the curator event thread.

Full log: https://dev.azure.com/sewen0794/Flink/_build/results?buildId=176&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374

Relevant Stack Trace:
{code}
Found one Java-level deadlock:
=============================
""main-EventThread"":
  waiting to lock monitor 0x00007f74c00045e8 (object 0x000000008ed14cb0, a java.lang.Object),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x00007f74e401a1f8 (object 0x000000008ed15008, a org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch),
  which is held by ""main-EventThread""

Java stack information for the threads listed above:
===================================================
""main-EventThread"":
	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:186)
	- waiting to lock <0x000000008ed14cb0> (a java.lang.Object)
	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:158)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:693)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:689)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
	at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:688)
	- locked <0x000000008ed15008> (a org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:567)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:618)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:883)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:653)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:601)
	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508)


""main"":
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:203)
	- waiting to lock <0x000000008ed15008> (a org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:190)
	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.close(ZooKeeperLeaderElectionDriver.java:140)
	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.stop(DefaultLeaderElectionService.java:103)
	- locked <0x000000008ed14cb0> (a java.lang.Object)
	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement(ZooKeeperLeaderElectionTest.java:310)

{code}
",,guoyangze,sewen,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 07 15:21:18 UTC 2020,,,,,,,,,,"0|z0kbo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 09:02;trohrmann;The problem is that {{curator.framework.leader.LeaderLatch}} calls {{ZooKeeperLeaderElectionDriver.isLeader --> DefaultLeaderElectionService.onGrantLeadership}} holding an internal lock {{a}}. {{ZooKeeperLeaderElectionDriver.isLeader --> DefaultLeaderElectionService.onGrantLeadership}} will try to acquire the lock {{b}} of {{DefaultLeaderElectionService}}. Therefore, if there is a concurrent close operation first acquiring lock {{b}} and then trying to close {{curator.framework.leader.LeaderLatch}} which requires lock {{a}}, we run into the deadlock.;;;","06/Nov/20 09:18;trohrmann;The solution I propose is to move {{leaderElectionDriver.close()}} out of the {{lock}} scope in {{DefaultLeaderElectionService.stop}} method.;;;","06/Nov/20 10:57;wangyang0918;[~trohrmann] 's right. I didn't notice that curator {{LeaderLatch}} also have a internal lock. Currently, we are closing the {{LeaderLatch}} in the another lock. This could cause dead lock. Moving {{DefaultLeaderElectionService#stop}} out of {{lock}} is reasonable.;;;","07/Nov/20 15:21;trohrmann;Fixed via

6dc5a13abfb0f44b5920bee4c4467a92bffe8eb9
980a2c906928d9124a176a286a34f768fb56ad8b
9d435f4506c8bf628b30606f442ce27b9bde8666;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FileSinkITCase.testFileSink: The record 0 should occur 4 times,  but only occurs 8time expected:<4> but was:<8>",FLINK-20006,13339042,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,rmetzger,rmetzger,05/Nov/20 15:44,09/Nov/20 11:27,13/Jul/23 08:12,09/Nov/20 11:27,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9082&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf
{code}
2020-11-05T13:31:16.7006473Z [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.565 s <<< FAILURE! - in org.apache.flink.connector.file.sink.FileSinkITCase
2020-11-05T13:31:16.7007237Z [ERROR] testFileSink[executionMode = STREAMING, triggerFailover = true](org.apache.flink.connector.file.sink.FileSinkITCase)  Time elapsed: 0.548 s  <<< FAILURE!
2020-11-05T13:31:16.7007897Z java.lang.AssertionError: The record 0 should occur 4 times,  but only occurs 8time expected:<4> but was:<8>
2020-11-05T13:31:16.7008317Z 	at org.junit.Assert.fail(Assert.java:88)
2020-11-05T13:31:16.7008644Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-11-05T13:31:16.7008987Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-11-05T13:31:16.7009392Z 	at org.apache.flink.connector.file.sink.FileSinkITCase.checkResult(FileSinkITCase.java:218)
2020-11-05T13:31:16.7009889Z 	at org.apache.flink.connector.file.sink.FileSinkITCase.testFileSink(FileSinkITCase.java:132)
2020-11-05T13:31:16.7010316Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}",,aljoscha,dian.fu,gaoyunhaii,godfreyhe,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 11:27:37 UTC 2020,,,,,,,,,,"0|z0kbi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 15:46;rmetzger;CC [~gaoyunhaii] [~aljoscha];;;","05/Nov/20 16:23;gaoyunhaii;Very sorry for the inconvenient brought, I'll have a look.;;;","06/Nov/20 02:53;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9082&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","06/Nov/20 03:01;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9102&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","06/Nov/20 03:02;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9121&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","06/Nov/20 11:33;godfreyhe;https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/9139/logs/95;;;","06/Nov/20 12:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9143&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","06/Nov/20 12:30;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9136&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","06/Nov/20 13:51;aljoscha;master: 19eb503079d9341fd0126284f22b1ac70e09d6c4;;;","08/Nov/20 01:56;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9279&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91

{code}
2020-11-07T22:18:57.6776989Z [ERROR] testFileSink[triggerFailover = false](org.apache.flink.connector.file.sink.StreamingExecutionFileSinkITCase)  Time elapsed: 2.773 s  <<< FAILURE!
2020-11-07T22:18:57.6777584Z java.lang.AssertionError: expected:<2500> but was:<0>
2020-11-07T22:18:57.6777929Z 	at org.junit.Assert.fail(Assert.java:88)
2020-11-07T22:18:57.6778280Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-11-07T22:18:57.6778647Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-11-07T22:18:57.6778997Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2020-11-07T22:18:57.6779449Z 	at org.apache.flink.connector.file.sink.FileSinkITBase.checkResult(FileSinkITBase.java:152)
2020-11-07T22:18:57.6780189Z 	at org.apache.flink.connector.file.sink.FileSinkITBase.testFileSink(FileSinkITBase.java:104)
{code};;;","08/Nov/20 01:58;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9279&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f;;;","09/Nov/20 02:44;gaoyunhaii;The latter issue should also get fixed now.;;;","09/Nov/20 11:27;aljoscha;Those last two runs that were posted didn't have the recently merged fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid Links in docs,FLINK-19998,13338990,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,vintageplayer,vintageplayer,vintageplayer,05/Nov/20 10:03,26/Nov/20 03:02,13/Jul/23 08:12,26/Nov/20 03:02,1.11.2,,,,,,,,Documentation,,,,,0,documentation,pull-request-available,,,,"Multiple broken links matching pattern: *site.baseurl }}{% link*

All these need to be replaced. Eg:

In the *docs/concepts/stateful-stream-processing.md* file, under the first section (What is State), the following two links are broken:
 # Checkpoints: *[checkpoints](\{{ site.baseurl}}\{% link dev/stream/state/checkpointing.md %})*
 # Savepoints: *[savepoints](\{{ site.baseurl }}\{%link ops/state/savepoints.md %})*

This results in the target link as follows:
 # For Checkpoints: [https://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/checkpointing.html]
 # [https://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/ops/state/savepoints.html]",,aljoscha,vintageplayer,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 03:01:55 UTC 2020,,,,,,,,,,"0|z0kb6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 10:05;vintageplayer;I wanted to submit a pull request removing *{\{ site.baseurl}}* from both the links, keeping it in lines with the link templates in the doc. ;;;","05/Nov/20 16:19;aljoscha;Please go ahead with a PR!;;;","26/Nov/20 03:01;yunta;merged in master c008907d2a629449c8d0ad9725d13b0604fc2141;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All vertices in an DataSet iteration job will be eagerly scheduled,FLINK-19994,13338973,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,zhuzh,zhuzh,05/Nov/20 09:29,10/Nov/20 19:09,13/Jul/23 08:12,10/Nov/20 19:09,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"After switching to pipelined region scheduling, all vertices in an DataSet iteration job will be eagerly scheduled, which means BLOCKING result consumers can be deployed even before the result finishes and resource waste happens. This is because all vertices will be put into one pipelined region if the job contains {{ColocationConstraint}}, see [PipelinedRegionComputeUtil|https://github.com/apache/flink/blob/c0f382f5f0072441ef8933f6993f1c34168004d6/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/failover/flip1/PipelinedRegionComputeUtil.java#L52].

IIUC, this {{makeAllOneRegion()}} behavior was introduced to ensure co-located iteration head and tail to be restarted together in pipelined region failover. However, given that edges within an iteration will always be PIPELINED ([ref|https://github.com/apache/flink/blob/0523ef6451a93da450c6bdf5dd4757c3702f3962/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java#L1188]), co-located iteration head and tail will always be in the same region. So I think we can drop the {{PipelinedRegionComputeUtil#makeAllOneRegion()}} code path and build regions in the the same way no matter if there is co-location constraints or not.",,azagrebin,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 19:09:33 UTC 2020,,,,,,,,,,"0|z0kb2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 09:32;zhuzh;cc [~trohrmann] [~azagrebin]

WDYT?
Correct me if I missed anything about why we need to make all vertices in one region for an iteration job.;;;","05/Nov/20 12:39;azagrebin;This makes sense to me. We already have computeStronglyConnectedComponents to put vertexes of one iteration into one region. I am running [CI|https://dev.azure.com/azagrebin/azagrebin/_build/results?buildId=341&view=results] to see if there are any failures for existing iteration tests. We probably also need some test with failover for iterations if there is none yet.

[~zhuzh] was buildOneRegionForAllVertices done because it was not clear whether one iteration can contain blocking edges or not? or what was the possible reason for head/tail being not in the same pipelined failover region?;;;","05/Nov/20 15:41;trohrmann;Is it somewhere enforced that the iteration step can only use pipelined data exchanges?;;;","05/Nov/20 16:14;zhuzh;[~azagrebin] {{computeStronglyConnectedComponents}} does not see iteration feedback edges so it does not help here. However, iteration edges will always be PIPELINED so that head and tail task will be pipelined connected and then will be in one region.
I have also run a [CI|https://dev.azure.com/zhuzh/flink-zz/_build/results?buildId=268&view=results] for the proposed change and it has passed.
The behavior to {{buildOneRegionForAllVertices}} has been there for quite some time since the legacy RestartPipelinedRegionStrategy which was already removed. I am not quite sure about the initial purpose. But from what I see, it is not needed now. 

[~trohrmann] [This|https://github.com/apache/flink/blob/0523ef6451a93da450c6bdf5dd4757c3702f3962/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java#L1188] is where edges within an iteration will be set to PIPELINED for DataSet jobs. And for streaming jobs, edges are always PIPELINED.;;;","05/Nov/20 16:43;trohrmann;Thanks [~zhuzh] for the pointer. I think it should then be fine to remove {{makeAllOneRegion()}}.;;;","10/Nov/20 19:09;zhuzh;Fixed via:
0b0a6d90c7819be3b31e64e8aba83ee8ae2d757c
a9d859b36b47cb433d2bb159d4a8848b769c8710;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleInputNodeCreationProcessor#isChainableSource should consider DataStreamScanProvider,FLINK-19990,13338948,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,05/Nov/20 07:09,08/Nov/20 07:08,13/Jul/23 08:12,08/Nov/20 07:08,,,,,,1.12.0,,,,,,,,0,pull-request-available,,,,,"{{MultipleInputNodeCreationProcessor#isChainableSource}} now only considers {{SourceProvider}}. However {{DataStreamScanProvider}} providing {{DataStream}} with {{SourceTransformation}} are also chainable sources, and we need to take them into consideration.",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 08 07:08:39 UTC 2020,,,,,,,,,,"0|z0kaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/20 07:08;lzljs3620320;master (1.12): 83174261b33f942f6fd7475d7115ccd430f27bab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBaseDynamicTableFactoryTest.testTableSourceFactory failed with ""NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V""",FLINK-19987,13338922,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,dian.fu,dian.fu,05/Nov/20 02:21,05/Nov/20 15:48,13/Jul/23 08:12,05/Nov/20 15:48,1.12.0,,,,,1.12.0,,,Connectors / HBase,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]

{code}

2020-11-04T22:30:58.3273231Z testTableSourceFactory(org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest) Time elapsed: 0.894 sec <<< ERROR! 2020-11-04T22:30:58.3274285Z org.apache.flink.table.api.ValidationException: 2020-11-04T22:30:58.3274931Z Unable to create a source for reading table 'default.default.t1'. 2020-11-04T22:30:58.3275159Z 2020-11-04T22:30:58.3275364Z Table options are: 2020-11-04T22:30:58.3275523Z 2020-11-04T22:30:58.3275882Z 'connector'='hbase-1.4' 2020-11-04T22:30:58.3276283Z 'table-name'='testHBastTable' 2020-11-04T22:30:58.3276712Z 'zookeeper.quorum'='localhost:2181' 2020-11-04T22:30:58.3277158Z 'zookeeper.znode.parent'='/flink' 2020-11-04T22:30:58.3277553Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) 2020-11-04T22:30:58.3278358Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.createTableSource(HBaseDynamicTableFactoryTest.java:332) 2020-11-04T22:30:58.3279046Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.testTableSourceFactory(HBaseDynamicTableFactoryTest.java:104) 2020-11-04T22:30:58.3279623Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2020-11-04T22:30:58.3280067Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2020-11-04T22:30:58.3280703Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2020-11-04T22:30:58.3281149Z at java.lang.reflect.Method.invoke(Method.java:498) 2020-11-04T22:30:58.3281615Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 2020-11-04T22:30:58.3282135Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 2020-11-04T22:30:58.3282637Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 2020-11-04T22:30:58.3283151Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 2020-11-04T22:30:58.3283749Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) 2020-11-04T22:30:58.3284221Z at org.junit.rules.RunRules.evaluate(RunRules.java:20) 2020-11-04T22:30:58.3284624Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 2020-11-04T22:30:58.3285091Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 2020-11-04T22:30:58.3285590Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 2020-11-04T22:30:58.3286053Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 2020-11-04T22:30:58.3286487Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 2020-11-04T22:30:58.3286911Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 2020-11-04T22:30:58.3287348Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 2020-11-04T22:30:58.3287779Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 2020-11-04T22:30:58.3288262Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 2020-11-04T22:30:58.3288721Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367) 2020-11-04T22:30:58.3289254Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274) 2020-11-04T22:30:58.3289781Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 2020-11-04T22:30:58.3290312Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161) 2020-11-04T22:30:58.3291103Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290) 2020-11-04T22:30:58.3291649Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242) 2020-11-04T22:30:58.3292158Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121) 2020-11-04T22:30:58.3292703Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V 2020-11-04T22:30:58.3293230Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357) 2020-11-04T22:30:58.3293876Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338) 2020-11-04T22:30:58.3294441Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113) 2020-11-04T22:30:58.3295032Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122) 2020-11-04T22:30:58.3295401Z ... 28 more
{code}",,dian.fu,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19849,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 15:48:12 UTC 2020,,,,,,,,,,"0|z0kark:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 15:48;rmetzger;Fixed in https://github.com/apache/flink/commit/11935306396c6b5ecb363938b68e3aeb72b5f83c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The automatic license check failed,FLINK-19986,13338920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,dian.fu,dian.fu,05/Nov/20 02:15,05/Nov/20 11:55,13/Jul/23 08:12,05/Nov/20 11:55,1.12.0,,,,,1.12.0,,,Build System,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=ffa46b9d-687d-5a4a-3e77-010e0da138d3

{code}

2020-11-04T21:34:32.2175854Z [INFO] ------------------------------------------------------------------------ 2020-11-04T21:34:32.2176981Z [INFO] Building Flink : Tools : CI : Java 1.12-SNAPSHOT 2020-11-04T21:34:32.2177899Z [INFO] ------------------------------------------------------------------------ 2020-11-04T21:34:32.2738277Z [INFO] 2020-11-04T21:34:32.2739975Z [INFO] --- exec-maven-plugin:3.0.0:java (default-cli) @ java-ci-tools --- 2020-11-04T21:34:33.3354165Z 21:34:33,332 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Loaded 20 items from resource modules-skipping-deployment.modulelist 2020-11-04T21:34:33.3377191Z 21:34:33,337 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Loaded 7 items from resource modules-defining-excess-dependencies.modulelist 2020-11-04T21:34:33.3413748Z 21:34:33,341 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - THIS UTILITY IS ONLY CHECKING FOR COMMON LICENSING MISTAKES. A MANUAL CHECK OF THE NOTICE FILES, DEPLOYED ARTIFACTS, ETC. IS STILL NEEDED! 2020-11-04T21:34:37.9134087Z 21:34:37,912 INFO org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Extracted 40 modules with a total of 574 dependencies 2020-11-04T21:34:39.1450119Z 21:34:39,144 INFO org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found 40 NOTICE files to check 2020-11-04T21:34:39.1669092Z 21:34:39,166 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-remote_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-runtime/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.1671308Z 21:34:39,166 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-remote_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-runtime/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.1721569Z 21:34:39,171 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.apache.orc:orc-core:nohive:1.5.6 in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.1724119Z 21:34:39,171 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1727247Z 21:34:39,171 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1730087Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-storage-api:2.3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1732575Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1735187Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:spark-client:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1738561Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-hadoop-bundle:1.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1741197Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:2.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1743602Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1745716Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libfb303:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1747800Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1749957Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-orc:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1752214Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1754327Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1756535Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:minlog:1.3.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1758620Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-metastore:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1760690Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1762758Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1766619Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1768581Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javax.jdo:jdo-api:3.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1770474Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:14.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1772375Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1774296Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:kryo-shaded:3.0.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1780026Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1782109Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1784013Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1786050Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1787992Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1789940Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-common:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1791842Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.tdunning:json:1.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1793774Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1795709Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.iq80.snappy:snappy:0.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1797616Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-service-rpc:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1799566Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-client:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1801981Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1804111Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-storage-api:2.7.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1810514Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:2.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1812619Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1814584Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1921350Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1928903Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1931355Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1936369Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1938695Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1940907Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1943072Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:reflectasm:1.10.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1945230Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:minlog:1.3.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1947637Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-common:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1951216Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.airlift:aircompressor:0.10 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1953109Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1955504Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-spark-client:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1957544Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javax.jdo:jdo-api:3.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1959833Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-core:1.5.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1962036Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-standalone-metastore:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1963988Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1966012Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-metastore:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1968158Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-service-rpc:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1970206Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-shims:1.5.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1977414Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-client:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1980778Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:kryo-shaded:3.0.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1986490Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:19.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1988628Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.9.9 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2005476Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-tools:1.5.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2009369Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2011337Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2013255Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2015177Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-jackson:1.10.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2017747Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2019652Z 21:34:39,182 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.tdunning:json:1.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2021577Z 21:34:39,183 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2023752Z 21:34:39,183 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-hadoop-bundle:1.10.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2025783Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2027637Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2029567Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware.minlog:minlog:1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2031535Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware.reflectasm:reflectasm:1.07 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2033533Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2035487Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2037441Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.20S:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2039405Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2041359Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2043271Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2045257Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.twitter:parquet-hadoop-bundle:1.6.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2047134Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:14.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2049083Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2051616Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2053616Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2055720Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2059933Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2061926Z 21:34:39,188 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:spark-client:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2064924Z 21:34:39,188 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2066900Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2068820Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2070725Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2072836Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2074892Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2076807Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.iq80.snappy:snappy:0.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2078716Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware.kryo:kryo:2.22 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2080694Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-annotations:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2082656Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-logging:commons-logging:1.1.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2084605Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-core:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2086594Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-databind:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2088548Z 21:34:39,192 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-jackson:1.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2090444Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2092341Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-storage-api:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2094323Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2098788Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-hadoop-bundle:1.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2101068Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:2.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2106636Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:spark-client:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2111497Z 21:34:39,194 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2115394Z 21:34:39,194 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.slf4j:slf4j-api:1.7.10 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2118884Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.airlift:aircompressor:0.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2120864Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2122837Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2124804Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2126776Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:reflectasm:1.10.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2128858Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:minlog:1.3.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2130795Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2132825Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2134776Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2136933Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-core:1.3.4 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2138823Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2140745Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-service-rpc:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2143089Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javax.jdo:jdo-api:3.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2145084Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-metastore:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2146996Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:14.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2148883Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2152546Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:kryo-shaded:3.0.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2154818Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-common:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2157822Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2159743Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2161674Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-client:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2163608Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2165499Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-tools:1.3.4 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2167420Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2169889Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2171870Z 21:34:39,199 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.tdunning:json:1.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2173827Z 21:34:39,199 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3020239Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-annotations:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3022559Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-logging:commons-logging:1.1.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3027098Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3031232Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-core:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3033953Z 21:34:39,302 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hbase:hbase-hadoop-compat:2.2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3042063Z 21:34:39,302 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hbase:hbase-hadoop2-compat:2.2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3044366Z 21:34:39,302 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-databind:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.4959013Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang:scala-reflect:2.12.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4960999Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang:scala-library:2.12.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4962531Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang:scala-compiler:2.12.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4964055Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang.modules:scala-xml_2.12:1.0.6 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4965574Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-actor_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4967134Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang.modules:scala-java8-compat_2.12:0.8.0 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4968696Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-stream_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4970496Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe:ssl-config-core_2.12:0.3.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4972066Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang.modules:scala-parser-combinators_2.12:1.1.1 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4973618Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-protobuf_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4975535Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-slf4j_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4977058Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.clapper:grizzled-slf4j_2.12:1.3.2 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4978538Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.github.scopt:scopt_2.12:3.5.0 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4979983Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.twitter:chill_2.12:0.7.6 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4981577Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang.modules:scala-java8-compat_2.11:0.7.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4983204Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang:scala-library:2.11.12 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4984775Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.clapper:grizzled-slf4j_2.11:1.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4988396Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang:scala-compiler:2.11.12 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4990047Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-actor_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4991752Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang.modules:scala-parser-combinators_2.11:1.1.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4993404Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-slf4j_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4995012Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang:scala-reflect:2.11.12 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4996813Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe:ssl-config-core_2.11:0.3.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4998368Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.twitter:chill_2.11:0.7.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5000029Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang.modules:scala-xml_2.11:1.0.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5001654Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-protobuf_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5003248Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.github.scopt:scopt_2.11:3.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5004840Z 21:34:39,494 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-stream_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5644298Z 21:34:39,561 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-transport-native-epoll:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5646181Z 21:34:39,561 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-context:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5647912Z 21:34:39,561 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.code.gson:gson:2.8.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5649691Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec-http:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5651431Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-auth:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5653230Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-transport-native-unix-common:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5655017Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-protobuf:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5656990Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.opencensus:opencensus-contrib-grpc-metrics:0.24.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5659084Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-transport:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5660968Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-handler-proxy:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5662802Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-tcnative-boringssl-static:2.0.26.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5664600Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.opencensus:opencensus-api:0.24.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5666388Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java-util:3.11.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5668152Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-testing:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5669896Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-resolver:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5671698Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.api.grpc:proto-google-common-protos:1.12.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5673464Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-netty:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5675154Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-core:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5676892Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:3.11.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5678657Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-buffer:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5680433Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-stub:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5684365Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec-http2:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5686780Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5688614Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-common:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5690345Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:26.0-jre is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5692167Z 21:34:39,563 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.auth:google-auth-library-credentials:0.18.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5693969Z 21:34:39,563 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec-socks:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5695934Z 21:34:39,563 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-handler:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5697230Z 21:34:39,563 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found a total of 16 severe license issues 2020-11-04T21:34:39.5840050Z ============================================================================== 2020-11-04T21:34:39.5843291Z License Check failed. See previous output for details. 2020-11-04T21:34:39.5843752Z ============================================================================== 2020-11-04T21:34:39.5873836Z 2020-11-04T21:34:39.6011858Z ##[error]Bash exited with code '1'. 2020-11-04T21:34:39.6028742Z ##[section]Finishing: Build Flink
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19810,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 11:55:59 UTC 2020,,,,,,,,,,"0|z0kar4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 02:16;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065;;;","05/Nov/20 02:17;dian.fu;cc [~rmetzger];;;","05/Nov/20 06:39;rmetzger;Looks like the check failed on the scala 2.12 profile. I'll open a PR to fix it!;;;","05/Nov/20 11:55;rmetzger;Resolved in https://github.com/apache/flink/commit/4044a75cc46c0f39ce3b3721e2bdefcdceee5149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShuffleCompressionITCase.testDataCompressionForSortMergeBlockingShuffle unstable,FLINK-19983,13338883,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,rmetzger,rmetzger,04/Nov/20 18:16,22/Jun/21 13:55,13/Jul/23 08:12,16/Nov/20 14:23,1.12.0,,,,,1.12.0,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8997&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2020-11-04T14:32:19.7227316Z [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 16.882 s <<< FAILURE! - in org.apache.flink.test.runtime.ShuffleCompressionITCase
2020-11-04T14:32:19.7228708Z [ERROR] testDataCompressionForSortMergeBlockingShuffle[useBroadcastPartitioner = true](org.apache.flink.test.runtime.ShuffleCompressionITCase)  Time elapsed: 5.058 s  <<< FAILURE!
2020-11-04T14:32:19.7230032Z java.lang.AssertionError: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-11-04T14:32:19.7230580Z 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:58)
2020-11-04T14:32:19.7231173Z 	at org.apache.flink.test.runtime.ShuffleCompressionITCase.testDataCompressionForSortMergeBlockingShuffle(ShuffleCompressionITCase.java:98)
2020-11-04T14:32:19.7232076Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-04T14:32:19.7232624Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-04T14:32:19.7233242Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-04T14:32:19.7233741Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-04T14:32:19.7234353Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-11-04T14:32:19.7235141Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-11-04T14:32:19.7238521Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-11-04T14:32:19.7239371Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-11-04T14:32:19.7240010Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-11-04T14:32:19.7240688Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-11-04T14:32:19.7241396Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-11-04T14:32:19.7242019Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-04T14:32:19.7242623Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-04T14:32:19.7243379Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-04T14:32:19.7244051Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-04T14:32:19.7244631Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-04T14:32:19.7245313Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-04T14:32:19.7245844Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-11-04T14:32:19.7246341Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-11-04T14:32:19.7246868Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-04T14:32:19.7247616Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-04T14:32:19.7248223Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-04T14:32:19.7248826Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-04T14:32:19.7249393Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-04T14:32:19.7249963Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-04T14:32:19.7250586Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-11-04T14:32:19.7251277Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-11-04T14:32:19.7252024Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-11-04T14:32:19.7252839Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-11-04T14:32:19.7253584Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-11-04T14:32:19.7254365Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-11-04T14:32:19.7255165Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-11-04T14:32:19.7255806Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-11-04T14:32:19.7256609Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-11-04T14:32:19.7257950Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-04T14:32:19.7258918Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-04T14:32:19.7259799Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-11-04T14:32:19.7260745Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-11-04T14:32:19.7261565Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-11-04T14:32:19.7262363Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534)
2020-11-04T14:32:19.7263192Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
2020-11-04T14:32:19.7263936Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
2020-11-04T14:32:19.7264543Z 	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
2020-11-04T14:32:19.7265215Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-04T14:32:19.7265827Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-04T14:32:19.7266473Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-04T14:32:19.7267334Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-04T14:32:19.7268117Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-04T14:32:19.7268898Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-04T14:32:19.7269511Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-04T14:32:19.7270106Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-04T14:32:19.7270708Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-04T14:32:19.7271257Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-04T14:32:19.7271812Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-04T14:32:19.7272376Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-04T14:32:19.7273048Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-04T14:32:19.7273583Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-04T14:32:19.7274140Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-04T14:32:19.7274679Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-04T14:32:19.7275310Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-04T14:32:19.7275792Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-04T14:32:19.7276343Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-04T14:32:19.7276864Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-04T14:32:19.7277531Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-04T14:32:19.7278195Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-04T14:32:19.7278856Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-04T14:32:19.7279542Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-04T14:32:19.7281428Z Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager 'localhost/127.0.0.1:33368'.
2020-11-04T14:32:19.7282544Z 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:283)
2020-11-04T14:32:19.7283723Z 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:183)
2020-11-04T14:32:19.7285215Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
2020-11-04T14:32:19.7286265Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
2020-11-04T14:32:19.7287520Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
2020-11-04T14:32:19.7288686Z 	at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:115)
2020-11-04T14:32:19.7289722Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
2020-11-04T14:32:19.7290742Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
2020-11-04T14:32:19.7291776Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
2020-11-04T14:32:19.7292884Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
2020-11-04T14:32:19.7293919Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
2020-11-04T14:32:19.7295079Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
2020-11-04T14:32:19.7296076Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
2020-11-04T14:32:19.7297214Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792)
2020-11-04T14:32:19.7298245Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475)
2020-11-04T14:32:19.7299068Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
2020-11-04T14:32:19.7299963Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
2020-11-04T14:32:19.7300879Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-11-04T14:32:19.7301516Z 	at java.lang.Thread.run(Thread.java:748)
2020-11-04T14:32:19.7302033Z Caused by: java.io.IOException: Reader is already released.
2020-11-04T14:32:19.7302880Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:255)
2020-11-04T14:32:19.7303825Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:108)
2020-11-04T14:32:19.7304871Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:173)
2020-11-04T14:32:19.7305873Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
2020-11-04T14:32:19.7306932Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
2020-11-04T14:32:19.7308179Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
2020-11-04T14:32:19.7309255Z 	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
2020-11-04T14:32:19.7310249Z 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
2020-11-04T14:32:19.7311499Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
2020-11-04T14:32:19.7312573Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
2020-11-04T14:32:19.7313711Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
2020-11-04T14:32:19.7314967Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
2020-11-04T14:32:19.7316038Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
2020-11-04T14:32:19.7317216Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
2020-11-04T14:32:19.7318253Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
2020-11-04T14:32:19.7319209Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:87)
2020-11-04T14:32:19.7320177Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
2020-11-04T14:32:19.7321167Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
2020-11-04T14:32:19.7322060Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
2020-11-04T14:32:19.7322615Z 	... 3 more
2020-11-04T14:32:19.7323125Z Caused by: java.lang.IllegalStateException: Reader is already released.
2020-11-04T14:32:19.7323760Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:198)
2020-11-04T14:32:19.7324559Z 	at org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.getNextBuffer(SortMergeSubpartitionReader.java:98)
2020-11-04T14:32:19.7325691Z 	at org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.getNextBuffer(CreditBasedSequenceNumberingViewReader.java:170)
2020-11-04T14:32:19.7326735Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:216)
2020-11-04T14:32:19.7327496Z 	... 21 more
{code}",,AHeise,dian.fu,kevin.cyj,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 16 14:23:42 UTC 2020,,,,,,,,,,"0|z0kaiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/20 08:49;kevin.cyj;I will take a look at this issue.;;;","06/Nov/20 12:17;rmetzger;Thanks a lot. I assigned you.;;;","09/Nov/20 03:33;kevin.cyj;After some investigation, I find that the state check is not true, we just need to remove it. The caller, including CreditBasedSequenceNumberingViewReader and LocalInputChannel can handle the case correctly. BoundedBlockingSubpartitionReader does the same thing. The PR is available for review now.;;;","16/Nov/20 14:23;arvid;Merged into master as e4f525e7c5270e33972cbbc9f6360435d0ff87ae.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLClientKafkaITCase.testKafka times out while creating topic caused by ""PyFlink end-to-end test""",FLINK-19974,13338839,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dian.fu,rmetzger,rmetzger,04/Nov/20 14:29,09/Nov/20 09:44,13/Jul/23 08:12,09/Nov/20 09:44,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8967&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-11-04T12:00:36.0501135Z Nov 04 12:00:36 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 57.959 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
2020-11-04T12:00:36.0538557Z Nov 04 12:00:36 [ERROR] testKafka[0: kafka-version:2.4.1 kafka-sql-version:universal](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 57.949 s  <<< ERROR!
2020-11-04T12:00:36.0539781Z Nov 04 12:00:36 java.io.IOException: Process failed due to timeout.
2020-11-04T12:00:36.0540659Z Nov 04 12:00:36 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:130)
2020-11-04T12:00:36.0548817Z Nov 04 12:00:36 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
2020-11-04T12:00:36.0549582Z Nov 04 12:00:36 	at org.apache.flink.tests.util.AutoClosableProcess.runBlocking(AutoClosableProcess.java:70)
2020-11-04T12:00:36.0550231Z Nov 04 12:00:36 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.createTopic(LocalStandaloneKafkaResource.java:261)
2020-11-04T12:00:36.0550909Z Nov 04 12:00:36 	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:136)
{code}
",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19838,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 09:44:31 UTC 2020,,,,,,,,,,"0|z0ka94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 15:36;rmetzger;In the attached log files, the {{zookeeper.out}} says:

{code}
[2020-11-04 11:59:41,521] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-11-04 11:59:41,522] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:461)
	at sun.nio.ch.Net.bind(Net.java:453)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:687)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:143)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:106)
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:128)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
{code}

Zookeeper is still running from the {{PyFlink end-to-end test}}:

{code}
2020-11-04T11:52:27.9189075Z Nov 04 11:52:27 [PASS] 'PyFlink end-to-end test' passed after 26 minutes and 22 seconds! Test exited with exit code 0.
2020-11-04T11:52:27.9189635Z Nov 04 11:52:27 
2020-11-04T11:52:28.2266995Z Nov 04 11:52:28 No taskexecutor daemon (pid: 72141) is running anymore on fv-az668-178.
2020-11-04T11:52:28.4107336Z Nov 04 11:52:28 No standalonesession daemon to stop on host fv-az668-178.
2020-11-04T11:52:29.0652923Z Nov 04 11:52:29 Deleted all files under /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/
2020-11-04T11:52:29.0795925Z Nov 04 11:52:29 Deleted /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-04854976498
2020-11-04T11:52:29.0797725Z Nov 04 11:52:29 ##[group]Environment Information
2020-11-04T11:52:29.0798431Z Nov 04 11:52:29 Jps
2020-11-04T11:52:29.2079973Z Nov 04 11:52:29 95041 Kafka
2020-11-04T11:52:29.2285220Z Nov 04 11:52:29 94455 QuorumPeerMain
2020-11-04T11:52:29.2374681Z Nov 04 11:52:29 120263 Jps
2020-11-04T11:52:29.2411676Z Nov 04 11:52:29 Disk information
2020-11-04T11:52:29.2442325Z Nov 04 11:52:29 Filesystem      Size  Used Avail Use% Mounted on
2020-11-04T11:52:29.2444296Z Nov 04 11:52:29 udev            3.7G     0  3.7G   0% /dev
2020-11-04T11:52:29.2445121Z Nov 04 11:52:29 tmpfs           729M   18M  712M   3% /run
2020-11-04T11:52:29.2445800Z Nov 04 11:52:29 /dev/sda1        90G   66G   25G  74% /
2020-11-04T11:52:29.2446330Z Nov 04 11:52:29 tmpfs           3.7G  8.2k  3.7G   1% /dev/shm
2020-11-04T11:52:29.2446781Z Nov 04 11:52:29 tmpfs           5.3M     0  5.3M   0% /run/lock
2020-11-04T11:52:29.2447218Z Nov 04 11:52:29 tmpfs           3.7G     0  3.7G   0% /sys/fs/cgroup
2020-11-04T11:52:29.2447652Z Nov 04 11:52:29 /dev/sda15      110M  3.8M  106M   4% /boot/efi
2020-11-04T11:52:29.2448048Z Nov 04 11:52:29 /dev/sdb1        15G  4.4G  9.6G  32% /mnt
2020-11-04T11:52:29.2448401Z Nov 04 11:52:29 Allocated ports
2020-11-04T11:52:29.2982104Z Nov 04 11:52:29 Active Internet connections (only servers)
2020-11-04T11:52:29.2983531Z Nov 04 11:52:29 Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
2020-11-04T11:52:29.2984193Z Nov 04 11:52:29 tcp        0      0 127.0.0.1:45157         0.0.0.0:*               LISTEN      1547/containerd 
2020-11-04T11:52:29.2984844Z Nov 04 11:52:29 tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1543/sshd       
2020-11-04T11:52:29.2985510Z Nov 04 11:52:29 tcp6       0      0 :::39011                :::*                    LISTEN      94455/java      
2020-11-04T11:52:29.2986450Z Nov 04 11:52:29 tcp6       0      0 :::9092                 :::*                    LISTEN      95041/java      
2020-11-04T11:52:29.2987149Z Nov 04 11:52:29 tcp6       0      0 :::2181                 :::*                    LISTEN      94455/java      
2020-11-04T11:52:29.2987793Z Nov 04 11:52:29 tcp6       0      0 :::22                   :::*                    LISTEN      1543/sshd       
2020-11-04T11:52:29.2988416Z Nov 04 11:52:29 tcp6       0      0 :::43579                :::*                    LISTEN      95041/java      
2020-11-04T11:52:29.2989057Z Nov 04 11:52:29 udp        0      0 0.0.0.0:68              0.0.0.0:*                           1112/dhclient  
{code};;;","04/Nov/20 18:25;rmetzger;I will introduce a sanity check for bash e2e tests: https://issues.apache.org/jira/browse/FLINK-19979;;;","09/Nov/20 09:44;dian.fu;Fixed in master(1.12.0) via 7b4c97b091f38f5e1d650b02a15e778ac085875d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-connector-hbase-base depends on hbase-server, instead of hbase-client",FLINK-19971,13338820,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mgergely,rmetzger,rmetzger,04/Nov/20 12:12,28/May/21 07:08,13/Jul/23 08:12,15/Jan/21 08:02,,,,,,1.13.0,,,Connectors / HBase,,,,,0,pull-request-available,,,,,"flink-connector-hbase-base depends on hbase-server, instead of hbase-client and hbase-common.

I believe hbase-server is only needed for starting a Minicluster and therefore should be in the test scope.

This was introduced by FLINK-1928.",,mgergely,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 08:02:03 UTC 2021,,,,,,,,,,"0|z0ka4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 12:36;chesnay;It actually seems as if hbase-base doesn't need this dependency at all just like all test-scoped dependencies.

It does need hbase-client though.;;;","11/Jan/21 20:45;mgergely;[~rmetzger] could you please review?;;;","15/Jan/21 08:02;rmetzger;Merged to master in https://github.com/apache/flink/commit/7fe1bca22a2093113ff268ef6e33d89cea1c4d48

Thanks for addressing this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State leak in CEP Operators (expired events/keys not removed from state),FLINK-19970,13338818,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,Jamalarm,Jamalarm,04/Nov/20 11:38,24/Feb/21 10:30,13/Jul/23 08:12,24/Feb/21 10:30,1.11.2,,,,,1.13.0,,,Library / CEP,,,,,0,pull-request-available,,,,,"We have been observing instability in our production environment recently, seemingly related to state backends. We ended up building a load testing environment to isolate factors and have discovered that the CEP library appears to have some serious problems with state expiry.

h2. Job Topology

Source: Kinesis (standard connector) -> keyBy() and forward to...
CEP: Array of simple Keyed CEP Pattern operators (details below) -> forward output to...
Sink: SQS (custom connector)

The CEP Patterns in the test look like this:

{code:java}
Pattern.begin(SCANS_SEQUENCE, AfterMatchSkipStrategy.skipPastLastEvent())
    .times(20)
    .subtype(ScanEvent.class)
    .within(Duration.minutes(30));
{code}

h2. Taskmanager Config

{code:java}
taskmanager.numberOfTaskSlots: $numberOfTaskSlots
taskmanager.data.port: 6121
taskmanager.rpc.port: 6122
taskmanager.exit-on-fatal-akka-error: true
taskmanager.memory.process.size: $memoryProcessSize
taskmanager.memory.jvm-metaspace.size: 256m
taskmanager.memory.managed.size: 0m
jobmanager.rpc.port: 6123
blob.server.port: 6130
rest.port: 8081
web.submit.enable: true
fs.s3a.connection.maximum: 50
fs.s3a.threads.max: 50
akka.framesize: 250m
akka.watch.threshold: 14

state.checkpoints.dir: s3://$savepointBucketName/checkpoints
state.savepoints.dir: s3://$savepointBucketName/savepoints
state.backend: filesystem
state.backend.async: true

s3.access-key: $s3AccessKey
s3.secret-key: $s3SecretKey
{code}

(the substitutions are controlled by terraform).

h2. Tests

h4. Test 1 (No key rotation)
8192 actors (different keys) emitting 1 Scan Event every 10 minutes indefinitely. Actors (keys) never rotate in or out.

h4. Test 2 (Constant key rotation)
8192 actors that produce 2 Scan events 10 minutes apart, then retire and never emit again. The setup creates new actors (keys) as soon as one finishes so we always have 8192. This test basically constantly rotates the key space.

h2. Results

For both tests, the state size (checkpoint size) grows unbounded and linearly well past the 30 minute threshold that should have caused old keys or events to be discard from the state. In the chart below, the left (steep) half is the 24 hours we ran Test 1, the right (shallow) half is Test 2.  My understanding is that the checkpoint size should level off after ~45 minutes or so then stay constant.

!image-2020-11-04-11-35-12-126.png! 

Could someone please assist us with this? Unless we have dramatically misunderstood how the CEP library is supposed to function this seems like a pretty severe bug.","Flink 1.11.2 run using the official docker containers in AWS ECS Fargate.

1 Job Manager, 1 Taskmanager with 2vCPUs and 8GB memory",aljoscha,alpinegizmo,dwysakowicz,Jamalarm,klion26,knaufk,maguowei,michallyson,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19293,,,,,,,,,,,,,"04/Nov/20 11:35;Jamalarm;image-2020-11-04-11-35-12-126.png;https://issues.apache.org/jira/secure/attachment/13014703/image-2020-11-04-11-35-12-126.png","03/Dec/20 16:37;Jamalarm;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13016430/screenshot-1.png","04/Dec/20 10:13;Jamalarm;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13016487/screenshot-2.png","04/Dec/20 16:17;Jamalarm;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13016502/screenshot-3.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 10:30:19 UTC 2021,,,,,,,,,,"0|z0ka4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 12:00;dwysakowicz;Do you look for patterns in event or processing time? If it is in event time, what is your Watermark strategy? The incoming elements are buffered for sorting until an appropriate Watermark comes in. The {{within}} timeour is applied only after the events were sorted.;;;","04/Nov/20 12:34;Jamalarm;Hi [~dwysakowicz],

Sorry for not including that info in the original post.

We are using event time with a custom watermarking strategy based on an average of the last 10 events' timestamps + a  constant buffer of 15 minutes.

The watermarking strategy is working just fine. Test #2 is actually still running and I can see the Low Watermark of the CEP operators is 1604492129185 (15 minutes ago) as expected.

Note that this setup is also producing matches just fine (with increased frequency of event emission). If the watermarks weren't being correctly assigned then we would never see matches coming out the other end of the CEP operators, right?;;;","04/Nov/20 12:39;dwysakowicz;Yes, you are right watermarking would affect the matches as well.;;;","06/Nov/20 11:01;Jamalarm;[~dwysakowicz] Please give me a shout if there's any more diagnostic info I can attach to assist with debugging this. This one is blocking us quite severely so I'm more than happy to help any way I can.;;;","16/Nov/20 15:55;dwysakowicz;Sorry for not getting back earlier. I did some experiments and I was able to replicate the problem. Unfortunately, 1) I don't have a solution yet 2) because we are late in the 1.12 release cycle it won't make it into 1.12.0. I will get back to the problem once we are done with the 1.12.;;;","16/Nov/20 17:05;Jamalarm;Hey [~dwysakowicz] - thanks for the update. Please let us know if there's anything we can do to help.

Happy to test out a branch version of Flink in our load test environment when it is available. The only issue is that we use the docker images so I'd need some way to build a branch docker image.;;;","01/Dec/20 10:32;wind_ljy;[~Jamalarm]  Do you figure out the root cause of the state leak?   Maybe I can spare some time to look into this problem because we're also using CEP on production and this is actually very severe from our side.;;;","01/Dec/20 11:03;Jamalarm;Hi [~wind_ljy],

No, we didn't identify a fix. We're not really familiar with the Flink codebase and our team is pretty small, so our plan was to wait until [~dwysakowicz] was finished with the 1.12.0 release and had some time to look at the issue.

We would obviously be very grateful if you were able to spare some time to dig into this. Our production system is effectively a ticking time bomb at the moment with this issue.;;;","02/Dec/20 15:08;dwysakowicz;Hey [~Jamalarm],
I worked a little bit on the issue. I prepared a first approach for fixing the issue. I'd appreciate if you could test it out. You can find it in my branch here: https://github.com/dawidwys/flink/tree/flink-19970 Only the last commit is relevant. You should also be able to cherry-pick it on flink 1.11 if you cannot run your tests on flink 1.12.

Unfortunately it requires changes in the state format, thus it is not ready to create a PR for the master. It would be nice though if you could verify that my suspicion for the cause is correct.;;;","02/Dec/20 15:20;Jamalarm;Hey [~dwysakowicz] - great to hear you have a candidate fix!

In order for us to test it I would need to package your branch as a docker container. Our load testing environment runs exclusively on containers and it would be... a profound headache to try and make it run any other way.

We can publish it to an internal ECR repo on our side, that shouldn't be too hard. Is there a relatively straightforward way to check your branch out and package it up as an image locally?

Thanks;;;","02/Dec/20 15:24;dwysakowicz;I think it should be rather easy to use this script (https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/common_docker.sh) from our e2e setup to create a docker image from a local build.;;;","02/Dec/20 15:53;Jamalarm;Do I need to rebuild our Job JAR using the CEP library from your branch? Or is this a TaskManager-side fix?;;;","02/Dec/20 15:58;dwysakowicz;Yeah, you're actually right. It's enough to build only the CEP library from my branch and submit it with the job jar.;;;","02/Dec/20 16:18;Jamalarm;For some reason I can't compile your branch, I get:


{code}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/jamalarm/src/open/flink/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java:[39,19] package javafx.util does not exist
[INFO] 1 error
[INFO] -------------------------------------------------------------
{code}

Any ideas?
;;;","02/Dec/20 16:41;dwysakowicz;Sorry, my bad. Please try again.;;;","03/Dec/20 05:02;wind_ljy;[~dwysakowicz] Does this bug affect all the patterns？;;;","03/Dec/20 14:11;Jamalarm;Ok, our load test is running now after a surprisingly painful setup process importing branch code. I'm re-running scenario #1 above and will post the results after a few hours (it should be clear if the problem is fixed);;;","03/Dec/20 16:37;Jamalarm; !screenshot-1.png! 

Unfortunately it doesn't appear to have stopped the state leak. I haven't let the test run for the full period but you can see the size continue to grow past the 30-45 minute mark where it should start discarding events. I'll keep it running for the full 24 to be representative but I think we might need to keep digging.;;;","04/Dec/20 10:13;Jamalarm;Results over a longer period, state still growing linearly:

 !screenshot-2.png! ;;;","04/Dec/20 13:32;dwysakowicz;Could you please double check that you've run my branch? If you are confident that you run my branch, I would need a program that I can run locally that can help me reproduce the problem. I've tried preparing one myself, but I cannot reproduce the state leak with the fix from my branch (I could do it without the fix). You can see the program I tried below:

{code}
public class CepBug {
	public static void main(String[] args) throws Exception {
		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		env.enableCheckpointing(100);
		env.getCheckpointConfig().enableExternalizedCheckpoints(RETAIN_ON_CANCELLATION);

		Pattern<Event, ?> pattern = Pattern.<Event>begin(""start"", AfterMatchSkipStrategy.skipPastLastEvent())
				.times(20)
				.within(Time.milliseconds(10));

		KeyedStream<Event, Object> events = env.addSource(new RichParallelSourceFunction<Event>() {
			private volatile boolean isRunning = true;
			private long currentTimestamp = 0;
			private int currentId = 0;

			@Override
			public void run(SourceContext<Event> sourceContext) throws Exception {
				while (isRunning) {
					sourceContext.collectWithTimestamp(
							new Event(
									currentId++,
									getRuntimeContext().getIndexOfThisSubtask(),
									currentTimestamp
							),
							currentTimestamp++
					);
					Thread.sleep(10);
				}
			}

			@Override
			public void cancel() {
				this.isRunning = false;
			}
		})
				.assignTimestampsAndWatermarks(
						WatermarkStrategy.forMonotonousTimestamps()
				)
				.keyBy(e -> e.key);

		SingleOutputStreamOperator<String> start = CEP.pattern(events, pattern)
				.process(new PatternProcessFunction<Event, String>() {
					@Override
					public void processMatch(
							Map<String, List<Event>> map,
							Context context,
							Collector<String> collector) throws Exception {
						collector.collect(
								map.get(""start"")
										.stream()
										.map(e -> String.format(""%s,%s"", e.id, e.timestamp))
										.collect(Collectors.joining("";""))
						);
					}
				});

		start.print();
		env.execute();
	}

	public static class Event {
		public final int id;
		public final int key;
		public final long timestamp;

		public Event(int id, int key, long timestamp) {
			this.id = id;
			this.key = key;
			this.timestamp = timestamp;
		}
	}
}
{code}

If I am correct it follows your scenario and is actually even more aggressive. The checkpoint size of this program stays stable ~47kB for over 20 minutes.;;;","04/Dec/20 15:07;Jamalarm;Hey [~dwysakowicz],

Just to confirm, it's just the `flink-cep` module that needs to be replaced in my Job JAR? No other flink libraries need to be swapped out and it's ok to run the actual cluster on the release version?;;;","04/Dec/20 15:13;dwysakowicz;The changes apply only to the flink-cep module yes. It's best to build the module against the version of your cluster.

I don't know your setup, so can't tell if it is enough to replace it in the Job JAR. Generally it should be unless you e.g. put the flink-cep library into the lib folder of your cluster.;;;","04/Dec/20 16:04;Jamalarm;Hey [~dwysakowicz]

Am I correct in saying that your scenario above matches my scenario #2 from the original description? That is to say the ""constant key rotation""  scenario, where keys only have one or two events before they go dormant and should be cleaned up?

The test I ran above was scenario #1 which was the ""no key rotation"" one. Just the same keys emitting events over and over forever. The equivalent in your test would be to hold the `id` value in your generated events constant.

I will rerun with the branch version on scenario #2 (to match your test setup) and see how it behaves.;;;","04/Dec/20 16:12;dwysakowicz;No, my test case matches your scenario #1. In my test there is no key rotation. I am using the Event#key as a key, which I set equal to the index of the parallel instance of the source. I run the test with parallelism of 4 so there were 4 constant keys.

Number of keys or keys becoming dormant should not play any role for the root cause. All keys are processed independently. 

The only difference in your two scenarios is how many events are assigned to a key and in turn into a pattern. The bug I fixed was that long chains of events were blocking past timed out events. Moreover the problem occured only if in such a chain there was a single event that was not timed-out. Then it was blocking all the past events. Have a look at an example:

Imagine partial matches:
{code}
1. (t=1), (t=2), (t=3) ... (t=4)
2. (t=2), (t=3) ... (t=4) (t=5)
3. (t=3) ... (t=4)(t=5)(t=6)
{code}
Assume timeout happens after 3 units of time.
Let's advance time to t = 4, then the 1. partial match gets timed out. We could prune event (t=1) but it was not pruned because it was blocked by e.g. (t=2) event in partial match 2. After advancing to t=5 the match 2. gets timed out, we could prune (t=2) again it is blocked by (t=3) from match 3. etc...

In your scenario #1 with much more events and basically never timing out all the events, the state was never pruned. That is why in scenario #1 it is more severe.
;;;","04/Dec/20 16:19;Jamalarm;Ah sorry, I mixed up the `key` and `id` value when I was reading your code. 

This is a screenshot from IntelliJ of the imported libraries from my test JAR:

 !screenshot-3.png! 

I pushed your branch to our internal artifactory under the snapshot version. I'm going to try pushing again with a more specific version name to make sure I'm not somehow pulling in snapshot versions from somewhere else, but getting this far was extremely painful already due to getting the maven build to work nice with our artifactory.

Would it be possible to stick a temporary log line somewhere in an init function for one of the CEP operators that I could look out for to confirm 100% that it's running with the right branch version on my remote test cluster?;;;","07/Dec/20 16:28;Jamalarm;Ok, I've built another version of our app this time using every artefact built from your branch (all parts of Flink). The bug is still visible (state still grows unbounded).

I'm going to try and get approval internally to send you a cut down version of our app that exhibits this behaviour. Do you just need a job JAR that you can start on a cluster and see the effect?

Also it would be great if there is a slightly more confidential way I can send the code to you, it's not hyper sensitive or anything but I'd rather not post it on a public JIRA ticket.;;;","08/Dec/20 08:28;dwysakowicz;I'd need the code of a job that I can reproduce the problem with. It would be best if I could reproduce the program locally on my machine. The most important part is though that it does not use any external systems. That it does not read or write from/to Kafka or any other queue or other systems.

You can email me at dwysakowicz@apache.org;;;","08/Dec/20 14:06;Jamalarm;Ok, I am working on this now. It's going to take me a while to cut down everything to get it into one self-contained JAR but I'll send it over as soon as I'm done.;;;","11/Dec/20 16:08;Jamalarm;[~dwysakowicz] I've emailed you over the code + JAR. Please give me a shout here or over email if you need anything else from me.;;;","24/Feb/21 10:30;dwysakowicz;Fixed in fab2e55bddc5495353a443c808c99e9bdaac4209;;;",,,,,,,,,,,,,,,,,,,,,,,
CliFrontendParser does not provide any help for run-application,FLINK-19969,13338816,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kkl0u,f.pompermaier,f.pompermaier,04/Nov/20 10:57,23/Dec/20 09:17,13/Jul/23 08:12,01/Dec/20 10:43,1.11.2,,,,,1.12.0,1.13.0,,Command Line Client,,,,,0,pull-request-available,,,,,flink CLI doesn't show any help about run-application,,aljoscha,f.pompermaier,kkl0u,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20686,FLINK-20450,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 09:17:06 UTC 2020,,,,,,,,,,"0|z0ka40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/20 09:32;rmetzger;+1 to fix this! It is very confusing that this command is documented but not printed by the tool.;;;","01/Dec/20 10:43;kkl0u;master: 3ecb27dc1ef7cb838d556c53aabb7bf327bc2491
release-1.12: 6c2fa6603cc1bd85f7a87322b229543ff62f62b8;;;","23/Dec/20 09:17;chesnay;Small extension for the list of possible actions in the help message merged in:

master: 243c05bea0d63f44b8bdb9305dc3b04b4afce40e

1.12: 300762cd68346ddf5afc6fd2027f997877a828b6 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gelly ITCase stuck on Azure in HITSITCase.testPrintWithRMatGraph,FLINK-19964,13338793,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,chesnay,chesnay,04/Nov/20 09:31,22/Jun/21 14:05,13/Jul/23 08:12,11/Nov/20 06:36,1.12.0,,,,,1.12.0,,,Library / Graph Processing,Runtime / Network,Tests,,,0,pull-request-available,test-stability,,,,"The HITSITCase has gotten stuck on Azure. Chances are that something in the scheduling or network has broken it.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8919&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5",,AHeise,dian.fu,pnowojski,rmetzger,roman,ym,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20011,,,,,,,FLINK-16972,,,,,,,,,,FLINK-20011,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 07:15:45 UTC 2020,,,,,,,,,,"0|z0k9yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 14:03;rmetzger;From the logs, you can see that the job seems to make progress without any issues, until we reach this point, where we don't see any new log events:

{code}
19:58:21,930 [CHAIN CoGroup (Hub) -> Combine (Sum) (3/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN CoGroup (Hub) -> Combine (Sum) (3/4)
19:58:21,930 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Reduce (Sum) (3/4) (852e861d142fcff331c85ee4b12949b9_9d7788ce41d2b7d2b242bfb2589af1cb_2_0) switched from RUNNING to FINISHED.
19:58:21,931 [Reduce (Sum) (2/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  Reduce (Sum) (2/4)
19:58:21,931 [Reduce (Sum) (1/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  Reduce (Sum) (1/4)
19:58:21,931 [CHAIN Map (Square) -> Combine (Sum) (2/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN Map (Square) -> Combine (Sum) (2/4)
19:58:21,931 [Reduce (Sum) (4/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  Reduce (Sum) (4/4)
19:58:21,931 [Reduce (Sum) (3/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  Reduce (Sum) (3/4)
19:58:21,931 [CHAIN Map (Square) -> Combine (Sum) (1/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN Map (Square) -> Combine (Sum) (1/4)
19:58:21,931 [CHAIN Map (Square) -> Combine (Sum) (4/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN Map (Square) -> Combine (Sum) (4/4)
19:58:21,931 [CHAIN Map (Square) -> Combine (Sum) (3/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN Map (Square) -> Combine (Sum) (3/4)
19:58:21,931 [Reduce (Sum) (1/1)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  Reduce (Sum) (1/1)
19:58:21,931 [CHAIN CoGroup (Authority) -> Combine (Sum) (3/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - starting iteration [1]:  CHAIN CoGroup (Authority) -> Combine (Sum) (3/4)
19:58:21,932 [CHAIN CoGroup (Authority) -> Combine (Sum) (1/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - starting iteration [1]:  CHAIN CoGroup (Authority) -> Combine (Sum) (1/4)
19:58:21,932 [CHAIN CoGroup (Authority) -> Combine (Sum) (2/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - starting iteration [1]:  CHAIN CoGroup (Authority) -> Combine (Sum) (2/4)
19:58:21,932 [CHAIN CoGroup (Authority) -> Combine (Sum) (4/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - starting iteration [1]:  CHAIN CoGroup (Authority) -> Combine (Sum) (4/4)
19:58:21,932 [CHAIN CoGroup (Authority) -> Combine (Sum) (3/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN CoGroup (Authority) -> Combine (Sum) (3/4)
19:58:21,933 [CHAIN CoGroup (Authority) -> Combine (Sum) (1/4)#0] INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - finishing iteration [1]:  CHAIN CoGroup (Authority) -> Combine (Sum) (1/4)
{code}

At this point, it seems that two iterations are finished:
- CHAIN CoGroup (Authority) -> Combine (Sum) (3/4)
- CHAIN CoGroup (Authority) -> Combine (Sum) (1/4)

and two are not finished:
- CHAIN CoGroup (Authority) -> Combine (Sum) (2/4)
- CHAIN CoGroup (Authority) -> Combine (Sum) (4/4)

Searching for ""CHAIN CoGroup (Authority) -> Combine (Sum) (4/4)"" in the threaddumps reveals:
{code}
2020-11-03T20:15:40.9815769Z ""CHAIN CoGroup (Authority) -> Combine (Sum) (4/4)#0"" #10722 prio=5 os_prio=0 tid=0x00007f8bf00db000 nid=0x6ce8 waiting on condition [0x00007f8a257d5000]
2020-11-03T20:15:40.9816219Z    java.lang.Thread.State: WAITING (parking)
2020-11-03T20:15:40.9816467Z 	at sun.misc.Unsafe.park(Native Method)
2020-11-03T20:15:40.9817005Z 	- parking to wait for  <0x000000008728caf8> (a java.util.concurrent.CompletableFuture$Signaller)
2020-11-03T20:15:40.9817404Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-11-03T20:15:40.9818064Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-11-03T20:15:40.9818484Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-11-03T20:15:40.9819044Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-11-03T20:15:40.9819455Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-03T20:15:40.9819938Z 	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:313)
2020-11-03T20:15:40.9820498Z 	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:286)
2020-11-03T20:15:40.9821095Z 	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:315)
2020-11-03T20:15:40.9821765Z 	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:292)
2020-11-03T20:15:40.9822490Z 	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:232)
2020-11-03T20:15:40.9823432Z 	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:131)
2020-11-03T20:15:40.9823980Z 	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:100)
2020-11-03T20:15:40.9824612Z 	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:53)
2020-11-03T20:15:40.9825143Z 	at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)
2020-11-03T20:15:40.9825627Z 	at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)
2020-11-03T20:15:40.9826167Z 	at org.apache.flink.runtime.operators.hash.InPlaceMutableHashTable$ReduceFacade.emit(InPlaceMutableHashTable.java:1060)
2020-11-03T20:15:40.9826721Z 	at org.apache.flink.runtime.operators.chaining.ChainedReduceCombineDriver.close(ChainedReduceCombineDriver.java:269)
2020-11-03T20:15:40.9827201Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:517)
2020-11-03T20:15:40.9827637Z 	at org.apache.flink.runtime.iterative.task.AbstractIterativeTask.run(AbstractIterativeTask.java:159)
2020-11-03T20:15:40.9828152Z 	at org.apache.flink.runtime.iterative.task.IterationIntermediateTask.run(IterationIntermediateTask.java:107)
2020-11-03T20:15:40.9828606Z 	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:370)
2020-11-03T20:15:40.9828998Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-11-03T20:15:40.9829371Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-11-03T20:15:40.9829680Z 	at java.lang.Thread.run(Thread.java:748)
{code}

Both threads ran into this. It seems that the {{LocalBufferPool}} ran out of buffers?

[~AHeise] can you take a look?;;;","04/Nov/20 14:29;arvid;[~roman_khachatryan] is looking into it.;;;","05/Nov/20 10:54;roman;I assumed at first that it's caused by my recent addition of waiting for EndOfChannelState event.

But git bisect gave a rather old commit:
{code:java}
e17dbab24f4f71c5472d27267e938791686e45c3 is the first bad commit
commit e17dbab24f4f71c5472d27267e938791686e45c3
Author: Arvid Heise <arvid@ververica.com>
Date:   Fri Sep 25 14:39:17 2020 +0200

    [FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.

    Before this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).

    The solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.

    Additionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.

    Finally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.

:040000 040000 1331ab5652c4bfbdbed02576f4e57a87ccaa1170 4f767acd0262ba07eefbdee6b8bd717ef1957765 M      flink-runtime

{code}
Reverting it on master solves the problem.

The failure itself doesn't happen all the time. Also adding logging or running in debug mode prevents it.

Given that, and that it's quite old, I'd lower the priority. WDYT [~rmetzger], [~pnowojski]?;;;","05/Nov/20 10:58;pnowojski;Thanks for investigation [~roman_khachatryan]. FLINK-16972 Is not that old, it was only merged in 1.12, so we should keep this ticket as release blocker. Otherwise we might release 1.12 with some serious new bug.;;;","05/Nov/20 11:07;rmetzger;Yes, let's keep it as release blocker and fix it asap. All iterative batch jobs are probably affected by this.;;;","05/Nov/20 16:01;rmetzger;[~roman_khachatryan] Is this blocker expected to be resolved before the feature freeze on Sunday?
;;;","05/Nov/20 22:25;roman;I'm not sure, probably not.;;;","06/Nov/20 04:03;zhuzh;We recently noticed the issue FLINK-19994 that pipelined region scheduling will eagerly schedule all the vertices in a DataSet iteration job.
[~roman_khachatryan] Is it possible that the problem is caused by downstream task allocated all available network buffers from global pool, and then the upstream task cannot obtain any buffer and get stuck? If so, I think FLINK-19994 can fix this problem.
However, I cannot reproduce the problem after 1700+ runs locally. So I'm not sure whether my guess is correct.;;;","06/Nov/20 12:24;rmetzger;Another gelly test is affected too: https://issues.apache.org/jira/browse/FLINK-20011;;;","09/Nov/20 05:54;rmetzger;The iterations deadlocks are happening quite frequently now. The commit Roman found bisecting is quite old. I believe a more recent change to the pipelined region scheduling is more likely to cause this instability.;;;","09/Nov/20 07:25;zhuzh;The change FLINK-19189 to ""enable pipelined region scheduling by default"" has been merged since 09/24, which is even older.
So possibly there are also other causes, which together result in this problem.;;;","10/Nov/20 15:08;rmetzger;You are right. Given the frequency of these deadlocks, something must have been merged in the last week that triggers this.;;;","10/Nov/20 15:19;arvid;I can confirm [~roman_khachatryan]'s find that this is caused by e17dbab24f4f71c5472d27267e938791686e45c3. It's easy to produce locally.
I'm assuming recent commits just changed the timing and made it more likely to appear.

However, it's also interesting that locally, the test only gets stuck after >10 tests are run on the same mini cluster. So it may also be related to some memory leaks.;;;","10/Nov/20 19:25;arvid;Okay here is what happens:
* Two {{LocalBufferPools}} are concurrently destroyed.
* The second {{LocalBufferPool}} still gets buffer assigned while the first one is destroyed in {{NetworkBufferPool}} (always has been like this).
* With the change in e17dbab24f4f71c5472d27267e938791686e45c3, the second {{LocalBufferPool}} proactively acquires one of the assigned buffers.
* Since it already has been destroyed this buffer is never returned to the {{NetworkBufferPool}} and simply vanishes from heap.
* With each repeated test run, the {{NetworkBufferPool}} of the same mini-cluster has less buffers available (1 less for each concurrent release).
* Eventually, there are too few buffers available to progress.

So there is a serious bug happening that needs to be fixed asap. The two obvious choices are either to not assign buffers to destroyed pools (which is expensive to detect because of different threads) and to not pro-actively take a buffer on destroyed local pool (easy to implement = old behavior).

There could also be a follow-up work related to why fewer available buffer cause a deadlock instead of a fast failure.

Note that the recent changes might have caused more concurrent releases of pools and thus made the issue visible.
Note2: no dev probably executed one of the tests locally in the last 3 months. (Yes, they happen that often locally!);;;","10/Nov/20 19:30;rmetzger;Thanks a lot for your analysis. I still don't understand why the issue didn't occur earlier. It has been merged at the beginning of October, but we saw the first failures in November, and that quite frequently. ;;;","10/Nov/20 19:31;rmetzger;Okay, it seems that you've edited your comment while I posted :) ;;;","11/Nov/20 06:36;arvid;Yes sorry about that, but I realized that exactly that question was unanswered ;).

Merged the fix into master as 18ffebb3dbecc21d2d33c436628176c4971cebbd. Backport not applicable.;;;","11/Nov/20 07:15;rmetzger;Thanks a lot for the fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple input creation algorithm will deduce an incorrect input order if the inputs are related under PIPELINED shuffle mode,FLINK-19959,13338765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,04/Nov/20 04:34,05/Nov/20 06:40,13/Jul/23 08:12,05/Nov/20 06:40,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Consider the following SQL
{code:sql}
WITH
  T1 AS (SELECT x.a AS a, y.d AS b FROM y LEFT JOIN x ON y.d = x.a),
  T2 AS (SELECT a, b FROM (SELECT a, b FROM T1) UNION ALL (SELECT x.a AS a, x.b AS b FROM x))
SELECT * FROM T2 LEFT JOIN t ON T2.a = t.a
{code}

The multiple input creation algorithm will currently deduce the following plan under the PIPELINED shuffle mode:
{code}
MultipleInputNode(readOrder=[0,1,1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, b, a0, b0, c], build=[right])\n:- Union(all=[true], union=[a, b])\n:  :- Calc(select=[a, CAST(d) AS b])\n:  :  +- NestedLoopJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, a], build=[right])\n:  :     :- [#3] Calc(select=[d])\n:  :     +- [#4] Exchange(distribution=[broadcast])\n:  +- [#2] Calc(select=[a, b])\n+- [#1] Exchange(distribution=[broadcast])\n])
:- Exchange(distribution=[broadcast])
:  +- BoundedStreamScan(table=[[default_catalog, default_database, t]], fields=[a, b, c])
:- Calc(select=[a, b])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx], reuse_id=[1])
:- Calc(select=[d])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f, ny)]]], fields=[d, e, f, ny])
+- Exchange(distribution=[broadcast])
   +- Calc(select=[a])
      +- Reused(reference_id=[1])
{code}

It's obvious that the 2nd and the 4th input for the multiple input node should have the same input priority, otherwise a deadlock will occur.

This is because the current algorithm fails to consider the case when the inputs are related out of the multiple input node.",,godfreyhe,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 06:40:35 UTC 2020,,,,,,,,,,"0|z0k9so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 06:40;godfreyhe;master: 854f523a76edc4c0dbfe9f2691823b91499f3590;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling NOW() function throws compile exception,FLINK-19948,13338581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,03/Nov/20 08:00,02/Jun/21 08:33,13/Jul/23 08:12,04/Nov/20 06:54,,,,,,1.11.3,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The following test code in {{ScalarOperatorsTest}} will fail with a compile exception

{code:scala}
testSqlApi(""CAST(NOW() AS BIGINT)"", ""??"")
{code}

{code}
java.lang.RuntimeException: Could not instantiate generated class 'TestFunction$24'

	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:57)
	at org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.evaluateExprs(ExpressionTestBase.scala:143)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52)
	... 25 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
	... 27 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 30 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 64, Column 22: Assignment conversion not possible from type ""long"" to type ""org.apache.flink.table.data.TimestampData""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78)
	... 36 more
{code}

This should be a bug in the code generation for {{NOW()}} function.",,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19488,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 02:12:34 UTC 2020,,,,,,,,,,"0|z0k8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 02:12;jark;Fixed in 
 - master (1.12.0): 6c1b3b67ef1d2401ece3bea9fc53b78ab36c3239
 - 1.11.3: 0a38f7362e9f83656e8bc233711f7d0886fbb237;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task names on web UI should change when an operator chain is chained with sources,FLINK-19940,13338558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,TsReaper,TsReaper,03/Nov/20 06:33,24/Nov/20 08:38,13/Jul/23 08:12,24/Nov/20 08:38,,,,,,1.12.0,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,Currently task names on web UI doesn't change even if the operator chain is chained with sources. We should change its name to show that the sources are chained.,,aljoscha,godfreyhe,jark,pnowojski,roman,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/20 02:19;TsReaper;multi-web-ui.png;https://issues.apache.org/jira/secure/attachment/13015588/multi-web-ui.png","20/Nov/20 13:50;aljoscha;with-multi-source-chaining.png;https://issues.apache.org/jira/secure/attachment/13015714/with-multi-source-chaining.png","20/Nov/20 13:50;aljoscha;without-chaining.png;https://issues.apache.org/jira/secure/attachment/13015715/without-chaining.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 08:38:07 UTC 2020,,,,,,,,,,"0|z0k8iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/20 06:34;TsReaper;cc [~pnowojski] [~AHeise];;;","13/Nov/20 19:22;roman;While implementing the fix I [found|https://github.com/apache/flink/pull/14056#issuecomment-726595759] that name is supplied to MultipleInputTransformation by ""client"" code, and blink already has some meaningful name (it uses explain plain functionality).
 So I'm wondering what was the setup, can you describe it [~TsReaper]?;;;","17/Nov/20 10:34;TsReaper;[~roman_khachatryan] The ""explain"" functionality does not have any specific logic for chained source. The following is an example for sql {{SELECT * FROM chainable LEFT JOIN x ON chainable.a = x.a}}, where ""chainable"" is a chainable source.

{code}
== Abstract Syntax Tree ==
LogicalProject(a=[$0], a0=[$1], b=[$2], c=[$3], nx=[$4])
+- LogicalJoin(condition=[=($0, $1)], joinType=[left])
   :- LogicalTableScan(table=[[default_catalog, default_database, chainable]])
   +- LogicalTableScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]])

== Optimized Logical Plan ==
MultipleInput(readOrder=[1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, a0, b, c, nx], build=[right])\n:- [#1] BoundedStreamScan(table=[[default_catalog, default_database, chainable]], fields=[a])\n+- [#2] Exchange(distribution=[broadcast])\n])
:- BoundedStreamScan(table=[[default_catalog, default_database, chainable]], fields=[a])
+- Exchange(distribution=[broadcast])
   +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx])

== Physical Execution Plan ==
 : Data Source
	content : Source: chainable

	 : Operator
		content : SourceConversion(table=[default_catalog.default_database.chainable], fields=[a])
		ship_strategy : FORWARD

 : Data Source
	content : Source: Collection Source

	 : Operator
		content : SourceConversion(table=[default_catalog.default_database.x, source: [TestTableSource(a, b, c, nx)]], fields=[a, b, c, nx])
		ship_strategy : FORWARD

		 : Operator
			content : MultipleInput(readOrder=[1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, a0, b, c, nx], build=[right])\n:- [#1] BoundedStreamScan(table=[[default_catalog, default_database, chainable]], fields=[a])\n+- [#2] Exchange(distribution=[broadcast])\n])
			ship_strategy : FORWARD


{code};;;","17/Nov/20 10:42;roman;Yes, exactly.

The question is, whether this plan is what is shown in the web UI?

(it's possible to construct a transformation without using it).;;;","18/Nov/20 11:55;TsReaper;[~roman_khachatryan] No. The plan shown on the web UI is different. For the task containing multiple input task, it will only show {{MultipleInput(readOrder=[1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, a0, b, c, nx], build=[right])\n:- [#1] BoundedStreamScan(table=[[default_catalog, default_database, chainable]], fields=[a])\n+- [#2] Exchange(distribution=[broadcast])\n])}}. If this multiple input operator is a predecessor of a {{Calc}} then the web UI will display {{MultipleInput(...) -> Calc(...)}}. It shows nothing about the chained source.;;;","18/Nov/20 14:38;roman;So it looks like this name is generated by blink, and the truncation also happens there, right?

 

cc: [~jark], [~godfreyhe];;;","18/Nov/20 15:45;jark;Hi [~TsReaper], I think it would be easier to understand if you can show what is displayed currently and what do you want to display. ;;;","19/Nov/20 02:29;TsReaper;[~roman_khachatryan] This is what is currently displayed on the web and what I'm expecting.

 !multi-web-ui.png! 
;;;","19/Nov/20 13:53;roman;Thanks for sharing this. So the issue is that name in the rectangle doesn't match the description, right?

 

On the server side, they are generated by the code in most cases (JobVertex.name or prettyName).

In the UI,  name in the rectangle is truncated to 300 symbols (flink-runtime-web/web-dashboard/src/app/share/common/dagre/node.component.ts).

This is I think what happens here: the name is just truncated.

Which seems reasonable to me, I don't think we should change this.

 

cc: [~aljoscha];;;","19/Nov/20 14:12;aljoscha;Isn't the problem that the {{TableSourceScan(...}} part is not shown?;;;","19/Nov/20 14:25;roman;I think so. [~TsReaper] can you confirm?

 

But it should appear after the ""Calc"" node, which is also truncated (note there are two ""Calc"" nodes, only 1st is shown on the left).;;;","20/Nov/20 07:53;TsReaper;[~roman_khachatryan] It's not truncated. It's just that the TableSourceScan part does not appear. The task name in the detail tab on the right will not be truncated, however the TableSourceScan part still doesn't appear. The #2 TableSourceScan indicates that its an input of the multiple input operator and is included in the name of the MultipleInputOperator (we display all inputs of the multiple input operator in its name), which is not that obvious for the user to tell that there is a chained source. The whole string before ""-> Calc"" is the name of the multiple input operator, which has nothing to do with source chaining. We can just name the operator to ""MultipleInput"" and all the string before ""> Calc"" will disappear. We name the operator like so just to expose the inner structure of a multiple input operator.

Indeed, one can argue that source chaining is currently only implemented for multiple input tasks, so it seems to be sufficient that chained sources are hinted in the multiple input operators in table API. However source chaining is related more to operator chaining rather than an operator in the table API. If we implement source chaining for two-input operators in the future, or if the users write a multiple input transformation themselves, then users will have no way to tell whether there is a chained source or not. So I think we should display the chained sources on the level of operator chains.;;;","20/Nov/20 13:55;aljoscha;What [~TsReaper] is referring to is that some part of the stack cuts of the names of sources when multiple sources are chained to a multi-input operator. You can see for yourself with this example:

{code}
Configuration config = new Configuration();

config.set(RestOptions.PORT, 8081);

final StreamExecutionEnvironment env =
		StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(config);

env.getConfig().enableObjectReuse();

env.setParallelism(1);

final FileSource<String> source = FileSource
		.forRecordStreamFormat(new TextLineFormat(), Path.fromLocalFile(new File(""/Users/aljoscha/Downloads/some-files"")))
		.monitorContinuously(Duration.ofMillis(5))
		.build();

DataStreamSource<String> source1 = env.fromSource(source, WatermarkStrategy.noWatermarks(), ""my source 1"");
DataStreamSource<String> source2 = env.fromSource(source, WatermarkStrategy.noWatermarks(), ""my source 2"");
DataStreamSource<String> source3 = env.fromSource(source, WatermarkStrategy.noWatermarks(), ""my source 3"");

MultipleInputTransformation<String> multipleInputTransformation = new MultipleInputTransformation<>(
		""operator"",
		new StreamOperatorFactory<String>() {
			@Override
			public <T extends StreamOperator<String>> T createStreamOperator(
					StreamOperatorParameters<String> parameters) {
				return (T) new TestMultipleInputOperator(""the name"", parameters);
			}

			@Override
			public void setChainingStrategy(ChainingStrategy strategy) {}

			@Override
			public ChainingStrategy getChainingStrategy() { return ChainingStrategy.HEAD_WITH_SOURCES; }

			@Override
			public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {
				return TestMultipleInputOperator.class;
			}
		},
		BasicTypeInfo.STRING_TYPE_INFO,
		1
);
multipleInputTransformation.addInput(source1.getTransformation());
multipleInputTransformation.addInput(source2.getTransformation());
multipleInputTransformation.addInput(source3.getTransformation());

new MultipleConnectedStreams(env)
		.transform(multipleInputTransformation)
		.print();

env.execute();
{code}

When you run it in the IDE you can check {{localhost:8081}} and see for yourself. I also uploaded two screenshots that show what's happening with chaining and without chaining.;;;","23/Nov/20 15:50;roman;Thanks for the clarification [~aljoscha] and [~TsReaper].

I published a new [PR|https://github.com/apache/flink/pull/14180], can you please take a look?;;;","24/Nov/20 08:38;aljoscha;master: 542b9fd5e5614db7c7f8bbe99f7e8ef73721182a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execute and collect with limit fails on bounded datastream jobs,FLINK-19933,13338475,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,02/Nov/20 18:47,09/Nov/20 14:13,13/Jul/23 08:12,09/Nov/20 14:13,1.12.0,,,,,,,,API / DataStream,,,,,0,pull-request-available,,,,,,,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 14:12:55 UTC 2020,,,,,,,,,,"0|z0k80g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/20 14:12;sjwiesman;fixed in master: cba124a508928497a2f235fc0783c6cc4e0c809a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionStateUpdateListener is only updated when legacy scheduling is enabled,FLINK-19927,13338393,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,rmetzger,rmetzger,02/Nov/20 10:40,04/Nov/20 08:02,13/Jul/23 08:12,04/Nov/20 08:02,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"This is a finding from FLINK-19805.

The {{ExecutionDeploymentTracker}} is never notified about executions reaching terminal state, when using the default scheduler.
This can potentially lead to invalid execution reconciliation behavior.

Fixing this ticket probably involves switching the statements here: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java#L1688-L1692

As part of the this tickets resolution, I suggest to also introduce a test case.
",,azagrebin,klion26,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19805,,FLINK-19954,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 08:02:11 UTC 2020,,,,,,,,,,"0|z0k7i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 14:32;azagrebin;It seems we need unit tests for how EG integrates with ExecutionStateUpdateListener and also ExecutionDeploymentListener after FLINK-17075.

I suggest to add at least tests for integration of ExecutionStateUpdateListener with EG in the scope of this issue, e.g. in ExecutionGraphStateUpdateTest. We can address ExecutionDeploymentListener integration in other ticket.

wdyt? or was the plan to continue write more high level cases in JobMasterExecutionDeploymentReconciliationTest, [~chesnay]?;;;","02/Nov/20 14:51;chesnay;There should definitely be a test that ensures that the EG issues notifications for all state transitions, but we first of course have to decide whether the new scheduler should even rely on this mechanism. EG#notifyExecutionChange is a no-op outside of legacy scheduling, so my assumption was that the new scheduler would handle this in some other form.

In the reconcilation test we should add a case to ensure that the tracker is properly cleared when the job is suspended or, more generally, if the job terminates.;;;","03/Nov/20 10:40;azagrebin;True, the recent state handling logic resides in the new SchedulerNG, currently DefaultScheduler. The execution state handling in EG is partially inactive, like the problematic notifyExecutionChange in this issue. We could reconsider how the execution tracking for reconciliation is integrated with the scheduling. I think the tracking logic could be moved from Execution#deploy and EG#notifyExecutionChange to either SchedulerNG#updateTaskExecutionState or DefaultScheduler#deployTaskSafe. The latter looks to me currently more natural. ExecutionVertexOperations.deploy could return submission future for deployment completion in ExecutionDeploymentTracker and Execution#getTerminalFuture to stop the tracking. This would be easier to unit test as well.

Nonetheless, this is not a quick fix. The fix, which [~rmetzger] mentions in the issue description, would be quick, I already tried it:
 * Doing the tracking stop in EG#notifyExecutionChange w/o legacy scheduling check
 * Testing it in JobMasterExecutionDeploymentReconciliationTest by intercepting the tracking stop in DefaultExecutionDeploymentTracker;;;","03/Nov/20 13:36;azagrebin;After offline discussion with [~chesnay] and [~trohrmann], the decision is to proceed with the quick fix of the original intention in FLINK-17075 to stop tracking in EG#notifyExecutionChange, also for SchedulerNG. The refactoring suggestions to move tracking to DefaultScheduler are moved to the follow-up ticket FLINK-19954.;;;","04/Nov/20 08:02;azagrebin;merged into master by 375c96c0c1e3ef3555b756c90d58c17c6a2e47e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong comments of cep test,FLINK-19915,13338343,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jackylau,jackylau,jackylau,02/Nov/20 06:37,13/Apr/21 20:40,13/Jul/23 08:12,02/Nov/20 09:16,1.11.0,,,,,1.12.0,,,Library / CEP,,,,,0,pull-request-available,,,,,"{code:java}
// code placeholder

@Test
public void testNFACompilerPatternEndsWithNotFollowedBy() {

   // adjust the rule
   expectedException.expect(MalformedPatternException.class);
   expectedException.expectMessage(""NotFollowedBy is not supported as a last part of a Pattern!"");

   Pattern<Event, ?> invalidPattern = Pattern.<Event>begin(""start"").where(new TestFilter())
      .followedBy(""middle"").where(new TestFilter())
      .notFollowedBy(""end"").where(new TestFilter());

   // here we must have an exception because of the two ""start"" patterns with the same name.
   compile(invalidPattern, false);
}

{code}
 

 

// here we must have an exception because of the two ""start"" patterns with the same name.

It is not right",,dwysakowicz,jackylau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 09:16:58 UTC 2020,,,,,,,,,,"0|z0k774:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 06:58;jackylau;https://github.com/apache/flink/pull/13874;;;","02/Nov/20 09:16;dwysakowicz;Fixed in f84acf6de3a46f5439bcc43120011c527dbd47a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TemporalJoinITCase.testEventTimeTemporalJoinChangelogUsingBeforeTime is instable,FLINK-19914,13338329,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,dian.fu,dian.fu,02/Nov/20 02:14,12/Nov/20 15:25,13/Jul/23 08:12,12/Nov/20 15:25,1.12.0,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8714&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=39857031-7f0c-5fd5-d730-a19c5794f839

{code}

[ERROR] testEventTimeTemporalJoinChangelogUsingBeforeTime[StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase)  Time elapsed: 0.225 s  <<< FAILURE!
java.lang.AssertionError: expected:<List(+I(1,Euro,12,2020-08-15T00:01,null,null), +I(2,US Dollar,1,2020-08-15T00:02,102,2020-08-15T00:00:02), +I(3,RMB,40,2020-08-15T00:03,702,2020-08-15T00:00:04), +I(4,Euro,14,2020-08-16T00:04,118,2020-08-16T00:01), +I(5,RMB,40,2020-08-16T00:03,null,null), +I(6,RMB,40,2020-08-16T00:04,null,null), +U(2,US Dollar,18,2020-08-16T00:03,106,2020-08-16T00:02), -D(6,RMB,40,2020-08-16T00:04,null,null), -U(2,US Dollar,1,2020-08-16T00:03,106,2020-08-16T00:02))> but was:<ArrayBuffer(+I(1,Euro,12,2020-08-15T00:01,null,null), +I(2,US Dollar,1,2020-08-15T00:02,null,null), +I(3,RMB,40,2020-08-15T00:03,null,null), +I(4,Euro,14,2020-08-16T00:04,118,2020-08-16T00:01), +I(5,RMB,40,2020-08-16T00:03,null,null), +I(6,RMB,40,2020-08-16T00:04,null,null), +U(2,US Dollar,18,2020-08-16T00:03,106,2020-08-16T00:02), -D(6,RMB,40,2020-08-16T00:04,null,null), -U(2,US Dollar,1,2020-08-16T00:03,106,2020-08-16T00:02))>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.testEventTimeTemporalJoinChangelogUsingBeforeTime(TemporalJoinITCase.scala:498)
{code}",,dian.fu,jark,leonard,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 15:25:46 UTC 2020,,,,,,,,,,"0|z0k740:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 02:14;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8714&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a;;;","02/Nov/20 05:31;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8736&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8736&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53;;;","02/Nov/20 07:33;mapohl;And another one: https://dev.azure.com/mapohl/flink/_build/results?buildId=102&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=5129dea2-618b-5c74-1b8f-9ec63a37a8a6;;;","02/Nov/20 09:05;jark;Temporarily disable the unstable tests in master: bfc8a9e510a9616f88b087e3e0bf5c39a7ffad11
Will fix the unstable tests later. ;;;","10/Nov/20 12:55;dian.fu;Any update on this issue? ;;;","10/Nov/20 13:55;leonard;[~dian.fu] I'm working to fix this one;;;","11/Nov/20 01:06;dian.fu;[~Leonard Xu] Thanks a lot~;;;","12/Nov/20 15:25;jark;Fixed in master: 1c89ab369f24a321a7ce54f4d29f79da54dc0191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink application in attach mode could not terminate when the only job is canceled,FLINK-19909,13338278,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,wangyang0918,wangyang0918,01/Nov/20 07:45,06/Nov/20 09:25,13/Jul/23 08:12,06/Nov/20 08:55,1.11.3,1.12.0,,,,1.11.3,1.12.0,,Deployment / Kubernetes,Deployment / YARN,Runtime / Coordination,,,0,pull-request-available,,,,,"Currently, the Yarn and Kubernetes application in attach mode could not terminate the Flink cluster after the only job is canceled. Because we are throwing {{ApplicationExecutionException}} in {{ApplicationDispatcherBootstrap#runApplicationEntryPoint}}. However, we are only checking {{ApplicationFailureException}} in {{runApplicationAndShutdownClusterAsync}}. Then we will go to fatal error handler which make the jobmanager directly exits. And it has no chance to deregister itself to the cluster manager(Yarn/Kubernetes). That means the jobmanager will be relaunched by cluster manager again and again until it exhausts the retry attempts.

 

cc [~kkl0u], I am not sure is this an expected change? I think it could work in 1.11.",,kkl0u,rmetzger,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19154,,,,,,,,,,,,,,,"02/Nov/20 05:54;wangyang0918;log.jm;https://issues.apache.org/jira/secure/attachment/13014534/log.jm",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 08:55:59 UTC 2020,,,,,,,,,,"0|z0k6so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/20 09:54;kkl0u;Thanks for opening this [~fly_in_gis]! I changed this after a comment on my PR (https://github.com/apache/flink/pull/13699) after a comment during review. Before, in this case the error handler I was using was completing the shutdown future of the {{Dispatcher}} exceptionally (see https://github.com/apache/flink/pull/13699#discussion_r508494946).

BTW if the job gets cancelled, shouldn't we go throw [here|https://github.com/kl0u/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L280], which is expected to put the correct exception?;;;","01/Nov/20 10:17;wangyang0918;[~kkl0u] Thanks for your response. Hmm. Actually, I find this issue when I am testing K8s native HA. I have two questions here.
 * Why we are throwing {{ApplicationExecutionException}} in {{ApplicationDispatcherBootstrap#runApplicationEntryPoint}}, not {{ApplicationFailureException}}? If we submit the Flink application with {{execution.attached: true}}, and we cancel the job, then we still go into the fatal error handler. Because we only try to find the {{ApplicationFailureException}}.
 * Using the {{ClusterEntrypoint#onFatalError}} is not reasonable, it will make us have no chance to deregister the Flink application.;;;","01/Nov/20 12:57;kkl0u;[~fly_in_gis] We want different behaviour when we cancel a job (delete HA data) and when a job fails (restart). So even if we throw {{ApplicationFailureException}} in the {{runApplicationEntryPoint}}, in the case of cancellation it has to contain the {{Status}} of the job as {{Cancelled}} when the job gets cancelled. 

In your case, you see a {{JobCancellationException}} being thrown in the {{runApplicationEntryPoint()}} ?;;;","02/Nov/20 05:55;wangyang0918;[~kkl0u] I know we need to have different behaviors for canceled job or failed. My point here is for ""detached"" and ""attached"" mode, we have different exception. I have uploaded a jobmanager log, which could show that even the job is canceled, we still go into the fatal error handle.

In detached mode, it works quite well.;;;","02/Nov/20 08:21;kkl0u;Thanks for the logs [~fly_in_gis], I am having a look now.;;;","02/Nov/20 08:54;trohrmann;I think the behaviour should not be different if the job is canceled or has failed terminally. In both cases, the HA data should be cleaned up and the cluster should shut down. Only if the system encounters a framework exception we should call the fatal error handler.;;;","02/Nov/20 09:14;kkl0u;Yes [~trohrmann] I phrased it wrongly before. With the ""restart"" in parenthesis I meant transient failures like framework failures from which the application is expected to restart. I am working on a fix and ping you or [~fly_in_gis] for a review.;;;","03/Nov/20 03:16;xtsong;Hi folks, thanks for reporting this issue and the discuss.

[~kkl0u], any estimation when this could be fixed?;;;","03/Nov/20 18:52;kkl0u;There is an open PR [~xintongsong]. It would be great if [~fly_in_gis] who reported it could have a look and verify that this fixes it.;;;","04/Nov/20 02:00;xtsong;Sounds great, thanks [~kkl0u].;;;","05/Nov/20 15:59;rmetzger;[~fly_in_gis] What's the status here?
Is this blocker expected to be merged before the feature freeze on Sunday?
;;;","05/Nov/20 16:40;trohrmann;I've reviewed the PR and I guess that it will be merged before the feature freeze [~rmetzger].;;;","06/Nov/20 03:06;wangyang0918;Sorry for the late response. I have gone through the PR and verified in a K8s cluster with application attach/detach mode. It works well.;;;","06/Nov/20 08:55;kkl0u;master: 2422fb5fc9da3aac95bbd52a6cac7d11479c1ae1
1.11: eb5914419a33a7358384bfc20d6dda0259baff0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Channel state (upstream) can be restored after emission of new elements (watermarks),FLINK-19907,13338219,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,31/Oct/20 16:56,22/Jun/21 13:55,13/Jul/23 08:12,03/Nov/20 12:12,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Runtime / Network,,,,,0,pull-request-available,,,,,"In StreamTask.beforeInvoke:

1. operatorChain.initializeStateAndOpenOperators(createStreamTaskStateInitializer());

2. readRecoveredChannelState();

 But operatorChain.initializeStateAndOpenOperators can emit watermarks (or potentially some other stream elements).

 I've encountered this issue while adding an EndOfRecovery marker - in some runs of in OverWindowITCase.testRowTimeBoundedPartitionedRangeOver the marker was emitted after the watermark.

 

cc: [~zjwang], [~pnowojski]",,AHeise,roman,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 12:12:37 UTC 2020,,,,,,,,,,"0|z0k6fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 08:50;zjwang;I am not quite sure the actual semantic for the new emitted elements while initializing operator state here. But I think we should consider two issues for guaranteeing the precision.

* Determinacy for repeated restore: That means the behavior should be consistent while executing the state restore multiple times.
* Consistency with normal running: Assume the new emitted elements also exist while state snapshot, what is the sequence between them and in-flight channel state, then we should also obey the same sequence after restoring.;;;","02/Nov/20 09:30;arvid;[~roman_khachatryan]'s PR fixes the second case while maintaining the first.

The issue is that the watermarks are already emitted from a state that is after the in-flight data of the output and input.;;;","02/Nov/20 09:30;arvid;Merged into master as 3ab31a0473c01c27feb3b64c3f379ebe481a71ce.;;;","03/Nov/20 12:12;arvid;Also merged in release-1.11 as 8addf891ad97ce83bb22c559aef28411f3953014. Closing issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result when compare two binary fields,FLINK-19906,13338218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hailong wang,hailong wang,hailong wang,31/Oct/20 16:19,17/Nov/20 14:37,13/Jul/23 08:12,17/Nov/20 14:37,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Currently, we use `Arrays.equals()` function to compare two binary fields in ScalarOperatorGens#generateComparison.
This will  lead to the Incorrect result in '<', '>', '>=', '<=' operator.",,hailong wang,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 04:54:51 UTC 2020,,,,,,,,,,"0|z0k6fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 02:06;jark;Fixed in 
 - master (1.12.0): 92c4fafd84ad31a225fd768273474d1977ed7f84
 - 1.11.3: 98ae79056929ef0a17968b26619dbd3d92274af1;;;","17/Nov/20 02:15;jark;[~hailong wang], could you create a pull request for release-1.11? ;;;","17/Nov/20 03:20;hailong wang;okay [~jark], I will do it later.;;;","17/Nov/20 04:54;hailong wang;Created for release-1.11~

https://github.com/apache/flink/pull/14090;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Jdbc-connector's 'lookup.max-retries' option initial value is 1 in JdbcLookupFunction,FLINK-19905,13338200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,873925389@qq.com,lsy,lsy,31/Oct/20 08:25,20/Dec/20 02:51,13/Jul/23 08:12,19/Dec/20 14:48,1.11.2,1.11.3,,,,1.13.0,,,Connectors / JDBC,,,,,0,pull-request-available,,,,,"As describe in FLINK-19684, we should init the begin value to 0 in JdbcLookupFunction. The PR only correct the retry value to 0 in JdbcRowDataLookupFunction and JdbcBatchingOutputFormat class, maybe the contributor forgets to correct it in JdbcLookupFunction class.",,hailong wang,jark,libenchao,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 19 14:48:49 UTC 2020,,,,,,,,,,"0|z0k6bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/20 08:26;lsy;CC [~jark];;;","31/Oct/20 12:16;hailong wang;Thanks [~lsy] for reporting this. For `JdbcLookupFunction ` has be deprecated, so we don't need to do that?;;;","01/Nov/20 03:26;jark;I think it's in a low priority to fix the legacy {{JdbcLookupFunction}}. But it's welcome to contribute a bug fix. ;;;","14/Dec/20 07:57;873925389@qq.com;hi,[~jark], i will  contribute a bug fix. This is a valuable experience to understand the pr process for me ，now i will create a pr;;;","19/Dec/20 14:48;jark;Fixed in master: cc9ccc42e8c8f7221c6d35f4c011d573577d0505;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to exclude metrics variables for the last metrics reporter.,FLINK-19901,13338051,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,kien_truong,kien_truong,30/Oct/20 10:56,03/Nov/20 18:18,13/Jul/23 08:12,03/Nov/20 18:18,1.10.0,,,,,1.10.3,1.11.3,1.12.0,Runtime / Metrics,,,,,0,pull-request-available,,,,,"We discovered a bug that leads to the setting {{scope.variables.excludes}} being ignored for the very last metric reporter.

Because {{reporterIndex}} was incremented before the length check, the last metrics reporter setting is overflowed back to 0.

Interestingly, this bug does not trigger when there's only one metric reporter, because slot 0 is actually overwritten with that reporter's variables instead of being used to store all variables in that case.
{code:java}
public abstract class AbstractMetricGroup<A extends AbstractMetricGroup<?>> implements MetricGroup {
...
	public Map<String, String> getAllVariables(int reporterIndex, Set<String> excludedVariables) {
		// offset cache location to account for general cache at position 0
		reporterIndex += 1;
		if (reporterIndex < 0 || reporterIndex >= logicalScopeStrings.length) {
			reporterIndex = 0;
		}
		// if no variables are excluded (which is the default!) we re-use the general variables map to save space
		return internalGetAllVariables(excludedVariables.isEmpty() ? 0 : reporterIndex, excludedVariables);
	}

...
{code}
 [Github link to the above code|https://github.com/apache/flink/blob/3bf5786655c3bb914ce02ebb0e4a1863b205b829/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/groups/AbstractMetricGroup.java#L122]",,kien_truong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 18:18:01 UTC 2020,,,,,,,,,,"0|z0k5eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/20 11:00;kien_truong;[~chesnay] Can you take look ? Thanks.;;;","03/Nov/20 18:18;chesnay;master: 477d37d503b76cf03131b4b8d3b3a75f86c55ba0
1.11: cdd537c0fc2fca306b19b79b99cc6ab31217c85b
1.10: 85fc5c1adb1a4e4fe2427f05f8493459099c1825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong property name for surefire log4j configuration,FLINK-19900,13338050,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,30/Oct/20 10:37,13/Apr/21 20:40,13/Jul/23 08:12,02/Nov/20 08:46,,,,,,1.12.0,,,Build System,,,,,0,pull-request-available,,,,,"Log4j uses {{log4j.configurationFile}} system property for passing a configuration file. In our surefire configuration we use {{log4j.configuration}} property instead which has no effect.

{code}
<plugin>
	<groupId>org.apache.maven.plugins</groupId>
	<artifactId>maven-surefire-plugin</artifactId>
	<version>2.22.1</version>
	<configuration>
         ....
		<systemPropertyVariables>
                ....
			<log4j.configuration>${log4j.configuration}</log4j.configuration>
		</systemPropertyVariables>
                ....
         </configuration>
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 08:46:14 UTC 2020,,,,,,,,,,"0|z0k5e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/20 10:48;chesnay;We can just remove it; the log4j file is picked up automatically in both maven and the IDE, and on CI we explicitly use the correct log4j property.;;;","02/Nov/20 08:46;dwysakowicz;Fixed in 46b0f8b05b263225cdcbd4769ba19aebc3f27bb7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use iloc for positional slicing instead of direct slicing in from_pandas,FLINK-19894,13338024,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,30/Oct/20 07:55,02/Nov/20 02:36,13/Jul/23 08:12,02/Nov/20 02:35,1.11.0,1.12.0,,,,1.11.3,1.12.0,,API / Python,,,,,0,pull-request-available,,,,,"When you use floats are index of pandas, it produces a wrong results:

 
{code:java}
>>> import pandas as pd
>>> t_env.from_pandas(pd.DataFrame({'a': [1, 2, 3]}, index=[2., 3., 4.])).to_pandas()
   a
0  1
1  2
{code}
 

This is because direct slicing uses the value as index when the index contains floats:

 
{code:java}
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:]
     a
2.0  1
3.0  2
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:]
     a
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:]
   a
4  3{code}
 ",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 02:35:40 UTC 2020,,,,,,,,,,"0|z0k58g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 02:35;dian.fu;Fixed in 
- master via ea43832f2304561d0df29cacbb347182ad03db6f
- release-1.11 via 6b1f7e0dde1b89880634baf4836244aba9356691;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalarOperatorGens should generate a not-null type for IS NULL and IS NOT NULL,FLINK-19891,13338004,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,30/Oct/20 05:57,04/Nov/20 03:40,13/Jul/23 08:12,04/Nov/20 03:40,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Otherwise, calling a function expecting a not null boolean argument will fail with argument type mismatch.",,jark,libenchao,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 03:40:05 UTC 2020,,,,,,,,,,"0|z0k540:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 03:40;lzljs3620320;master (1.12): a2eb2c10c87981dcaf118b8ccc1a2c0ae26058e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ignore-parse-errors not work for the legacy JSON format,FLINK-19880,13337841,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cxiiiiiii,MingZhangOk,MingZhangOk,29/Oct/20 09:45,18/Dec/20 15:11,13/Jul/23 08:12,18/Dec/20 15:11,1.11.2,,,,,1.12.1,1.13.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,starter,,,,"when I call
 new Json(). ignoreParseErrors (true)
 When used to ignore exceptions, the following exceptions always occur:
 org.apache.flink. formats.json.JsonRowFormatFactory
 Unsupported property keys：
 format.ignore-parse-errors
 Modify this line of code to solve my problem .

 Add a line of code after 52 lines in this class(org.apache.flink.formats.json.JsonRowFormatFactory):
properties.add(JsonValidator.FORMAT_IGNORE_PARSE_ERRORS);


 First time to participate in flink submission code, do not understand what rules, directly submitted, sorry",,cxiiiiiii,jark,leonard,MingZhangOk,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 18 15:11:00 UTC 2020,,,,,,,,,,"0|z0k43s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/20 10:42;jark;Hi [~MingZhangOk], I think this might be a bug in Json format. However, the {{Json()}} descriptor API is not recommended to use at this point. It is legacy feature and may has some bugs in it. This will be refactored by FLIP-129 in the next release.

It is recommend to use DDL to register tables which is more stable and powerful. ;;;","17/Dec/20 05:51;jark;The problem lies in the {{JsonRowFormatFactory#supportedFormatProperties}} forgets to include {{JsonValidator.FORMAT_IGNORE_PARSE_ERRORS}}.
So the fix should be very simple and we can add a test to verify this in {{JsonRowFormatFactoryTest}}.;;;","17/Dec/20 06:35;cxiiiiiii;Hi [~jark] ,I found that there are two places to fix, one is to add support for FORMAT_IGNORE_PARSE_ERRORS as you said, but also need to handle the return value when parsing fails in JsonRowDeserializationSchema#deserialize, which currently returns null, so that the processing in source will be more in line with expectations, so, I intend to return Row.of(new Row(1)) instead of null.

Please assign this issue to me.

 

Best, xiao cai;;;","17/Dec/20 12:36;jark;What value do you suggest to return?  IIUC, returning null matches the expected behavior documented in the docs: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/json.html#json-ignore-parse-errors;;;","17/Dec/20 13:21;cxiiiiiii;hi [~jark] I looked carefully at the KafkaDeserializationSchema#deserialize will make a null determination on the deserialized message and only handles the non-null values. So I think it is ok to return null.;;;","17/Dec/20 13:36;jark;Yes [~cxiiiiiii].;;;","18/Dec/20 15:11;jark;Fixed in
 - master: fc660274a5c7b4cdea00335cac87bdf5d421db92
 - release-1.12: 3bfb2ab611acff9f0db28f95878032c80f3ae815;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Csv Serialization schema contains line delimiter,FLINK-19868,13337795,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,29/Oct/20 04:04,07/Nov/20 05:43,13/Jul/23 08:12,06/Nov/20 03:30,,,,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"CsvRowSerializationSchema.serialize(Row.of(""f0"", ""f1"")) => f0,f1\n

Csv Serialization schema is for one line, why contains line delimiter?",,caozhen1937,jark,liufangliang,lzljs3620320,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 03:30:05 UTC 2020,,,,,,,,,,"0|z0k3tk:",9223372036854775807,"The 'csv.line-delimiter' option has been removed from CSV format. Because the line delimiter should be defined by the connector instead of format. If users have been using this option in previous Flink version, they should alter such table to remove this option when upgrading to Flink 1.12. There should not much users using this option. ",,,,,,,,,,,,,,,,,,,"29/Oct/20 04:09;lzljs3620320;Now the filesystem wants to reuse serialization schema, but the behavior of each format is inconsistent. For example, the line separator is written in CSV, but JSON does not.;;;","29/Oct/20 04:16;jark;I think {{csv.line-delimiter}} is not a good option for csv format, the {{line-delimiter}} should be an option of filesystem, because it is never used for message queues. 
For the row-wise formats, it should consume and produce the bytes for a row. The line splitting should be handled by connectors. 

I also searched the mailing list, and I didn't find any user asking questions about {{csv.line-delimiter}}, only some people are asking {{csv.field-delimiter}}. 
Therefore, I think we can remove {{csv.line-delimiter}} option (mention this in release note) and not append a new line in {{CsvRowDataSerializationSchema}}. 

What do you think?;;;","30/Oct/20 03:51;liufangliang;+1;;;","06/Nov/20 03:30;jark;Fixed in master: 2d12d325d77e4727cb74d8a58597eb0ce9ab5aa2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation fails for UDF that accepts var-args,FLINK-19867,13337794,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,29/Oct/20 03:57,05/Nov/20 02:15,13/Jul/23 08:12,04/Nov/20 13:53,,,,,,1.11.3,1.12.0,,Table SQL / API,,,,,0,pull-request-available,,,,,,,lirui,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 13:53:09 UTC 2020,,,,,,,,,,"0|z0k3tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 13:53;twalthr;Fixed in 1.12.0: 36c3326e129b64f29dbe0653b984e99870261c87
Fixed in 1.11.3: f488e700d9e80909640776b1d33975f85153990d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FunctionsStateBootstrapOperator.createStateAccessor fails due to uninitialized runtimeContext,FLINK-19866,13337793,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,molin,molin,molin,29/Oct/20 03:46,04/Nov/20 08:51,13/Jul/23 08:12,04/Nov/20 08:51,statefun-2.2.0,,,,,statefun-2.2.1,,,Stateful Functions,,,,,0,pull-request-available,,,,,"It has bugs similar to [FLINK-19330|https://issues.apache.org/jira/browse/FLINK-19330]

In Flink 1.11.2, statefun-flink-state-processor 2.2.0, the AbstractStreamOperator's runtimeContext is not fully initialized when executing
 AbstractStreamOperator#intializeState()
in particular KeyedStateStore is set after intializeState was finished.
See:
[https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java#L258,L259]
This behaviour was changed from Flink 1.10->Flink 1.11.

StateFun's FunctionsStateBootstrapOperator performs its initialization logic at initalizeState, and it requires an already initialized runtimeContext to create stateAccessor.

This situation causes the following failure: 
{code:java}
Caused by: java.lang.NullPointerException: Keyed state can only be used on a 'keyed stream', i.e., after a 'keyBy()' operation.Caused by: java.lang.NullPointerException: Keyed state can only be used on a 'keyed stream', i.e., after a 'keyBy()' operation. at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:75) at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.checkPreconditionsAndGetKeyedStateStore(StreamingRuntimeContext.java:223) at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:188) at org.apache.flink.statefun.flink.core.state.FlinkState.createFlinkStateAccessor(FlinkState.java:69) at org.apache.flink.statefun.flink.core.state.FlinkStateBinder.bindValue(FlinkStateBinder.java:48) at org.apache.flink.statefun.sdk.state.StateBinder.bind(StateBinder.java:30) at org.apache.flink.statefun.flink.core.state.PersistedStates.findReflectivelyAndBind(PersistedStates.java:46) at org.apache.flink.statefun.flink.state.processor.operator.StateBootstrapFunctionRegistry.bindState(StateBootstrapFunctionRegistry.java:120) at org.apache.flink.statefun.flink.state.processor.operator.StateBootstrapFunctionRegistry.initialize(StateBootstrapFunctionRegistry.java:103) at org.apache.flink.statefun.flink.state.processor.operator.StateBootstrapper.<init>(StateBootstrapper.java:39) at org.apache.flink.statefun.flink.state.processor.operator.FunctionsStateBootstrapOperator.initializeState(FunctionsStateBootstrapOperator.java:67) at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:106) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:258) at org.apache.flink.state.api.output.BoundedStreamTask.init(BoundedStreamTask.java:85) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:457) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522) at org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.mapPartition(BoundedOneInputStreamTaskRunner.java:76) at org.apache.flink.runtime.operators.MapPartitionDriver.run(MapPartitionDriver.java:103) at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:504) at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) at java.lang.Thread.run(Thread.java:748){code}
 

 

 

 ",,molin,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,19330,https://issues.apache.org/jira/browse/FLINK-19330,,,,,,,,,9223372036854775807,,,Wed Nov 04 08:51:02 UTC 2020,,,,,,,,,,"0|z0k3t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 08:51;tzulitai;statefun/master: 315718ca1d81e431204ec0384e88408dfcbcb77e
statefun/release-2.2: 653393fcfb9242d0d9169e4b2133de8bf5a08399;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"YARN tests failed with ""java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}""",FLINK-19865,13337781,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,dian.fu,dian.fu,29/Oct/20 01:56,03/Nov/20 13:42,13/Jul/23 08:12,02/Nov/20 15:59,1.12.0,,,,,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354

{code}
2020-10-28T22:58:39.4927767Z 2020-10-28 22:57:33,866 main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy for element DefaultRolloverStrategy: java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}"" java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}""
2020-10-28T22:58:39.4929252Z 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
2020-10-28T22:58:39.4929823Z 	at java.lang.Integer.parseInt(Integer.java:569)
2020-10-28T22:58:39.4930327Z 	at java.lang.Integer.parseInt(Integer.java:615)
2020-10-28T22:58:39.4931047Z 	at org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy$Builder.build(DefaultRolloverStrategy.java:137)
2020-10-28T22:58:39.4931866Z 	at org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy$Builder.build(DefaultRolloverStrategy.java:90)
2020-10-28T22:58:39.4932720Z 	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:122)
2020-10-28T22:58:39.4933446Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1002)
2020-10-28T22:58:39.4934275Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:942)
2020-10-28T22:58:39.4935029Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:934)
2020-10-28T22:58:39.4935837Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:934)
2020-10-28T22:58:39.4936605Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:552)
2020-10-28T22:58:39.4937573Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:241)
2020-10-28T22:58:39.4938429Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:288)
2020-10-28T22:58:39.4939206Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:579)
2020-10-28T22:58:39.4939885Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-10-28T22:58:39.4940490Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-10-28T22:58:39.4941087Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-10-28T22:58:39.4941733Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-10-28T22:58:39.4942534Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-10-28T22:58:39.4943154Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-10-28T22:58:39.4943820Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-10-28T22:58:39.4944540Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-10-28T22:58:39.4945199Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-10-28T22:58:39.4945858Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-10-28T22:58:39.4946426Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-10-28T22:58:39.4946965Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
2020-10-28T22:58:39.4947698Z 	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.<clinit>(ClusterEntrypoint.java:108)
{code}",,dian.fu,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 15:59:29 UTC 2020,,,,,,,,,,"0|z0k3qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/20 06:30;rmetzger;This seems to be a problem in the Hadoop 3.1.3 profile only.;;;","30/Oct/20 01:26;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8626&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354;;;","30/Oct/20 16:10;trohrmann;Could you take a look at the problem [~rmetzger]? It might be related to the rolling log file changes FLINK-8357.;;;","31/Oct/20 02:16;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8698&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354;;;","31/Oct/20 02:18;dian.fu;Upgrade to ""Blocker"" as this test case is continuously failing in Hadoop 3.1.3.;;;","31/Oct/20 07:20;rmetzger;Yes, I'll take a look.;;;","31/Oct/20 13:02;rmetzger;The error message is ""only"" printed to the "".out"" file, everything else works as expected.
I just tried it with a real Hadoop 3.1.3 cluster, and the problem occurs there as well. So this isn't just a test problem.;;;","02/Nov/20 02:09;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8714&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354;;;","02/Nov/20 05:33;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8736&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354;;;","02/Nov/20 15:59;rmetzger;Resolved in https://github.com/apache/flink/commit/133593a396ea5f0c27a240cf6413b5dba3253f72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TwoInputStreamTaskTest.testWatermarkMetrics failed with ""expected:<1> but was:<-9223372036854775808>""",FLINK-19864,13337779,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kezhuw,dian.fu,dian.fu,29/Oct/20 01:45,22/Jun/21 13:55,13/Jul/23 08:12,23/Nov/20 12:13,1.12.0,,,,,1.12.0,,,API / DataStream,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392

{code}
2020-10-28T22:40:44.2528420Z [ERROR] testWatermarkMetrics(org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest) Time elapsed: 1.528 s <<< FAILURE! 2020-10-28T22:40:44.2529225Z java.lang.AssertionError: expected:<1> but was:<-9223372036854775808> 2020-10-28T22:40:44.2541228Z at org.junit.Assert.fail(Assert.java:88) 2020-10-28T22:40:44.2542157Z at org.junit.Assert.failNotEquals(Assert.java:834) 2020-10-28T22:40:44.2542954Z at org.junit.Assert.assertEquals(Assert.java:645) 2020-10-28T22:40:44.2543456Z at org.junit.Assert.assertEquals(Assert.java:631) 2020-10-28T22:40:44.2544002Z at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkMetrics(TwoInputStreamTaskTest.java:540)


{code}",,AHeise,dian.fu,kezhuw,pnowojski,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 12:12:12 UTC 2020,,,,,,,,,,"0|z0k3q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/20 16:09;trohrmann;cc [~chesnay];;;","02/Nov/20 08:47;trohrmann;[~pnowojski] said that he is watching this ticket as well and might take a look once he and [~AHeise] are done with the unaligned checkpoints work.;;;","02/Nov/20 12:33;chesnay;From what I can tell the metrics work as intended, and this test failure also cannot be explained with threading since long field in the watermark metric is volatile. I couldn'T reproduce it locally after ~40k executions.

The only theoretical possibility I can come up with is that the task crashed while consuming the input, emptying the queue but not updating metrics.

The test uses {{StreamTaskTestHarness#waitForInputProcessing}} to wait for the task to have finished processing of the input. This method roughly does the following:
{code}
while (true) {
	checkForErrorInTaskThread()
	if (allInputConsumed()) {
		break
	}
}
{code}
If a task consumes data after checkForError, and then fails before reaching the point where metrics are updated, then this is not noticed anywhere.
 ;;;","05/Nov/20 08:47;rmetzger;This issue only seems to occur on Azure hosted VMs (azure nightly cron in Flink and personal azure accounts)

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8626&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374;;;","05/Nov/20 09:08;arvid;From my experience so far, there are no Azure-only issues. It's rather that you quite often run into completely overloaded VMs on Azure and rare errors pop up.
What I usually do is to tightly run things in parallel to overload my system as well:

{noformat}
for i in {0..99}; do mkdir logs/$i; for j in {0..7}; do FLINK_DIR=../flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT mvn surefire:test -nsu -Dfast -Drat.skip=true  -Dtest=org.apache.flink.test.checkpointing.UnalignedCheckpointITCase#shouldPerformUnalignedCheckpointOnNonParallelLocalChannel -pl flink-tests &>logs/$i/test_$j.txt &; ((j == 7)) && wait; done; done
{noformat}

This runs 100 waves of 8 parallel tests. I can usually provoke errors that do not pop up with >1k single process repetitions.;;;","09/Nov/20 10:06;trohrmann;Do you have time looking at this problem [~AHeise] (assuming that the unaligned checkpoint work has been concluded)?;;;","12/Nov/20 01:11;kezhuw;I think this is probably caused by misuse of {{Thread.getState}} as synchronization tool in {{StreamTaskTestHarness.waitForInputProcessing}}. 

{code:java}
	public void waitForInputProcessing() throws Exception {
		while (true) {
			checkForErrorInTaskThread()
			if (allInputConsumed()) {
				break
			}
		}

		// then wait for the Task Thread to be in a blocked state
		// Check whether the state is blocked, this should be the case if it cannot
		// notifyNonEmpty more input, i.e. all currently available input has been processed.
		while (true) {
			Thread.State state = taskThread.getState();
			if (state == Thread.State.BLOCKED || state == Thread.State.TERMINATED ||
					state == Thread.State.WAITING || state == Thread.State.TIMED_WAITING) {
				break;
			}

			try {
				Thread.sleep(1);
			} catch (InterruptedException ignored) {}
		}
	}
{code}

Herre is what javadoc says about {{Thread.getState}}:

{quote}
Returns the state of this thread. This method is designed for use in monitoring of the system state, not for synchronization control.
{quote}

Even though {{Thread.threadStatus}} is volatile in JDK, it is not in JVM side.
{code:c++}
// Write the thread status value to threadStatus field in java.lang.Thread java class.
void java_lang_Thread::set_thread_status(oop java_thread,
                                         java_lang_Thread::ThreadStatus status) {
  // The threadStatus is only present starting in 1.5
  if (_thread_status_offset > 0) {
    java_thread->int_field_put(_thread_status_offset, status);
  }
}
{code}

I can't give an reliable example to prove JVM code without help of additional synchronization tool, it is a is a chicken-and-egg problem in my know knowledge. This is also not the case we encounter here, as we have explicit synchronization tool in this test case: {{ConcurrentLinkedList.size}} and {{ConcurrentLinkedList.poll}}.

Also I didn't find explicit blocking statement after {{ConcurrentLinkedList.poll}} and before {{inputWatermarkGauge.setCurrentWatermark}}. But *there are implicit blocking entry points: concurrent class loading.* I writes following code to verify this:

{code:java}
import java.util.Arrays;
import java.util.List;
import java.util.concurrent.CountDownLatch;

public class Main {
    private static final List<String> unloadedClassNames = Arrays.asList(
        ""java.sql.DriverManager"",
        ""java.io.Console"",
        ""java.io.FileInputStream"",
        ""java.io.FilePermission""
    );

    public static void main(String[] args) throws Exception {
        final CountDownLatch readyLatch = new CountDownLatch(1);
        final CountDownLatch classLoadingLatch = new CountDownLatch(1);
        final CountDownLatch doneLatch = new CountDownLatch(1);
        Thread pollingThread = new Thread(() -> {
            try {
                readyLatch.countDown();
                while (classLoadingLatch.getCount() != 0) {
                    Thread.yield();
                }
                unloadedClassNames.forEach(className -> {
                    try {
                        Class.forName(className);
                        Thread.yield();
                    } catch (Exception ex) {
                        ex.printStackTrace();
                        System.exit(2);
                    }
                });
                while (doneLatch.getCount() != 0) {
                    Thread.yield();
                }
            } catch (Exception ex) {
                ex.printStackTrace();
                System.exit(2);
            }
        });
        pollingThread.start();

        readyLatch.await();

        classLoadingLatch.countDown();

        unloadedClassNames.forEach(className -> {
            try {
                Class.forName(className);
            } catch (Exception ex) {
                ex.printStackTrace();
                System.exit(2);
            }
        });

        Thread.State pollingThreadState = pollingThread.getState();
        if (pollingThreadState != Thread.State.RUNNABLE) {
            System.err.format(""polling thread state: %s\n"", pollingThreadState);
            System.exit(1);
        }

        doneLatch.countDown();
        pollingThread.join();
    }
}
{code}

Here, I choose four classes, which both have static initialization block. The above code fails quite often, roughly rate 10%, in my local environment. This is probably why JDK declares that statement, *class loading is everywhere in java*.;;;","12/Nov/20 08:18;arvid;The find is most likely correct, although I don't think it's used as a synchronization tool. The javadoc of the harness also clearly states 
{noformat}
This class id deprecated because of it's threading model. Please use
{@link StreamTaskMailboxTestHarness}.{noformat}
I'm assuming a proper fix is to migrate all 53 tests... I'm happy to hear alternative suggestions.

If someone has the capacity to do so, please feel free to start. I'm not sure if I'd find the time before the release to do so. But I also suggest to decrease the priority, given that we only had one failed instance so far and it's not indicating a production issue.;;;","12/Nov/20 14:43;rmetzger;[~kezhuw] excellent analysis, thank you! 

I agree with [~AHeise]'s proposal to decrease the priority of this issue: It is caused by the underlying testing framework, not by production code.;;;","12/Nov/20 14:44;rmetzger;If we see this specific test fail more frequently, does it make sense to ignore it, or allow it retry a few times?;;;","13/Nov/20 11:41;trohrmann;I've updated the components to reflect that the problem lies in the {{StreamTaskTestHarness}}.;;;","17/Nov/20 01:05;kezhuw;How about using a combination of {{Thread.isAlive}} and {{TaskMailbox.isMailboxThreadBlocked}} to replace thread state judgement ? Changes are similar to following code snippets:
{code:java}
public interface TaskMailbox {
	/**
	 * Check whether mailbox thread is blocking on waiting for new mails.
	 *
	 * @return true if mailbox thread is blocking on waiting for new mails.
	 */
	@VisibleForTesting
	boolean isMailboxThreadBlocked();
}

public class TaskMailboxImpl implements TaskMailbox {
        @Override
	public boolean isMailboxThreadBlocked() {
		final ReentrantLock lock = this.lock;
		lock.lock();
		try {
			return lock.hasWaiters(notEmpty);
		} finally {
			lock.unlock();
		}
	}
}

public class StreamTaskTestHarness<OUT> {
	public void waitForInputProcessing() throws Exception {
		while (true) {
			checkForErrorInTaskThread()
			if (allInputConsumed()) {
				break
			}
		}

		// Wait for the Task Thread to be blocked in mailbox loop, this should be the case if
		// it cannot notifyNonEmpty more input before reaching here, i.e. all currently available
		// input has been processed.
		while (taskThread.isAlive()) {
			if (taskThread.task.isMailboxLoopBlocked()) {
				break;
			}

			try {
				Thread.sleep(1);
			} catch (InterruptedException ignored) {}
		}

		Throwable error = taskThread.getError();
		if (error != null) {
			throw new Exception(""Exception in the task thread"", error);
		}
	}
}
{code}

It should be equivalent to thread state checking version but sticking to mailbox loop.;;;","17/Nov/20 14:57;kezhuw;[~trohrmann] [~pnowojski] [~chesnay] [~AHeise] What do you think ? Could you assign this to me if above sound viable ?;;;","17/Nov/20 15:31;arvid;I wouldn't like to change production code to get deprecated test code working. I could assign the ticket nonetheless and you could try out if it's working and maybe check if there are alternatives that don't need changes to the production code.

 

Another option could be to check the StreamStatus/InputStatus for detecting that all input has been processed and remove this thread status fiddling altogether.;;;","17/Nov/20 17:39;kezhuw;[~AHeise] -I think {{InputStatus}} is not enough. It only means that all input has been *fetched*. We need assertion happens-after all input processed.- I think we can query {{MailboxProcessor.isDefaultActionUnavailable}} through {{MaiilboxExecutor}}. This should provide enough guarantee for assertion since {{controller.suspendDefaultAction()}} executes after all input processed in mailbox thread. All we need to do is changing {{StreamTaskTestHarness.waitForInputProcessing}}, it should similar to following code:
{code:java}
public class StreamTaskTestHarness<OUT> {
	public void waitForInputProcessing() throws Exception {
		while (true) {
			checkForErrorInTaskThread()
			if (allInputConsumed()) {
				break
			}
		}

		// Wait for all currently available input has been processed.
		final MailboxProcessor mailboxProcessor = taskThread.task.mailboxProcessor;
		final MailboxExecutor mailboxExecutor = mailboxProcessor.getMainMailboxExecutor();
		while (taskThread.isAlive()) {
			final AtomicBoolean allInputProcessed = new AtomicBoolean();
			final CountDownLatch latch = new CountDownLatch(1);
			try {
				mailboxExecutor.execute(() -> {
					allInputProcessed.set(mailboxProcessor.isDefaultActionUnavailable());
					latch.countDown();
				}, ""query-whether-processInput-has-suspend-itself"");
				// Mail could be dropped due to task exception.
				latch.await(1, TimeUnit.SECONDS);
			} catch (RejectedExecutionException ex) {
				// Loop until task thread exit for possible task exception.
			}

			if (allInputProcessed.get()) {
				break;
			}

			try {
				Thread.sleep(1);
			} catch (InterruptedException ignored) {}
		}

		Throwable error = taskThread.getError();
		if (error != null) {
			throw new Exception(""Exception in the task thread"", error);
		}
	}
}
{code}
How about this approach ? I think it adheres what mailbox modeling encourage.;;;","19/Nov/20 13:41;arvid;I assigned the ticket to you as you have done so much investigation already. Could you please try out your approach in a PR? Then let's discuss code changes and alternatives there.;;;","19/Nov/20 13:49;kezhuw;[~AHeise] I will open a PR as soon as possible and mention you at that PR.;;;","23/Nov/20 05:17;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9912&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","23/Nov/20 12:12;arvid;Merged into master as eb500b84311717cfbc64d15683621b812e76ca55.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible semantics of channelIndex in UnionInputGate.resumeConsumption and its clients,FLINK-19855,13337645,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,roman,roman,roman,28/Oct/20 11:19,22/Jun/21 13:55,13/Jul/23 08:12,02/Nov/20 08:41,1.12.0,,,,,1.12.0,,,Runtime / Network,,,,,0,pull-request-available,,,,,"Given channelIndex only, UnionInputGate has to guess which wrapped input
gate this channel belongs to. For that, UnionInputGate expects channel
index with an inputGate offset. This contradicts with the contract of
other resumeConsumption() implementations.
UnionInputGate.resumeConsumption isn't used currently but is planned to
be used in FLINK-19856.",,AHeise,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 08:40:58 UTC 2020,,,,,,,,,,"0|z0k2w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 08:40;arvid;Merged into master as 6b5d4c0c795d50f8ff988b98c2800511b745ecdd .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableScanTest.testTemporalJoinOnUpsertSource fails,FLINK-19854,13337636,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,rmetzger,rmetzger,28/Oct/20 10:46,28/Oct/20 14:09,13/Jul/23 08:12,28/Oct/20 11:44,1.12.0,,,,,1.12.0,,,Table SQL / Planner,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8482&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

org.junit.ComparisonFailure: planAfter expected:<...me, __TEMPORAL_JOIN_[]LEFT_KEY(currency), ...> but was:<...me, __TEMPORAL_JOIN_[CONDITION_PRIMARY_KEY(currency0), __TEMPORAL_JOIN_]LEFT_KEY(currency), ...>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.apache.flink.table.planner.utils.DiffRepository.assertEquals(DiffRepository.java:436)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertEqualsOrExpand(TableTestBase.scala:478)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:362)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:275)
	at org.apache.flink.table.planner.plan.stream.sql.TableScanTest.testTemporalJoinOnUpsertSource(TableScanTest.scala:515)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
",,jark,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 14:09:07 UTC 2020,,,,,,,,,,"0|z0k2u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/20 10:48;rmetzger;2 failures in a row. Probably a broken test:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8488&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29
[~jark];;;","28/Oct/20 11:44;jark;Fixed in master: d13f66be552eac89f45469c199ae036087baa38d;;;","28/Oct/20 11:49;jark;Hi [~rmetzger], I'm wondering will the pull request build not rebase to the latest master branch? 
The case {{testTemporalJoinOnUpsertSource}} is merged last night, but this build for the pull request https://github.com/apache/flink/pull/13300 (which breaks the test) is triggered and passed today. ;;;","28/Oct/20 12:01;rmetzger;Pull requests are not rebased to the latest master when they are build.;;;","28/Oct/20 12:01;rmetzger;Thanks a lot for fixing this so quickly!;;;","28/Oct/20 12:16;jark;Thanks [~rmetzger], I just know that. Then we have to rebase the pull request manually after it is accepted. ;;;","28/Oct/20 12:26;rmetzger;Yes, I recommend that if the change is likely to break due to other changes.;;;","28/Oct/20 14:09;leonard;Sorry for the broken tests, I didn't rebase the latest master timely, I rebased two day before.

I'll keep this in mind. Thanks very much [~rmetzger] and [~jark];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed memory released check can block IterativeTask,FLINK-19852,13337630,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,shaomeng.wang,shaomeng.wang,28/Oct/20 10:11,22/Jun/21 13:55,13/Jul/23 08:12,27/Nov/20 15:04,1.10.2,1.11.0,1.11.1,1.11.2,1.12.0,1.11.3,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,,"UnsafeMemoryBudget#reserveMemory, called on TempBarrier, needs time to wait on GC of all allocated/released managed memory at every iteration.

 

stack:

!image-2020-10-28-17-48-48-583.png!

new TempBarrier in BatchTask

!image-2020-10-28-17-48-28-395.png!

 

These will be very slow than before.",,AHeise,azagrebin,gaoyunhaii,roman,sewen,shaomeng.wang,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15758,,,,,,,,,,,,,,,"28/Oct/20 09:48;shaomeng.wang;image-2020-10-28-17-48-28-395.png;https://issues.apache.org/jira/secure/attachment/13014277/image-2020-10-28-17-48-28-395.png","28/Oct/20 09:48;shaomeng.wang;image-2020-10-28-17-48-48-583.png;https://issues.apache.org/jira/secure/attachment/13014276/image-2020-10-28-17-48-48-583.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 15:04:38 UTC 2020,,,,,,,,,,"0|z0k2sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 07:58;azagrebin;UnsafeMemory usage has indeed become more safe after 1.11. We do not just expect users of UnsafeMemory to always explicitly release it. MemorySegments are tracked by JVM GC to make sure that they are reused only once no other code refers to them, basically when they are GC'ed but GC takes time, of course, this is the price for safety. It is very similar to JVM direct memory. The problem here is that the limit is relatively small per operator and it is exact (no playground to over-allocate). The usage pattern in TempBarrier is the worst for this safe approach because it tries to (re)-allocate all segments at once. Hence, it has to wait for GC of all segments between iterations (stop the world event). From what I see in SpillingBuffer/ListMemorySegmentSource it does not really need all segments at once, the segments are just pulled on-demand one-by-one. If ListMemorySegmentSource reserved segments also one-by-one then GC would be amortised between segments allocation. Ideally using code should also release segments asap once they are not needed anymore, this would give GC more time.;;;","13/Nov/20 08:59;arvid;Another option that I discussed with Andrey offline is to not reallocate pages at all during batch iterations. Memory usage on batch-side should be rather safe. [~sewen] WDYT?;;;","16/Nov/20 03:27;xtsong;[~gaoyunhaii] and I also had a discussion around this issue. We have similar opinion as yours, that the least invasive fix might be to have iterative tasks cache the pages and do not re-allocate them on each iteration.;;;","17/Nov/20 19:51;roman;I took at the code and I think it can be solved by:
 # ""transferring"" memory segments from an old TempBarrier to the new one in BatchTask.resetAllInputs()
 # For that, add TempBarrier.closeForReuse() method, from which return the segments instead of calling memManager.release()
 # In TempBarrier constructor, some memory still has to be allocated because some segments might have been returned to reader/writer

I don't see a clean way to collect old segments and create a new TB instance atomically. In between initInputLocalStrategy should be called. Reusing a TempBarrier instance seems to be error-prone.

WDYT?

 

Besides that, I have some concerns regarding the issue:
 # initInputLocalStrategy() might also allocate memory; Are we sure that degradation is not caused by this? (e.g. ExternalSorterBuilder.doBuild())
 # at least one thread is created - for each TempBarrier - the same question
 # How big is the regression, are there any numbers? Critical Priority seems a bit subjective given that this issue appeared first in 1.10 

[~shaomeng.wang], can you maybe clarify this?;;;","21/Nov/20 13:34;sewen;Transferring the memory sounds reasonable here.

I would go for the simplest solution here, not necessarily for the most long-term maintainable, because this code is not expected to be evolved too much any more. DataStream is taking over more and more functionality from DataSet.;;;","23/Nov/20 21:00;roman;Thanks for your feedback [~sewen].

I've created a PR to implement this approach: https://github.com/apache/flink/pull/14184.;;;","25/Nov/20 13:17;arvid;Merged into master as e46ee85824796db83b994d37cd757d4fe52a0dd2.;;;","27/Nov/20 15:04;arvid;Merged into 1.11 as 05a7875448b6f92450083c17153e7f21e7de2e31.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check NOTICE files for 1.12 release,FLINK-19849,13337604,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,28/Oct/20 08:28,05/Nov/20 06:19,13/Jul/23 08:12,04/Nov/20 19:30,1.12.0,,,,,1.12.0,,,Build System,,,,,0,,,,,,This will be automated through FLINK-19810,,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19987,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 19:30:11 UTC 2020,,,,,,,,,,"0|z0k2n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/20 13:09;rmetzger;PR https://github.com/apache/flink/pull/13796;;;","04/Nov/20 19:30;rmetzger;Fixed in https://github.com/apache/flink/commit/77e9e6505e1f6a33be4872587806ab18ac0a33a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ParquetFsStreamingSinkITCase.testPart failed with ""Trying to access closed classloader""",FLINK-19843,13337544,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,dian.fu,dian.fu,28/Oct/20 01:33,09/Apr/21 04:21,13/Jul/23 08:12,09/Apr/21 04:21,1.12.0,1.13.0,,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8431&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51

{code}

2020-10-27T22:51:46.7422561Z [ERROR] testPart(org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase) Time elapsed: 7.031 s <<< ERROR! 2020-10-27T22:51:46.7423062Z java.lang.RuntimeException: Failed to fetch next result 2020-10-27T22:51:46.7425294Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) 2020-10-27T22:51:46.7426708Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) 2020-10-27T22:51:46.7427791Z at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) 2020-10-27T22:51:46.7428869Z at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) 2020-10-27T22:51:46.7429957Z at java.util.Iterator.forEachRemaining(Iterator.java:115) 2020-10-27T22:51:46.7430652Z at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:114) 2020-10-27T22:51:46.7431826Z at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.check(FsStreamingSinkITCaseBase.scala:141) 2020-10-27T22:51:46.7432859Z at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:122) 2020-10-27T22:51:46.7433902Z at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPart(FsStreamingSinkITCaseBase.scala:86) 2020-10-27T22:51:46.7434702Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2020-10-27T22:51:46.7435452Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2020-10-27T22:51:46.7436661Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2020-10-27T22:51:46.7437367Z at java.lang.reflect.Method.invoke(Method.java:498) 2020-10-27T22:51:46.7438119Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 2020-10-27T22:51:46.7438966Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 2020-10-27T22:51:46.7439789Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 2020-10-27T22:51:46.7440666Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 2020-10-27T22:51:46.7441740Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 2020-10-27T22:51:46.7442533Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) 2020-10-27T22:51:46.7443290Z at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) 2020-10-27T22:51:46.7444227Z at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) 2020-10-27T22:51:46.7445043Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) 2020-10-27T22:51:46.7445631Z at java.lang.Thread.run(Thread.java:748) 2020-10-27T22:51:46.7446383Z Caused by: java.io.IOException: Failed to fetch job execution result 2020-10-27T22:51:46.7447239Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) 2020-10-27T22:51:46.7448233Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126) 2020-10-27T22:51:46.7449239Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) 2020-10-27T22:51:46.7449963Z ... 22 more 2020-10-27T22:51:46.7450619Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. 2020-10-27T22:51:46.7451795Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) 2020-10-27T22:51:46.7452573Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) 2020-10-27T22:51:46.7453500Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172) 2020-10-27T22:51:46.7454213Z ... 24 more 2020-10-27T22:51:46.7454773Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. 2020-10-27T22:51:46.7455573Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) 2020-10-27T22:51:46.7456621Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119) 2020-10-27T22:51:46.7457526Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) 2020-10-27T22:51:46.7458304Z at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628) 2020-10-27T22:51:46.7459124Z at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996) 2020-10-27T22:51:46.7460037Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:117) 2020-10-27T22:51:46.7461155Z ... 25 more 2020-10-27T22:51:46.7461778Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy 2020-10-27T22:51:46.7462882Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) 2020-10-27T22:51:46.7464036Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) 2020-10-27T22:51:46.7465069Z at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:218) 2020-10-27T22:51:46.7466097Z at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:211) 2020-10-27T22:51:46.7467159Z at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:205) 2020-10-27T22:51:46.7468108Z at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523) 2020-10-27T22:51:46.7468999Z at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419) 2020-10-27T22:51:46.7469731Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2020-10-27T22:51:46.7470414Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2020-10-27T22:51:46.7471424Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2020-10-27T22:51:46.7472158Z at java.lang.reflect.Method.invoke(Method.java:498) 2020-10-27T22:51:46.7472887Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286) 2020-10-27T22:51:46.7473757Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201) 2020-10-27T22:51:46.7474636Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) 2020-10-27T22:51:46.7475487Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154) 2020-10-27T22:51:46.7476376Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) 2020-10-27T22:51:46.7477064Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) 2020-10-27T22:51:46.7477756Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) 2020-10-27T22:51:46.7478476Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) 2020-10-27T22:51:46.7479191Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) 2020-10-27T22:51:46.7479882Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 2020-10-27T22:51:46.7480591Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 2020-10-27T22:51:46.7481457Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517) 2020-10-27T22:51:46.7482108Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) 2020-10-27T22:51:46.7482807Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) 2020-10-27T22:51:46.7483544Z at akka.actor.ActorCell.invoke(ActorCell.scala:561) 2020-10-27T22:51:46.7484210Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) 2020-10-27T22:51:46.7484803Z at akka.dispatch.Mailbox.run(Mailbox.scala:225) 2020-10-27T22:51:46.7485362Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235) 2020-10-27T22:51:46.7486135Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 2020-10-27T22:51:46.7486871Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) 2020-10-27T22:51:46.7487561Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 2020-10-27T22:51:46.7488288Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 2020-10-27T22:51:46.7491259Z Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'. 2020-10-27T22:51:46.7492610Z at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:161) 2020-10-27T22:51:46.7493370Z at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:179) 2020-10-27T22:51:46.7493961Z at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780) 2020-10-27T22:51:46.7494430Z at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036) 2020-10-27T22:51:46.7494986Z at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995) 2020-10-27T22:51:46.7495434Z at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968) 2020-10-27T22:51:46.7495987Z at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848) 2020-10-27T22:51:46.7496428Z at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200) 2020-10-27T22:51:46.7496851Z at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1254) 2020-10-27T22:51:46.7497298Z at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1479) 2020-10-27T22:51:46.7497762Z at org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182) 2020-10-27T22:51:46.7498255Z at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:109) 2020-10-27T22:51:46.7498838Z at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader$1.visit(ColumnChunkPageReadStore.java:103) 2020-10-27T22:51:46.7499449Z at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader$1.visit(ColumnChunkPageReadStore.java:99) 2020-10-27T22:51:46.7499964Z at org.apache.parquet.column.page.DataPageV1.accept(DataPageV1.java:120) 2020-10-27T22:51:46.7500494Z at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readPage(ColumnChunkPageReadStore.java:99) 2020-10-27T22:51:46.7501343Z at org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.readToVector(AbstractColumnReader.java:171) 2020-10-27T22:51:46.7501958Z at org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.nextBatch(ParquetColumnarRowSplitReader.java:299) 2020-10-27T22:51:46.7502608Z at org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.ensureBatch(ParquetColumnarRowSplitReader.java:270) 2020-10-27T22:51:46.7503255Z at org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.reachedEnd(ParquetColumnarRowSplitReader.java:251) 2020-10-27T22:51:46.7503984Z at org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory$ParquetInputFormat.reachedEnd(ParquetFileSystemFormatFactory.java:198) 2020-10-27T22:51:46.7504643Z at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:90) 2020-10-27T22:51:46.7505201Z at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) 2020-10-27T22:51:46.7505681Z at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) 2020-10-27T22:51:46.7506287Z at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213) 2020-10-27T22:51:46.7506648Z 2020-10-27T22:51:46.7507973Z Exception in thread ""Thread-14"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'. 2020-10-27T22:51:46.7509055Z at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:161) 2020-10-27T22:51:46.7509903Z at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:179) 2020-10-27T22:51:46.7510510Z at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780) 2020-10-27T22:51:46.7511109Z at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036) 2020-10-27T22:51:46.7511579Z at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995) 2020-10-27T22:51:46.7512048Z at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968) 2020-10-27T22:51:46.7512482Z at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848) 2020-10-27T22:51:46.7512921Z at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200) 2020-10-27T22:51:46.7513444Z at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812) 2020-10-27T22:51:46.7513895Z at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789) 2020-10-27T22:51:46.7514390Z at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183) 2020-10-27T22:51:46.7514909Z at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145) 2020-10-27T22:51:46.7515398Z at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65) 2020-10-27T22:51:46.7516021Z at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102) 2020-10-27T22:51:47.4802370Z [INFO] Running org.apache.flink.formats.parquet.ParquetTableSourceITCase


{code}",,dian.fu,dwysakowicz,LesTR,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 04:21:39 UTC 2021,,,,,,,,,,"0|z0k29s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/20 01:47;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","30/Oct/20 01:23;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8626&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","30/Oct/20 01:26;dian.fu;cc [~lzljs3620320] Could you help to take a look at this issue? ;;;","30/Oct/20 01:29;dian.fu;Seems that it only fails in hadoop313. This test is continuously failing in hadoop313, upgrade it to ""Blocker""!;;;","31/Oct/20 02:14;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8698&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","02/Nov/20 02:08;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8714&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","02/Nov/20 02:28;lzljs3620320;Thanks for reporting, [~dian.fu], I'll take a look~;;;","02/Nov/20 02:37;dian.fu;Thanks a lot [~lzljs3620320] ~;;;","02/Nov/20 03:08;lzljs3620320;The classloader was closed by:
{code:java}
Exception in thread ""Thread-11"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.Close stack: java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.close(FlinkUserCodeClassLoaders.java:156)
org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$ResolvedClassLoader.releaseClassLoader(BlobLibraryCacheManager.java:430)
org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$ResolvedClassLoader.access$1000(BlobLibraryCacheManager.java:348)
org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.releaseClassLoader(BlobLibraryCacheManager.java:296)
org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.release(BlobLibraryCacheManager.java:287)
org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.access$1200(BlobLibraryCacheManager.java:204)
org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$DefaultClassLoaderLease.release(BlobLibraryCacheManager.java:340)
org.apache.flink.runtime.taskexecutor.TaskExecutor$TaskExecutorJobServices.close(TaskExecutor.java:2049)
org.apache.flink.runtime.taskexecutor.DefaultJobTable$JobOrConnection.close(DefaultJobTable.java:232)
org.apache.flink.runtime.taskexecutor.TaskExecutor.closeJob(TaskExecutor.java:1408)
org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$closeJobManagerConnectionIfNoAllocatedResources$23(TaskExecutor.java:1647)
java.util.Optional.ifPresent(Optional.java:159)
org.apache.flink.runtime.taskexecutor.TaskExecutor.closeJobManagerConnectionIfNoAllocatedResources(TaskExecutor.java:1646)
org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlotInternal(TaskExecutor.java:1630)
org.apache.flink.runtime.taskexecutor.TaskExecutor.freeSlot(TaskExecutor.java:993)

{code};;;","02/Nov/20 03:09;lzljs3620320;Can be re-produced by:
{code:java}
in flink-parquet: mvn -Dflink.forkCount=2 -Dflink.forkCountTestPackage=2 -Dfast -Dinclude_hadoop_aws -Dhadoop.version=3.1.3 install
{code};;;","02/Nov/20 06:37;lzljs3620320;PR: https://github.com/apache/flink/pull/13873;;;","02/Nov/20 12:50;dian.fu;Fixed in master via ad1c4f0848b03d4833a51ae2fc85cf2a6e2787e4;;;","07/Apr/21 06:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16093&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=13504;;;","08/Apr/21 12:56;lzljs3620320;I'll take a look~;;;","09/Apr/21 04:21;lzljs3620320;This should be a new problem. _*TestTimedOut*_ in the test.

It should be the same as FLINK-22129, except that there is a timeout in ParquetFsStreamingSinkITCase, but there is no timeout in FLINK-22129.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlinkStreamUserDefinedTableFunctionTests.test_table_function_with_sql_query is unstable,FLINK-19842,13337486,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,rmetzger,rmetzger,27/Oct/20 18:03,11/Nov/20 02:05,13/Jul/23 08:12,11/Nov/20 02:05,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8401&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3

{code}
=================================== FAILURES ===================================
_ PyFlinkStreamUserDefinedTableFunctionTests.test_table_function_with_sql_query _

self = <pyflink.table.tests.test_udtf.PyFlinkStreamUserDefinedTableFunctionTests testMethod=test_table_function_with_sql_query>

    def test_table_function_with_sql_query(self):
        self._register_table_sink(
            ['a', 'b', 'c'],
            [DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
    
        self.t_env.create_temporary_system_function(
            ""multi_emit"", udtf(MultiEmit(), result_types=[DataTypes.BIGINT(), DataTypes.BIGINT()]))
    
        t = self.t_env.from_elements([(1, 1, 3), (2, 1, 6), (3, 2, 9)], ['a', 'b', 'c'])
        self.t_env.register_table(""MyTable"", t)
        t = self.t_env.sql_query(
            ""SELECT a, x, y FROM MyTable LEFT JOIN LATERAL TABLE(multi_emit(a, b)) as T(x, y)""
            "" ON TRUE"")
        actual = self._get_output(t)
>       self.assert_equals(actual, [""1,1,0"", ""2,2,0"", ""3,3,0"", ""3,3,1""])

pyflink/table/tests/test_udtf.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'pyflink.table.tests.test_udtf.PyFlinkStreamUserDefinedTableFunctionTests'>
actual = JavaObject id=o37759, expected = ['1,1,0', '2,2,0', '3,3,0', '3,3,1']

{code}",,dian.fu,hxbks2ks,nicholasjiang,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20066,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 02:05:25 UTC 2020,,,,,,,,,,"0|z0k1ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/20 01:05;nicholasjiang;cc [~dianfu], [~hxbks2ks];;;","28/Oct/20 01:55;hxbks2ks;[~rmetzger] [~nicholasjiang] Thanks a lot. I will take a look.;;;","08/Nov/20 01:46;dian.fu;Seems another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9279&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3

{code}
2020-11-07T23:17:06.7806808Z self = <pyflink.table.tests.test_udtf.PyFlinkBlinkStreamUserDefinedFunctionTests testMethod=test_table_function>
2020-11-07T23:17:06.7807102Z 
2020-11-07T23:17:06.7807304Z     def test_table_function(self):
2020-11-07T23:17:06.7807580Z         self._register_table_sink(
2020-11-07T23:17:06.7807959Z             ['a', 'b', 'c'],
2020-11-07T23:17:06.7808316Z             [DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
2020-11-07T23:17:06.7808665Z     
2020-11-07T23:17:06.7808944Z         multi_emit = udtf(MultiEmit(), result_types=[DataTypes.BIGINT(), DataTypes.BIGINT()])
2020-11-07T23:17:06.7809335Z         multi_num = udf(MultiNum(), result_type=DataTypes.BIGINT())
2020-11-07T23:17:06.7809595Z     
2020-11-07T23:17:06.7810056Z         t = self.t_env.from_elements([(1, 1, 3), (2, 1, 6), (3, 2, 9)], ['a', 'b', 'c'])
2020-11-07T23:17:06.7810613Z         t = t.join_lateral(multi_emit(t.a, multi_num(t.b)).alias('x', 'y'))
2020-11-07T23:17:06.7811170Z         t = t.left_outer_join_lateral(condition_multi_emit(t.x, t.y).alias('m')) \
2020-11-07T23:17:06.7811479Z             .select(""x, y, m"")
2020-11-07T23:17:06.7812029Z         t = t.left_outer_join_lateral(identity(t.m).alias('n')) \
2020-11-07T23:17:06.7812326Z             .select(""x, y, n"")
2020-11-07T23:17:06.7812560Z         actual = self._get_output(t)
2020-11-07T23:17:06.7812820Z         self.assert_equals(actual,
2020-11-07T23:17:06.7813237Z                            [""1,0,null"", ""1,1,null"", ""2,0,null"", ""2,1,null"", ""3,0,0"", ""3,0,1"",
2020-11-07T23:17:06.7813628Z >                           ""3,0,2"", ""3,1,1"", ""3,1,2"", ""3,2,2"", ""3,3,null""])
{code};;;","11/Nov/20 02:05;dian.fu;Fixed in master(1.12.0) via 0ab238919dbceb0180df74482c88f3c3dcbfed52;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test failures are not causing the build to fail,FLINK-19839,13337443,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,27/Oct/20 14:23,28/Oct/20 06:40,13/Jul/23 08:12,28/Oct/20 06:40,1.12.0,,,,,1.12.0,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,,"Example: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8385&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
Oct 27 13:37:05 Killing JM watchdog @ 11717
Oct 27 13:37:05 Killing TM watchdog @ 11801
Oct 27 13:37:05 [FAIL] Test script contains errors.
Oct 27 13:37:05 Checking of logs skipped.
Oct 27 13:37:05 
Oct 27 13:37:05 [FAIL] 'Running HA (file, sync) end-to-end test' failed after 15 minutes and 0 seconds! Test exited with exit code 1
Oct 27 13:37:05 
13:37:05 ##[group]Environment Information
Oct 27 13:37:06 Published e2e logs into debug logs artifact:
Oct 27 13:37:06 flink-vsts-client-fv-az678.log
Oct 27 13:37:06 flink-vsts-standalonesession-0-fv-az678.log
Oct 27 13:37:06 flink-vsts-standalonesession-0-fv-az678.out
Oct 27 13:37:06 flink-vsts-taskexecutor-0-fv-az678.log
Oct 27 13:37:06 flink-vsts-taskexecutor-0-fv-az678.out
Oct 27 13:37:06 flink-vsts-zookeeper-0-fv-az678.log
Oct 27 13:37:06 flink-vsts-zookeeper-0-fv-az678.out
Oct 27 13:37:06 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
Oct 27 13:37:12 COMPRESSING build artifacts.
{code}

Despite the FAIL, the stage itself is green.
",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19664,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 06:40:47 UTC 2020,,,,,,,,,,"0|z0k1nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/20 15:12;chesnay;Is this what you are referring to?
{code:java}
Oct 27 13:54:12 Waiting for Dispatcher REST endpoint to come up...
Oct 27 13:54:13 Dispatcher REST endpoint has not started within a timeout of 30 sec
Oct 27 13:54:13 Checking of logs skipped.
Oct 27 13:54:13 
Oct 27 13:54:13 [PASS] 'Local recovery and sticky scheduling end-to-end test' passed after 0 minutes and 34 seconds! Test exited with exit code 0. {code}
This should fail the test. So it's not that a failed tests is not noticed by the CI scripts, it is that the test does not officially fail.;;;","27/Oct/20 15:15;rmetzger;Snap, I posted the wrong link. This is the right example: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8385&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

Another example: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8243&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","28/Oct/20 06:40;rmetzger;Resolved in https://github.com/apache/flink/commit/bb4e2e8dceb503215aa66aee5e1acd7e75b91fae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink restored from a wrong checkpoint (a very old one and not the last completed one),FLINK-19816,13337309,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,stevenz3wu,stevenz3wu,27/Oct/20 01:14,02/Sep/21 12:57,13/Jul/23 08:12,18/Nov/20 08:00,1.11.0,1.12.0,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"h2. Summary

Upon failure, it seems that Flink didn't restore from the last completed checkpoint. Instead, it restored from a very old checkpoint. As a result, Kafka offsets are invalid and caused the job to replay from the beginning as Kafka consumer ""auto.offset.reset"" was set to ""EARLIEST"".

This is an embarrassingly parallel stateless job. Parallelism is over 1,000. I have the full log file from jobmanager at INFO level available upon request.

h2. Sequence of events from the logs

Just before the failure, checkpoint *210768* completed.

{code}
2020-10-25 02:35:05,970 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [jobmanager-future-thread-5] - Completed checkpoint 210768 for job 233b4938179c06974e4535ac8a868675 (4623776 bytes in 120402 ms).
{code}

During restart, somehow it decided to restore from a very old checkpoint *203531*.
{code:java}
2020-10-25 02:36:03,301 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-3]  - Start SessionDispatcherLeaderProcess.
2020-10-25 02:36:03,302 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-5]  - Recover all persisted job graphs.
2020-10-25 02:36:03,304 INFO  com.netflix.bdp.s3fs.BdpS3FileSystem                         [cluster-io-thread-25]  - Deleting path: s3://<bucket>/checkpoints/XM3B/clapp_avro-clapp_avro_nontvui/1593/233b4938179c06974e4535ac8a868675/chk-210758/c31aec1e-07a7-4193-aa00-3fbe83f9e2e6
2020-10-25 02:36:03,307 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-5]  - Trying to recover job with job id 233b4938179c06974e4535ac8a868675.

2020-10-25 02:36:03,381 INFO  com.netflix.bdp.s3fs.BdpS3FileSystem                         [cluster-io-thread-25]  - Deleting path: s3://<bucket>/checkpoints/Hh86/clapp_avro-clapp_avro_nontvui/1593/233b4938179c06974e4535ac8a868675/chk-210758/4ab92f70-dfcd-4212-9b7f-bdbecb9257fd
...
2020-10-25 02:36:03,427 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Recovering checkpoints from ZooKeeper.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Found 0 checkpoints in ZooKeeper.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Trying to fetch 0 checkpoints from storage.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [flink-akka.actor.default-dispatcher-82003]  - Starting job 233b4938179c06974e4535ac8a868675 from savepoint s3://<bucket>/checkpoints/metadata/clapp_avro-clapp_avro_nontvui/1113/47e2a25a8d0b696c7d0d423722bb6f54/chk-203531/_metadata ()
{code}

",,bryanck,kezhuw,nikobearrr,Paul Lin,stevenz3wu,trohrmann,wind_ljy,xtsong,yunta,zhenzhongxu,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19778,,,,,,,,,,,FLINK-21979,FLINK-11813,,,,,FLINK-20033,,,,,"02/Sep/21 08:51;Paul Lin;jm.log;https://issues.apache.org/jira/secure/attachment/13032889/jm.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 02 12:56:39 UTC 2021,,,,,,,,,,"0|z0k0tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 02:25;stevenz3wu;This happened again for the same job in production. I noticed both failures started with zookeeper failure. Here are some observations on the sequence of events. My hypothesis is that the race condition / interactions between recovering from zk failure and failure-rate restart-strategy caused this problem of restoring a wrong and very old checkpoint. [~trohrmann] any comment?

1. initially, there were some problems with zookeeper that caused the job to fail

{code}
2020-10-25 02:35:59,266 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [main-SendThread(ip-100-81-150-64.us-west-2.compute.internal:2181)]  - Client session timed out, have not heard from server in 26676
ms for sessionid 0x363850d0d034d9d
2020-10-25 02:35:59,268 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [main-SendThread(ip-100-81-150-64.us-west-2.compute.internal:2181)]  - Client session timed out, have not heard from server in 26676
ms for sessionid 0x363850d0d034d9d, closing socket connection and attempting reconnect
2020-10-25 02:35:59,282 INFO  com.netflix.bdp.s3fs.BdpS3FileSystem                         [cluster-io-thread-25]  - Deleting path: s3://us-west-2.spaas.prod/checkpoints/r7E1/clapp_avro-clapp_avro_nontvui/1593/233b4938179c06974e4
535ac8a868675/chk-210758/16d4e138-4199-4fd2-a014-4b394189f72b
2020-10-25 02:35:59,369 INFO  org.apache.flink.shaded.curator4.org.apache.curator.framework.state.ConnectionStateManager [main-EventThread]  - State change: SUSPENDED
{code}


2. This job is configured with `restart-strategy=failure-rate`. and there are enough task restarts to trigger the terminal condition canRestart() to return false. This should eventually lead the Flink job to halt/terminal state.

{code}
2020-10-25 02:35:59,641 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [flink-akka.actor.default-dispatcher-81991]  - Job clapp-avro-nontvui (233b4938179c06974e4535ac8a868675) switched from state FAILING to FA
ILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by FailureRateRestartBackoffTimeStrategy(FailureRateRestartBackoffTimeStrategy(failuresIntervalMS=1800000,backoffTimeMS=30000,maxFailuresPerInterval=20)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:203)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:508)
        at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)
        at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1725)
        at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)
        at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)
        at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:954)
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:173)
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:165)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:732)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)
        at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:149)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseTaskManagerInternal(SlotPoolImpl.java:818)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseTaskManager(SlotPoolImpl.java:777)
        at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:435)
        at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:352)
        at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:216)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:514)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:176)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:517)
        at akka.actor.Actor.aroundReceive$(Actor.scala:515)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: Stopping JobMaster for job clapp-avro-nontvui(233b4938179c06974e4535ac8a868675).
        at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:349)
        ... 22 more
2020-10-25 02:35:59,641 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [flink-akka.actor.default-dispatcher-81991]  - Stopping checkpoint coordinator for job 233b4938179c06974e4535ac8a868675.
2020-10-25 02:35:59,641 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-81991]  - Shutting down
{code}

3. Flink then removes the checkpoint-counter from zk, as it tries to fail/halt the job

{code}
2020-10-25 02:36:03,262 INFO  org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore [flink-akka.actor.default-dispatcher-81991]  - Removing /spaas/clapp_avro-clapp_avro_nontvui/1593/default/checkpoints/233b4938179c06974e45
35ac8a868675 from ZooKeeper
2020-10-25 02:36:03,269 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter [flink-akka.actor.default-dispatcher-81991]  - Shutting down.
2020-10-25 02:36:03,269 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter [flink-akka.actor.default-dispatcher-81991]  - Removing /checkpoint-counter/233b4938179c06974e4535ac8a868675 from ZooKeeper
{code}

4. but somehow Flink tries to start the job again even though failure-rate restart-strategy reached terminal state for canRestart(). maybe it is because the zookeeper reconnect caused another path of job recovery? because the proper checkpoint info in zookeeper is already cleaned in previous step, now Flink recovered a wrong checkpoint.

{code}
2020-10-25 02:36:03,301 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-3]  - Start SessionDispatcherLeaderProcess.
2020-10-25 02:36:03,302 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-5]  - Recover all persisted job graphs.
2020-10-25 02:36:03,307 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-5]  - Trying to recover job with job id 233b4938179c06974e4535ac8a868675.
...
2020-10-25 02:36:03,427 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Recovering checkpoints from ZooKeeper.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Found 0 checkpoints in ZooKeeper.
{code};;;","05/Nov/20 09:17;trohrmann;Hi [~stevenz3wu], thanks a lot for this analysis. Have you gotten hold of the complete logs? I will go through the code to see whether I see a problem and the logs might speed this process up a bit. In any case, this sounds like a bug in Flink to me.;;;","05/Nov/20 09:51;trohrmann;Looking at the code, I believe that the scenario you are describing can actually happen. If the job is about to complete successfully, it will notify the {{Dispatcher}} and then remove all checkpoints from the {{CompletedCheckpointStore}}. If at the same time, the Dispatcher loses the leadership, it will stop all running jobs but w/o cleaning up the persisted jobs. Depending on which action is executed first (losing the leadership and stopping jobs w/o cleaning up the persisted JobGraph or completing successfully and cleaning up the persisted JobGraph) one can end up with a cluster where one has deleted all checkpoint data but not the {{JobGraph}} itself.

I think the problem is that we don't look at the final job status when deciding whether to clean up the persisted {{JobGraph}} or not but we decide on {{cleanupHA}} when the {{Dispatcher}} is being stopped or when the job reaches a globally terminal state. In the former case, this can lead to ignoring a successful termination of the job.;;;","05/Nov/20 14:40;stevenz3wu;[~trohrmann] thanks a lot for taking a look. forwarded the logs to you. but it seems that we may not need them;;;","06/Nov/20 09:20;xtsong;Thanks for the discussion, [~stevenz3wu] & [~trohrmann].

[~trohrmann], is this a new problem introduced after 1.11.0? If not, maybe we can downgrade it to a critical issue, given that the problem only affects under zookeeper failures.

WDYT?;;;","06/Nov/20 13:54;trohrmann;This problem is not new. However, it affects all HA setups and can effectively cause data duplication in exactly once sinks because we might simply run a job twice because of this bug. Hence, I believe that it is a true blocker which we must fix before releasing.;;;","06/Nov/20 14:12;xtsong;Alright, thanks for the explanation.;;;","06/Nov/20 15:18;trohrmann;It turns out that we actually have two problems:

1) What we have already described: A job reaching a globally terminal state while being suspended and the resulting race condition is a problem.

2) Stopping the JobMaster causing the job to fail and thereby to exceed the maximum allowed number of restarts causing it to go to {{FAILED}}. That's also what happened in your case [~stevenz3wu]. This problem has been unintentionally fixed via FLINK-19237 for {{1.12}}. I will create a new ticket to also fix it for {{1.10}} and {{1.11}}.;;;","06/Nov/20 15:33;trohrmann;The new ticket for tracking the second problem is FLINK-20033.;;;","09/Nov/20 13:56;trohrmann;After fixing FLINK-20033, I've downgraded this issue to critical because the problem should now be very unlikely to happen. Still we should fix this problem asap.;;;","18/Nov/20 08:00;trohrmann;Fixed via

1.12.0: cb850fdda2b40866f3b0782e038ae4bce35c9eb0
1.11.3: a2925a0d2e894bf28aaced2993ec453589d143de;;;","02/Sep/21 08:51;Paul Lin;Still getting this error with Flink 1.12.3. The jobmanager logs are attached, please take a look. [~trohrmann] [^jm.log];;;","02/Sep/21 12:56;trohrmann;Hi [~Paul Lin], I think you are not running into the concrete issue that is fixed with this ticket. Instead I believe that you are running into FLINK-11813 that will be fixed with the next release. 

I think the following is happening: The job reaches a globally terminal state (FAILED). Then it tells the {{Dispatcher}} that triggers the clean up of HA information. After the cleanup has happened, the {{Dispatcher}} process loses the leadership and is restarted. Since you seem to use the application mode/per job mode, Flink will be started with the same job but with no checkpoint information since it has been cleaned up. This will ultimately result in a restart.

Part of the problem has been solved via FLINK-21979 but the last remaining piece is FLINK-11813 that will most likely introduce a {{JobResultStore}} that can outlive Flink. Only by persisting information about the job status that can survive a cluster failure, we are able to properly resolve the situation. As a consequence, there will be some bookkeeping information that needs to be taken care of by the user/operator of Flink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job may try to leave SUSPENDED state in ExecutionGraph#failJob(),FLINK-19806,13337144,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,26/Oct/20 06:44,17/Nov/20 10:05,13/Jul/23 08:12,17/Nov/20 10:05,1.10.0,,,,,1.11.3,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"{{SUSPENDED}} is a terminal state which a job is not supposed to leave this state once entering. However, {{ExecutionGraph#failJob()}} did not check it and may try to transition a job out from {{SUSPENDED}} state. This will cause unexpected errors and may lead to JM crash.
The problem can be visible if we rework {{ExecutionGraphSuspendTest}} to be based on {{DefaultScheduler}}.
We should harden the check in {{ExecutionGraph#failJob()}}.",,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 10:05:05 UTC 2020,,,,,,,,,,"0|z0jzsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/20 10:05;zhuzh;Fixed via:
1.12: 1538e5695c561d10c4e92fed9d0cf561b3125f0a
1.11: 4054aa01ebb70aa7e4bf9b3bf96916f96869b42d
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderChangeClusterComponentsTest.testReelectionOfJobMaster is instable,FLINK-19805,13337120,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,dian.fu,dian.fu,26/Oct/20 01:40,05/Nov/20 09:27,13/Jul/23 08:12,05/Nov/20 09:27,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8214&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034

{code}
2020-10-23T21:07:32.6861747Z [ERROR] testReelectionOfJobMaster(org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest)  Time elapsed: 30.182 s  <<< FAILURE!
2020-10-23T21:07:32.6862546Z java.lang.AssertionError: Job failed.
2020-10-23T21:07:32.6865424Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.throwAssertionErrorOnFailedResult(JobResultUtils.java:54)
2020-10-23T21:07:32.6866512Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.assertSuccess(JobResultUtils.java:30)
2020-10-23T21:07:32.6867720Z 	at org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.testReelectionOfJobMaster(LeaderChangeClusterComponentsTest.java:152)
2020-10-23T21:07:32.6868707Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T21:07:32.6869428Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T21:07:32.6870293Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T21:07:32.6871062Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T21:07:32.6871954Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-23T21:07:32.6872726Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-23T21:07:32.6873503Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-23T21:07:32.6874393Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-23T21:07:32.6875218Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-10-23T21:07:32.6876001Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-10-23T21:07:32.6876816Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-23T21:07:32.6877475Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-23T21:07:32.6878216Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-23T21:07:32.6879061Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-23T21:07:32.6879819Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-23T21:07:32.6880502Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-23T21:07:32.6881215Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-23T21:07:32.6882109Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-23T21:07:32.6882850Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-23T21:07:32.6884171Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-10-23T21:07:32.6884969Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-23T21:07:32.6885641Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-23T21:07:32.6886201Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-23T21:07:32.6886841Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-23T21:07:32.6887378Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-23T21:07:32.6887913Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-23T21:07:32.6888478Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-23T21:07:32.6889109Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-23T21:07:32.6889625Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-23T21:07:32.6890110Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-23T21:07:32.6890607Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-10-23T21:07:32.6891237Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-10-23T21:07:32.6892166Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-10-23T21:07:32.6892827Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217)
2020-10-23T21:07:32.6893382Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210)
2020-10-23T21:07:32.6894048Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204)
2020-10-23T21:07:32.6894667Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526)
2020-10-23T21:07:32.6895205Z 	at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:240)
2020-10-23T21:07:32.6895872Z 	at org.apache.flink.runtime.jobmaster.DefaultExecutionDeploymentReconciler.reconcileExecutionDeployments(DefaultExecutionDeploymentReconciler.java:55)
2020-10-23T21:07:32.6896633Z 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.reportPayload(JobMaster.java:1234)
2020-10-23T21:07:32.6897239Z 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.reportPayload(JobMaster.java:1221)
2020-10-23T21:07:32.6897834Z 	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.receiveHeartbeat(HeartbeatManagerImpl.java:199)
2020-10-23T21:07:32.6898395Z 	at org.apache.flink.runtime.jobmaster.JobMaster.heartbeatFromTaskManager(JobMaster.java:672)
2020-10-23T21:07:32.6898846Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T21:07:32.6899289Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T21:07:32.6899816Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T21:07:32.6900257Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T21:07:32.6900723Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:281)
2020-10-23T21:07:32.6901265Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-10-23T21:07:32.6901938Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-10-23T21:07:32.6902497Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-10-23T21:07:32.6902977Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-10-23T21:07:32.6903678Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-10-23T21:07:32.6904110Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2020-10-23T21:07:32.6904545Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2020-10-23T21:07:32.6904974Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-10-23T21:07:32.6905435Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-10-23T21:07:32.6905893Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2020-10-23T21:07:32.6906325Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2020-10-23T21:07:32.6906830Z 	at akka.actor.Actor.aroundReceive(Actor.scala:517)
2020-10-23T21:07:32.6907220Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
2020-10-23T21:07:32.6907618Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-10-23T21:07:32.6908050Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-10-23T21:07:32.6908458Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-10-23T21:07:32.6908831Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-10-23T21:07:32.6909213Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-10-23T21:07:32.6909554Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-10-23T21:07:32.6909959Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-10-23T21:07:32.6910436Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-10-23T21:07:32.6910887Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-10-23T21:07:32.6911360Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-10-23T21:07:32.6913020Z Caused by: org.apache.flink.util.FlinkException: Execution ff7439fa86b9f67e46b2b6715829af00_dccf9918c07aa47eb2b28a1de42a640f_3_0 is unexpectedly no longer running on task executor 3d5d979c-6898-4593-935e-f0914738d325.
2020-10-23T21:07:32.6913798Z 	at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:244)
2020-10-23T21:07:32.6914167Z 	... 33 more
{code}",,dian.fu,guoyangze,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17295,FLINK-19927,,,,,,,,,FLINK-19928,,,,,"02/Nov/20 06:23;rmetzger;mvn-2.log;https://issues.apache.org/jira/secure/attachment/13014539/mvn-2.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 09:27:51 UTC 2020,,,,,,,,,,"0|z0jznk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/20 06:22;rmetzger;I assume this failure is caused by FLINK-18293
{code}
20:33:44,161 [flink-akka.actor.default-dispatcher-2] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - blocking operator (1/4)#0 (5e24756980880a09400046e85eac327a_b56395e1225b36d417c0d2b7dffefc06_0_0) switched from CREATED to FAILED.
java.lang.IllegalStateException: Task is being remove from TaskManager.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.unregisterTaskAndNotifyFinalState(TaskExecutor.java:1582) ~[classes/:?]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$2400(TaskExecutor.java:174) ~[classes/:?]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$TaskManagerActionsImpl.lambda$updateTaskExecutionState$1(TaskExecutor.java:1925) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:404) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:197) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
{code};;;","02/Nov/20 06:21;rmetzger;I'm upgrading this ticket to a blocker.

The problem is reproducible by running it in a loop on Azure. It is most likely caused by a bug in the executiongraph / slot protocol / task reconciliation. I'm trying to figure out the exact issue.;;;","02/Nov/20 08:02;rmetzger;What is the problem?
In rare cases, the test fails, because the DefaultExecutionDeploymentReconciler expects certain executions to be present on the TaskExecutor, while they are not.
The problem is likely more on the JobManager side, tracking the executions wrongly.

Current findings:
a) For reproducing the issue locally, I noticed that no heartbeats are send from the TaskManager to the JobManager. I initially thought the default heartbeat frequency is too low, but it rather seems that in {{HeartbeatManagerSenderImpl}} the map of targets is always empty (even at an interval of 100 ms).

b) I introduced a log statement into {{SchedulerBase.suspend()}} to log the number of tracked deployments by the ExecutionDeploymentTracker after suspension, and the number is 0 when the test succeeds and 1 when it fails. However, in both cases {{stopTrackingDeploymentOf}} is never called. Maybe a new instance is introduced somewhere? I'm currently validating this with a new test run.
Also note that in successful cases, the number of tracked deployments can be 1. I'm investigating if this is a condition of the failure.

c) In successful cases, heartbeats are only triggered by the JobMaster, but never responded by any TaskExecutor (probably because they are not registered). The problem can only surface if heartbeats are received by the JobMaster.

d) Even if a heartbeat is received by the JobMaster, and the reconciliation detects a mismatch, the job does not always fail: If the job finishes ""concurrently"", the reconciler exception is not reported:
{code}
19:40:02,257 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received heartbeat request from 2c0b814278e98801af7cf7a663d486aa.
19:40:02,257 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.jobmaster.JobMaster                 [] - Received heartbeat from 69b562cc-9497-4ea9-9820-1518c8a6da34.
19:40:02,257 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.jobmaster.DefaultExecutionDeploymentTracker [] - executionsByHost = {69b562cc-9497-4ea9-9820-1518c8a6da34=[25c51994b6e81cdc39017bd42618bb86_390ab88bdf3f62c745ec70954a480407_0_0, 25c51994b6e81cdc39017bd42618bb86_390ab88bdf3f62c745ec70954a480407_1_0]}
19:40:02,257 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.DefaultExecutionDeploymentReconciler [] - Reconciling with taskExecutorHost=69b562cc-9497-4ea9-9820-1518c8a6da34, executionDeploymentReport=ExecutionDeploymentReport{executions=[]}, expectedDeployedExecutions={25c51994b6e81cdc39017bd42618bb86_390ab88bdf3f62c745ec70954a480407_0_0=DEPLOYED, 25c51994b6e81cdc39017bd42618bb86_390ab88bdf3f62c745ec70954a480407_1_0=DEPLOYED}
19:40:02,257 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.jobmaster.JobMaster                 [] - Failing deployments [25c51994b6e81cdc39017bd42618bb86_390ab88bdf3f62c745ec70954a480407_0_0, 25c51994b6e81cdc39017bd42618bb86_390ab88bdf3f62c745ec70954a480407_1_0] due to no longer being deployed.
19:40:02,258 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 25c51994b6e81cdc39017bd42618bb86 reached globally terminal state FINISHED.
{code}

;;;","02/Nov/20 08:36;rmetzger;I'm currently investigating why {{stopTrackingDeploymentOf}} gets never called.;;;","02/Nov/20 10:03;rmetzger;[~chesnay] It seems that execution transitions to terminal states (which would stop the deployment tracking) are never received for non-legacy scheduling: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java#L1688-L1692
Does it make sense to move the {{executionStateUpdateListener.onStateUpdate}} above the legacy scheduler check?;;;","02/Nov/20 10:05;chesnay;Probably, yes.;;;","02/Nov/20 10:40;rmetzger;Since this also doesn't seem to be covered by any test, I've filed a ticket: https://issues.apache.org/jira/browse/FLINK-19927;;;","02/Nov/20 10:43;rmetzger;With FLINK-19927 fixed, the test now fails with 

{code}
Test repeatMe(org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest) failed with:
java.lang.AssertionError: Job failed.
	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.throwAssertionErrorOnFailedResult(JobResultUtils.java:54)
	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.assertSuccess(JobResultUtils.java:30)
	at org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.testReelectionOfJobMaster(LeaderChangeClusterComponentsTest.java:172)
	at org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.repeatMe(LeaderChangeClusterComponentsTest.java:143)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:218)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:211)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:205)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:527)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token mismatch: Ignoring message LocalFencedMessage(abf41c6beb770a340a2c567891af4007, LocalRpcInvocation(updateTaskExecutionState(TaskExecutionState))) because the fencing token abf41c6beb770a340a2c567891af4007 did not match the expected fencing token 84a9b4942a1f5ab9444f58d841ff4672.
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:81)
	... 19 more
{code}

I'll keep investigating.;;;","02/Nov/20 11:40;rmetzger;After an offline discussion we concluded that this error is caused by the ExecutionAttemptId being reused across different leader sessions. The reported error is the Task's failure to report it's status to the JobManager. This gets reported through the new leader session to the JobMaster, which can not distinguish if this failure is coming from the current of previous execution attempt.

This problem has been introduced by FLINK-17295. I will now try to revert FLINK-17295 to see if that makes the test stable.

I will also introduce an assertion into the ExecutionGraph that the number of tracked deployments by the DefaultExecutionDeploymentTracker is always 0 after suspending the execution. This might help uncover further problems.;;;","02/Nov/20 13:55;rmetzger;[~trohrmann] The problem is indeed resolved by reverting FLINK-17295. Shall I open a PR for the revert?
We can then reopen FLINK-17295 and discuss there how to solve the problem.;;;","02/Nov/20 14:31;trohrmann;Yes, let's do it like this [~rmetzger]. Please re-open FLINK-17295 and describe the problem we have seen here.;;;","02/Nov/20 19:04;rmetzger;I also ran this a variant on CI that throws an IllegalStateException if the deployment tracker is tracking something after the suspension completed. It didn't reveal any additional issues. This is the commit: https://github.com/rmetzger/flink/commit/3bfb92af93faf0fb9d92f9e3553ffa0eb6e2adf2.

;;;","05/Nov/20 09:27;rmetzger;I'm now closing this ticket:
- FLINK-17295 has been reverted
- FLINK-19927 has been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error results when use constant decimal array,FLINK-19796,13337083,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,FrankZou,FrankZou,25/Oct/20 09:33,28/Aug/21 12:08,13/Jul/23 08:12,14/May/21 07:34,1.11.2,,,,,1.14.0,,,Table SQL / Runtime,,,,,0,auto-unassigned,pull-request-available,,,,"The result is wrong when use constant decimal array with different precisions.
 Here is an example:
{code:sql}
create table print_sink(
	data array<decimal(3,2)>
) with (
      'connector' = 'print'
);
insert into print_sink
select array[0.12, 0.5, 0.99];
{code}
The result will be:
{code:java}
+I([0.12, 0.05, 0.99])
{code}",,FrankZou,icshuo,jark,jkillers,leonard,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19797,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 14 07:34:35 UTC 2021,,,,,,,,,,"0|z0jzfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/20 09:38;libenchao;[~FrankZou] Thanks for reporting this bug, would you like to fix this?;;;","26/Oct/20 02:01;FrankZou;[~libenchao] Yes, I'd like to.;;;","26/Oct/20 02:20;libenchao;[~FrankZou] assigned to you~;;;","05/Nov/20 12:48;jkillers;{code:java}
// precision=3,scale=2
create table print_sink(
	data array<decimal(3,2)>
) with (
      'connector' = 'print'
);

// 0.12(precision=3,scale=2)
// 0.5(precision=2,scale=1)
// 0.50(precision=3,scale=2)
insert into print_sink
select array[0.12, 0.5, 0.50];

// result  +I([0.12, 0.05, 0.50]){code}
Precision when parsing create statement: < Precision:3 , Scale:2 >
 Precision when parsing query statements: < Precision:2 , Scale:1 > 
 When using the converter, the accuracy: < Precision:3 , Scale:2 > [check|https://github.com/apache/flink/blob/61b1106615773268eb08ffcd2c9b6fe74a6fd663/flink-table/flink-table-common/src/main/java/org/apache/flink/table/data/ArrayData.java#L186]
 The accuracy of both sides does not match

@[~jark]  @[~lzljs3620320]  ;;;","16/Apr/21 10:50;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:50;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","13/May/21 01:53;icshuo;This jira seems silent for long time. I'll take it, cc [~lzljs3620320].;;;","14/May/21 06:27;lzljs3620320;Thanks [~icshuo] assigned;;;","14/May/21 07:34;lzljs3620320;Fixed via master: 0f51995bc156435eb9e97d2ac15e0225117877e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testKafkaSourceSinkWithMetadata fails on AZP,FLINK-19793,13336949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,trohrmann,trohrmann,23/Oct/20 15:52,27/Oct/20 19:48,13/Jul/23 08:12,27/Oct/20 19:46,1.12.0,,,,,1.12.0,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"The {{KafkaTableITCase.testKafkaSourceSinkWithMetadata}} seems to fail on AZP:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8197&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

{code}
Expected: <data 1,1,CreateTime,2020-03-08T13:12:11.123,0,0,{k1=[B@4ea4e0f3, k2=[B@7c9ecd9e},0,metadata_topic_avro,true>
<data 2,2,CreateTime,2020-03-09T13:12:11.123,1,0,{},0,metadata_topic_avro,false>
<data 3,3,CreateTime,2020-03-10T13:12:11.123,2,0,{k1=[B@27cd156a, k2=[B@4af44e42},0,metadata_topic_avro,true>
     but: was <data 1,1,CreateTime,2020-03-08T13:12:11.123,0,0,{k1=[B@4ea4e0f3, k2=[B@7c9ecd9e},0,metadata_topic_avro,true>
<data 2,2,CreateTime,2020-03-09T13:12:11.123,1,0,{},0,metadata_topic_avro,false>
<data 3,3,CreateTime,2020-03-10T13:12:11.123,2,0,{k1=[B@27cd156a, k2=[B@4af44e42},0,metadata_topic_avro,true>
{code}",,dian.fu,jark,kezhuw,leslieyuan,pnowojski,rmetzger,trohrmann,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19275,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 19:48:15 UTC 2020,,,,,,,,,,"0|z0jylk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/20 17:06;pnowojski;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8243&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","26/Oct/20 01:34;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8185&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","26/Oct/20 01:35;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8214&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","26/Oct/20 01:44;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8231&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=c1d93a6a-ba91-515d-3196-2ee8019fbda7;;;","26/Oct/20 01:47;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8249&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19;;;","26/Oct/20 18:05;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8307&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","27/Oct/20 01:27;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8323&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","27/Oct/20 02:52;twalthr;Will take care of this.;;;","27/Oct/20 11:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8360&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","27/Oct/20 11:30;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8352&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","27/Oct/20 11:49;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8366&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","27/Oct/20 15:10;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8379&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","27/Oct/20 19:46;twalthr;Fixed in 1.12.0: b60c1f942c9119c68fc8bce9d2e6c43bf7ba93b4;;;","27/Oct/20 19:48;rmetzger;Thanks for merging a fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interval join with equal time attributes is not recognized,FLINK-19792,13336943,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,slinkydeveloper,twalthr,twalthr,23/Oct/20 14:56,30/Nov/21 05:56,13/Jul/23 08:12,01/Oct/21 08:44,,,,,,1.15.0,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,pull-request-available,,,,"A user reported that interval joins with equal time attribute predicate are not recognized, instead a regular inner join is used:

For example:
{code}
table1 = table_env.from_path(""table1"")
table2 = table_env.from_path(""table2"")

print(table1.join(table2).where(""ts = ts2 && id = id2"").select(""id, ts"")
{code}

The documentation clearly states that this should be supported:
{code}
For example, the following predicates are valid interval join conditions:

ltime === rtime
ltime >= rtime && ltime < rtime + 10.minutes
{code}
Source: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#joins

See also the discussion here:
https://stackoverflow.com/q/64445207/806430",,alexmojaki,alpinegizmo,jark,kezhuw,knaufk,libenchao,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 01 08:44:21 UTC 2021,,,,,,,,,,"0|z0jyk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 14:58;alexmojaki;Fortunately just ""ts >= ts2 && ts <= ts2"" seems to work fine.;;;","22/Apr/21 11:04;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:05;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Sep/21 08:14;slinkydeveloper;I tried to replicate the issue with the following query (rowtime is just a TIMESTAMP typed attribute computed from another attribute):
{code:java}
SELECT t1.a, t2.b FROM A t1, B t2 WHERE t1.rowtime = t2.rowtime AND t1.a = t2.a
{code}
And I see a proper interval join is planned:
{code:java}
Calc(select=[a, b])
+- IntervalJoin(joinType=[InnerJoin], windowBounds=[isRowTime=true, leftLowerBound=0, leftUpperBound=0, leftTimeIndex=1, rightTimeIndex=2], where=[((rowtime = rowtime0) AND (a = a0))], select=[a, rowtime, a0, b, rowtime0])
   :- Exchange(distribution=[hash[rowtime, a]])
   :  +- WatermarkAssigner(rowtime=[rowtime], watermark=[(rowtime - 1000:INTERVAL SECOND)])
   :     +- Calc(select=[a, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
   :        +- TableSourceScan(table=[[default_catalog, default_database, A, project=[a, c], metadata=[]]], fields=[a, c])
   +- Exchange(distribution=[hash[rowtime, a]])
      +- WatermarkAssigner(rowtime=[rowtime], watermark=[(rowtime - 1000:INTERVAL SECOND)])
         +- Calc(select=[a, b, TO_TIMESTAMP(FROM_UNIXTIME(c)) AS rowtime])
            +- TableSourceScan(table=[[default_catalog, default_database, B]], fields=[a, b, c])
{code}
Am I overlooking something?

 ;;;","01/Oct/21 08:44;twalthr;Fixed in master: 7033cbfe404bea1519d3342a611e2f92768d70f9

I did not back port this change due to the involved plan changes. A workaround has been mentioned in the comments.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitionRequestClientFactoryTest.testInterruptsNotCached fails with NullPointerException,FLINK-19791,13336925,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,rmetzger,rmetzger,23/Oct/20 13:32,22/Jun/21 13:55,13/Jul/23 08:12,23/Nov/20 13:12,1.12.0,,,,,1.12.0,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8517&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374

{code}
2020-10-23T13:25:12.0774554Z [ERROR] testInterruptsNotCached(org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest)  Time elapsed: 0.762 s  <<< ERROR!
2020-10-23T13:25:12.0775695Z java.io.IOException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0776455Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:95)
2020-10-23T13:25:12.0777038Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.testInterruptsNotCached(PartitionRequestClientFactoryTest.java:72)
2020-10-23T13:25:12.0777465Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T13:25:12.0777815Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T13:25:12.0778221Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T13:25:12.0778581Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T13:25:12.0778921Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-23T13:25:12.0779331Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-23T13:25:12.0779733Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-23T13:25:12.0780117Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-23T13:25:12.0780484Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-23T13:25:12.0780851Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-23T13:25:12.0781236Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-23T13:25:12.0781600Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-23T13:25:12.0781937Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-23T13:25:12.0782431Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-23T13:25:12.0782877Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-23T13:25:12.0783223Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-23T13:25:12.0783541Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-23T13:25:12.0783905Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-23T13:25:12.0784315Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-23T13:25:12.0784718Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-23T13:25:12.0785125Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-23T13:25:12.0785552Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-23T13:25:12.0785980Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-23T13:25:12.0786379Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-23T13:25:12.0786763Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-23T13:25:12.0787922Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0788575Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-10-23T13:25:12.0788954Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-10-23T13:25:12.0789431Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:88)
2020-10-23T13:25:12.0789808Z 	... 26 more
2020-10-23T13:25:12.0790546Z Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0791396Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:134)
2020-10-23T13:25:12.0791959Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111)
2020-10-23T13:25:12.0792732Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77)
2020-10-23T13:25:12.0793118Z 	... 26 more
2020-10-23T13:25:12.0793342Z Caused by: java.lang.NullPointerException
2020-10-23T13:25:12.0793681Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:61)
2020-10-23T13:25:12.0794319Z 	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.<init>(NettyPartitionRequestClient.java:73)
2020-10-23T13:25:12.0794854Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:126)
{code}
",,AHeise,dian.fu,rmetzger,roman,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19688,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 13:12:13 UTC 2020,,,,,,,,,,"0|z0jyg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 13:32;rmetzger;CC [~roman_khachatryan];;;","26/Oct/20 01:48;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8249&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada;;;","26/Oct/20 06:56;arvid;Merged a fix into master as 0184672733fb2417ca9c23c30f5183bb3dff5dd0 .;;;","13/Nov/20 13:44;rmetzger;I'm not sure if this problem has been really fixed. While testing the RC 1 of Flink 1.12.0, I saw the following exception:

{code}
2020-11-13 14:39:15,566 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Co-Flat Map (1/4) (0602ab4f0306596872a928c6375bd153) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@4102bd05.
org.apache.flink.runtime.io.network.partition.consumer.PartitionConnectionException: Connection for partition be51d31b9b1185e636f8b0e964615117#1@96cf744116e8d64d20ca53ccedac43c3 not reachable.
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:163) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:314) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:286) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:94) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:577) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:541) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_222]
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/192.168.1.25:57359' has failed. This might indicate that the remote task manager has been lost.
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:95) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:160) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	... 12 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/192.168.1.25:57359' has failed. This might indicate that the remote task manager has been lost.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_222]
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_222]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:88) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:160) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	... 12 more
Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/192.168.1.25:57359' has failed. This might indicate that the remote task manager has been lost.
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:134) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:160) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	... 12 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:61) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.<init>(NettyPartitionRequestClient.java:73) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:126) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:160) ~[flink-dist_2.11-1.12.0.jar:1.12.0]
	... 12 more
{code};;;","13/Nov/20 14:56;roman;I think it's indeed a network error as the message suggests.

The wrapped NullPointerException just means that the channel isn't connected (because of some previous error).;;;","16/Nov/20 18:16;roman;The original issue was caused by an incorrect test. Here, the test doesn't seem to be involved.

[~rmetzger], can you please open a separate ticket and close this one? (if you suspect this is not a network problem).;;;","23/Nov/20 13:12;arvid;I think the second issue is a duplicate of https://issues.apache.org/jira/browse/FLINK-19925 . I'm closing this ticket as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Writing MAP<STRING, STRING> to Kafka with JSON format produces incorrect data.",FLINK-19790,13336919,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,libenchao,fhueske,fhueske,23/Oct/20 12:48,05/Nov/20 02:31,13/Jul/23 08:12,05/Nov/20 02:31,1.11.2,,,,,1.11.3,1.12.0,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,,"Running the following SQL script writes incorrect data to Kafka:
{code:java}
CREATE TEMPORARY TABLE tmp_1 (m MAP<String, String>) WITH (
  'connector' = 'kafka',
  'format' = 'json',
  'properties.bootstrap.servers' = '...',
  'properties.group.id' = '...',
  'topic' = 'tmp-1'
);

CREATE TEMPORARY TABLE gen (k STRING, v STRING) WITH (
  'connector' = 'datagen'
);

CREATE TEMPORARY VIEW gen_short AS
SELECT SUBSTR(k, 0, 4) AS k, SUBSTR(v, 0, 4) AS v FROM gen;

INSERT INTO tmp_1
SELECT MAP[k, v] FROM gen_short; {code}
Printing the content of the {{tmp-1}} topics results in the following output:
{code:java}
$ kafka-console-consumer --bootstrap-server ... --from-beginning --topic tmp-1 | head -n 5
{""m"":{""8a93"":""6102""}}
{""m"":{""8a93"":""6102"",""7922"":""f737""}}
{""m"":{""8a93"":""6102"",""7922"":""f737"",""9b63"":""15b0""}}
{""m"":{""8a93"":""6102"",""7922"":""f737"",""9b63"":""15b0"",""c38b"":""b55c""}}
{""m"":{""8a93"":""6102"",""7922"":""f737"",""9b63"":""15b0"",""c38b"":""b55c"",""222c"":""f3e2""}}
{code}
As you can see, the map is not correctly encoded as JSON and written to Kafka.

I've run the query with the Blink planner with object reuse and operator pipelining disabled.
Writing with Avro works as expected.

Hence I assume that the JSON encoder/serializer reuses the Map object when encoding the JSON.

 

 
 ",,fhueske,jark,kezhuw,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 12:49:12 UTC 2020,,,,,,,,,,"0|z0jyew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 13:22;libenchao;[~fhueske] Sure, this is a bug in the map converter for JSON format, I'll fix it.;;;","23/Oct/20 13:26;fhueske;Thanks for the quick response and working on a fix [~libenchao]! (y);;;","04/Nov/20 12:49;libenchao;Fixed via 
- b7487bd85bedea8bd50e706c253a61ebbcf8a8bc (1.12.0)
- b50fc62e5b08363932815e0c68fbea3f08f0011f(1.11.3);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink doesn't set proper nullability for Logical types for Confluent Avro Serialization,FLINK-19786,13336895,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,maver1ck,maver1ck,23/Oct/20 09:50,29/Oct/20 15:55,13/Jul/23 08:12,29/Oct/20 15:55,1.12.0,,,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,,,"When Flink is creating schema in registry nullability is not properly set for logical types.
Examples. Table:


{code:sql}
create table `test_logical_null` (
	`string_field` STRING,
	`timestamp_field` TIMESTAMP(3)
) WITH (
  'connector' = 'kafka', 
  'topic' = 'test-logical-null', 
  'properties.bootstrap.servers' = 'localhost:9092', 
  'properties.group.id' = 'test12345', 
   'scan.startup.mode' = 'earliest-offset', 
  'format' = 'avro-confluent', -- Must be set to 'avro-confluent' to configure this format.
  'avro-confluent.schema-registry.url' = 'http://localhost:8081', -- URL to connect to Confluent Schema Registry
  'avro-confluent.schema-registry.subject' = 'test-logical-null' -- Subject name to write to the Schema Registry service; required for sinks
)
{code}

Schema:


{code:json}
{
  ""type"": ""record"",
  ""name"": ""record"",
  ""fields"": [
    {
      ""name"": ""string_field"",
      ""type"": [
        ""string"",
        ""null""
      ]
    },
    {
      ""name"": ""timestamp_field"",
      ""type"": {
        ""type"": ""long"",
        ""logicalType"": ""timestamp-millis""
      }
    }
  ]
}
{code}

For not null fields:

{code:sql}
create table `test_logical_notnull` (
	`string_field` STRING NOT NULL,
	`timestamp_field` TIMESTAMP(3) NOT NULL
) WITH (
  'connector' = 'kafka', 
  'topic' = 'test-logical-notnull', 
  'properties.bootstrap.servers' = 'localhost:9092', 
  'properties.group.id' = 'test12345', 
   'scan.startup.mode' = 'earliest-offset', 
  'format' = 'avro-confluent', -- Must be set to 'avro-confluent' to configure this format.
  'avro-confluent.schema-registry.url' = 'http://localhost:8081', -- URL to connect to Confluent Schema Registry
  'avro-confluent.schema-registry.subject' = 'test-logical-notnull-value' -- Subject name to write to the Schema Registry service; required for sinks
);
{code}
Schema

{code:json}
{
  ""type"": ""record"",
  ""name"": ""record"",
  ""fields"": [
    {
      ""name"": ""string_field"",
      ""type"": ""string""
    },
    {
      ""name"": ""timestamp_field"",
      ""type"": {
        ""type"": ""long"",
        ""logicalType"": ""timestamp-millis""
      }
    }
  ]
}
{code}
As we can see for string_field we have proper union with null (for nullable field). For timestamp_field in both examples union is missing.",,danny0405,dwysakowicz,maver1ck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 15:55:25 UTC 2020,,,,,,,,,,"0|z0jy9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/20 15:55;dwysakowicz;Fixed in 146269db821f975af65b61f7dde8720591abdda6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove the ""record_"" field name prefix for Confluent Avro format deserialization",FLINK-19779,13336864,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,danny0405,danny0405,23/Oct/20 07:04,29/Oct/20 15:54,13/Jul/23 08:12,29/Oct/20 15:54,1.12.0,,,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"Reported by Maciej Bryński :

Problem is this is not compatible. I'm unable to read anything from Kafka using Confluent Registry. Example:
I have data in Kafka with following value schema:


{code:java}
{
  ""type"": ""record"",
  ""name"": ""myrecord"",
  ""fields"": [
    {
      ""name"": ""f1"",
      ""type"": ""string""
    }
  ]
}
{code}

I'm creating table using this avro-confluent format:


{code:sql}
create table `test` (
	`f1` STRING
) WITH (
  'connector' = 'kafka', 
  'topic' = 'test', 
  'properties.bootstrap.servers' = 'localhost:9092', 
  'properties.group.id' = 'test1234', 
   'scan.startup.mode' = 'earliest-offset', 
  'format' = 'avro-confluent'
  'avro-confluent.schema-registry.url' = 'http://localhost:8081'
);
{code}

When trying to select data I'm getting error:


{code:noformat}
SELECT * FROM test;
[ERROR] Could not execute SQL statement. Reason:
org.apache.avro.AvroTypeException: Found myrecord, expecting record, missing required field record_f1
{code}
",,danny0405,dwysakowicz,jark,limbo,maver1ck,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 15:54:54 UTC 2020,,,,,,,,,,"0|z0jy2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 07:07;maver1ck;Thanks [~danny0405] for putting this bug here.

I was using flink from master branch. Commit: 
78b3f2e7d5d0a5a4d341587951b2dc54a5ef427b;;;","23/Oct/20 07:12;jark;Not compatible with what? ;;;","23/Oct/20 08:03;maver1ck;[~jark] 
This was an answer to this comment:

https://github.com/apache/flink/pull/12919#issuecomment-714889392;;;","23/Oct/20 09:52;xiaozilong;Hi [~danny0405] , I think there is still existed a bug in PR. The code what cause the problem like: 
{code:java}
RowType rowType = (RowType) TableSchema.builder()
   .field(""row1"", DataTypes.ROW(DataTypes.FIELD(""a"", DataTypes.STRING())))
   .field(""row2"", DataTypes.ROW(DataTypes.FIELD(""b"", DataTypes.STRING())))
   .field(""row3"", DataTypes.ROW(
      DataTypes.FIELD(""row2"", DataTypes.ROW(DataTypes.FIELD(""c"", DataTypes.STRING())))))
   .build().toRowDataType().getLogicalType();
Schema schema = AvroSchemaConverter.convertToSchema(rowType);
System.out.println(schema.toString(true));{code}
it will throw:
{code:java}
org.apache.avro.SchemaParseException: Can't redefine: row2{code}
I think we can use a special string as the top name to replace `record_`? Thanks.
 ;;;","23/Oct/20 10:02;jark;Thanks [~maver1ck] for the link and reporting this problem. Let's keep discussion in this JIRA issue, because the notification of pull request is rather rarely received. 

Regarding to the problem and the fix [~danny0405] provided, I'm still confused what's the root cause of {{org.apache.avro.AvroTypeException: Found myrecord, expecting record, missing required field record_f1}}. 

I guess you have written avro data using Flink SQL into Kafka days ago (maybe Flink 1.11.x?), and upgrade Flink to the latest master version, and using the new Flink SQL to consume the avro data in Kafka, the exception is thrown. Is that the ""compatible"" problem you mentioned? As the Avro schema are not compatible now. ;;;","23/Oct/20 10:52;maver1ck;No.

I've written data to Kafka manually.

I wasn't able to read it without this patch as Flink was expecting record_ prefix for all fields. With this patch I'm able to read data.

 I'm using following command line to write data to Kafka
{code:java}
kafka-avro-console-producer --broker-list localhost:9092 --topic test --property value.schema='{""type"":""record"",""name"":""myrecord"",""fields"":[{""name"":""f1"",""type"":""string""}]}' --property schema.registry.url=http://localhost:8081{code}
Then I'm sending following record
{code:java}
{""f1"":""xyz""}{code}
From the error message only this part is important:

*missing required field record_f1*

 ;;;","23/Oct/20 14:11;jark;Thanks [~maver1ck], will dig into the issue. ;;;","26/Oct/20 07:50;danny0405;[~xiaozilong] Yes, the avro schema builder does not allow same name field names, even if they are in different scope (different layer).

[~maver1ck] I have fixed the nullability and precision problem, please check.;;;","26/Oct/20 08:45;xiaozilong;[~danny0405] So, we should update the code? I think the code that I report should be allowed in practice, but it doesn't work in [this PR|https://github.com/apache/flink/pull/13763].;;;","29/Oct/20 15:54;dwysakowicz;Fixed in f570d04885aa1c63c9b9c0d1ad702f49aa0b75a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NullPointException for WindowOperator.close(),FLINK-19777,13336854,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,frank wang,frank wang,23/Oct/20 06:07,27/Oct/20 07:22,13/Jul/23 08:12,27/Oct/20 07:22,1.11.2,,,,,1.11.3,1.12.0,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"i use flink sql run a job,the sql and metadata is :
 meta :

1>soure: kafka
 create table metric_source_window_table(

`metricName` String,

`namespace` String,

`timestamp` BIGINT,

`doubleValue` DOUBLE,

`longValue` BIGINT,

`metricsValue` String,

`tags` MAP<String, String>,

`meta` Map<String, String>,

t as TO_TIMESTAMP(FROM_UNIXTIME(`timestamp`/1000,'yyyy-MM-dd HH:mm:ss')),

WATERMARK FOR t AS t) WITH (

'connector' = 'kafka',

'topic' = 'ai-platform',

'properties.bootstrap.servers' = 'xxx',

'properties.group.id' = 'metricgroup',

'scan.startup.mode'='earliest-offset',

'format' = 'json',

'json.fail-on-missing-field' = 'false',

'json.ignore-parse-errors' = 'true')

2>sink to clickhouse(the clickhouse-connector was developed by ourself)

create table flink_metric_window_table(

`timestamp` BIGINT,

`longValue` BIGINT,

`metricName` String,

`metricsValueSum` DOUBLE,

`metricsValueMin` DOUBLE,

`metricsValueMax` DOUBLE,

`tag_record_id` String,

`tag_host_ip` String,

`tag_instance` String,

`tag_job_name` String,

`tag_ai_app_name` String,

`tag_namespace` String,

`tag_ai_type` String,

`tag_host_name` String,

`tag_alarm_domain` String) WITH (

'connector.type' = 'clickhouse',

'connector.property-version' = '1',

'connector.url' = 'jdbc:clickhouse://xxx:8123/dataeye',

'connector.cluster'='ck_cluster',

'connector.write.flush.max-rows'='6000',

'connector.write.flush.interval'='1000',

'connector.table' = 'flink_metric_table_all')

my sql is :

insert into
 hive.temp_vipflink.flink_metric_window_table
select
 cast(HOP_ROWTIME(t, INTERVAL '60' SECOND, INTERVAL '15' MINUTE) AS BIGINT) AS `timestamps`,
 sum(COALESCE( `longValue`, 0)) AS longValue,
 metricName,
 sum(IF(IS_DIGIT(metricsValue), cast(metricsValue AS DOUBLE), 0)) AS metricsValueSum,
 min(IF(IS_DIGIT(metricsValue), cast(metricsValue AS DOUBLE), 0)) AS metricsValueMin,
 max(IF(IS_DIGIT(metricsValue), cast(metricsValue AS DOUBLE), 0)) AS metricsValueMax,
 tags ['record_id'],
 tags ['host_ip'],
 tags ['instance'],
 tags ['job_name'],
 tags ['ai_app_name'],
 tags ['namespace'],
 tags ['ai_type'],
 tags ['host_name'],
 tags ['alarm_domain']
from
 hive.temp_vipflink.metric_source_window_table
 group by 
 metricName,
 tags ['record_id'],
 tags ['host_ip'],
 tags ['instance'],
 tags ['job_name'],
 tags ['ai_app_name'],
 tags ['namespace'],
 tags ['ai_type'],
 tags ['host_name'],
 tags ['alarm_domain'],
 HOP(t, INTERVAL '60' SECOND, INTERVAL '15' MINUTE)

 

when i run this sql for a long hours, it will appear a exception like this:

[2020-10-22 20:54:52.089] [ERROR] [GroupWindowAggregate(groupBy=[metricName, $f1, $f2, $f3, $f4, $f5, $f6, $f7, $f8, $f9], window=[SlidingGroupWindow('w$, t, 900000, 60000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[metricName, $f1, $f2, $f3, $f4, $f5, $f6, $f7, $f8, $f9, SUM($f11) AS longValue, SUM($f12) AS metricsValueSum, MIN($f12) AS metricsValueMin, MAX($f12) AS metricsValueMax, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]) -> Calc(select=[CAST(CAST(w$rowtime)) AS timestamps, longValue, metricName, metricsValueSum, metricsValueMin, metricsValueMax, $f1 AS EXPR$6, $f2 AS EXPR$7, $f3 AS EXPR$8, $f4 AS EXPR$9, $f5 AS EXPR$10, $f6 AS EXPR$11, $f7 AS EXPR$12, $f8 AS EXPR$13, $f9 AS EXPR$14]) -> SinkConversionToTuple2 -> Sink: JdbcUpsertTableSink(timestamp, longValue, metricName, metricsValueSum, metricsValueMin, metricsValueMax, tag_record_id, tag_host_ip, tag_instance, tag_job_name, tag_ai_app_name, tag_namespace, tag_ai_type, tag_host_name, tag_alarm_domain) (23/44)] [org.apache.flink.streaming.runtime.tasks.StreamTask] >>> Error during disposal of stream operator. java.lang.NullPointerException: null at org.apache.flink.table.runtime.operators.window.WindowOperator.dispose(WindowOperator.java:318) ~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:729) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:645) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:549) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]

 

finally ,this job is error, and this job will be failed","jdk 1.8.0_262
flink 1.11.1",frank wang,hailong wang,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 02:34:09 UTC 2020,,,,,,,,,,"0|z0jy0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 11:27;hailong wang;Hi [~jark], I wonder under what circumstances will the close method be called before the open method?;;;","23/Oct/20 14:08;jark;If the job is failed before the window operator initialized, this might happen. [~hailong wang];;;","26/Oct/20 02:22;hailong wang;You are right. More correctly, the dispose method is called before the open method when job is failed before the operator initialized. For the close method is invoked when job finished or is canceled, and when job is failed, the dispose method is invoked.;;;","27/Oct/20 02:34;jark;Fixed in
 - master: 1bdac236eb0eb19ab2125bf70ebb1099e0bca468
 - 1.11: ea96f48b38fad49cfdbbac842a730ba2b298b796;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemProcessingTimeServiceTest.testImmediateShutdown is instable,FLINK-19775,13336827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,dian.fu,dian.fu,23/Oct/20 02:01,25/Nov/20 09:32,13/Jul/23 08:12,25/Nov/20 09:32,1.11.0,,,,,1.11.3,1.12.0,,API / DataStream,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8131&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=66b5c59a-0094-561d-0e44-b149dfdd586d

{code}
2020-10-22T21:12:54.9462382Z [ERROR] testImmediateShutdown(org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest)  Time elapsed: 0.009 s  <<< ERROR!
2020-10-22T21:12:54.9463024Z java.lang.InterruptedException
2020-10-22T21:12:54.9463331Z 	at java.lang.Object.wait(Native Method)
2020-10-22T21:12:54.9463766Z 	at java.lang.Object.wait(Object.java:502)
2020-10-22T21:12:54.9464140Z 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:63)
2020-10-22T21:12:54.9466014Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.testImmediateShutdown(SystemProcessingTimeServiceTest.java:154)
{code}",,dian.fu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 09:32:50 UTC 2020,,,,,,,,,,"0|z0jxug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/20 10:00;873925389@qq.com;Hi [~dian.fu] , i'm interested in this issus , could you assignee to me to repair.;;;","13/Nov/20 13:50;dian.fu;[~873925389@qq.com] Thanks a lot, have assigned it to you~;;;","16/Nov/20 05:26;873925389@qq.com;Hi,[~dian.fu], Looking forward to your follow-up to this pr （https://github.com/apache/flink/pull/14076）;;;","19/Nov/20 12:57;873925389@qq.com;[~dian.fu]，Is there a way to reproduce it locally?;;;","19/Nov/20 13:02;dian.fu;I have no idea how to reproduce it locally. You could try to run it repeatedly to see if it could be reproduced.;;;","20/Nov/20 09:40;873925389@qq.com;[@tillrohrmann,|https://github.com/tillrohrmann] [~dian.fu]  ,Maybe I found the reason for the instability. It depends on when the thread A calling the await() method acquires the lock of the lock object in the jvm lock pool. According to the lock.wait() code in the await() method, we will know that it will cause the current thread A to enter the blocking state. At this point, trigger() will perform its work in the thread B to which it belongs. Suppose that when thread B executes to lock.notifyAll(), thread A will be awakened, but the lock of the lock object is not allocated in time and is still blocked. As long as the interrupt flag of thread A is true, InterruptedException will be thrown. So I thought of two solutions. The first point: Thread B changes the interrupt flag of thread A to false before executing lock.notifyAll(). This method can ensure that the A thread will not throw InterruptedException after being awakened, but how to restore the thread interruption flag for different threads still needs to be considered. The second point: Use catch code to catch the exception in the A thread, it will automatically restore the thread interruption flag, and make the compilation pass. But the second method cannot solve the problem of exception throwing.;;;","20/Nov/20 14:47;trohrmann;I think this is not what is happening here. I think the test case {{testShutdownServiceUninterruptible}} which runs before the failing test did not properly clear the {{isInterrupted}} flag. I will create a PR to fix it.;;;","23/Nov/20 01:56;873925389@qq.com;Okay, that sounds good;;;","25/Nov/20 09:32;trohrmann;Fixed via

1.12.0: c8521674e42f2a9a55b9ee0d2ffc2ef0911f4b60
1.11.3: 306bc8dc3863cc95bc7276d1aa3b32ace78bc4ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when accessing null array from postgres in JDBC Connector,FLINK-19771,13336732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dforciea,dforciea,dforciea,22/Oct/20 13:51,28/May/21 07:12,13/Jul/23 08:12,22/Jan/21 09:56,1.11.2,,,,,1.11.4,1.12.2,1.13.0,Connectors / JDBC,,,,,0,pull-request-available,,,,,"When trying to utilize the JDBC Connector for Postgres, I tried to read in a text array. When a row that was null was attempted to be read in, the connector threw an exception and execution stopped. It appears looking at the source code that if the row is null that it will still attempt to grab the contents out: [https://github.com/apache/flink/blob/release-1.11.2/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java#L97]

The stack trace is as follows:
{code:java}
[error] Caused by: java.io.IOException: Couldn't access resultSet
[error]   at org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat.nextRecord(JdbcRowDataInputFormat.java:266)
[error]   at org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat.nextRecord(JdbcRowDataInputFormat.java:57)
[error]   at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:91)
[error]   at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
[error]   at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
[error]   at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
[error] Caused by: java.lang.NullPointerException
[error]   at org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.lambda$createPostgresArrayConverter$c06ce9f4$2(PostgresRowConverter.java:97)
[error]   at org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.toInternal(AbstractJdbcRowConverter.java:79)
[error]   at org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat.nextRecord(JdbcRowDataInputFormat.java:259)
[error]   ... 5 more {code}",,dforciea,jark,leonard,nicholasjiang,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 09:56:30 UTC 2021,,,,,,,,,,"0|z0jx9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 13:31;nicholasjiang;[~dforciea], IMO, PostgresRowConverter#createPostgresArrayConverter doesn't check whether the pgArray is null. If pgArray is null, the converter should return null. Therefore, createPostgresArrayConverter should the check whether the pgArray is null.
cc [~jark], [~Leonard Xu];;;","23/Oct/20 14:17;jark;I think {{PostgresRowConverter}} should override {{AbstractJdbcRowConverter#createInternalConverter}} instead of {{createNullableInternalConverter}}.;;;","23/Oct/20 14:44;dforciea;I wouldn't mind taking this one on... but I would have a question about adding unit tests for this bug. Digging in, I don't see anything that directly tests out the {{PostgresRowConverter}}. It looks like the existing tests only test the dialect itself, and otherwise just uses Derby.

Should a unit test for {{PostgresRowConverter}} be added? Or did I miss seeing an end to end test that spins up a postgres container to perform validation?;;;","26/Oct/20 00:06;dforciea;[~nicholasjiang] [~jark] [~Leonard Xu]

I created a branch with the fix as [~jark] described in his comment, and I also created some unit tests for the PostgresRowConverter that were missing. Let me know if you think this is a proper approach, and I'll gladly create a PR for this.

 

https://github.com/dforciea/flink/compare/master...dforciea:FLINK-19771;;;","26/Oct/20 02:18;leonard;Thanks [~dforciea] for reporting,

I think we should override `createInternalConverter` and reuse null check in `createNullableInternalConverter`.



The `DerbyRowConverter` and `MySQLRowConverter` have same logic and so use derby is enough, For `PostgresRowConverter`  test,  `PostgresCatalogITCase` cover it, I think we can add null array test in `PostgresCatalogITCase.testArrayTypes`.

 ;;;","26/Oct/20 02:56;dforciea;[~Leonard Xu] Thank you for the pointer on where there are postgres-specific tests. I have updated my branch accordingly to accomplish what you've described. Let me know if you want me to submit it as a PR.;;;","26/Oct/20 03:01;leonard;[~dforciea] 

Your branch makes sense to me, I think you can submit a PR to fix this, thanks.

 ;;;","05/Nov/20 19:27;dforciea;Sorry, I think I had put in the wrong issue number on my commit message in the PR. That is now fixed, if that is holding this up.;;;","22/Jan/21 09:56;twalthr;Fixed in 1.13.0: 2bbac78b9eaa812b113cfc34f57c14ef3a3c35bc
Fixed in 1.12.2: d11137ea1f68a0480ada713e65065c092b281e1e
Fixed in 1.11.4: 428c67369bb1863e0d5e67b7b197fbd73f140e42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonProgramOptionsTest requires package phase before execution,FLINK-19770,13336723,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,csq,juha.mynttinen,juha.mynttinen,22/Oct/20 12:57,29/Oct/20 02:12,13/Jul/23 08:12,29/Oct/20 02:12,1.12.0,,,,,1.12.0,,,API / Python,Tests,,,,0,pull-request-available,starter,,,,"The PR [https://github.com/apache/flink/pull/13322] lately added the test method  testConfigurePythonExecution in org.apache.flink.client.cli.PythonProgramOptionsTest.
 
""mvn clean verify"" fails for me in  testConfigurePythonExecution:
 
...
INFO] Running org.apache.flink.client.cli.PythonProgramOptionsTest
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.433 s <<< FAILURE! - in org.apache.flink.client.cli.PythonProgramOptionsTest
[ERROR] testConfigurePythonExecution(org.apache.flink.client.cli.PythonProgramOptionsTest)  Time elapsed: 0.019 s  <<< ERROR!
java.nio.file.NoSuchFileException: target/dummy-job-jar
at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
at java.base/java.nio.file.Files.readAttributes(Files.java:1763)
at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
at java.base/java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322)
at java.base/java.nio.file.Files.walkFileTree(Files.java:2716)
at java.base/java.nio.file.Files.walkFileTree(Files.java:2796)
at org.apache.flink.client.cli.PythonProgramOptionsTest.testConfigurePythonExecution(PythonProgramOptionsTest.java:131)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runners.Suite.runChild(Suite.java:128)
at org.junit.runners.Suite.runChild(Suite.java:27)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
...
[ERROR] Errors:
[ERROR]   PythonProgramOptionsTest.testConfigurePythonExecution:131 » NoSuchFile target/...
 
The issue is that the test expects a file to exist in {{target}}, but it's not there. This happens when running mvn in local env.

The test passes in CI environment, because there the process first builds Flink creating the file and then this test is run. ",,csq,dian.fu,juha.mynttinen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 02:12:03 UTC 2020,,,,,,,,,,"0|z0jx7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/20 13:29;chesnay;Essentially we just have to change the class name suffix to ITCase to ensure it always runs after the packaging phase.;;;","23/Oct/20 01:50;csq;[~juha.mynttinen] Thank you for reporting this issue, we should make these test cases as integration tests that would be executed after the package building phase completed as [~chesnay] mentioned. I would like to fix it. ;;;","29/Oct/20 02:12;dian.fu;Merged to master via 789d8b65fde1c86bca2fa1ec43b55686d1584c97;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeadlockBreakupTest.testSubplanReuse_AddSingletonExchange is instable,FLINK-19759,13336638,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,dian.fu,dian.fu,22/Oct/20 05:50,22/Oct/20 05:59,13/Jul/23 08:12,22/Oct/20 05:59,1.12.0,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8052&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

{code}
[ERROR]   DeadlockBreakupTest.testSubplanReuse_AddSingletonExchange:217 planAfter expected:<...=[>(cnt, 3)])
:  +- [SortAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS cnt], reuse_id=[1])
:     +- Exchange(distribution=[single])
:        +- LocalSort]Aggregate(select=[Pa...> but was:<...=[>(cnt, 3)])
:  +- [HashAggregate(isMerge=[true], select=[Final_COUNT(count1$0) AS cnt], reuse_id=[1])
:     +- Exchange(distribution=[single])
:        +- LocalHash]Aggregate(select=[Pa...>
[INFO] 
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19624,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 05:59:34 UTC 2020,,,,,,,,,,"0|z0jwoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/20 05:51;dian.fu;cc [~TsReaper] [~godfreyhe];;;","22/Oct/20 05:53;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8053&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29;;;","22/Oct/20 05:54;dian.fu;Upgrade to blocker as it's continuously failing.;;;","22/Oct/20 05:59;dian.fu;Just noticed that it's already fixed in https://github.com/apache/flink/commit/0b1c7327e9d0efa9c32c95815a0cff5f5ad0856b, closing this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CEP documentation error of the example in 'After Match Strategy' section,FLINK-19755,13336612,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,22/Oct/20 01:57,27/Oct/20 02:07,13/Jul/23 08:12,27/Oct/20 02:06,1.11.0,,,,,1.11.3,1.12.0,,Documentation,Library / CEP,,,,0,pull-request-available,,,,,"{code:java}
 symbol   tax   price         rowtime
======== ===== ======= =====================
 XYZ      1     7       2018-09-17 10:00:01
 XYZ      2     9       2018-09-17 10:00:02
 XYZ      1     10      2018-09-17 10:00:03
 XYZ      2     5       2018-09-17 10:00:04
 XYZ      2     17      2018-09-17 10:00:05
 XYZ      2     14      2018-09-17 10:00:06


SELECT *
FROM Ticker
    MATCH_RECOGNIZE(
        PARTITION BY symbol
        ORDER BY rowtime
        MEASURES
            SUM(A.price) AS sumPrice,
            FIRST(rowtime) AS startTime,
            LAST(rowtime) AS endTime
        ONE ROW PER MATCH
        [AFTER MATCH STRATEGY]
        PATTERN (A+ C)
        DEFINE
            A AS SUM(A.price) < 30
    )
 

{code}
{code:java}
AFTER MATCH SKIP TO LAST A 

symbol   sumPrice        startTime              endTime
======== ========== ===================== =====================
 XYZ      26         2018-09-17 10:00:01   2018-09-17 10:00:04
 XYZ      15         2018-09-17 10:00:03   2018-09-17 10:00:05
 XYZ      22         2018-09-17 10:00:04   2018-09-17 10:00:06
 XYZ      17         2018-09-17 10:00:05   2018-09-17 10:00:06

Again, the first result matched against the rows #1, #2, #3, #4.Compared to the previous strategy, the next match includes only row #3 (mapped to A) again for the next matching.

Therefore, the second result matched against the rows #3, #4, #5.

The third result matched against the rows #4, #5, #6.

The last result matched against the rows #5, #6.{code}
h5. i think it will exist looping match when coming to 17, 14 using ""AFTER MATCH SKIP TO LAST A """,,dian.fu,jackylau,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/20 02:42;jackylau;0F2917C8-6C35-43a9-BDD1-FD506906257E.png;https://issues.apache.org/jira/secure/attachment/13014017/0F2917C8-6C35-43a9-BDD1-FD506906257E.png","23/Oct/20 02:42;jackylau;7CA31459-B125-4cea-8C75-3D101C27A9F9.png;https://issues.apache.org/jira/secure/attachment/13014018/7CA31459-B125-4cea-8C75-3D101C27A9F9.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 02:06:08 UTC 2020,,,,,,,,,,"0|z0jwiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/20 02:00;jackylau;h5. i think it will exist looping match when coming to 17, 14 using ""AFTER MATCH SKIP TO LAST A "" 

https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/match_recognize.html#logical-offsets;;;","23/Oct/20 02:41;jackylau;hi [~jark] [~dianfu], i modify the data to solve the looping match, could you please review it ,thanks?;;;","27/Oct/20 02:06;dian.fu;Fixed in master via 691350854efbf52dba07c77ac28554edc7c733f2 and release-1.11 via 6e67b50eaba6b4c23181566bc2fb96130464f876;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix document error for GroupBy Window Aggregation,FLINK-19751,13336445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,appleyuchi,appleyuchi,21/Oct/20 10:04,29/Apr/21 15:33,13/Jul/23 08:12,29/Apr/21 15:33,,,,,,,,,Documentation,Table SQL / API,,,,0,stale-major,,,,,"I'm learning [Official document|https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/tableApi.html]

For the part ""GroupBy Window Aggregation""

change
.as(""w"")) -> .as(""w"")
please
",,appleyuchi,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 15:33:44 UTC 2021,,,,,,,,,,"0|z0jvi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/20 12:34;jark;Feel free to open pull request [~appleyuchi]. 
I found you created series documentation issues, it's a great contribution. 
You can create {{[hotfix][docs]}} pull request directly without creating JIRA issues for such documentation typos in the next time. ;;;","22/Apr/21 11:05;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 15:33;jark;This has been fixed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deserializer is not opened in Kafka consumer when restoring from state,FLINK-19750,13336430,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,renqs,renqs,21/Oct/20 08:02,26/Oct/20 04:04,13/Jul/23 08:12,26/Oct/20 04:04,1.11.0,1.11.1,1.11.2,,,1.11.3,1.12.0,,Connectors / Kafka,,,,,0,pull-request-available,,,,,"When a job using Kafka consumer is recovered from a checkpoint or savepoint, the {{open}} method of the record deserializer is not called. This is possibly because {{this.deserializer.open}} is put into the else clause by mistake, which will only be called if the job has a clean start. ",,hackergin,jark,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 12:31:50 UTC 2020,,,,,,,,,,"0|z0jveo:",9223372036854775807,"Patch merged.
master: 200be714ff7476f66b7f56cd9b266670b752afe0
release-1.11: 78cd4628285c9dfdc0ada3f59cdefc4e8a7eb0bc",,,,,,,,,,,,,,,,,,,"21/Oct/20 12:29;jark;It seems it is a notable bug, because this will lead ""avro"" format doesn't work once job is failed. I increased the priority.;;;","21/Oct/20 12:29;jark;Are you interested to provide a fix? [~renqs];;;","21/Oct/20 12:31;renqs;Sure [~jark] , I'll create a PR about this. Thank you~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyGroupRangeOffsets#KeyGroupOffsetsIterator should skip key groups that don't have a defined offset,FLINK-19748,13336415,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,21/Oct/20 06:40,08/Oct/21 11:08,13/Jul/23 08:12,07/Nov/20 02:31,,,,,,1.11.3,1.12.0,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,,,"Currently, on commit the {{UnboundedFeedbackLogger}} only calls {{startNewKeyGroup}} on the raw keyed stream for key groups that actually have logged messages:
https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/logger/UnboundedFeedbackLogger.java#L102

This means that it might skip some key groups, if a key group doesn't have any logged messages.

This doesn't conform with the expected usage of Flink's {{KeyedStateCheckpointOutputStream}}, where it expects that for ALL key groups within the range, {{startNewKeyGroup}} needs to be invoked.
The reason for this is that underneath, calling {{startNewKeyGroup}} would also record the starting stream offset position for the key group.
However, when iterating through a raw keyed stream, the key group offsets iterator {{KeyGroupRangeOffsets#KeyGroupOffsetsIterator}} doesn't take into account that some key groups weren't written and therefore do not have offsets defined, and the streams will be seeked to incorrect positions.

Ultimately, if some key groups were skipped while writing to the raw keyed stream, the following error will be thrown on restore:
{code}
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:204)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:247)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.io.IOException: position out of bounds
	at org.apache.flink.runtime.state.StatePartitionStreamProvider.getStream(StatePartitionStreamProvider.java:58)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:235)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:167)
	... 9 more
Caused by: java.io.IOException: position out of bounds
	at org.apache.flink.runtime.state.memory.ByteStreamStateHandle$ByteStateHandleInputStream.seek(ByteStreamStateHandle.java:124)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl$KeyGroupStreamIterator.next(StreamTaskStateInitializerImpl.java:442)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl$KeyGroupStreamIterator.next(StreamTaskStateInitializerImpl.java:395)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:228)
	... 10 more
{code}

h2. *Solution*

We change the {{KeyGroupRangeOffsets#KeyGroupOffsetsIterator}} in Flink to skip key groups that don't have a defined offset (i.e. {{startNewKeyGroup}} wasn't called for these key groups).",,liyu,rmetzger,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19692,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 07 02:31:26 UTC 2020,,,,,,,,,,"0|z0jvbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 16:00;rmetzger;Is this blocker expected to be merged before the feature freeze on Sunday?
;;;","07/Nov/20 02:31;tzulitai;flink/master: 3367c238ad9de0dc50735df33ff09852a3427b1b
flink/release-1.11: 12a2ade31b4ba3beb9eb3fe1d26064223fc02fec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid command in flink-end-to-end-tests README,FLINK-19742,13336345,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,dhill@promoted.ai,dhill@promoted.ai,20/Oct/20 17:43,06/Nov/20 10:13,13/Jul/23 08:12,06/Nov/20 10:13,1.12.0,,,,,1.12.0,,,Tests,,,,,0,pull-request-available,,,,,"The [README|https://github.com/apache/flink/tree/master/flink-end-to-end-tests#running-tests] refers to `{{flink-end-to-end-tests/run-pre-commit-tests.sh}}` but that file no longer exists.

 ",,aljoscha,dhill@promoted.ai,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 10:13:03 UTC 2020,,,,,,,,,,"0|z0juvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/20 02:27;xtsong;Thanks for reporting this [~dhill@promoted.ai]. This is indeed a valid problem.
Would you like to help fixing this issue?;;;","26/Oct/20 14:18;aljoscha;cc [~rmetzger];;;","26/Oct/20 14:20;aljoscha;After https://github.com/apache/flink/commit/a22b130941365813bd055bb8b7a77f6c1d499c33. Are all pre-commit tests integrated and we don't need to run them separately anymore?;;;","26/Oct/20 14:22;rmetzger;I will update the readme.;;;","06/Nov/20 10:13;chesnay;master: 2ad11e541bfd1780380517915145360cda9279ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalTimeServiceManager fails to restore due to corrupt reads if there are other users of raw keyed state streams,FLINK-19741,13336336,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,20/Oct/20 16:29,06/Sep/21 09:55,13/Jul/23 08:12,07/Nov/20 02:30,1.10.2,1.11.2,1.9.3,,,1.11.3,1.12.0,,Runtime / State Backends,,,,,0,pull-request-available,,,,,"h2. *Diagnosis*

Currently, when restoring a {{InternalTimeServiceManager}}, we always attempt to read from the provided raw keyed state streams (using {{InternalTimerServiceSerializationProxy}}):
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/InternalTimeServiceManagerImpl.java#L117

This is incorrect, since we don't write with the {{InternalTimerServiceSerializationProxy}} if the timers do not require legacy synchronous snapshots:
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/InternalTimeServiceManagerImpl.java#L192
(we currently only require that when users use RocksDB backend + heap timers).

Therefore, the {{InternalTimeServiceManager}} can fail to be created on restore due to corrupt reads in the case where:
* a checkpoint was taken where {{useLegacySynchronousSnapshots}} is false (hence nothing was written, and the time service manager does not use the raw keyed stream)
* the raw keyed stream is used elsewhere (e.g. in the Flink application's user code)
* on restore from the checkpoint, {{InternalTimeServiceManagerImpl.create()}} attempts to read from the raw keyed stream with the {{InternalTimerServiceSerializationProxy}}.

Full error stack trace (with Flink 1.11.1):
{code}
2020-10-21 13:16:51
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:204)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:247)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:479)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:475)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:528)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at java.io.DataInputStream.readUTF(DataInputStream.java:609)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.read(InternalTimerServiceSerializationProxy.java:110)
	at org.apache.flink.core.io.PostVersionedIOReadableWritable.read(PostVersionedIOReadableWritable.java:76)
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManager.restoreStateForKeyGroup(InternalTimeServiceManager.java:217)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:234)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:167)
	... 9 more
{code}

h2. *Reproducing*

- Have an application with any operator that uses and writes to raw keyed state streams
- Use heap backend + any timer factory or RocksDB backend + RocksDB timers
- Take a savepoint or wait for a checkpoint, and trigger a restore

h2. *Proposed Fix*

The fix would be to also respect the {{useLegacySynchronousSnapshots}} flag in:
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/InternalTimeServiceManagerImpl.java#L231",,aljoscha,Antti-Kaikkonen,kezhuw,klion26,liyu,nkruber,rmetzger,tzulitai,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19692,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 07 02:30:47 UTC 2020,,,,,,,,,,"0|z0juts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 16:00;rmetzger;Is this blocker expected to be merged before the feature freeze on Sunday?
It seems that the PR got approval already?;;;","06/Nov/20 02:23;tzulitai;[~rmetzger] yes, I'm running some last tests against StateFun to ensure that this works properly.
I have on my backlog to make sure this gets merged by end of week. Same for FLINK-19748.;;;","06/Nov/20 12:19;rmetzger;Great, thank you!;;;","07/Nov/20 02:30;tzulitai;flink/master: c151abc5bd6b4b642100cf75f8981f08535c5936
flink/release-1.11: 6675979838ade22cd6e069950bee362ba52ef747;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in to_pandas for table containing event time: class java.time.LocalDateTime cannot be cast to class java.sql.Timestamp,FLINK-19740,13336329,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,alexmojaki,alexmojaki,20/Oct/20 15:51,03/Nov/20 14:09,13/Jul/23 08:12,03/Nov/20 11:21,1.11.2,1.12.0,,,,1.11.3,1.12.0,,API / Python,Table SQL / API,,,,0,pull-request-available,,,,,"In a nutshell, if I create a table with an event time column:

{{CREATE TABLE simple_table (}}
 {{   ts TIMESTAMP(3),}}
 {{   WATERMARK FOR ts AS ts - INTERVAL '5' SECOND}}
 {{)}}

then it fails to serialize with .to_pandas(). This only happens with the watermark line and in streaming mode.

Full code:

from pyflink.table import EnvironmentSettings, StreamTableEnvironment

env_settings = (
 EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
)
table_env = StreamTableEnvironment.create(environment_settings=env_settings)
table_env.execute_sql(
 """"""
 CREATE TABLE simple_table (
 ts TIMESTAMP(3),
 WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
 ) WITH (
 'connector.type' = 'filesystem',
 'format.type' = 'csv',
 'connector.path' = '/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_2.csv'
 )
""""""
)

print(table_env.from_path(""simple_table"").to_pandas())

Output:
  
 WARNING: An illegal reflective access operation has occurred
 WARNING: Illegal reflective access by org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil ([file:/home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar|file:///home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar]) to constructor java.nio.DirectByteBuffer(long,int)
 WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil
 WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
 WARNING: All illegal access operations will be denied in a future release
 Traceback (most recent call last):
 File ""/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_903.py"", line 20, in <module>
 print(table_env.from_path(""simple_table"").to_pandas())
 File ""/home/alex/work/flink/flink-python/pyflink/table/table.py"", line 839, in to_pandas
 table = pa.Table.from_batches(serializer.load_from_iterator(batches))
 File ""pyarrow/table.pxi"", line 1576, in pyarrow.lib.Table.from_batches
 File ""/home/alex/work/flink/flink-python/pyflink/table/serializers.py"", line 76, in load_from_iterator
 reader = pa.ipc.open_stream(
 File ""/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/pyarrow/ipc.py"", line 146, in open_stream
 return RecordBatchStreamReader(source)
 File ""/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/pyarrow/ipc.py"", line 62, in __init__
 self._open(source)
 File ""pyarrow/ipc.pxi"", line 360, in pyarrow.lib._RecordBatchStreamReader._open
 File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status
 File ""/home/alex/work/flink/flink-python/pyflink/table/serializers.py"", line 69, in readinto
 input = self.leftover or (self.itor.next() if self.itor.hasNext() else None)
 File ""/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1285, in __call__
 return_value = get_return_value(
 File ""/home/alex/work/flink/flink-python/pyflink/util/exceptions.py"", line 147, in deco
 return f(*a, **kw)
 File ""/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value
 raise Py4JJavaError(
 py4j.protocol.Py4JJavaError: An error occurred while calling o39.next.
 : java.lang.RuntimeException: Failed to serialize the data of the table
 at org.apache.flink.table.runtime.arrow.ArrowUtils$2.next(ArrowUtils.java:683)
 at org.apache.flink.table.runtime.arrow.ArrowUtils$2.next(ArrowUtils.java:663)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base/java.lang.reflect.Method.invoke(Method.java:566)
 at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
 at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
 at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
 at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
 at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
 at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
 at java.base/java.lang.Thread.run(Thread.java:834)
 Caused by: java.lang.ClassCastException: class java.time.LocalDateTime cannot be cast to class java.sql.Timestamp (java.time.LocalDateTime is in module java.base of loader 'bootstrap'; java.sql.Timestamp is in module java.sql of loader 'platform')
 at org.apache.flink.table.data.util.DataFormatConverters$TimestampConverter.toInternalImpl(DataFormatConverters.java:897)
 at org.apache.flink.table.data.util.DataFormatConverters$DataFormatConverter.toInternal(DataFormatConverters.java:381)
 at org.apache.flink.table.data.util.DataFormatConverters$RowConverter.toInternalImpl(DataFormatConverters.java:1426)
 at org.apache.flink.table.data.util.DataFormatConverters$RowConverter.toInternalImpl(DataFormatConverters.java:1414)
 at org.apache.flink.table.data.util.DataFormatConverters$DataFormatConverter.toInternal(DataFormatConverters.java:381)
 at org.apache.flink.table.runtime.arrow.ArrowUtils$1.next(ArrowUtils.java:655)
 at org.apache.flink.table.runtime.arrow.ArrowUtils$1.next(ArrowUtils.java:641)
 at org.apache.flink.table.runtime.arrow.ArrowUtils$2.next(ArrowUtils.java:675)
 ... 12 more","Ubuntu 18.04

Python 3.8, jar built from master yesterday.

Or Python 3.7, installed latest version from pip.",alexmojaki,dian.fu,hxbks2ks,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 11:21:52 UTC 2020,,,,,,,,,,"0|z0jus8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/20 03:45;nicholasjiang;cc [~dian.fu];;;","02/Nov/20 11:26;hxbks2ks;[~alexmojaki] [~nicholasjiang] Thanks a lot for reporting this error. I will take a look.;;;","02/Nov/20 12:52;dian.fu;[~hxbks2ks] Thanks for taking this issue. I will assign it to you~;;;","03/Nov/20 11:21;dian.fu;Fixed in 
- master via c7154214cd4996472178ed7091ee8cf4be864816
- release-1.11 via 5333fbfe48542aceda3b6d21e76b48f7bde002ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompileException when windowing in batch mode: A method named ""replace"" is not declared in any enclosing class nor any supertype ",FLINK-19739,13336312,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,TsReaper,alexmojaki,alexmojaki,20/Oct/20 15:16,15/Dec/21 01:40,13/Jul/23 08:12,28/Jul/21 11:26,1.11.2,1.12.0,,,,1.12.8,1.13.3,1.14.0,Table SQL / Runtime,,,,,0,auto-deprioritized-major,auto-unassigned,pull-request-available,,,"Example script:

{code:python}
from pyflink.table import EnvironmentSettings, BatchTableEnvironment
from pyflink.table.window import Tumble


env_settings = (
    EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build()
)
table_env = BatchTableEnvironment.create(environment_settings=env_settings)

table_env.execute_sql(
    """"""
    CREATE TABLE table1 (
        amount INT,
        ts TIMESTAMP(3),
        WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
    ) WITH (
        'connector.type' = 'filesystem',
        'format.type' = 'csv',
        'connector.path' = '/home/alex/work/test-flink/data1.csv'
    )
""""""
)


table1 = table_env.from_path(""table1"")
table = (
    table1
        .window(Tumble.over(""5.days"").on(""ts"").alias(""__window""))
        .group_by(""__window"")
        .select(""amount.sum"")
)
print(table.to_pandas())
{code}

Output:

{code}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil (file:/home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
/* 1 */
/* 2 */      public class LocalHashWinAggWithoutKeys$59 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator, org.apache.flink.streaming.api.operators.BoundedOneInput {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        private static final org.slf4j.Logger LOG$2 =
/* 8 */          org.slf4j.LoggerFactory.getLogger(""LocalHashWinAgg"");
/* 9 */        
/* 10 */        private transient org.apache.flink.table.types.logical.LogicalType[] aggMapKeyTypes$5;
/* 11 */        private transient org.apache.flink.table.types.logical.LogicalType[] aggBufferTypes$6;
/* 12 */        private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap aggregateMap$7;
/* 13 */        org.apache.flink.table.data.binary.BinaryRowData emptyAggBuffer$9 = new org.apache.flink.table.data.binary.BinaryRowData(1);
/* 14 */        org.apache.flink.table.data.writer.BinaryRowWriter emptyAggBufferWriterTerm$10 = new org.apache.flink.table.data.writer.BinaryRowWriter(emptyAggBuffer$9);
/* 15 */        org.apache.flink.table.data.GenericRowData hashAggOutput = new org.apache.flink.table.data.GenericRowData(2);
/* 16 */        private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggMapKey$17 = new org.apache.flink.table.data.binary.BinaryRowData(1);
/* 17 */        private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggBuffer$18 = new org.apache.flink.table.data.binary.BinaryRowData(1);
/* 18 */        private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry reuseAggMapEntry$19 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry(reuseAggMapKey$17, reuseAggBuffer$18);
/* 19 */        org.apache.flink.table.data.binary.BinaryRowData aggMapKey$3 = new org.apache.flink.table.data.binary.BinaryRowData(1);
/* 20 */        org.apache.flink.table.data.writer.BinaryRowWriter aggMapKeyWriter$4 = new org.apache.flink.table.data.writer.BinaryRowWriter(aggMapKey$3);
/* 21 */        private boolean hasInput = false;
/* 22 */        org.apache.flink.streaming.runtime.streamrecord.StreamRecord element = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord((Object)null);
/* 23 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 24 */
/* 25 */        public LocalHashWinAggWithoutKeys$59(
/* 26 */            Object[] references,
/* 27 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 28 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 29 */            org.apache.flink.streaming.api.operators.Output output,
/* 30 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 31 */          this.references = references;
/* 32 */          aggMapKeyTypes$5 = (((org.apache.flink.table.types.logical.LogicalType[]) references[0]));
/* 33 */          aggBufferTypes$6 = (((org.apache.flink.table.types.logical.LogicalType[]) references[1]));
/* 34 */          this.setup(task, config, output);
/* 35 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 36 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 37 */              .setProcessingTimeService(processingTimeService);
/* 38 */          }
/* 39 */        }
/* 40 */
/* 41 */        @Override
/* 42 */        public void open() throws Exception {
/* 43 */          super.open();
/* 44 */          aggregateMap$7 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap(this.getContainingTask(),this.getContainingTask().getEnvironment().getMemoryManager(),computeMemorySize(), aggMapKeyTypes$5, aggBufferTypes$6);
/* 45 */          
/* 46 */          
/* 47 */          emptyAggBufferWriterTerm$10.reset();
/* 48 */          
/* 49 */          
/* 50 */          if (true) {
/* 51 */            emptyAggBufferWriterTerm$10.setNullAt(0);
/* 52 */          } else {
/* 53 */            emptyAggBufferWriterTerm$10.writeInt(0, ((int) -1));
/* 54 */          }
/* 55 */                       
/* 56 */          emptyAggBufferWriterTerm$10.complete();
/* 57 */                  
/* 58 */        }
/* 59 */
/* 60 */        @Override
/* 61 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 62 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 63 */          
/* 64 */          org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;
/* 65 */          int field$11;
/* 66 */          boolean isNull$11;
/* 67 */          int field$12;
/* 68 */          boolean isNull$12;
/* 69 */          boolean isNull$13;
/* 70 */          int result$14;
/* 71 */          org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;
/* 72 */          org.apache.flink.table.data.TimestampData field$21;
/* 73 */          boolean isNull$21;
/* 74 */          boolean isNull$22;
/* 75 */          long result$23;
/* 76 */          boolean isNull$24;
/* 77 */          long result$25;
/* 78 */          boolean isNull$26;
/* 79 */          long result$27;
/* 80 */          boolean isNull$28;
/* 81 */          long result$29;
/* 82 */          boolean isNull$30;
/* 83 */          long result$31;
/* 84 */          boolean isNull$32;
/* 85 */          long result$33;
/* 86 */          boolean isNull$34;
/* 87 */          boolean result$35;
/* 88 */          boolean isNull$36;
/* 89 */          long result$37;
/* 90 */          boolean isNull$38;
/* 91 */          long result$39;
/* 92 */          boolean isNull$40;
/* 93 */          long result$41;
/* 94 */          boolean isNull$42;
/* 95 */          long result$43;
/* 96 */          boolean isNull$44;
/* 97 */          long result$45;
/* 98 */          boolean isNull$46;
/* 99 */          long result$47;
/* 100 */          boolean isNull$48;
/* 101 */          long result$49;
/* 102 */          boolean isNull$50;
/* 103 */          long result$51;
/* 104 */          boolean isNull$52;
/* 105 */          long result$53;
/* 106 */          boolean isNull$55;
/* 107 */          long result$56;
/* 108 */          boolean isNull$57;
/* 109 */          long result$58;
/* 110 */          
/* 111 */          
/* 112 */          if (!in1.isNullAt(1)) {
/* 113 */            hasInput = true;
/* 114 */            // input field access for group key projection, window/pane assign
/* 115 */             // and aggregate map update
/* 116 */            isNull$11 = in1.isNullAt(0);
/* 117 */          field$11 = -1;
/* 118 */          if (!isNull$11) {
/* 119 */            field$11 = in1.getInt(0);
/* 120 */          }
/* 121 */          isNull$21 = in1.isNullAt(1);
/* 122 */          field$21 = null;
/* 123 */          if (!isNull$21) {
/* 124 */            field$21 = in1.getTimestamp(1, 3);
/* 125 */          }
/* 126 */            // assign timestamp(window or pane)
/* 127 */            
/* 128 */          
/* 129 */          
/* 130 */          
/* 131 */          
/* 132 */          isNull$22 = isNull$21;
/* 133 */          result$23 = -1L;
/* 134 */          if (!isNull$22) {
/* 135 */            
/* 136 */            result$23 = field$21.getMillisecond();
/* 137 */            
/* 138 */          }
/* 139 */          
/* 140 */          
/* 141 */          isNull$24 = isNull$22 || false;
/* 142 */          result$25 = -1L;
/* 143 */          if (!isNull$24) {
/* 144 */            
/* 145 */            result$25 = (long) (result$23 * ((long) 1L));
/* 146 */            
/* 147 */          }
/* 148 */          
/* 149 */          isNull$26 = isNull$21;
/* 150 */          result$27 = -1L;
/* 151 */          if (!isNull$26) {
/* 152 */            
/* 153 */            result$27 = field$21.getMillisecond();
/* 154 */            
/* 155 */          }
/* 156 */          
/* 157 */          
/* 158 */          isNull$28 = isNull$26 || false;
/* 159 */          result$29 = -1L;
/* 160 */          if (!isNull$28) {
/* 161 */            
/* 162 */            result$29 = (long) (result$27 * ((long) 1L));
/* 163 */            
/* 164 */          }
/* 165 */          
/* 166 */          
/* 167 */          isNull$30 = isNull$28 || false;
/* 168 */          result$31 = -1L;
/* 169 */          if (!isNull$30) {
/* 170 */            
/* 171 */            result$31 = (long) (result$29 - ((long) 0L));
/* 172 */            
/* 173 */          }
/* 174 */          
/* 175 */          
/* 176 */          isNull$32 = isNull$30 || false;
/* 177 */          result$33 = -1L;
/* 178 */          if (!isNull$32) {
/* 179 */            
/* 180 */            result$33 = (long) (result$31 % ((long) 432000000L));
/* 181 */            
/* 182 */          }
/* 183 */          
/* 184 */          
/* 185 */          isNull$34 = isNull$32 || false;
/* 186 */          result$35 = false;
/* 187 */          if (!isNull$34) {
/* 188 */            
/* 189 */            result$35 = result$33 < ((int) 0);
/* 190 */            
/* 191 */          }
/* 192 */          
/* 193 */          long result$54 = -1L;
/* 194 */          boolean isNull$54;
/* 195 */          if (result$35) {
/* 196 */            
/* 197 */          
/* 198 */          
/* 199 */          
/* 200 */          
/* 201 */          
/* 202 */          isNull$36 = isNull$21;
/* 203 */          result$37 = -1L;
/* 204 */          if (!isNull$36) {
/* 205 */            
/* 206 */            result$37 = field$21.getMillisecond();
/* 207 */            
/* 208 */          }
/* 209 */          
/* 210 */          
/* 211 */          isNull$38 = isNull$36 || false;
/* 212 */          result$39 = -1L;
/* 213 */          if (!isNull$38) {
/* 214 */            
/* 215 */            result$39 = (long) (result$37 * ((long) 1L));
/* 216 */            
/* 217 */          }
/* 218 */          
/* 219 */          
/* 220 */          isNull$40 = isNull$38 || false;
/* 221 */          result$41 = -1L;
/* 222 */          if (!isNull$40) {
/* 223 */            
/* 224 */            result$41 = (long) (result$39 - ((long) 0L));
/* 225 */            
/* 226 */          }
/* 227 */          
/* 228 */          
/* 229 */          isNull$42 = isNull$40 || false;
/* 230 */          result$43 = -1L;
/* 231 */          if (!isNull$42) {
/* 232 */            
/* 233 */            result$43 = (long) (result$41 % ((long) 432000000L));
/* 234 */            
/* 235 */          }
/* 236 */          
/* 237 */          
/* 238 */          isNull$44 = isNull$42 || false;
/* 239 */          result$45 = -1L;
/* 240 */          if (!isNull$44) {
/* 241 */            
/* 242 */            result$45 = (long) (result$43 + ((long) 432000000L));
/* 243 */            
/* 244 */          }
/* 245 */          
/* 246 */            isNull$54 = isNull$44;
/* 247 */            if (!isNull$54) {
/* 248 */              result$54 = result$45;
/* 249 */            }
/* 250 */          }
/* 251 */          else {
/* 252 */            
/* 253 */          
/* 254 */          
/* 255 */          
/* 256 */          
/* 257 */          isNull$46 = isNull$21;
/* 258 */          result$47 = -1L;
/* 259 */          if (!isNull$46) {
/* 260 */            
/* 261 */            result$47 = field$21.getMillisecond();
/* 262 */            
/* 263 */          }
/* 264 */          
/* 265 */          
/* 266 */          isNull$48 = isNull$46 || false;
/* 267 */          result$49 = -1L;
/* 268 */          if (!isNull$48) {
/* 269 */            
/* 270 */            result$49 = (long) (result$47 * ((long) 1L));
/* 271 */            
/* 272 */          }
/* 273 */          
/* 274 */          
/* 275 */          isNull$50 = isNull$48 || false;
/* 276 */          result$51 = -1L;
/* 277 */          if (!isNull$50) {
/* 278 */            
/* 279 */            result$51 = (long) (result$49 - ((long) 0L));
/* 280 */            
/* 281 */          }
/* 282 */          
/* 283 */          
/* 284 */          isNull$52 = isNull$50 || false;
/* 285 */          result$53 = -1L;
/* 286 */          if (!isNull$52) {
/* 287 */            
/* 288 */            result$53 = (long) (result$51 % ((long) 432000000L));
/* 289 */            
/* 290 */          }
/* 291 */          
/* 292 */            isNull$54 = isNull$52;
/* 293 */            if (!isNull$54) {
/* 294 */              result$54 = result$53;
/* 295 */            }
/* 296 */          }
/* 297 */          isNull$55 = isNull$24 || isNull$54;
/* 298 */          result$56 = -1L;
/* 299 */          if (!isNull$55) {
/* 300 */            
/* 301 */            result$56 = (long) (result$25 - result$54);
/* 302 */            
/* 303 */          }
/* 304 */          
/* 305 */          
/* 306 */          isNull$57 = isNull$55 || false;
/* 307 */          result$58 = -1L;
/* 308 */          if (!isNull$57) {
/* 309 */            
/* 310 */            result$58 = (long) (result$56 - ((long) 0L));
/* 311 */            
/* 312 */          }
/* 313 */          
/* 314 */            // process each input
/* 315 */            
/* 316 */            // build aggregate map key
/* 317 */            
/* 318 */          
/* 319 */          aggMapKeyWriter$4.reset();
/* 320 */          
/* 321 */          
/* 322 */          if (false) {
/* 323 */            aggMapKeyWriter$4.setNullAt(0);
/* 324 */          } else {
/* 325 */            aggMapKeyWriter$4.writeLong(0, result$58);
/* 326 */          }
/* 327 */                       
/* 328 */          aggMapKeyWriter$4.complete();
/* 329 */                  
/* 330 */            // aggregate by each input with assigned timestamp
/* 331 */            // look up output buffer using current key (grouping keys ..., assigned timestamp)
/* 332 */          lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);
/* 333 */          currentAggBuffer$8 = lookupInfo$20.getValue();
/* 334 */          if (!lookupInfo$20.isFound()) {
/* 335 */            
/* 336 */            // append empty agg buffer into aggregate map for current group key
/* 337 */            try {
/* 338 */              currentAggBuffer$8 =
/* 339 */                aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);
/* 340 */            } catch (java.io.EOFException exp) {
/* 341 */              
/* 342 */          LOG$2.info(""BytesHashMap out of memory with {} entries, output directly."", aggregateMap$7.getNumElements());
/* 343 */           // hash map out of memory, output directly
/* 344 */          
/* 345 */          org.apache.flink.util.MutableObjectIterator<org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry> iterator =
/* 346 */            aggregateMap$7.getEntryIterator();
/* 347 */          while (iterator.next(reuseAggMapEntry$19) != null) {
/* 348 */             
/* 349 */             
/* 350 */          
/* 351 */          hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);
/* 352 */                   
/* 353 */             output.collect(outElement.replace(hashAggOutput));
/* 354 */          }
/* 355 */                 
/* 356 */           // retry append
/* 357 */          
/* 358 */           // reset aggregate map retry append
/* 359 */          aggregateMap$7.reset();
/* 360 */          lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);
/* 361 */          try {
/* 362 */            currentAggBuffer$8 =
/* 363 */              aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);
/* 364 */          } catch (java.io.EOFException e) {
/* 365 */            throw new OutOfMemoryError(""BytesHashMap Out of Memory."");
/* 366 */          }
/* 367 */                 
/* 368 */                    
/* 369 */            }
/* 370 */          }
/* 371 */          // aggregate buffer fields access
/* 372 */          isNull$12 = currentAggBuffer$8.isNullAt(0);
/* 373 */          field$12 = -1;
/* 374 */          if (!isNull$12) {
/* 375 */            field$12 = currentAggBuffer$8.getInt(0);
/* 376 */          }
/* 377 */          // do aggregate and update agg buffer
/* 378 */          int result$16 = -1;
/* 379 */          boolean isNull$16;
/* 380 */          if (isNull$11) {
/* 381 */            
/* 382 */            isNull$16 = isNull$12;
/* 383 */            if (!isNull$16) {
/* 384 */              result$16 = field$12;
/* 385 */            }
/* 386 */          }
/* 387 */          else {
/* 388 */            int result$15 = -1;
/* 389 */          boolean isNull$15;
/* 390 */          if (isNull$12) {
/* 391 */            
/* 392 */            isNull$15 = isNull$11;
/* 393 */            if (!isNull$15) {
/* 394 */              result$15 = field$11;
/* 395 */            }
/* 396 */          }
/* 397 */          else {
/* 398 */            
/* 399 */          
/* 400 */          
/* 401 */          isNull$13 = isNull$12 || isNull$11;
/* 402 */          result$14 = -1;
/* 403 */          if (!isNull$13) {
/* 404 */            
/* 405 */            result$14 = (int) (field$12 + field$11);
/* 406 */            
/* 407 */          }
/* 408 */          
/* 409 */            isNull$15 = isNull$13;
/* 410 */            if (!isNull$15) {
/* 411 */              result$15 = result$14;
/* 412 */            }
/* 413 */          }
/* 414 */            isNull$16 = isNull$15;
/* 415 */            if (!isNull$16) {
/* 416 */              result$16 = result$15;
/* 417 */            }
/* 418 */          }
/* 419 */          if (isNull$16) {
/* 420 */            currentAggBuffer$8.setNullAt(0);
/* 421 */          } else {
/* 422 */            currentAggBuffer$8.setInt(0, result$16);
/* 423 */          }
/* 424 */                     
/* 425 */          }
/* 426 */        }
/* 427 */
/* 428 */        
/* 429 */        @Override
/* 430 */        public void endInput() throws Exception {
/* 431 */          org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;
/* 432 */        int field$11;
/* 433 */        boolean isNull$11;
/* 434 */        int field$12;
/* 435 */        boolean isNull$12;
/* 436 */        boolean isNull$13;
/* 437 */        int result$14;
/* 438 */        org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;
/* 439 */        org.apache.flink.table.data.TimestampData field$21;
/* 440 */        boolean isNull$21;
/* 441 */        boolean isNull$22;
/* 442 */        long result$23;
/* 443 */        boolean isNull$24;
/* 444 */        long result$25;
/* 445 */        boolean isNull$26;
/* 446 */        long result$27;
/* 447 */        boolean isNull$28;
/* 448 */        long result$29;
/* 449 */        boolean isNull$30;
/* 450 */        long result$31;
/* 451 */        boolean isNull$32;
/* 452 */        long result$33;
/* 453 */        boolean isNull$34;
/* 454 */        boolean result$35;
/* 455 */        boolean isNull$36;
/* 456 */        long result$37;
/* 457 */        boolean isNull$38;
/* 458 */        long result$39;
/* 459 */        boolean isNull$40;
/* 460 */        long result$41;
/* 461 */        boolean isNull$42;
/* 462 */        long result$43;
/* 463 */        boolean isNull$44;
/* 464 */        long result$45;
/* 465 */        boolean isNull$46;
/* 466 */        long result$47;
/* 467 */        boolean isNull$48;
/* 468 */        long result$49;
/* 469 */        boolean isNull$50;
/* 470 */        long result$51;
/* 471 */        boolean isNull$52;
/* 472 */        long result$53;
/* 473 */        boolean isNull$55;
/* 474 */        long result$56;
/* 475 */        boolean isNull$57;
/* 476 */        long result$58;
/* 477 */          
/* 478 */        org.apache.flink.util.MutableObjectIterator<org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry> iterator =
/* 479 */          aggregateMap$7.getEntryIterator();
/* 480 */        while (iterator.next(reuseAggMapEntry$19) != null) {
/* 481 */           
/* 482 */           
/* 483 */        
/* 484 */        hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);
/* 485 */                 
/* 486 */           output.collect(outElement.replace(hashAggOutput));
/* 487 */        }
/* 488 */               
/* 489 */        }
/* 490 */                 
/* 491 */
/* 492 */        @Override
/* 493 */        public void close() throws Exception {
/* 494 */           super.close();
/* 495 */          aggregateMap$7.free();
/* 496 */          
/* 497 */        }
/* 498 */
/* 499 */        
/* 500 */      }
/* 501 */    

Traceback (most recent call last):
  File ""/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_903.py"", line 32, in <module>
    print(table.to_pandas())
  File ""/home/alex/work/flink/flink-python/pyflink/table/table.py"", line 829, in to_pandas
    if batches.hasNext():
  File ""/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1285, in __call__
    return_value = get_return_value(
  File ""/home/alex/work/flink/flink-python/pyflink/util/exceptions.py"", line 147, in deco
    return f(*a, **kw)
  File ""/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o51.hasNext.
: java.lang.RuntimeException: Failed to fetch next result
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77)
	at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355)
	at org.apache.flink.table.runtime.arrow.ArrowUtils$1.hasNext(ArrowUtils.java:644)
	at org.apache.flink.table.runtime.arrow.ArrowUtils$2.hasNext(ArrowUtils.java:666)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103)
	... 16 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172)
	... 18 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)
	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
	at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:117)
	... 19 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:413)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'LocalHashWinAggWithoutKeys$59'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:70)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:613)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:583)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:574)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:164)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
	... 13 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
	... 15 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 18 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 351, Column 33: A method named ""replace"" is not declared in any enclosing class nor any supertype, nor through a static import
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781)
	at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760)
	at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2776)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1842)
	at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1498)
	at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3052)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2776)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileTryCatch(UnitCompiler.java:3136)
	at org.codehaus.janino.UnitCompiler.compileTryCatchFinally(UnitCompiler.java:2966)
	at org.codehaus.janino.UnitCompiler.compileTryCatchFinallyWithResources(UnitCompiler.java:2770)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2742)
	at org.codehaus.janino.UnitCompiler.access$2300(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1499)
	at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$TryStatement.accept(Java.java:3238)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2776)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2776)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78)
	... 24 more
{code}

However it works fine in streaming mode:

{code:python}
env_settings = (
    EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
)
table_env = StreamTableEnvironment.create(environment_settings=env_settings)
{code}

How the table is created seems irrelevant - this raises the same error:

{code:python}
from datetime import datetime

from pyflink.table import DataTypes, BatchTableEnvironment, EnvironmentSettings
from pyflink.table.window import Tumble

env_settings = (
    EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build()
)
table_environment = BatchTableEnvironment.create(environment_settings=env_settings)
transactions = table_environment.from_elements(
    [
        (1, datetime(2000, 1, 1, 0, 0, 0)),
        (-2, datetime(2000, 1, 2, 0, 0, 0)),
        (3, datetime(2000, 1, 3, 0, 0, 0)),
        (-4, datetime(2000, 1, 4, 0, 0, 0)),
    ],
    DataTypes.ROW(
        [
            DataTypes.FIELD(""amount"", DataTypes.BIGINT()),
            DataTypes.FIELD(""ts"", DataTypes.TIMESTAMP(3)),
        ]
    ),
)
table = (
    transactions
        .window(Tumble.over(""5.days"").on(""ts"").alias(""__window""))
        .group_by(""__window"")
        .select(""amount.sum"")
)
print(table.to_pandas())
{code}","Ubuntu 18.04

Python 3.8, jar built from master yesterday.

Or Python 3.7, installed latest version from pip.",alexmojaki,dian.fu,jark,leonard,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 28 11:26:05 UTC 2021,,,,,,,,,,"0|z0juog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/20 12:15;dian.fu;cc [~jark];;;","02/Nov/20 12:22;jark;I think this is a bug in batch code generator for window aggregate. cc [~lzljs3620320];;;","16/Apr/21 22:43;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:51;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","29/May/21 11:25;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Jun/21 10:53;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Jul/21 09:15;TsReaper;Hi!

I've looked into this issue and found that {{WindowCodeGenerator}} forgets to consider if the current operator is a local window aggregate when choosing type for output records. I'm taking this issue.;;;","28/Jul/21 02:05;lzljs3620320;[~TsReaper] Can you cherry-pick to 1.13?;;;","28/Jul/21 11:26;lzljs3620320;master: 941e8b81cac761fab748c83b361ae8a2b35ce1cd
release-1.13: 68e0ee84e908c7ed64a9f8ae2dbf6823e4106d71
release-1.12: be672582a9b1926566b6462c01c6870cdf93e555;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"YARN tests failed with ""RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified""",FLINK-19728,13336206,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,dian.fu,dian.fu,20/Oct/20 06:31,20/Oct/20 13:12,13/Jul/23 08:12,20/Oct/20 13:12,1.12.0,,,,,,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7880&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354

{code}
2020-10-19T21:53:36.9360054Z [INFO] Running org.apache.flink.yarn.YARNHighAvailabilityITCase
2020-10-19T21:53:39.7175350Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified {memory-mb=name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 32, maximum allocation: 4096, testing-external-resource=name: testing-external-resource, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, vcores=name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 1, maximum allocation: 4}. Use DominantResourceCalculator instead to make effective use of these resource-types
2020-10-19T21:53:39.7177209Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:346)
2020-10-19T21:53:39.7177866Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:425)
2020-10-19T21:53:39.7178772Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:39.7179272Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:39.7179843Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:819)
2020-10-19T21:53:39.7180380Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:39.7180903Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1224)
2020-10-19T21:53:39.7181591Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:318)
2020-10-19T21:53:39.7182090Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:39.7182562Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
2020-10-19T21:53:39.7183084Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
2020-10-19T21:53:39.7183622Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
2020-10-19T21:53:39.7184153Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:39.7184722Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:39.7185218Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
2020-10-19T21:53:39.7185681Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:39.7186115Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:649)
2020-10-19T21:53:39.7186555Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:39.7187005Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:608)
2020-10-19T21:53:39.7187502Z 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.setup(YARNHighAvailabilityITCase.java:124)
2020-10-19T21:53:39.7187947Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:39.7188399Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:39.7188887Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:39.7189367Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:39.7189781Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:39.7190267Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:39.7190762Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:39.7191214Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:39.7191675Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:39.7192123Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:39.7192540Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:39.7192975Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:39.7193397Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:39.7193761Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:39.7194202Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:39.7194773Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:39.7195270Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:39.7195850Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:39.7196376Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:39.7196888Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:39.7197369Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:39.7197830Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:39.7302125Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.789 s <<< FAILURE! - in org.apache.flink.yarn.YARNHighAvailabilityITCase
2020-10-19T21:53:39.7303025Z [ERROR] org.apache.flink.yarn.YARNHighAvailabilityITCase  Time elapsed: 2.789 s  <<< FAILURE!
2020-10-19T21:53:39.7303403Z java.lang.AssertionError
2020-10-19T21:53:39.7303684Z 	at org.junit.Assert.fail(Assert.java:86)
2020-10-19T21:53:39.7303979Z 	at org.junit.Assert.fail(Assert.java:95)
2020-10-19T21:53:39.7315900Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:699)
2020-10-19T21:53:39.7316373Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:39.7316847Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:608)
2020-10-19T21:53:39.7317343Z 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.setup(YARNHighAvailabilityITCase.java:124)
2020-10-19T21:53:39.7317771Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:39.7318186Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:39.7318764Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:39.7319200Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:39.7319662Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:39.7320163Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:39.7320635Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:39.7321115Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:39.7321571Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:39.7322001Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:39.7322435Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:39.7322874Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:39.7323258Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:39.7323627Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:39.7324057Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:39.7324640Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:39.7325146Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:39.7325638Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:39.7326140Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:39.7326669Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:39.7327158Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:39.7327597Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:39.7327869Z 
2020-10-19T21:53:40.3750536Z [INFO] Running org.apache.flink.yarn.YARNFileReplicationITCase
2020-10-19T21:53:41.9922803Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified {memory-mb=name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 32, maximum allocation: 4096, testing-external-resource=name: testing-external-resource, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, vcores=name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 1, maximum allocation: 4}. Use DominantResourceCalculator instead to make effective use of these resource-types
2020-10-19T21:53:41.9925410Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:346)
2020-10-19T21:53:41.9926534Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:425)
2020-10-19T21:53:41.9927407Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:41.9928157Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:41.9929123Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:819)
2020-10-19T21:53:41.9929876Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:41.9930420Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1224)
2020-10-19T21:53:41.9931012Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:318)
2020-10-19T21:53:41.9931561Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:41.9932054Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
2020-10-19T21:53:41.9932582Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
2020-10-19T21:53:41.9933397Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
2020-10-19T21:53:41.9934171Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:41.9935038Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:41.9935855Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
2020-10-19T21:53:41.9936603Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:41.9937142Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:649)
2020-10-19T21:53:41.9937816Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:41.9938444Z 	at org.apache.flink.yarn.YARNFileReplicationITCase.setup(YARNFileReplicationITCase.java:65)
2020-10-19T21:53:41.9938869Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:41.9939289Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:41.9939789Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:41.9940206Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:41.9940638Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:41.9941174Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:41.9941651Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:41.9942135Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:41.9942598Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:41.9943025Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:41.9943463Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:41.9944051Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:41.9944513Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:41.9944953Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:41.9945434Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:41.9945955Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:41.9946525Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:41.9947129Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:41.9947649Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:41.9948138Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:41.9948664Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:42.0607300Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.682 s <<< FAILURE! - in org.apache.flink.yarn.YARNFileReplicationITCase
2020-10-19T21:53:42.0608115Z [ERROR] org.apache.flink.yarn.YARNFileReplicationITCase  Time elapsed: 1.682 s  <<< FAILURE!
2020-10-19T21:53:42.0608596Z java.lang.AssertionError
2020-10-19T21:53:42.0608891Z 	at org.junit.Assert.fail(Assert.java:86)
2020-10-19T21:53:42.0609213Z 	at org.junit.Assert.fail(Assert.java:95)
2020-10-19T21:53:42.0609604Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:699)
2020-10-19T21:53:42.0610096Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:42.0610630Z 	at org.apache.flink.yarn.YARNFileReplicationITCase.setup(YARNFileReplicationITCase.java:65)
2020-10-19T21:53:42.0611153Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:42.0611595Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:42.0612121Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:42.0612584Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:42.0613027Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:42.0613559Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:42.0614078Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:42.0614747Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:42.0615243Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:42.0615721Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:42.0616176Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:42.0616614Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:42.0617096Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:42.0617553Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:42.0618097Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:42.0618710Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:42.0619217Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:42.0619778Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:42.0620352Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:42.0620875Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:42.0621585Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:42.0621859Z 
2020-10-19T21:53:42.7061131Z [INFO] Running org.apache.flink.yarn.YarnConfigurationITCase
2020-10-19T21:53:44.3195961Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified {memory-mb=name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 32, maximum allocation: 4096, testing-external-resource=name: testing-external-resource, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, vcores=name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 1, maximum allocation: 4}. Use DominantResourceCalculator instead to make effective use of these resource-types
2020-10-19T21:53:44.3198447Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:346)
2020-10-19T21:53:44.3199434Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:425)
2020-10-19T21:53:44.3200008Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:44.3200501Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:44.3201075Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:819)
2020-10-19T21:53:44.3201617Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:44.3202187Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1224)
2020-10-19T21:53:44.3202870Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:318)
2020-10-19T21:53:44.3203424Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:44.3204125Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
2020-10-19T21:53:44.3204898Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
2020-10-19T21:53:44.3205458Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
2020-10-19T21:53:44.3206010Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:44.3206509Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:44.3207024Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
2020-10-19T21:53:44.3207528Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:44.3207995Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:649)
2020-10-19T21:53:44.3208520Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:44.3209045Z 	at org.apache.flink.yarn.YarnTestBase.setup(YarnTestBase.java:725)
2020-10-19T21:53:44.3209461Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:44.3209887Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:44.3210416Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:44.3210880Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:44.3211327Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:44.3211857Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:44.3212375Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:44.3212865Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:44.3213490Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:44.3214009Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:44.3214552Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:44.3214982Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:44.3215391Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:44.3215844Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:44.3216459Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:44.3217016Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:44.3217527Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:44.3218081Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:44.3218766Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:44.3219277Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:44.3219768Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:44.3908419Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.681 s <<< FAILURE! - in org.apache.flink.yarn.YarnConfigurationITCase
2020-10-19T21:53:44.3909218Z [ERROR] org.apache.flink.yarn.YarnConfigurationITCase  Time elapsed: 1.681 s  <<< FAILURE!
2020-10-19T21:53:44.3909609Z java.lang.AssertionError
2020-10-19T21:53:44.3909893Z 	at org.junit.Assert.fail(Assert.java:86)
2020-10-19T21:53:44.3910223Z 	at org.junit.Assert.fail(Assert.java:95)
2020-10-19T21:53:44.3910585Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:699)
2020-10-19T21:53:44.3911043Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:44.3911498Z 	at org.apache.flink.yarn.YarnTestBase.setup(YarnTestBase.java:725)
2020-10-19T21:53:44.3911884Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:44.3912307Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:44.3912910Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:44.3913382Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:44.3913839Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:44.3914470Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:44.3914997Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:44.3915502Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:44.3916052Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:44.3916597Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:44.3917060Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:44.3917478Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:44.3917880Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:44.3918449Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:44.3919031Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:44.3919571Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:44.3920098Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:44.3920828Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:44.3921399Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:44.3921920Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:44.3922401Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:44.3922699Z 
2020-10-19T21:53:45.0363811Z [INFO] Running org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase
2020-10-19T21:53:46.6520359Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified {memory-mb=name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 32, maximum allocation: 4096, testing-external-resource=name: testing-external-resource, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, vcores=name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 1, maximum allocation: 4}. Use DominantResourceCalculator instead to make effective use of these resource-types
2020-10-19T21:53:46.6522862Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:346)
2020-10-19T21:53:46.6523558Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:425)
2020-10-19T21:53:46.6524147Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:46.6525008Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:46.6525762Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:819)
2020-10-19T21:53:46.6526334Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:46.6527082Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1224)
2020-10-19T21:53:46.6527695Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:318)
2020-10-19T21:53:46.6528484Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:46.6528974Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
2020-10-19T21:53:46.6529515Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
2020-10-19T21:53:46.6530131Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
2020-10-19T21:53:46.6530650Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:46.6531138Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:46.6531646Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
2020-10-19T21:53:46.6532119Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:46.6532569Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:649)
2020-10-19T21:53:46.6533027Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:46.6533493Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:608)
2020-10-19T21:53:46.6534037Z 	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.setup(YARNSessionCapacitySchedulerITCase.java:130)
2020-10-19T21:53:46.6534620Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:46.6535033Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:46.6535528Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:46.6535979Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:46.6536559Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:46.6537073Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:46.6537573Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:46.6538091Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:46.6538642Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:46.6539110Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:46.6539626Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:46.6540081Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:46.6540478Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:46.6540916Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:46.6541444Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:46.6541968Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:46.6542458Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:46.6542993Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:46.6543554Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:46.6544041Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:46.6544586Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:46.7247677Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.686 s <<< FAILURE! - in org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase
2020-10-19T21:53:46.7248506Z [ERROR] org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase  Time elapsed: 1.686 s  <<< FAILURE!
2020-10-19T21:53:46.7248899Z java.lang.AssertionError
2020-10-19T21:53:46.7249223Z 	at org.junit.Assert.fail(Assert.java:86)
2020-10-19T21:53:46.7249522Z 	at org.junit.Assert.fail(Assert.java:95)
2020-10-19T21:53:46.7249935Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:699)
2020-10-19T21:53:46.7250374Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:46.7250822Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:608)
2020-10-19T21:53:46.7251353Z 	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.setup(YARNSessionCapacitySchedulerITCase.java:130)
2020-10-19T21:53:46.7251932Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:46.7252360Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:46.7252882Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:46.7253353Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:46.7253800Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:46.7254467Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:46.7254991Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:46.7255509Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:46.7256008Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:46.7256488Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:46.7256941Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:46.7257541Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:46.7257945Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:46.7258475Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:46.7259018Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:46.7259566Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:46.7260136Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:46.7260848Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:46.7261419Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:46.7261924Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:46.7262415Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:46.7262694Z 
2020-10-19T21:53:47.3644907Z [INFO] Running org.apache.flink.yarn.YARNITCase
2020-10-19T21:53:48.9846587Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified {memory-mb=name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 32, maximum allocation: 4096, testing-external-resource=name: testing-external-resource, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, vcores=name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 1, maximum allocation: 4}. Use DominantResourceCalculator instead to make effective use of these resource-types
2020-10-19T21:53:48.9849883Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:346)
2020-10-19T21:53:48.9850867Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:425)
2020-10-19T21:53:48.9851487Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:48.9852002Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:48.9852585Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:819)
2020-10-19T21:53:48.9853162Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:48.9853900Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1224)
2020-10-19T21:53:48.9855009Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:318)
2020-10-19T21:53:48.9855895Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:48.9856558Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
2020-10-19T21:53:48.9857112Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
2020-10-19T21:53:48.9857673Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
2020-10-19T21:53:48.9858230Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:48.9858912Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:48.9859426Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
2020-10-19T21:53:48.9859946Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:48.9860408Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:649)
2020-10-19T21:53:48.9860864Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:48.9861581Z 	at org.apache.flink.yarn.YARNITCase.setup(YARNITCase.java:75)
2020-10-19T21:53:48.9861969Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:48.9862423Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:48.9862950Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:48.9863395Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:48.9863861Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:48.9864501Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:48.9865209Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:48.9865700Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:48.9866197Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:48.9866683Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:48.9867136Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:48.9867562Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:48.9867949Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:48.9868482Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:48.9869082Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:48.9869619Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:48.9870151Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:48.9870709Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:48.9871270Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:48.9871792Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:48.9872285Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:49.0546948Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.688 s <<< FAILURE! - in org.apache.flink.yarn.YARNITCase
2020-10-19T21:53:49.0547665Z [ERROR] org.apache.flink.yarn.YARNITCase  Time elapsed: 1.688 s  <<< FAILURE!
2020-10-19T21:53:49.0548046Z java.lang.AssertionError
2020-10-19T21:53:49.0548516Z 	at org.junit.Assert.fail(Assert.java:86)
2020-10-19T21:53:49.0548895Z 	at org.junit.Assert.fail(Assert.java:95)
2020-10-19T21:53:49.0549285Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:699)
2020-10-19T21:53:49.0549769Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:49.0550232Z 	at org.apache.flink.yarn.YARNITCase.setup(YARNITCase.java:75)
2020-10-19T21:53:49.0550638Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:49.0551088Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:49.0551595Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:49.0552063Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:49.0552527Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:49.0553042Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:49.0553572Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:49.0554079Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:49.0554664Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:49.0555349Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:49.0555823Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:49.0556233Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:49.0556641Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:49.0557111Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:49.0557632Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:49.0558269Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:49.0558899Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:49.0559444Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:49.0560025Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:49.0560541Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:49.0561047Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:49.0561340Z 
2020-10-19T21:53:49.7062373Z [INFO] Running org.apache.flink.yarn.YARNApplicationITCase
2020-10-19T21:53:51.3180617Z org.apache.hadoop.yarn.exceptions.YarnRuntimeException: RM uses DefaultResourceCalculator which used only memory as resource-type but invalid resource-types specified {memory-mb=name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 32, maximum allocation: 4096, testing-external-resource=name: testing-external-resource, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, vcores=name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 1, maximum allocation: 4}. Use DominantResourceCalculator instead to make effective use of these resource-types
2020-10-19T21:53:51.3182548Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:346)
2020-10-19T21:53:51.3183233Z 	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:425)
2020-10-19T21:53:51.3183794Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:51.3184412Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:51.3184993Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:819)
2020-10-19T21:53:51.3185544Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:51.3186099Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1224)
2020-10-19T21:53:51.3186708Z 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:318)
2020-10-19T21:53:51.3187207Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:51.3187713Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348)
2020-10-19T21:53:51.3188239Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128)
2020-10-19T21:53:51.3188844Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497)
2020-10-19T21:53:51.3189378Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:51.3189867Z 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
2020-10-19T21:53:51.3190370Z 	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316)
2020-10-19T21:53:51.3190886Z 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
2020-10-19T21:53:51.3191593Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:649)
2020-10-19T21:53:51.3192033Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:51.3192531Z 	at org.apache.flink.yarn.YARNApplicationITCase.setup(YARNApplicationITCase.java:57)
2020-10-19T21:53:51.3192968Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:51.3193377Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:51.3193880Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:51.3194498Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:51.3194926Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:51.3195436Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:51.3195931Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:51.3196438Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:51.3196914Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:51.3197387Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:51.3197823Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:51.3198235Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:51.3198693Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:51.3199135Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:51.3199651Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:51.3200175Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:51.3200720Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:51.3201248Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:51.3201798Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:51.3202282Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:51.3202759Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-19T21:53:51.3861339Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.677 s <<< FAILURE! - in org.apache.flink.yarn.YARNApplicationITCase
2020-10-19T21:53:51.3862099Z [ERROR] org.apache.flink.yarn.YARNApplicationITCase  Time elapsed: 1.677 s  <<< FAILURE!
2020-10-19T21:53:51.3862459Z java.lang.AssertionError
2020-10-19T21:53:51.3862740Z 	at org.junit.Assert.fail(Assert.java:86)
2020-10-19T21:53:51.3863076Z 	at org.junit.Assert.fail(Assert.java:95)
2020-10-19T21:53:51.3863439Z 	at org.apache.flink.yarn.YarnTestBase.start(YarnTestBase.java:699)
2020-10-19T21:53:51.3863895Z 	at org.apache.flink.yarn.YarnTestBase.startYARNWithConfig(YarnTestBase.java:612)
2020-10-19T21:53:51.3864523Z 	at org.apache.flink.yarn.YARNApplicationITCase.setup(YARNApplicationITCase.java:57)
2020-10-19T21:53:51.3864946Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T21:53:51.3865378Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T21:53:51.3865887Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T21:53:51.3866328Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T21:53:51.3866766Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-19T21:53:51.3867269Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-19T21:53:51.3867948Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-19T21:53:51.3868608Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-19T21:53:51.3869082Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-19T21:53:51.3869527Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:51.3869975Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-19T21:53:51.3870415Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-19T21:53:51.3870899Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-19T21:53:51.3871355Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-19T21:53:51.3871870Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-19T21:53:51.3872384Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-19T21:53:51.3872897Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-19T21:53:51.3873435Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-19T21:53:51.3873972Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-19T21:53:51.3874560Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-19T21:53:51.3875041Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dian.fu,guoyangze,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 13:12:23 UTC 2020,,,,,,,,,,"0|z0ju14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 06:51;rmetzger;[~xintongsong] can you take a look at this issue or ping somebody who could look into this.;;;","20/Oct/20 07:14;xtsong;Sure. I'll look into this.;;;","20/Oct/20 08:07;xtsong;I think this is introduced by FLINK-19689. The added {{resource-types.xml}} does not work with {{MiniYARNCluster}}. Sorry for not discovering this before merging. I was verifying locally with `mvn test` rather than `mvn verify` so the IT cases are not executed.

We would need to come up with another solution for fixing FLINK-19689 without breaking the IT cases, which could use some time. To avoid blocking the cron tests, I would suggest the following actions.
* Revert the fix for FLINK-19689 and close this ticket.
* Deactivate the problematic test cases in FLINK-19689 and reopen FLINK-19689, until we worked out a proper fix and re-activate.

[~rmetzger], [~dian.fu], WDTY?;;;","20/Oct/20 08:15;dian.fu;[~xintongsong] Thanks a lot for looking into this issue and the quick response. It makes sense to me.;;;","20/Oct/20 08:19;rmetzger;I agree with your proposed resolution!;;;","20/Oct/20 08:28;xtsong;Thanks for the quick response, [~rmetzger], [~dian.fu].
I'll take the actions.;;;","20/Oct/20 13:12;xtsong;Fixed via
* master: ffa94e0fc4ea39695a8ea21f528e8917f3159746;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logger cannot be initialized due to timeout: LoggerInitializationException is thrown,FLINK-19725,13336186,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,jark,jark,20/Oct/20 03:50,18/Jun/21 14:39,13/Jul/23 08:12,03/Dec/20 13:08,1.12.0,,,,,1.12.1,1.13.0,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"The actual failure is an error in instantiating the logger:
{code:java}
java.lang.Exception: Could not create actor system
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:263)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:341)
	at org.apache.flink.runtime.minicluster.MiniCluster.createLocalRpcService(MiniCluster.java:792)
	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:273)
	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:180)
	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:91)
	at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:51)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: akka.ConfigurationException: Could not start logger due to [akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [akka.event.Logging$LoggerInitializationException: Logger log1-Slf4jLogger did not respond with LoggerInitialized, sent instead [TIMEOUT]]]
	at akka.event.LoggingBus$class.startDefaultLoggers(Logging.scala:147)
	at akka.event.EventStream.startDefaultLoggers(EventStream.scala:22)
	at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:662)
	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:874)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:870)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:870)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:891)
	at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:96)
	at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70)
	at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125)
	at org.apache.flink.runtime.akka.AkkaUtils.createActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:276)
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:260)
	... 17 more

{code}
 

Here is an instance:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7877&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=20270]
{code:java}
2020-10-19T18:06:30.6704453Z =================================== FAILURES ===================================
2020-10-19T18:06:30.6705106Z ________________ DataStreamTests.test_key_by_on_connect_stream _________________
2020-10-19T18:06:30.6705465Z 
2020-10-19T18:06:30.6705934Z self = <pyflink.datastream.tests.test_data_stream.DataStreamTests testMethod=test_key_by_on_connect_stream>
2020-10-19T18:06:30.6707720Z 
2020-10-19T18:06:30.6708558Z     def test_key_by_on_connect_stream(self):
2020-10-19T18:06:30.6710053Z         ds1 = self.env.from_collection([('a', 0), ('b', 0), ('c', 1), ('d', 1), ('e', 2)],
2020-10-19T18:06:30.6710691Z                                        type_info=Types.ROW([Types.STRING(), Types.INT()])) \
2020-10-19T18:06:30.6711381Z             .key_by(MyKeySelector(), key_type_info=Types.INT())
2020-10-19T18:06:30.6717965Z         ds2 = self.env.from_collection([('a', 0), ('b', 0), ('c', 1), ('d', 1), ('e', 2)],
2020-10-19T18:06:30.6718712Z                                        type_info=Types.ROW([Types.STRING(), Types.INT()]))
2020-10-19T18:06:30.6719278Z     
2020-10-19T18:06:30.6719679Z         class AssertKeyCoMapFunction(CoMapFunction):
2020-10-19T18:06:30.6720136Z             def __init__(self):
2020-10-19T18:06:30.6720528Z                 self.pre1 = None
2020-10-19T18:06:30.6720881Z                 self.pre2 = None
2020-10-19T18:06:30.6721195Z     
2020-10-19T18:06:30.6721510Z             def map1(self, value):
2020-10-19T18:06:30.6722215Z                 if value[0] == 'b':
2020-10-19T18:06:30.6722849Z                     assert self.pre1 == 'a'
2020-10-19T18:06:30.6723621Z                 if value[0] == 'd':
2020-10-19T18:06:30.6724247Z                     assert self.pre1 == 'c'
2020-10-19T18:06:30.6724652Z                 self.pre1 = value[0]
2020-10-19T18:06:30.6725002Z                 return value
2020-10-19T18:06:30.6730397Z     
2020-10-19T18:06:30.6730856Z             def map2(self, value):
2020-10-19T18:06:30.6731728Z                 if value[0] == 'b':
2020-10-19T18:06:30.6732399Z                     assert self.pre2 == 'a'
2020-10-19T18:06:30.6733025Z                 if value[0] == 'd':
2020-10-19T18:06:30.6733769Z                     assert self.pre2 == 'c'
2020-10-19T18:06:30.6734159Z                 self.pre2 = value[0]
2020-10-19T18:06:30.6734521Z                 return value
2020-10-19T18:06:30.6735084Z     
2020-10-19T18:06:30.6735376Z         ds1.connect(ds2)\
2020-10-19T18:06:30.6735862Z             .key_by(MyKeySelector(), MyKeySelector(), key_type_info=Types.INT())\
2020-10-19T18:06:30.6736369Z             .map(AssertKeyCoMapFunction())\
2020-10-19T18:06:30.6736761Z             .add_sink(self.test_sink)
2020-10-19T18:06:30.6737079Z     
2020-10-19T18:06:30.6737718Z >       self.env.execute('key_by_test')
2020-10-19T18:06:30.6737996Z 
2020-10-19T18:06:30.6738342Z pyflink/datastream/tests/test_data_stream.py:206: 
2020-10-19T18:06:30.6738880Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-10-19T18:06:30.6739598Z pyflink/datastream/stream_execution_environment.py:621: in execute
2020-10-19T18:06:30.6740205Z     return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
2020-10-19T18:06:30.6741118Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__
2020-10-19T18:06:30.6741690Z     answer, self.gateway_client, self.target_id, self.name)
2020-10-19T18:06:30.6745640Z pyflink/util/exceptions.py:147: in deco
2020-10-19T18:06:30.6746059Z     return f(*a, **kw)
2020-10-19T18:06:30.6746516Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-10-19T18:06:30.6746870Z 
2020-10-19T18:06:30.6747564Z answer = 'xro4008'
2020-10-19T18:06:30.6748011Z gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f3087c37748>
2020-10-19T18:06:30.6748717Z target_id = 'o3660', name = 'execute'
2020-10-19T18:06:30.6749062Z 
2020-10-19T18:06:30.6749495Z     def get_return_value(answer, gateway_client, target_id=None, name=None):
2020-10-19T18:06:30.6750188Z         """"""Converts an answer received from the Java gateway into a Python object.
2020-10-19T18:06:30.6750598Z     
2020-10-19T18:06:30.6751006Z         For example, string representation of integers are converted to Python
2020-10-19T18:06:30.6751559Z         integer, string representation of objects are converted to JavaObject
2020-10-19T18:06:30.6752020Z         instances, etc.
2020-10-19T18:06:30.6752313Z     
2020-10-19T18:06:30.6752693Z         :param answer: the string returned by the Java gateway
2020-10-19T18:06:30.6753379Z         :param gateway_client: the gateway client used to communicate with the Java
2020-10-19T18:06:30.6753980Z             Gateway. Only necessary if the answer is a reference (e.g., object,
2020-10-19T18:06:30.6754411Z             list, map)
2020-10-19T18:06:30.6754857Z         :param target_id: the name of the object from which the answer comes from
2020-10-19T18:06:30.6755430Z             (e.g., *object1* in `object1.hello()`). Optional.
2020-10-19T18:06:30.6755969Z         :param name: the name of the member from which the answer comes from
2020-10-19T18:06:30.6756522Z             (e.g., *hello* in `object1.hello()`). Optional.
2020-10-19T18:06:30.6756888Z         """"""
2020-10-19T18:06:30.6757327Z         if is_error(answer)[0]:
2020-10-19T18:06:30.6757721Z             if len(answer) > 1:
2020-10-19T18:06:30.6758122Z                 type = answer[1]
2020-10-19T18:06:30.6758610Z                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2020-10-19T18:06:30.6759288Z                 if answer[1] == REFERENCE_TYPE:
2020-10-19T18:06:30.6759725Z                     raise Py4JJavaError(
2020-10-19T18:06:30.6760202Z                         ""An error occurred while calling {0}{1}{2}.\n"".
2020-10-19T18:06:30.6760710Z >                       format(target_id, ""."", name), value)
2020-10-19T18:06:30.6761374Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling o3660.execute.
2020-10-19T18:06:30.6762068Z E                   : java.lang.Exception: Could not create actor system
2020-10-19T18:06:30.6762857Z E                   	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:263)
2020-10-19T18:06:30.6763949Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:341)
2020-10-19T18:06:30.6765129Z E                   	at org.apache.flink.runtime.metrics.util.MetricUtils.startMetricRpcService(MetricUtils.java:152)
2020-10-19T18:06:30.6766058Z E                   	at org.apache.flink.runtime.metrics.util.MetricUtils.startLocalMetricsRpcService(MetricUtils.java:142)
2020-10-19T18:06:30.6766915Z E                   	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:275)
2020-10-19T18:06:30.6767883Z E                   	at org.apache.flink.client.program.PerJobMiniClusterFactory.submitJob(PerJobMiniClusterFactory.java:74)
2020-10-19T18:06:30.6768798Z E                   	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:81)
2020-10-19T18:06:30.6769938Z E                   	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1867)
2020-10-19T18:06:30.6770886Z E                   	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1763)
2020-10-19T18:06:30.6771888Z E                   	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-10-19T18:06:30.6772685Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T18:06:30.6773464Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T18:06:30.6774309Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T18:06:30.6775064Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T18:06:30.6775849Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-10-19T18:06:30.6776765Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-10-19T18:06:30.6777692Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-10-19T18:06:30.6778577Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-10-19T18:06:30.6779591Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-10-19T18:06:30.6780471Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-10-19T18:06:30.6781193Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-10-19T18:06:30.6783433Z E                   Caused by: akka.ConfigurationException: Could not start logger due to [akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [akka.event.Logging$LoggerInitializationException: Logger log1-Slf4jLogger did not respond with LoggerInitialized, sent instead [TIMEOUT]]]
2020-10-19T18:06:30.6784902Z E                   	at akka.event.LoggingBus$class.startDefaultLoggers(Logging.scala:147)
2020-10-19T18:06:30.6785646Z E                   	at akka.event.EventStream.startDefaultLoggers(EventStream.scala:22)
2020-10-19T18:06:30.6786417Z E                   	at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:662)
2020-10-19T18:06:30.6787228Z E                   	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:874)
2020-10-19T18:06:30.6787974Z E                   	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:870)
2020-10-19T18:06:30.6788680Z E                   	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:870)
2020-10-19T18:06:30.6789462Z E                   	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:891)
2020-10-19T18:06:30.6790182Z E                   	at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:96)
2020-10-19T18:06:30.6790915Z E                   	at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70)
2020-10-19T18:06:30.6791819Z E                   	at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55)
2020-10-19T18:06:30.6792597Z E                   	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125)
2020-10-19T18:06:30.6793524Z E                   	at org.apache.flink.runtime.akka.AkkaUtils.createActorSystem(AkkaUtils.scala)
2020-10-19T18:06:30.6794301Z E                   	at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:276)
2020-10-19T18:06:30.6795256Z E                   	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:260)
2020-10-19T18:06:30.6796016Z E                   	... 20 more
2020-10-19T18:06:30.6796252Z 
2020-10-19T18:06:30.6797013Z .tox/py35-cython/lib/python3.5/site-packages/py4j/protocol.py:328: Py4JJavaError
{code}",,dian.fu,jark,mapohl,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22979,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 13:08:37 UTC 2020,,,,,,,,,,"0|z0jtwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 03:50;jark;cc [~dianfu]
;;;","20/Oct/20 06:46;dian.fu;Thanks [~jark], I'll take a look at this issue.;;;","01/Dec/20 09:08;mapohl;Another build failure happened here: [https://dev.azure.com/mapohl/flink/_build/results?buildId=120&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87]

FYI: I updated the issue as it's not directly related to PyFlink. Instead, it seems to be caused by the Akka logger.;;;","01/Dec/20 15:51;mapohl;Several SO threads (e.g. [that one|https://stackoverflow.com/questions/27910526/logger-log1-slf4jlogger-did-not-respond-within-timeout5000-milliseconds-to-ini]) suggest increasing Akka's {{logger-startup-timeout}} configuration parameter. It's default value is set to 5 seconds. Increasing the timeout should fix this error in high-load scenarios.

Is [AkkaUtils.getBasicAkkaConfig(Configuration)|https://github.com/apache/flink/blob/13272cedd0928a4c5dde5b5463b607e9eb506cb8/flink-runtime/src/main/scala/org/apache/flink/runtime/akka/AkkaUtils.scala#L261] the right location to adapt the Akka configuration?;;;","02/Dec/20 14:38;trohrmann;Sounds like a good idea [~mapohl]. +1 for increasing the {{logger-startup-timeout}} value.;;;","03/Dec/20 13:08;trohrmann;Fixed via

1.13.0: 0119456d1b6e5f7317f4cfbfe04ab89df48edef6
1.12.1: 1a29a7f5b955f27b24b722b0ae6725f458ab091d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceITCase.testStreamPartitionRead is not stable on Azure,FLINK-19718,13336175,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,jark,jark,20/Oct/20 02:38,27/Oct/20 08:43,13/Jul/23 08:12,27/Oct/20 08:43,1.12.0,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Here are some instances:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7845&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7875&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code}
2020-10-19T09:17:36.2004157Z [INFO] Results:
2020-10-19T09:17:36.2004505Z [INFO] 
2020-10-19T09:17:36.2007981Z [ERROR] Failures: 
2020-10-19T09:17:36.2010669Z [ERROR]   HiveTableSourceITCase.testStreamPartitionRead:537 expected:<[+I(0,0,2020-05-06 00:00:00), +I(1,1,2020-05-06 00:10:00), +I(1,1_copy,2020-05-06 00:10:00), +I(2,2,2020-05-06 00:20:00), +I(2,2_copy,2020-05-06 00:20:00), +I(3,3,2020-05-06 00:30:00), +I(3,3_copy,2020-05-06 00:30:00), +I(4,4,2020-05-06 00:40:00), +I(4,4_copy,2020-05-06 00:40:00), +I(5,5,2020-05-06 00:50:00), +I(5,5_copy,2020-05-06 00:50:00)]> but was:<[]>
2020-10-19T09:17:36.2011985Z [INFO] 
2020-10-19T09:17:36.2012582Z [ERROR] Tests run: 80, Failures: 1, Errors: 0, Skipped: 3
2020-10-19T09:17:36.2012976Z [INFO] 
2020-10-19T09:17:36.2137222Z [INFO] ------------------------------------------------------------------------
2020-10-19T09:17:36.2140971Z [INFO] Reactor Summary:
2020-10-19T09:17:36.2141558Z [INFO] 
2020-10-19T09:17:36.2141987Z [INFO] Flink : Tools : Force Shading ...................... SUCCESS [  1.346 s]
2020-10-19T09:17:36.2142534Z [INFO] Flink : Test utils : ............................... SUCCESS [  1.845 s]
2020-10-19T09:17:36.2143098Z [INFO] Flink : Test utils : Junit ......................... SUCCESS [  3.265 s]
2020-10-19T09:17:36.2190677Z [INFO] Flink : Queryable state : .......................... SUCCESS [  0.077 s]
2020-10-19T09:17:36.2191261Z [INFO] Flink : FileSystems : Azure FS Hadoop .............. SUCCESS [ 12.600 s]
2020-10-19T09:17:36.2191821Z [INFO] Flink : Examples : ................................. SUCCESS [  0.249 s]
2020-10-19T09:17:36.2192380Z [INFO] Flink : Examples : Batch ........................... SUCCESS [  1.919 s]
{code}
",,dian.fu,jark,lirui,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 08:43:28 UTC 2020,,,,,,,,,,"0|z0jtu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 02:49;jark;cc [~lirui], [~lzljs3620320];;;","20/Oct/20 18:42;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7958&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","21/Oct/20 03:12;lzljs3620320;Thanks for reporting this issue, I think we can use {{TableResult.collect}};;;","21/Oct/20 11:34;lzljs3620320;master: 700f07aa449d7bf7c7f68ed70b8d33779ae25b69

Feel free to re-open this ticket if error re-produce.;;;","21/Oct/20 11:35;lzljs3620320;Observe two days first;;;","27/Oct/20 08:43;dian.fu;Thanks [~jark] [~lzljs3620320] for working on this issue. Closing this ticket as it's seems fixed well and doesn't occur any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceReaderBase.pollNext may return END_OF_INPUT if SplitReader.fetch throws,FLINK-19717,13336089,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kezhuw,kezhuw,kezhuw,19/Oct/20 14:48,08/Dec/20 09:46,13/Jul/23 08:12,08/Dec/20 09:41,1.11.2,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,,,,,"Here are my imaginative execution flows:
1. In mailbox thread, we enters {{SourceReaderBase.getNextFetch}}. After executes {{splitFetcherManager.checkErrors()}} but before {{elementsQueue.poll()}}, {{SplitFetcher}} gets its chance to run.
2. {{SplitFetcher}} terminates due to exception from {{SplitReader.fetch}}. {{SplitFetcher.shutdownHook}} will removes this exceptional fetcher from {{SplitFetcherManager}}.
3. In mailbox thread,  {{elementsQueue.poll()}} executes. If there is no elements in queue, {{elementsQueue}} will be reset to unavailable.
4. After getting no elements from {{SourceReaderBase.getNextFetch}}, we will enter {{SourceReaderBase.finishedOrAvailableLater}}. If the exceptional fetcher is last alive fetcher, then {{SourceReaderBase.finishedOrAvailableLater}} may evaluate to {{InputStatus.END_OF_INPUT}}
5. {{StreamTask}} will terminate itself due to {{InputStatus.END_OF_INPUT}}.

Here is revised {{SourceReaderBaseTest.testExceptionInSplitReader}} which will fails in rate about 1/2.
{code:java}
	@Test
	public void testExceptionInSplitReader() throws Exception {
		expectedException.expect(RuntimeException.class);
		expectedException.expectMessage(""One or more fetchers have encountered exception"");
		final String errMsg = ""Testing Exception"";

		FutureCompletingBlockingQueue<RecordsWithSplitIds<int[]>> elementsQueue =
			new FutureCompletingBlockingQueue<>();
		// We have to handle split changes first, otherwise fetch will not be called.
		try (MockSourceReader reader = new MockSourceReader(
			elementsQueue,
			() -> new SplitReader<int[], MockSourceSplit>() {
				@Override
				public RecordsWithSplitIds<int[]> fetch() {
					throw new RuntimeException(errMsg);
				}

				@Override
				public void handleSplitsChanges(SplitsChange<MockSourceSplit> splitsChanges) {}

				@Override
				public void wakeUp() {
				}
			},
			getConfig(),
			null)) {
			ValidatingSourceOutput output = new ValidatingSourceOutput();
			reader.addSplits(Collections.singletonList(getSplit(0,
				NUM_RECORDS_PER_SPLIT,
				Boundedness.CONTINUOUS_UNBOUNDED)));
			reader.handleSourceEvents(new NoMoreSplitsEvent());
			// This is not a real infinite loop, it is supposed to throw exception after some polls.
			while (true) {
				InputStatus inputStatus = reader.pollNext(output);
				assertNotEquals(InputStatus.END_OF_INPUT, inputStatus);
				// Add a sleep to avoid tight loop.
				Thread.sleep(0);
			}
		}
	}
{code}

This revised {{SourceReaderBaseTest.testExceptionInSplitReader}} differs from existing one in three places:
 1. {{reader.handleSourceEvents(new NoMoreSplitsEvent())}} sets {{SourceReaderBase.noMoreSplitsAssignment}} to true.
 2. Add assertion to assert that {{reader.pollNext}} will not return {{InputStatus.END_OF_INPUT}}.
 3. Modify {{Thread.sleep(1)}} to {{Thread.sleep(0)}} to increase failure rate from 1/200 to 1/2.

See [FLINK-19448|https://issues.apache.org/jira/browse/FLINK-19448] for initial discussion.",,becket_qin,dian.fu,kezhuw,rmetzger,sewen,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 09:41:54 UTC 2020,,,,,,,,,,"0|z0jtb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 15:51;kezhuw;{quote}Maybe the fix would be to always check for errors before returning NOTHING_AVAILABLE.
{quote}
[~sewen] I guess you means checking for errors before returning *{{END_OF_INPUT}}* ? I think it is no enough since what this error-checking expect is that error-setting happens before fetcher-removing. Before error-checking, all error-setting operations should completed, otherwise we are facing undetermined result. We can't get this guarantee from current implementation. Currently, we do fetcher-removing in {{SplitFetcher.shutdownHook}} and error-setting in {{ThrowableCatchingRunnable.exceptionHandler}}. Both operations are concurrent safe, but fetcher-removing happens before error-setting. This means that there is no 'happen before' relationship between error-checking and error-setting. If we reverses 'happen before' relationship between error-setting and fetcher-removing, then, after detecting {{allFetchersHaveShutdown}} and before error-checking, we known that error-setting has completed. So, if we are going to check for errors before returning {{END_OF_INPUT}}, we should also reverse 'happen before' relationship between error-setting and fetcher-removing.

[~dianfu] [~rmetzger] [~sewen] [~becket_qin] Should this be a blocker for 1.12 ?;;;","21/Oct/20 08:09;rmetzger;Thanks a lot for your investigation and for opening this ticket.
I can not comment on the priority of the ticket. Let's wait for Stephan or Jiangjie.;;;","22/Oct/20 00:59;kezhuw;[~rmetzger] Thanks for response. Let's wait their decision.

[~sewen] [~becket_qin] I think we can solve this by moving {{exceptionHandler}} from {{ThrowableCatchingRunnable}} to {{SplitFetcher}}, this way we can rearrange 'happen before' relationship between error-setting and fetcher-removing inside {{SplitFetcher}} with no interference from extra structure. I have created [a branch in my clone|https://github.com/kezhuw/flink/tree/test-case-source-reader-end-of-input-caused-by-split-reader-exception] to demonstrate test case and possible fix. Commit [3c65b2d|https://github.com/kezhuw/flink/commit/3c65b2d8dddd3c18c787fd590ece17304110e3fc] modifies and fails {{SourceReaderBaseTest.testExceptionInSplitReader}}, [e2e383|https://github.com/kezhuw/flink/commit/e2e3832c7a4c726405111fb9198e6737417877d5] fixes it. Does this and above analysis make sense to you ? If it is, could you mind assign this issue to me ?;;;","23/Oct/20 13:59;sewen;[~kezhuw] I agree with your analysis and with the suggested fix. Good work!

Do you want to prepare a PR for this? Otherwise I could work on this in a few days.;;;","23/Oct/20 14:05;kezhuw;[~sewen] I am willing to take over this.;;;","26/Oct/20 15:57;rmetzger;Thank you. I assigned you to the ticket.;;;","27/Oct/20 08:46;dian.fu;[~kezhuw] Any update on this ticket? It would be great if we can fix this issue by this week as we're planning to building the first RC of 1.12 on next Monday.;;;","27/Oct/20 09:52;rmetzger;[~dian.fu] I believe the issue here is that [~kezhuw] is waiting for a reviewer of the PR. It has been open for 4 days.;;;","27/Oct/20 11:01;dian.fu;[~rmetzger] You are right. I missed the PR.;;;","05/Nov/20 15:57;rmetzger;Is this blocker expected to be merged before the feature freeze on Sunday?
I guess we could also fix this bug after the feature freeze in the stabilization phase.;;;","08/Nov/20 15:47;sewen;We are about to merge this. There is consensus on problem and solution, so it'll be in very soon.

Given that this is not a feature, but a critical bug fix, I think the feature freeze deadline does not apply here.;;;","09/Nov/20 02:52;becket_qin;[~sewen] I have merged the patch to master. But cherry-picking the patch to release-1.11 has some conflicts. Do you want to sequentialize the backporting of this patch with other Source patches? Otherwise I can also rebase the patch on release-1.11.;;;","09/Nov/20 12:38;sewen;Yes, we need to pick back all changes from master to release-1.11 in order. Otherwise it will be impossible.
Let's coordinate between the two of us how we do this...;;;","12/Nov/20 15:00;sewen;Fixed in 1.12.0 via 2cce1aced0d6a311ff0803b773f1565e7f9d76fc;;;","08/Dec/20 09:41;becket_qin;Merged to release-1.11.
ceda80842723f3f621279d44ddcbb5210e2a3525;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use Assume Role with EFO record publisher,FLINK-19716,13336070,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny.cranmer,danny.cranmer,danny.cranmer,19/Oct/20 13:05,07/Dec/22 10:34,13/Jul/23 08:12,20/Oct/20 05:21,,,,,,1.12.0,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"*Reproduction Steps*
 * Setup an application to consume from a Kinesis Stream
 * Use ASSUME_ROLE credential provider 

{code:java}

consumerConfig.setProperty(AWSConfigConstants.AWS_CREDENTIALS_PROVIDER, ASSUME_ROLE.name());
consumerConfig.setProperty(AWSConfigConstants.AWS_ROLE_ARN, ""<role-arn>"");
consumerConfig.setProperty(AWSConfigConstants.AWS_ROLE_SESSION_NAME, ""test-efo"");

{code}

 

*Expected Result*
 * Consumer is able to authorise and consume from the stream

*Actual Result*
 * The following error is thrown (full stack attached)
 ** Caused by: org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.exception.SdkClientException: Unable to load an HTTP implementation from any provider in the chain. You must declare a dependency on an appropriate HTTP implementation or pass in an SdkHttpClient explicitly to the client builder.

*Diagnosis*

This issue occurs because Assume Role credential provider requires a Sync HTTP Client. The Apache HTTP Client is on the classpath but it is not detected due to the shading relocation. It is looking for:

- {{org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpService}}

However the service manifest is defining:

- {{software.amazon.awssdk.http.SdkHttpService}}

*Solution*
 * Add a service manifest such that the shaded HTTP client is used
 * Also needed to update the HTTP client/core version due to incompatibilities 

*Testing*

Tested using EFO and POLLING record consumer 

 ",,danny.cranmer,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/20 13:10;danny.cranmer;FLINK-19716-error.txt;https://issues.apache.org/jira/secure/attachment/13013787/FLINK-19716-error.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 05:21:46 UTC 2020,,,,,,,,,,"0|z0jt6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 13:16;danny.cranmer;[~tzulitai] do you have time to take a look a this PR please? It is a relatively small one:

- [https://github.com/apache/flink/pull/13689]

 ;;;","20/Oct/20 05:21;tzulitai;Fixed in master via: f4e4046aed11c3f16a38f5f0202d7592cdbb5f2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A result partition is not untracked after its producer task failed in TaskManager,FLINK-19703,13336001,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,19/Oct/20 06:06,03/Nov/20 13:56,13/Jul/23 08:12,02/Nov/20 14:15,1.10.2,1.11.2,1.12.0,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"{{Execution#maybeReleasePartitionsAndSendCancelRpcCall(...)}} will be not invoked when a task is reported to be failed in TaskManager, which results in its partitions to still be tacked by the job manager partition tracker. ",,kezhuw,klion26,wind_ljy,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 14:15:27 UTC 2020,,,,,,,,,,"0|z0jsrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 09:45;chesnay;Could you explain why this happens? I can't seem to find a code-path where this can happen.;;;","23/Oct/20 10:11;zhuzh;[~chesnay] It can happen in code path {{JobMaster#updateTaskExecutionState(...)}} -> {{SchedulerBase#updateTaskExecutionState(...)}} -> {{executionGraph.updateState(...)}} ->  {{Execution#markFailed(...)}} -> {{Execution#processFail(...)}}, in the case that {{isLegacyScheduling == false}}.;;;","23/Oct/20 10:22;zhuzh;The problem can be exposed by reworking ExecutionPartitionLifecycleTest to be based on DefaultScheduler. https://github.com/apache/flink/pull/13766/commits/42f066384d04bcfe67ccbb5766f09ab5dde9e19c#diff-ea7fe46ae9e506408641ce4ad75933fc11c50202bc096ea9963cdcd3fd37cc73;;;","02/Nov/20 14:15;zhuzh;Fixed via:
dc50a2a84100441e6003df57e2ad667d275e5e3c
ed33697c1b8d74eb511cf88ec8e40792c128ad1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unaligned Checkpoint might misuse the number of buffers to persist from the previous barrier,FLINK-19701,13335991,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,19/Oct/20 03:48,03/Nov/20 05:50,13/Jul/23 08:12,03/Nov/20 05:50,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,,,,,0,,,,,,"Current CheckpointUnaligner interacts with RemoteInputChannel to persisting the input buffers. However, based the current implementation it seems if we have the following case:
{code:java}
1. There are 3 input channels.
2. Input channel 0 received barrier 1, and processed barrier 1 to start checkpoint 1.
3. Input channel 1 received barrier 1, and processed barrier 1. Now the state of input channel persister becomes BARRIER_RECEIVED and numBuffersOvertaken(channel 1) = n_1.
4. However, input 2 received nothing and the checkpoint expired, new checkpoint is trigger.
5. Input channel 0 received barrier 2, checkpoint 1 is deserted and checkpoint 2 is started. However, in this case the state of the input channels are not cleared. Thus now channel 1 is still BARRIER_RECEIVED and numBuffersOvertaken(channel 1) = n_1. Then channel 1 would only persist n_1 buffers in the channel for the new checkpoint 2. 

{code}",,aljoscha,gaoyunhaii,klion26,pnowojski,roman,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 05:50:27 UTC 2020,,,,,,,,,,"0|z0jspc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 03:59;gaoyunhaii;Hi [~AHeise] [~pnowojski], could you also have a look at whether this is truly a problem ? I test this case manually by blocking the barriers from the upstream tasks and it seems to be exist. ;;;","30/Oct/20 17:01;roman;-Another related issue occurs in the following scenario:- - not an issue (there is a check in getInflightBuffers)
 # -taskA and taskB receive AlignedCheckpoint barrier-
 # -taskB cancels it-
 # -JM sends (next) UnalignedCheckpoint barrier-
 # -now taskA will have AC barrier in queue AND will receive UC barrier - so RemoteInputChannel.receivedBuffers.getNumUnprioritizedElements() will include this barrier, which is currently illegal (ChannelStateWriter will throw an exception)-

-Probably, it's easier to solve it together. If not, please create a separate ticket.-;;;","02/Nov/20 19:32;roman;I looked at the code and it seems that this case is handled properly.

 

The next (6) step would be SingleCheckpointBarrierHandler.processBarrier() (from main task thread).

And there:
{code:java}
if (currentCheckpointId < newBarrierId) {
    if (isCheckpointPending()) {
        cancelSubsumedCheckpoint(barrierId);
...
{code}
Which eventually resets numBuffersOvertaken to ALL for all input channels and pendingCheckpointBarrierId to CHECKPOINT_COMPLETED for their channel persisters.

 

Could you check this in your test [~gaoyunhaii]?

Or could you share the test if the concern is not resolved?;;;","03/Nov/20 00:38;gaoyunhaii;Hi [~roman_khachatryan] very thanks investigating the issue! I re-run the test on the latest master and now we indeed will reset the channel info after checkpoints get subsumed. I re-checked the code and found that previously we do not call _resetPendingCheckpoint_ when canceling the subsumed checkpoint in _CheckpointUnaligner_, but now we should have also fixed this issue during the development. ;;;","03/Nov/20 05:50;roman;Hi [~gaoyunhaii], thanks for double-checking the issue.

I guess it was resolved by [~pnowojski] in [9dd2c3|https://github.com/apache/flink/commit/9dd2c351c1e1717d7229278a0975e8052808891c#diff-54795174913178db0c99b6f7ba9b9c263a4982941f90477fa453e8bea9717070R210] (thanks Piotr! :) ).

Closing.
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PrometheusReporterEndToEndITCase crashes with exit code 143,FLINK-19699,13335977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,dian.fu,dian.fu,19/Oct/20 01:26,10/Nov/20 15:11,13/Jul/23 08:12,10/Nov/20 15:10,1.12.0,,,,,1.12.0,,,Runtime / Metrics,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7814&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729]

{code}
2020-10-18T23:46:04.9667443Z [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-18T23:46:04.9669237Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp
2020-10-18T23:46:04.9670440Z [ERROR] Error occurred in starting fork, check output in log
2020-10-18T23:46:04.9671283Z [ERROR] Process Exit Code: 143
2020-10-18T23:46:04.9671614Z [ERROR] Crashed tests:
2020-10-18T23:46:04.9672025Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
2020-10-18T23:46:04.9672649Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-18T23:46:04.9674834Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp
2020-10-18T23:46:04.9676153Z [ERROR] Error occurred in starting fork, check output in log
2020-10-18T23:46:04.9676556Z [ERROR] Process Exit Code: 143
2020-10-18T23:46:04.9676882Z [ERROR] Crashed tests:
2020-10-18T23:46:04.9677288Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
2020-10-18T23:46:04.9677827Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-10-18T23:46:04.9678408Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:282)
2020-10-18T23:46:04.9678965Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
2020-10-18T23:46:04.9679575Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-10-18T23:46:04.9680983Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-10-18T23:46:04.9681749Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-10-18T23:46:04.9682246Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-10-18T23:46:04.9682728Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-10-18T23:46:04.9683179Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-10-18T23:46:04.9683609Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-10-18T23:46:04.9684102Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-10-18T23:46:04.9684639Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-10-18T23:46:04.9685180Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-10-18T23:46:04.9685711Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-10-18T23:46:04.9686145Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-10-18T23:46:04.9686516Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-10-18T23:46:04.9689517Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-10-18T23:46:04.9689917Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-10-18T23:46:04.9690262Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-10-18T23:46:04.9690606Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-18T23:46:04.9690994Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-18T23:46:04.9691435Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-18T23:46:04.9691856Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-18T23:46:04.9692450Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-10-18T23:46:04.9693419Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-10-18T23:46:04.9693885Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-10-18T23:46:04.9694334Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-10-18T23:46:04.9694939Z [ERROR] -> [Help 1]
{code}",,azagrebin,dian.fu,klion26,rmetzger,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19882,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 15:10:54 UTC 2020,,,,,,,,,,"0|z0jsm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 06:26;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7883&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","21/Oct/20 07:50;rmetzger;Improved debugging in https://github.com/apache/flink/commit/c8c93c5067f655e02f01f21eb17ef2fd74360cad;;;","21/Oct/20 12:46;trohrmann;I'd suggest to close this issue as cannot reproduce until we see another failure with the improved logging [~rmetzger].;;;","21/Oct/20 13:01;rmetzger;Yes, that's a good idea. Closing ticket for now ...;;;","29/Oct/20 13:23;azagrebin;seems I encountered another instance in my PR (unrelated) CI:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8572&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529]

passed in my CI:
https://dev.azure.com/azagrebin/azagrebin/_build/results?buildId=332&view=results;;;","02/Nov/20 02:05;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8714&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","02/Nov/20 08:55;trohrmann;cc [~rmetzger]. There might be a new failure with the improved debugging logs.;;;","02/Nov/20 09:34;rmetzger;Yes, I'll take a look soon.;;;","03/Nov/20 08:57;rmetzger;Sadly there are no crash files attached to the debug logs :(  

I'm trying to reproduce the issue on my CI with DEBUG level and printing the file system contents in the end to make sure the crash file collection isn't broken. I've tried reproducing the issue before, without luck. Let's see...

;;;","05/Nov/20 06:47;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8623&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d ... again no debug info.;;;","05/Nov/20 09:23;rmetzger;I did some further research: Signal 143 means the process got killed by a SIGTERM: 128 + 15 (SIGTERM) = 143.
In Flink's code, I could not find any place where we exit the JVM with 143.

I will add the signal handlers to the test code as well, and write the kernel ring buffer to the e2e debugging files to rule out the oom killer.
I bet it's the oomkiller that runs because the python test has leftover processes occupying the memory.;;;","06/Nov/20 03:03;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9121&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","06/Nov/20 12:00;chesnay;Additional debugging measures:
master:
43701398fc9b3fe8161d1e9fb583c3aeb16a9356
0b9a47ef481d366389b3d1e303974f6fe00e5224;;;","08/Nov/20 01:17;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9257&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","08/Nov/20 04:33;zhuzh;Another instance:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9256&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","09/Nov/20 09:02;rmetzger;This is actually caused by the same problem as: FLINK-19882 I'm testing a fix already.;;;","10/Nov/20 15:10;rmetzger;Closing this, it should be fixed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing Table with RowTime Column of type TIMESTAMP(3) to Kafka fails with ClassCastException,FLINK-19695,13335940,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,knaufk,knaufk,18/Oct/20 16:28,20/Oct/20 13:56,13/Jul/23 08:12,20/Oct/20 13:56,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"When I try to write a table to Kafka (JSON format) that has a rowtime attribute of type TIMESTAMP(3) the job fails with 

{noformat}
2020-10-18 18:02:08
java.lang.ClassCastException: org.apache.flink.table.data.TimestampData cannot be cast to java.lang.Long
	at org.apache.flink.table.data.GenericRowData.getLong(GenericRowData.java:154)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator$SimpleContext.timestamp(SinkOperator.java:144)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:866)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:86)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:123)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collectWithTimestamp(StreamSourceContexts.java:111)
	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:352)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:185)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
{noformat}

From looking at the relevant code in SinkOperator$SimpleContext#timestamp it seems that we can only deal with long type timestamps in SinkOperator?!
",,jark,knaufk,leonard,libenchao,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 08:57:59 UTC 2020,,,,,,,,,,"0|z0jse0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 08:57;jark;Fixed in 
 - master: 16f97fbfeb822b8f4d9db237dd37bd422812053f
 - 1.11: 69e3e91dc2197bd29e8b804d3fd5a2a43ec1b1af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't restore feedback channel from savepoint,FLINK-19692,13335913,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,igal,Antti-Kaikkonen,Antti-Kaikkonen,18/Oct/20 05:34,11/Nov/20 09:01,13/Jul/23 08:12,29/Oct/20 03:39,statefun-2.0.0,statefun-2.1.0,statefun-2.2.0,,,statefun-2.2.1,,,Stateful Functions,,,,,0,pull-request-available,,,,,"When using the new statefun-flink-datastream integration the following error is thrown by the *feedback -> union* task when trying to restore from a savepoint:
{code:java}
java.lang.Exception: Exception while creating StreamOperatorStateContext.
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:204)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:247)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:479)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:475)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:528)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.io.IOException: position out of bounds
    at org.apache.flink.runtime.state.StatePartitionStreamProvider.getStream(StatePartitionStreamProvider.java:58)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:235)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:167)
    ... 9 more
Caused by: java.io.IOException: position out of bounds
    at org.apache.flink.runtime.state.memory.ByteStreamStateHandle$ByteStateHandleInputStream.seek(ByteStreamStateHandle.java:124)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl$KeyGroupStreamIterator.next(StreamTaskStateInitializerImpl.java:442)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl$KeyGroupStreamIterator.next(StreamTaskStateInitializerImpl.java:395)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:228)
    ... 10 more
{code}
 The error is only thrown when the feedback channel has been used. 

I have tested with the [example application|https://github.com/apache/flink-statefun/blob/master/statefun-examples/statefun-flink-datastream-example/src/main/java/org/apache/flink/statefun/examples/datastream/Example.java] and the error is thrown only if it is modified to actually use the feedback channel. I simply modified the invoke method to sometimes forward the greeting to a random name: 
{code:java}
@Override
public void invoke(Context context, Object input) {
  int seen = seenCount.updateAndGet(MyFunction::increment);
  context.send(GREETINGS, String.format(""Hello %s at the %d-th time"", input, seen));
  String[] names = {""Stephan"", ""Igal"", ""Gordon"", ""Seth"", ""Marta""};
  ThreadLocalRandom random = ThreadLocalRandom.current();
  int index = random.nextInt(names.length);
  final String name2 = names[index];
  if (random.nextDouble() < 0.5) context.send(new Address(GREET, name2), input);
}
{code}",,Antti-Kaikkonen,igal,kezhuw,klion26,sjwiesman,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19741,FLINK-19748,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 09:01:02 UTC 2020,,,,,,,,,,"0|z0js80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 12:37;igal;Thanks for reporting this [~Antti-Kaikkonen],

I think that the problem is that StateFun can only work with RocksDB as the state backend.

Can you try to re-run your modified example with setting rocks db as the state backend:

(add that lines after: [https://github.com/apache/flink-statefun/blob/master/statefun-examples/statefun-flink-datastream-example/src/main/java/org/apache/flink/statefun/examples/datastream/Example.java#L58)]

 
{code:java}
 env.setStateBackend(new RocksDBStateBackend(""file:///tmp/checkpoint""));{code}
 

You may also need to add the following dependency to the example program:

 
{code:java}
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-statebackend-rocksdb_2.12</artifactId>
    <version>${flink.version}</version>
</dependency> {code}
 

Thanks.

 ;;;","19/Oct/20 17:48;Antti-Kaikkonen;Using rocksdb did not help. 

I also noticed that this error is only thrown when parallelism is more than 1 because the feedback channel is not used with parallelism 1. I was able to reproduce with a fresh 1.11.2 single node installation with *taskmanager.numberOfTaskSlots: 2* in flink-conf.yaml.

By the way I have not yet tested regular stateful functions with 1.11.2 so I'm not sure if this is only a problem with statefun-flink-datastream or flink-statefun in general.;;;","20/Oct/20 06:42;igal;Thanks for following up, we are investigating this and report back shortly.;;;","20/Oct/20 07:30;Antti-Kaikkonen;I did some testing with regular embedded stateful functions and got the same error. So either this is a bug in stateful functions or there is something wrong with my setup. Here is the program that I created for testing: [https://github.com/Antti-Kaikkonen/FlinkStatefunCountTo1M]

I don't remember having this issue with statefun 2.1 and flink 1.10 but I'm not 100% sure.

edit: I forgot to mention that the error is thrown even when the program doesn't use any persistent states.;;;","21/Oct/20 02:32;tzulitai;[~Antti-Kaikkonen] thanks a lot for reporting this.

We've currently identified FLINK-19741 as the cause for this error. This would require a new Flink minor / hotfix release, and updating the Flink dependency in StateFun with the new release.

In the meanwhile, [~igal] is also investigating if there is a possible solution for this in StateFun for 2.2.1, so that we don't have to wait for a new Flink release.;;;","21/Oct/20 03:19;Antti-Kaikkonen;Thanks for investigating and identifying the issue so quickly. I will keep an eye on these issues and update as soon as there is a solution available. I'm also interested in any workarounds to get it working as soon as possible.;;;","29/Oct/20 03:39;tzulitai;Fixed via -

statefun/master: ddeec3128524ecb41633c6a325c030c7b72d7e83
statefun/release-2.2: 8823b27defff64fab2532fb01d1b5f8ae5cfdd72

Thanks for the fix [~igal] and also the testing efforts [~Antti-Kaikkonen]!;;;","11/Nov/20 09:01;tzulitai;[~Antti-Kaikkonen] FYI, StateFun 2.2.1 was just released, which includes the fix for this issue: https://flink.apache.org/news/2020/11/11/release-statefun-2.2.1.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ParquetStreamingFileSinkITCase.testWriteParquetAvroReflect failed with ""expected:<1> but was:<2>""",FLINK-19690,13335908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dian.fu,dian.fu,18/Oct/20 01:23,29/Apr/21 13:34,13/Jul/23 08:12,29/Apr/21 13:34,1.11.2,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,stale-major,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7792&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=603cb7fd-6f38-5c99-efca-877e1439232f

{code}
[ERROR] testWriteParquetAvroReflect(org.apache.flink.formats.parquet.avro.ParquetStreamingFileSinkITCase) Time elapsed: 1.407 s <<< FAILURE! 
java.lang.AssertionError: expected:<1> but was:<2> 
 at org.junit.Assert.fail(Assert.java:88) 
 at org.junit.Assert.failNotEquals(Assert.java:834) 
 at org.junit.Assert.assertEquals(Assert.java:645) 
 at org.junit.Assert.assertEquals(Assert.java:631) 
 at org.apache.flink.formats.parquet.avro.ParquetStreamingFileSinkITCase.validateResults(ParquetStreamingFileSinkITCase.java:161) 
 at org.apache.flink.formats.parquet.avro.ParquetStreamingFileSinkITCase.testWriteParquetAvroReflect(ParquetStreamingFileSinkITCase.java:153)
{code}",,dian.fu,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 13:34:45 UTC 2021,,,,,,,,,,"0|z0js6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:05;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 13:34;lirui;The issue hasn't happened for a while. Closing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorProcessSpecContainerResourcePriorityAdapterTest fails,FLINK-19689,13335861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,rmetzger,rmetzger,17/Oct/20 09:52,27/Oct/20 10:47,13/Jul/23 08:12,21/Oct/20 14:21,1.12.0,,,,,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7776&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354

{code}
2020-10-16T21:48:16.5354451Z [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 0.022 s <<< FAILURE! - in org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest
2020-10-16T21:48:16.5355440Z [ERROR] testExternalResource(org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest)  Time elapsed: 0.005 s  <<< ERROR!
2020-10-16T21:48:16.5356271Z java.lang.IllegalStateException: External resource testing-external-resource is not supported by the Yarn cluster.
2020-10-16T21:48:16.5356834Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:220)
2020-10-16T21:48:16.5357490Z 	at org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapter.validateExternalResourceConfigs(TaskExecutorProcessSpecContainerResourcePriorityAdapter.java:85)
2020-10-16T21:48:16.5358469Z 	at org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapter.<init>(TaskExecutorProcessSpecContainerResourcePriorityAdapter.java:57)
2020-10-16T21:48:16.5359460Z 	at org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.getAdapterWithExternalResources(TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.java:170)
2020-10-16T21:48:16.5360791Z 	at org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.testExternalResource(TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.java:129)
2020-10-16T21:48:16.5361437Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-16T21:48:16.5362144Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-16T21:48:16.5363032Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-16T21:48:16.5363704Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-16T21:48:16.5364339Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-16T21:48:16.5365108Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-16T21:48:16.5365880Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-16T21:48:16.5366598Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-16T21:48:16.5367162Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-10-16T21:48:16.5367560Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-16T21:48:16.5367943Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-16T21:48:16.5368543Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-16T21:48:16.5369034Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-16T21:48:16.5369455Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-16T21:48:16.5369859Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-16T21:48:16.5370276Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-16T21:48:16.5370676Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-16T21:48:16.5371091Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-16T21:48:16.5371497Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-16T21:48:16.5371921Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-16T21:48:16.5372422Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-16T21:48:16.5373073Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-16T21:48:16.5373554Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-16T21:48:16.5374072Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-16T21:48:16.5374660Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-16T21:48:16.5375128Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-16T21:48:16.5375585Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dian.fu,guoyangze,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 14:21:41 UTC 2020,,,,,,,,,,"0|z0jrwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/20 01:27;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7791&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354;;;","18/Oct/20 01:30;dian.fu;cc [~xintongsong];;;","18/Oct/20 02:20;xtsong;Thanks for the pointer, [~dian.fu]. I'm looking into this.;;;","18/Oct/20 03:27;xtsong;The problem is that we try to set a {{testing-external-resource}} in the test case, while Hadoop 2.10+ checks it against the {{resource-types.xml}} and finds it not defined.
The solution is to add {{flink-yarn/src/test/resoruces/resource-types.xml}} with the testing resource type included.
I'm opening a PR to fix this.;;;","19/Oct/20 02:50;xtsong;Fixed via:
* master: f734f59488afaa7ad984362a0dd62e99c134f062;;;","19/Oct/20 09:08;rmetzger;Thanks for the quick fix!;;;","20/Oct/20 08:40;xtsong;Reopen the ticket.

The previous fix is reverted (in a5bc7d2ec6f7d7a069f64c4789665db3ff1f40b0) because it causes the IT cases fail.
https://issues.apache.org/jira/browse/FLINK-19728

The test cases are temporary deactivated, until we come up with a proper solution for this ticket.;;;","21/Oct/20 14:21;xtsong;Fixed via
* master: 34c765cc22c6033b4a0299ba6d49377293978d29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink batch job fails because of InterruptedExceptions from network stack,FLINK-19688,13335859,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,rmetzger,rmetzger,17/Oct/20 09:27,22/Jun/21 13:55,13/Jul/23 08:12,22/Oct/20 08:20,1.12.0,,,,,1.12.0,,,Runtime / Network,Runtime / Task,,,,0,,,,,,"I have a benchmarking test job, that throws RuntimeExceptions at any operator at a configured, random interval. When using low intervals, such as mean failure rate = 60 s, the job will get into a state where it frequently fails with InterruptedExceptions.

The same job does not have this problem on Flink 1.11.2 (at least not after running the job for 15 hours, on 1.12-SN, it happens within a few minutes)
This is the job: https://github.com/rmetzger/flip1-bench/blob/master/flip1-bench-jobs/src/main/java/com/ververica/TPCHQuery3.java

This is the exception:
{code}
2020-10-16 16:02:15,653 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (8/8)#1 (06d656f696bf4ed98831938a7ac2359d_c1c4a56fea0536703d37867c057f0cc8_7_1) switched from RUNNING to FAILED.
java.lang.Exception: The data preparation for task 'CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38))' , caused an error: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:481) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:370) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]
Caused by: org.apache.flink.util.WrappingRuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.
	at org.apache.flink.runtime.operators.sort.ExternalSorter.getIterator(ExternalSorter.java:253) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1122) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:99) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:475) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 4 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_222]
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_222]
	at org.apache.flink.runtime.operators.sort.ExternalSorter.getIterator(ExternalSorter.java:250) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1122) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:99) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:475) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 4 more
Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.
	at org.apache.flink.runtime.operators.sort.ExternalSorter.lambda$getIterator$1(ExternalSorter.java:247) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) ~[?:1.8.0_222]
	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) ~[?:1.8.0_222]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_222]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_222]
	at org.apache.flink.runtime.operators.sort.ExternalSorterBuilder.lambda$doBuild$1(ExternalSorterBuilder.java:363) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.internalHandleException(ThreadBase.java:123) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:82) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:83) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.runtime.io.network.partition.consumer.PartitionConnectionException: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:160) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:305) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:277) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:93) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:91) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ReadingThread.go(ReadingThread.java:68) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:79) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/192.168.2.172:52366' has failed. This might indicate that the remote task manager has been lost.
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:85) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:157) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:305) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:277) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:93) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:91) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ReadingThread.go(ReadingThread.java:68) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:79) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/192.168.2.172:52366' has failed. This might indicate that the remote task manager has been lost.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_222]
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_222]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:83) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:157) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:305) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:277) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:93) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:91) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ReadingThread.go(ReadingThread.java:68) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:79) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/192.168.2.172:52366' has failed. This might indicate that the remote task manager has been lost.
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:122) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:101) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.lambda$createPartitionRequestClient$1(PartitionRequestClientFactory.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.FutureUtils.completeFromCallable(FutureUtils.java:88) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:157) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:305) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:277) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:93) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:91) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ReadingThread.go(ReadingThread.java:68) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:79) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: java.lang.InterruptedException
	at java.lang.Object.wait(Native Method) ~[?:1.8.0_222]
	at java.lang.Object.wait(Object.java:502) ~[?:1.8.0_222]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:252) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:114) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:101) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.lambda$createPartitionRequestClient$1(PartitionRequestClientFactory.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.FutureUtils.completeFromCallable(FutureUtils.java:88) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:157) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:305) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:277) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:93) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:91) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ReadingThread.go(ReadingThread.java:68) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:79) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code}

I will attach all logs to this ticket.

",,AHeise,klion26,pnowojski,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19791,,,,,,,,FLINK-20615,,,,,"17/Oct/20 09:29;rmetzger;logs.tgz;https://issues.apache.org/jira/secure/attachment/13013707/logs.tgz",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 08:20:48 UTC 2020,,,,,,,,,,"0|z0jrw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 09:19;trohrmann;Could this be an instance of masking the actual root cause of the {{Task}} failure (in our case the user code exception)? cc [~pnowojski].;;;","19/Oct/20 14:04;rmetzger;I'm trying to investigate what's going on.
Here are some of my notes, without any exciting results (yet)

{code}
We have a test setup with 4 TaskManagers.

JM: 2020-10-16 16:01:31,015: TaskManager 2 is the first where an artificial failure is induced through a RuntimeException.

JM: The JobManager then cancels the execution on all TMs:
JM: 2020-10-16 16:01:31,049 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RUNNING to RESTARTING.

(side note)
TM3: 2020-10-16 16:01:31,190: TaskManager 3 is logging a InterruptedException (as a WARN from ExternalSorter) on ""CHAIN Join (Join at main(TPCHQuery3.java:183)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (7/8)#0""
(This WARN will be reduced to a DEBUG in https://github.com/apache/flink/pull/13668).

JM: cancellation complete, we are RUNNING again:
JM: 2020-10-16 16:01:31,448 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RESTARTING to RUNNING.

TM3: Now TM3 fails a task, with the root cause being an InterruptedException, while connecting to TaskManager 0 in PartitionRequestClientFactory.connect().
TM3: 2020-10-16 16:02:15,653 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (8/8)#1 (06d656f696bf4ed98831938a7ac2359d_c1c4a56fea0536703d37867c057f0cc8_7_1) switched from RUNNING to FAILED.
java.lang.Exception: The data preparation for task 'CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38))' , caused an error: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_1 not reachable.

JM: receives the report from TM3:
JM: 2020-10-16 16:02:15,860 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (8/8) (06d656f696bf4ed98831938a7ac2359d_c1c4a56fea0536703d37867c057f0cc8_7_1) switched from RUNNING to FAILED on

JM: 2020-10-16 16:02:15,888 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RUNNING to RESTARTING.

What is TM0 doing when TM3 reports the InterruptedException?
Nothing special:
TM0: 2020-10-16 16:02:15,633 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (2/8)#1 (06d656f696bf4ed98831938a7ac2359d_c1c4a56fea0536703d37867c057f0cc8_1_1) switched from DEPLOYING to RUNNING.
TM0: 2020-10-16 16:02:15,894 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task CHAIN DataSource (at getLineitemDataSet(TPCHQuery3.java:347) (org.apache.flink.api.java.io.TupleCsvInputFormat)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (2/8)#1 (06d656f696bf4ed98831938a7ac2359d_7d1f567aae11518e19b6b807800d3af7_1_1).

What is happening afterwards?

2020-10-16 16:02:16,150 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RESTARTING to RUNNING.
2020-10-16 16:02:23,241 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at getOrdersDataSet(TPCHQuery3.java:363) (org.apache.flink.api.java.io.TupleCsvInputFormat)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) -> Filter (Filter at main(TPCHQuery3.java:145)) (4/8) (06d656f696bf4ed98831938a7ac2359d_dbb16db93f572f3995dadfb433c8c1ff_3_2) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@2e3effa.
java.lang.RuntimeException: Kill requested
2020-10-16 16:02:23,243 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RUNNING to RESTARTING.
2020-10-16 16:02:23,562 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RESTARTING to RUNNING.
2020-10-16 16:03:06,051 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at getLineitemDataSet(TPCHQuery3.java:347) (org.apache.flink.api.java.io.TupleCsvInputFormat)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (7/8) (06d656f696bf4ed98831938a7ac2359d_12527f4a17e3fa06633ddc6f343514ed_6_3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@1de3a10.
java.lang.RuntimeException: Kill requested
2020-10-16 16:03:06,266 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RESTARTING to RUNNING.
2020-10-16 16:03:48,543 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (7/8) (06d656f696bf4ed98831938a7ac2359d_c1c4a56fea0536703d37867c057f0cc8_6_4) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@56d6a66.
java.lang.Exception: The data preparation for task 'CHAIN GroupReduce (GroupReduce at main(TPCHQuery3.java:199)) -> Map (Map at appendMapper(KillerClientMapper.java:38))' , caused an error: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Connection for partition 060d457c4163472f65a4b741993c83f8#0@06d656f696bf4ed98831938a7ac2359d_0bcc9fbf9ac242d5aac540917d980e44_0_4 not reachable.
2020-10-16 16:03:48,548 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RUNNING to RESTARTING.
2020-10-16 16:03:48,704 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RESTARTING to RUNNING.
2020-10-16 16:04:31,385 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at getLineitemDataSet(TPCHQuery3.java:347) (org.apache.flink.api.java.io.TupleCsvInputFormat)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (2/8) (06d656f696bf4ed98831938a7ac2359d_12527f4a17e3fa06633ddc6f343514ed_1_5) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@124567d2.
java.lang.RuntimeException: Kill requested
2020-10-16 16:04:31,387 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job TPCH Query 3 Example (06d656f696bf4ed98831938a7ac2359d) switched from state RUNNING to RESTARTING.
... and so on ...

It seems that we are not in a restart loop. Rather sometimes the execution gets interrupted.

Could it be an interrupt from the operating system?
Unlikely, we are logging signals:
2020-10-16 16:00:04,092 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Registered UNIX signal handlers for [TERM, HUP, INT]
there is only one signal logged
2020-10-16 16:10:31,680 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
{code}

I will now try to find out who's interrupting the thread.
;;;","22/Oct/20 08:18;roman;https://github.com/apache/flink/pull/13723;;;","22/Oct/20 08:20;arvid;Merged into master as 840e8af879e69c1bf9ad121b670a2703eb88b858.
Closing issue as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Jdbc-connector's  'lookup.max-retries' option implementation is different from the meaning,FLINK-19684,13335797,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,caozhen1937,caozhen1937,caozhen1937,16/Oct/20 15:09,27/Oct/20 14:09,13/Jul/23 08:12,27/Oct/20 14:09,,,,,,1.12.0,,,Connectors / JDBC,,,,,0,pull-request-available,,,,," 

The code of 'lookup.max-retries' option :
{code:java}
for (int retry = 1; retry <= maxRetryTimes; retry++) {
      statement.clearParameters();
      .....
}  
{code}
From the code, If this option is set to 0, the JDBC query will not be executed.

 

From documents,  the max retry times if lookup database failed. [1]

When set to 0, there is a query, but no retry.

 

So,the code of 'lookup.max-retries' option should be:
{code:java}
for (int retry = 0; retry <= maxRetryTimes; retry++) {
      statement.clearParameters();
      .....
}  
{code}
 

 

[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#lookup-max-retries",,caozhen1937,jark,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 14:09:39 UTC 2020,,,,,,,,,,"0|z0jrig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/20 16:03;jark;I think you are right. ;;;","27/Oct/20 14:09;jark;Fixed in master: bca8e472f943f59c0b226734a7a7c889ff4ba321;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java package name error in docs,FLINK-19676,13335752,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leiqiang,leiqiang,leiqiang,16/Oct/20 10:41,26/Oct/20 12:00,13/Jul/23 08:12,26/Oct/20 12:00,,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"_fix java package name error in types.md and types.zh.md . java Map in package java.util not in java.lang_

 

_such as:_

_!image-2020-10-16-19-12-32-613.png|width=759,height=76!_",,aljoscha,leiqiang,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/20 11:12;leiqiang;image-2020-10-16-19-12-32-613.png;https://issues.apache.org/jira/secure/attachment/13013668/image-2020-10-16-19-12-32-613.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 12:00:36 UTC 2020,,,,,,,,,,"0|z0jr8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/20 11:07;leiqiang;hi, [~rmetzger]. can you assign this issues to me?;;;","16/Oct/20 11:50;rmetzger;I assigned you, but for the future, it is okay if you just open a hotfix pull request without a jira ticket for such typo fixes.;;;","26/Oct/20 12:00;aljoscha;master: aee0e7050d562c2fd83b4c93071289225cc2bf1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The plan of is incorrect when Calc contains WHERE clause, composite fields access and Python UDF at the same time ",FLINK-19675,13335747,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,16/Oct/20 10:17,23/Oct/20 06:46,13/Jul/23 08:12,23/Oct/20 06:46,1.10.1,1.11.0,,,,1.10.3,1.11.3,1.12.0,API / Python,,,,,0,pull-request-available,,,,,"For the following job:
{code}
SELECT a, pyFunc1(b, d._1) FROM MyTable WHERE a + 1 > 0
{code}

The plan is as following:
{code}
FlinkLogicalCalc(select=[a, pyFunc1(b, f0) AS EXPR$1])
+- FlinkLogicalCalc(select=[a, b, d._1 AS f0])
 +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])
{code}
It's incorrect as the where condition is missing.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 23 06:46:17 UTC 2020,,,,,,,,,,"0|z0jr7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 06:46;dian.fu;Merged to 
- master via ae4080c409f9e548cf78dce0d1b8bf60d0c901b3
- release-1.11 via 525485c71a00fdda3cd2f3db5a4b2f52859b76a3
- release-1.10 via 85bc8362d98c66ace7b09a3de71ed5334c442db0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PipelinedRegionSchedulingStrategy#init ResultPartitionType blocking check should use isBlocking method,FLINK-19669,13335695,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,16/Oct/20 02:25,18/Oct/20 13:55,13/Jul/23 08:12,18/Oct/20 13:55,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,PipelinedRegionSchedulingStrategy#init ResultPartitionType blocking check should use isBlocking method instead of checking of the enum type.,,xuannan,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 18 13:55:45 UTC 2020,,,,,,,,,,"0|z0jqvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/20 02:58;zhuzh;Thanks for reporting this issue [~xuannan]. You are right that it should invoke {{isBlocking()}}.
I have assigned you the ticket. Go ahead to submit a fix.;;;","18/Oct/20 13:55;zhuzh;Fixed via 8133c0662c94d1d7dc225166130e4838b9361e04;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AlternatingCheckpointBarrierHandlerTest.testMetricsAlternation unstable,FLINK-19665,13335653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,rmetzger,rmetzger,15/Oct/20 18:10,06/Nov/20 07:08,13/Jul/23 08:12,06/Nov/20 07:08,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,,,,,0,test-stability,,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8479&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374
{code}
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.115 s <<< FAILURE! - in org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandlerTest
[ERROR] testMetricsAlternation(org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandlerTest)  Time elapsed: 0.017 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a value less than or equal to <74001L>
     but: <137102L> was greater than <74001L>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandlerTest.assertMetrics(AlternatingCheckpointBarrierHandlerTest.java:212)
	at org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandlerTest.testMetricsAlternation(AlternatingCheckpointBarrierHandlerTest.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
{code}",,kezhuw,klion26,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 07:08:25 UTC 2020,,,,,,,,,,"0|z0jqmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/20 14:09;kezhuw;[~pnowojski] [~rmetzger] Seems that it has been fixed in commit  [8ffe245|https://github.com/apache/flink/commit/8ffe24593931f58374afa6967b7ec9067834a0b0#diff-5b1b1fead0493f9a0cbaf8e58737f18fd0118093a4c5fa4ee9034fdf68ba8126R212] and [pr-13667|https://github.com/apache/flink/pull/13667].;;;","06/Nov/20 07:08;pnowojski;Yes, that's right. Thanks for pointing this out [~kezhuw];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Local recovery and sticky scheduling end-to-end test hangs with ""Expected to find info here.""",FLINK-19658,13335617,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,15/Oct/20 13:41,16/Oct/20 10:58,13/Jul/23 08:12,16/Oct/20 10:27,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"The reason for all these e2e test hangs recently seems to be the Local recovery and sticky scheduling end-to-end test.

It is in a restart loop with this error:
{code}
020-10-15T13:01:42.4079891Z 2020-10-15 12:54:06,099 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Unnamed (1/4) (78a56f7797be1d41b0b1b31a75bd90e1_20ba6b65f97481d5570070de90e4e791_0_1) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@65b70d8d.
2020-10-15T13:01:42.4080637Z java.lang.NullPointerException: Expected to find info here.
2020-10-15T13:01:42.4081365Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4082067Z 	at org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob$StateCreatingFlatMap.initializeState(StickyAllocationAndLocalRecoveryTestJob.java:343) ~[?:?]
2020-10-15T13:01:42.4083125Z 	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:185) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4103820Z 	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:167) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4104926Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4106020Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:107) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4107084Z 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:262) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4108295Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4109432Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:505) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4110458Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4111428Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4112328Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4113167Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4113962Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4114434Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16620,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 10:27:32 UTC 2020,,,,,,,,,,"0|z0jqeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/20 13:41;rmetzger;I disabled the test in a hotfix: https://github.com/apache/flink/commit/8aeb3ec0d2f021787e39c2887805c3528c067a76;;;","15/Oct/20 14:19;rmetzger;Looks like FLINK-16620 introduced a change to the taskName, which this e2e test depends on.;;;","15/Oct/20 14:40;rmetzger;Testing a fix already.;;;","16/Oct/20 10:27;rmetzger;Resolved in https://github.com/apache/flink/commit/487d8377dbd3dedee12c9d93801c1b6d05c28722;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase.checkForProhibitedLogContents: netty failed with java.io.IOException: Broken pipe,FLINK-19657,13335614,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,15/Oct/20 13:28,22/Oct/20 11:50,13/Jul/23 08:12,22/Oct/20 11:50,1.12.0,,,,,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7666&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf
{code}
2020-10-15T10:41:39.3168991Z [ERROR] Failures: 
2020-10-15T10:41:39.3172085Z [ERROR]   YARNSessionCapacitySchedulerITCase.checkForProhibitedLogContents:650->YarnTestBase.ensureNoProhibitedStringInLogFiles:479 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-logDir-nm-1_0/application_1602757819968_0002/container_1602757819968_0002_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2020-10-15T10:41:39.3173934Z [
2020-10-15T10:41:39.3175238Z 2020-10-15 10:31:08,883 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 5a24db6b51d7c697fe30e49aa1a8412c from job leader monitoring.
2020-10-15T10:41:39.3176858Z 2020-10-15 10:31:08,885 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 5a24db6b51d7c697fe30e49aa1a8412c.
2020-10-15T10:41:39.3178614Z 2020-10-15 10:31:09,531 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                 [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2020-10-15T10:41:39.3199037Z 2020-10-15 10:31:09,538 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3202316Z 2020-10-15 10:31:09,573 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-io-fa9b2b85-fbe7-429c-85a9-6fcfb4811b01
2020-10-15T10:41:39.3204499Z 2020-10-15 10:31:09,577 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2020-10-15T10:41:39.3240288Z 2020-10-15 10:31:09,579 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-netty-shuffle-7e95809e-e862-45b1-892e-09835297c82d
2020-10-15T10:41:39.3244086Z 2020-10-15 10:31:09,594 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-dist-cache-ed288b32-5c40-48b9-9bcb-7186f5a7bfc2
2020-10-15T10:41:39.3246636Z 2020-10-15 10:31:09,596 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3248653Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3250746Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3253551Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3255688Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3267975Z ]
2020-10-15T10:41:39.3270637Z [ERROR]   YARNSessionCapacitySchedulerITCase.checkForProhibitedLogContents:650->YarnTestBase.ensureNoProhibitedStringInLogFiles:479 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-logDir-nm-1_0/application_1602757819968_0002/container_1602757819968_0002_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2020-10-15T10:41:39.3272364Z [
2020-10-15T10:41:39.3273848Z 2020-10-15 10:31:08,883 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 5a24db6b51d7c697fe30e49aa1a8412c from job leader monitoring.
2020-10-15T10:41:39.3275694Z 2020-10-15 10:31:08,885 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 5a24db6b51d7c697fe30e49aa1a8412c.
2020-10-15T10:41:39.3277365Z 2020-10-15 10:31:09,531 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                 [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2020-10-15T10:41:39.3278813Z 2020-10-15 10:31:09,538 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3281136Z 2020-10-15 10:31:09,573 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-io-fa9b2b85-fbe7-429c-85a9-6fcfb4811b01
2020-10-15T10:41:39.3283728Z 2020-10-15 10:31:09,577 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2020-10-15T10:41:39.3285976Z 2020-10-15 10:31:09,579 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-netty-shuffle-7e95809e-e862-45b1-892e-09835297c82d
2020-10-15T10:41:39.3288937Z 2020-10-15 10:31:09,594 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-dist-cache-ed288b32-5c40-48b9-9bcb-7186f5a7bfc2
2020-10-15T10:41:39.3291038Z 2020-10-15 10:31:09,596 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3293240Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3295639Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3297824Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3299775Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3300726Z ]
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 11:50:45 UTC 2020,,,,,,,,,,"0|z0jqds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/20 12:51;rmetzger;The offending log messages are coming from the TaskManager:
{code}
2020-10-15 10:31:09,531 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                 [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2020-10-15 10:31:09,538 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2020-10-15 10:31:09,573 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-io-fa9b2b85-fbe7-429c-85a9-6fcfb4811b01
2020-10-15 10:31:09,577 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2020-10-15 10:31:09,579 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-netty-shuffle-7e95809e-e862-45b1-892e-09835297c82d
2020-10-15 10:31:09,594 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-dist-cache-ed288b32-5c40-48b9-9bcb-7186f5a7bfc2
2020-10-15 10:31:09,596 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
{code}
... which loses connection to the JobManager, which is faster in shutting down ...

{code}
2020-10-15 10:31:09,459 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:33241
2020-10-15 10:31:09,460 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2020-10-15 10:31:09,466 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2020-10-15 10:31:09,502 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2020-10-15 10:31:09,502 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2020-10-15 10:31:09,540 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2020-10-15 10:31:09,541 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2020-10-15 10:31:09,635 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2020-10-15 10:31:09,646 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2020-10-15 10:31:09,713 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2020-10-15 10:31:09,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Terminating cluster entrypoint process YarnJobClusterEntrypoint with exit code 0.
{code}

I will open a PR to whitelist this log string.;;;","22/Oct/20 11:50;rmetzger;Resolved in https://github.com/apache/flink/commit/e5192bf0b65dd9586e442dbbe4ee58f5c6779839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when using blink planner and TemporalTableFunction after setting IdleStateRetentionTime ,FLINK-19655,13335591,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,seunjjs,seunjjs,seunjjs,15/Oct/20 11:01,22/Oct/20 03:18,13/Jul/23 08:12,22/Oct/20 03:18,1.10.0,1.11.0,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"My Code here:

{code:java}
EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, bsSettings);
tableEnv.getConfig().setIdleStateRetentionTime(Time.seconds(60), Time.seconds(600));

final Table table = tableEnv.from(""tableName"");
final TableFunction<?> function = table.createTemporalTableFunction(
                    temporalTableEntry.getTimeAttribute(),
                    String.join("","", temporalTableEntry.getPrimaryKeyFields()));
tableEnv.registerFunction(temporalTableEntry.getName(), function);
{code}



And NPE throwed when I executed my program.
{code:java}
java.lang.NullPointerException
	at org.apache.flink.table.runtime.operators.join.temporal.BaseTwoInputStreamOperatorWithStateRetention.registerProcessingCleanupTimer(BaseTwoInputStreamOperatorWithStateRetention.java:109)
	at org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperator.processElement2(TemporalProcessTimeJoinOperator.java:98)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processRecord2(StreamTwoInputProcessor.java:145)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.lambda$new$1(StreamTwoInputProcessor.java:107)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessor.java:362)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:185)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:748)
{code}
    
And When I changed to useOldPlanner, it worked fine.And when I debuged the code ,I found BaseTwoInputStreamOperatorWithStateRetention#open did not be executed.
Here is BaseTwoInputStreamOperatorWithStateRetention#open code.
{code:java}
public void open() throws Exception {
		initializeTimerService();

		if (stateCleaningEnabled) {
			ValueStateDescriptor<Long> cleanupStateDescriptor =
				new ValueStateDescriptor<>(CLEANUP_TIMESTAMP, Types.LONG);
			latestRegisteredCleanupTimer = getRuntimeContext().getState(cleanupStateDescriptor);
		}
	}
{code}
Here is TemporalProcessTimeJoinOperator#open code.
{code:java}
public void open() throws Exception {
		this.joinCondition = generatedJoinCondition.newInstance(getRuntimeContext().getUserCodeClassLoader());
		FunctionUtils.setFunctionRuntimeContext(joinCondition, getRuntimeContext());
		FunctionUtils.openFunction(joinCondition, new Configuration());

		ValueStateDescriptor<BaseRow> rightStateDesc = new ValueStateDescriptor<>(""right"", rightType);
		this.rightState = getRuntimeContext().getState(rightStateDesc);
		this.collector = new TimestampedCollector<>(output);
		this.outRow = new JoinedRow();
		// consider watermark from left stream only.
		super.processWatermark2(Watermark.MAX_WATERMARK);
	}
{code}
I compared the code with oldplaner(TemporalProcessTimeJoin#open).May be TemporalProcessTimeJoinOperator#open should add super.open()?
Here is TemporalProcessTimeJoin#open code.
{code:scala}
override def open(): Unit = {
    LOG.debug(s""Compiling FlatJoinFunction: $genJoinFuncName \n\n Code:\n$genJoinFuncCode"")
    val clazz = compile(
      getRuntimeContext.getUserCodeClassLoader,
      genJoinFuncName,
      genJoinFuncCode)

    LOG.debug(""Instantiating FlatJoinFunction."")
    joinFunction = clazz.newInstance()
    FunctionUtils.setFunctionRuntimeContext(joinFunction, getRuntimeContext)
    FunctionUtils.openFunction(joinFunction, new Configuration())

    val rightStateDescriptor = new ValueStateDescriptor[Row](""right"", rightType)
    rightState = getRuntimeContext.getState(rightStateDescriptor)

    collector = new TimestampedCollector[CRow](output)
    cRowWrapper = new CRowWrappingCollector()
    cRowWrapper.out = collector

    super.open()
  }
{code}",,jark,leonard,libenchao,nicholasjiang,seunjjs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 03:18:24 UTC 2020,,,,,,,,,,"0|z0jq8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/20 02:27;libenchao;[~seunjjs] Thanks for reporting this, would you like to raise a PR to fix it?;;;","16/Oct/20 03:33;nicholasjiang;[~seunjjs], I agree with your point that super.open should be add in TemporalProcessTimeJoinOperator.open(). TemporalProcessTimeJoinOperator should also initialize timer service for processing time.
cc [~jark];;;","16/Oct/20 10:41;seunjjs;[~libenchao] OK,I will try it tomorrow.;;;","18/Oct/20 07:25;seunjjs;[~nicholasjiang] [~libenchao] Hi, I have raised a PR [13675|https://github.com/apache/flink/pull/13675], Please review it,thanks. 
BTW,Run all tests of flink project in local(Windows10 Laptop) is difficult, I found these problems:
- 1. The path of Flink Source Code should not have whitespace
- 2. On Win 10,  the code needs to be put on disk C(FileUtilsTest#testCompressionOnRelativePath use empty path to get root loc which is code loc)
- 3. AbstractRecoverableWriterTest#testCloseWithNoData always throw Exception: xxx/part-0: 另一个程序正在使用此文件，进程无法访问。

At last, I gave up executing all tests but only flink-table-runtime-blink tests.:(
;;;","19/Oct/20 01:40;leonard;[~seunjjs] I reviewed the PR, you can have a look. ^_^;;;","22/Oct/20 03:18;jark;Fixed in master: daeda68edf3466a3f9347c25bdf866ef4f620396;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the execution time of PyFlink end-to-end tests,FLINK-19654,13335551,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hxbks2ks,dian.fu,dian.fu,15/Oct/20 05:13,22/Jun/21 08:41,13/Jul/23 08:12,22/Jun/21 08:40,1.12.0,,,,,1.12.0,,,API / Python,Tests,,,,0,auto-deprioritized-major,auto-unassigned,pull-request-available,,,"Thanks for the sharing from [~rmetzger], currently the test duration for PyFlink end-to-end test is as following:

||test case||average execution-time||maximum execution-time||
|PyFlink Table end-to-end test|1340s|1877s|
|PyFlink DataStream end-to-end test|387s|575s|
|Kubernetes PyFlink application test|606s|694s|

We need to investigate how to improve them to reduce the execution time.
",,dian.fu,hxbks2ks,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19549,,,,,"15/Oct/20 08:16;rmetzger;image (7).png;https://issues.apache.org/jira/secure/attachment/13013610/image+%287%29.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 08:40:38 UTC 2021,,,,,,,,,,"0|z0jpzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/20 08:16;rmetzger;Thanks for opening this ticket. I attached the source of your data to this ticket.;;;","15/Oct/20 08:29;rmetzger;Looking at the PyFlink table test: It seems that the test has 2 parts
1st: start a cluster and run a number of jobs
2nd: start a Kerberos secured? flink on yarn on docker cluster to run 2 jobs.

In one example, the 1st part took 10 minutes, the 2nd part 14 minutes.

I propose to remove the 2nd part from the pyflink table test: Running Flink on a Kerberos secured cluster is already tested separately.


Looking at the PyFlink DataStream test: It seems to do a lot of the same python setup procedures (which take roughly 2 minutes).
Can't we do the DataStream test in the same test as the PyFlink Table test? (because we have already python set up, and a cluster running?) 
;;;","15/Oct/20 09:47;hxbks2ks;[~dian.fu] Thanks a lot for opening this ticket. [~rmetzger] Thanks a lot for the suggestions.

1. I very much agree to put the test of pyflink datastream and table together, which can save the time of compiling/installing the sdist package and the time of starting and stopping the cluster.

2. pyflink on yarn test
I think we can't remove this test which will cover some pyflink special features of on yarn. But I will optimize this test to remove the part of downloading dependency, which is the main reason for the slow test.

3. I will change the third party req package of `scipy` to other small package.
The `scipy` package is about 25MB. I think this is the main reason for the slow test.;;;","15/Oct/20 10:24;rmetzger;Thanks. I assigned you to the ticket.
Can you do 1. 2. and 3. in separate pull requests, so that we quickly get the time savings?

2. Can't we run the python tests with the regular YARN tests, where we are have a cluster running already? (flink-yarn-tests)
Currently, the pyflink on yarn test is using a secured yarn cluster on docker, which is extremely slow to set up and launch.;;;","16/Oct/20 07:20;hxbks2ks;[~rmetzger]Thanks a lot for sharing. I have created a PR to merge pyflink table tests and datastream tests.  Regarding whether the test of pyflink on yarn can be moved to flink-yarn-tests, I still need to investigate this. I think we can first do what we know can be optimized;;;","16/Oct/20 07:37;dian.fu;- Put the tests of Pyflink DataStream and Table together:
merged to master via c83a3683470a9702e6a4e4827bdb48545e1c3838;;;","18/Oct/20 14:48;dian.fu;-  Update the dependency test from *scipy* to *pytest* to reduce the execution time:
merged to master via ce4efe6099aeac3139f81c1047abe8511aac59f4;;;","27/Oct/20 05:12;dian.fu;- Set the parallelism to 2 to reduce the execution time:
merged to master via 16ed892245fa0ccd0319597f26f0ec193d5021c8;;;","16/Apr/21 10:51;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:51;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","31/May/21 11:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Jun/21 10:44;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Jun/21 08:40;dian.fu;Just closing this ticket for now as we have already did some improvements and there is no known TODO items currently. We could continue to improve if necessary. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogITCase fails on azure,FLINK-19653,13335537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,gaoyunhaii,gaoyunhaii,15/Oct/20 03:54,24/Nov/20 02:54,13/Jul/23 08:12,24/Nov/20 02:54,1.12.0,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7628&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf


{code:java}


2020-10-14T17:28:27.3065932Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.396 s <<< FAILURE! - in org.apache.flink.table.catalog.hive.HiveCatalogITCase
2020-10-14T17:28:27.3066739Z [ERROR] org.apache.flink.table.catalog.hive.HiveCatalogITCase  Time elapsed: 10.396 s  <<< ERROR!
2020-10-14T17:28:27.3067248Z java.lang.IllegalStateException: Failed to create HiveServer :Failed to get metastore connection
2020-10-14T17:28:27.3067925Z 	at com.klarna.hiverunner.HiveServerContainer.init(HiveServerContainer.java:101)
2020-10-14T17:28:27.3068360Z 	at com.klarna.hiverunner.builder.HiveShellBase.start(HiveShellBase.java:165)
2020-10-14T17:28:27.3068886Z 	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.createHiveServerContainer(FlinkStandaloneHiveRunner.java:217)
2020-10-14T17:28:27.3069678Z 	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.access$600(FlinkStandaloneHiveRunner.java:92)
2020-10-14T17:28:27.3070290Z 	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner$2.before(FlinkStandaloneHiveRunner.java:131)
2020-10-14T17:28:27.3070763Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
2020-10-14T17:28:27.3071177Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-14T17:28:27.3071576Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-14T17:28:27.3071961Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-14T17:28:27.3072432Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-14T17:28:27.3072852Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-14T17:28:27.3073316Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-14T17:28:27.3073810Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-14T17:28:27.3074287Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-14T17:28:27.3074768Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-14T17:28:27.3075281Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-14T17:28:27.3075798Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-14T17:28:27.3076239Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-14T17:28:27.3076648Z Caused by: java.lang.RuntimeException: Failed to get metastore connection
2020-10-14T17:28:27.3077099Z 	at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:169)
2020-10-14T17:28:27.3077650Z 	at com.klarna.hiverunner.HiveServerContainer.init(HiveServerContainer.java:84)
2020-10-14T17:28:27.3077947Z 	... 17 more
2020-10-14T17:28:27.3078655Z Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
2020-10-14T17:28:27.3079236Z 	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:236)
2020-10-14T17:28:27.3079655Z 	at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:388)
2020-10-14T17:28:27.3080038Z 	at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:332)
2020-10-14T17:28:27.3080610Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:312)
2020-10-14T17:28:27.3081099Z 	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288)
2020-10-14T17:28:27.3081501Z 	at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:166)
2020-10-14T17:28:27.3081784Z 	... 18 more
2020-10-14T17:28:27.3082140Z Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
2020-10-14T17:28:27.3082720Z 	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1708)
2020-10-14T17:28:27.3083344Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
2020-10-14T17:28:27.3083870Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
2020-10-14T17:28:27.3084379Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
2020-10-14T17:28:27.3084864Z 	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3600)
2020-10-14T17:28:27.3085274Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3652)
2020-10-14T17:28:27.3085665Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3632)
2020-10-14T17:28:27.3086055Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3894)
2020-10-14T17:28:27.3086477Z 	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)
2020-10-14T17:28:27.3086898Z 	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)
2020-10-14T17:28:27.3087217Z 	... 23 more
2020-10-14T17:28:27.3087509Z Caused by: java.lang.reflect.InvocationTargetException
2020-10-14T17:28:27.3087952Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-10-14T17:28:27.3088380Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-10-14T17:28:27.3088902Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-10-14T17:28:27.3089370Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-10-14T17:28:27.3089792Z 	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706)
2020-10-14T17:28:27.3090116Z 	... 32 more
2020-10-14T17:28:27.3090692Z Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
2020-10-14T17:28:27.3091386Z 	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
2020-10-14T17:28:27.3091802Z 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:480)
2020-10-14T17:28:27.3092373Z 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:247)
2020-10-14T17:28:27.3092901Z 	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
2020-10-14T17:28:27.3093690Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-10-14T17:28:27.3094130Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-10-14T17:28:27.3094635Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-10-14T17:28:27.3095099Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-10-14T17:28:27.3095518Z 	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706)
2020-10-14T17:28:27.3096126Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
2020-10-14T17:28:27.3096637Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
2020-10-14T17:28:27.3097164Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
2020-10-14T17:28:27.3097739Z 	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3600)
2020-10-14T17:28:27.3098242Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3652)
2020-10-14T17:28:27.3098626Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3632)
2020-10-14T17:28:27.3099018Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3894)
2020-10-14T17:28:27.3099437Z 	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)
2020-10-14T17:28:27.3099855Z 	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)
2020-10-14T17:28:27.3100275Z 	at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:388)
2020-10-14T17:28:27.3100644Z 	at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:332)
2020-10-14T17:28:27.3101042Z 	at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:312)
2020-10-14T17:28:27.3101416Z 	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288)
2020-10-14T17:28:27.3101808Z 	at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:166)
2020-10-14T17:28:27.3102229Z 	at com.klarna.hiverunner.HiveServerContainer.init(HiveServerContainer.java:84)
2020-10-14T17:28:27.3102745Z 	at com.klarna.hiverunner.builder.HiveShellBase.start(HiveShellBase.java:165)
2020-10-14T17:28:27.3103253Z 	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.createHiveServerContainer(FlinkStandaloneHiveRunner.java:217)
2020-10-14T17:28:27.3103835Z 	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.access$600(FlinkStandaloneHiveRunner.java:92)
2020-10-14T17:28:27.3104389Z 	at org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner$2.before(FlinkStandaloneHiveRunner.java:131)
2020-10-14T17:28:27.3104851Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
2020-10-14T17:28:27.3105268Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-14T17:28:27.3105663Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-14T17:28:27.3106040Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-14T17:28:27.3106387Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-14T17:28:27.3106801Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-14T17:28:27.3107267Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-14T17:28:27.3107869Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-14T17:28:27.3108325Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-14T17:28:27.3108822Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-14T17:28:27.3109326Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-14T17:28:27.3109772Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-14T17:28:27.3110211Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-14T17:28:27.3110625Z Caused by: java.net.ConnectException: Connection refused (Connection refused)
2020-10-14T17:28:27.3110980Z 	at java.net.PlainSocketImpl.socketConnect(Native Method)
2020-10-14T17:28:27.3111340Z 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
2020-10-14T17:28:27.3111790Z 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
2020-10-14T17:28:27.3112560Z 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
2020-10-14T17:28:27.3112977Z 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
2020-10-14T17:28:27.3113301Z 	at java.net.Socket.connect(Socket.java:607)
2020-10-14T17:28:27.3113642Z 	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
2020-10-14T17:28:27.3113900Z 	... 40 more
2020-10-14T17:28:27.3114057Z )
2020-10-14T17:28:27.3114358Z 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:529)
2020-10-14T17:28:27.3114923Z 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:247)
2020-10-14T17:28:27.3115431Z 	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
2020-10-14T17:28:27.3115798Z 	... 37 more


{code}
",,dian.fu,gaoyunhaii,leonard,lirui,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 02:54:26 UTC 2020,,,,,,,,,,"0|z0jpwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/20 05:32;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9036&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=fc5181b0-e452-5c8f-68de-1097947f6483;;;","05/Nov/20 05:32;dian.fu;cc [~lirui];;;","05/Nov/20 08:57;lirui;The error message basically just tells us the HMS failed to start. We don't log the output of HMS (in order to keep logs terse), so we can't know what goes wrong exactly.

To make the tests more stable, perhaps we should use embedded metastore. While a standalone HMS is closer to real world use case, embedded mode seems good enough for test coverage. [~lzljs3620320] any thoughts?;;;","05/Nov/20 09:03;lzljs3620320;+1 to embedded metastore.

A standalone HMS looks not so stable, and I think embedded metastore is enough to cover. (Correct me if there are somethings can not be covered);;;","05/Nov/20 10:48;lirui;OK, I'll do some investigation to migrate to embedded metastore for testing. Maybe not all tests can run with embedded mode, e.g. ACID tables, but at least we can only launch standalone HMS for fewer test cases.;;;","07/Nov/20 05:55;dian.fu;HiveTableSourceITCase failed with similar exception:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9221&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","16/Nov/20 19:28;rmetzger;[~lirui] are you going to open a pull request for the 1.12.0 release, or are we going to address this at a later point?;;;","17/Nov/20 02:37;lirui;[~rmetzger] Yeah I'll open a PR for 1.12;;;","17/Nov/20 07:33;rmetzger;Thanks a lot!;;;","24/Nov/20 02:54;lzljs3620320;master (1.12): e9b05e723fb02d9a6dae607ef28244eca6e9edc8

Feel free to re-open if re-produce.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In Flink's module flink-sql-parser/sqlCreateTable/unparse, the  tables without columns are not supported.",FLINK-19649,13335522,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gsralex,gsralex,gsralex,15/Oct/20 03:11,20/Oct/20 03:45,13/Jul/23 08:12,20/Oct/20 03:45,1.11.2,,,,,1.12.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"In Flink's module flink-sql-parser, the sqlCreateTable.class unparse method
 The create table statement does not match the new rules of the print connector.

The print connector supports this:
{code:java}
CREATE TABLE table_print WITH ('connector' = 'print') LIKE table (EXCLUDING ALL){code}
When a print table is created without any columns, the parentheses must be omitted.

However, the unparse generated statements in SqlCreateTable contain parentheses. This will affect the SqlCreateTable.toString() ,SqlCreateTable.toSqlString() methods.
{code:java}
CREATE TABLE table_print() WITH ('connector' = 'print') LIKE table (EXCLUDING ALL)  
{code}
Might need to add the following to the unparse method for columnList.size > 0.

 
{code:java}
if (columnList.size() > 0) {
   SqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.create(""sds""), ""("", "")"");
   for (SqlNode column : columnList) {
      printIndent(writer);
      if (column instanceof SqlBasicCall) {
         SqlCall call = (SqlCall) column;
         SqlCall newCall = call.getOperator().createCall(
               SqlParserPos.ZERO,
               call.operand(1),
               call.operand(0));
         newCall.unparse(writer, leftPrec, rightPrec);
      } else {
         column.unparse(writer, leftPrec, rightPrec);
      }
   }
   if (tableConstraints.size() > 0) {
      for (SqlTableConstraint constraint : tableConstraints) {
         printIndent(writer);
         constraint.unparse(writer, leftPrec, rightPrec);
      }
   }
   if (watermark != null) {
      printIndent(writer);
      watermark.unparse(writer, leftPrec, rightPrec);
   }

   writer.newlineAndIndent();
   writer.endList(frame);
}
{code}
 

My English isn't very good. I hope I've made myself clear. Thanks.

 

 ",,gsralex,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 03:45:48 UTC 2020,,,,,,,,,,"0|z0jptc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/20 06:37;ykt836;Good catch, feel free to open a pull request [~gsralex];;;","15/Oct/20 07:42;gsralex;Hi Kurt, Thanks for the reply, I'll try submitting a pull request.;;;","15/Oct/20 12:36;gsralex;Hi Kurt, I tried to submit a pull request, this is my first time submitting a pull request to Flink. I'm not sure what I did is right [~ykt836]

https://github.com/apache/flink/pull/13651;;;","20/Oct/20 03:45;ykt836;fixed in 2a647735e9545634ab5829d48086d0ac2dd0dfda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShuffleCompressionITCase.testDataCompressionForBlockingShuffle is instable,FLINK-19645,13335515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,dian.fu,dian.fu,15/Oct/20 02:15,30/Nov/21 20:38,13/Jul/23 08:12,05/Nov/20 16:54,1.12.0,,,,,1.12.0,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7631&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56

{code}
2020-10-14T21:46:25.3133019Z [ERROR] testDataCompressionForBlockingShuffle[useBroadcastPartitioner = true](org.apache.flink.test.runtime.ShuffleCompressionITCase)  Time elapsed: 34.949 s  <<< ERROR!
2020-10-14T21:46:25.3133688Z org.apache.flink.util.FlinkException: Could not close resource.
2020-10-14T21:46:25.3134146Z 	at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:42)
2020-10-14T21:46:25.3134716Z 	at org.apache.flink.test.runtime.ShuffleCompressionITCase.executeTest(ShuffleCompressionITCase.java:114)
2020-10-14T21:46:25.3135396Z 	at org.apache.flink.test.runtime.ShuffleCompressionITCase.testDataCompressionForBlockingShuffle(ShuffleCompressionITCase.java:89)
2020-10-14T21:46:25.3135941Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-14T21:46:25.3136410Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-14T21:46:25.3136959Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-14T21:46:25.3137435Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-14T21:46:25.3137915Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-14T21:46:25.3138459Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-14T21:46:25.3138987Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-14T21:46:25.3139524Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-14T21:46:25.3140017Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-14T21:46:25.3140495Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-14T21:46:25.3141037Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-14T21:46:25.3141520Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-14T21:46:25.3142213Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-14T21:46:25.3142672Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-14T21:46:25.3143134Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-14T21:46:25.3143578Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-14T21:46:25.3144026Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-14T21:46:25.3144437Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-10-14T21:46:25.3144815Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-10-14T21:46:25.3147907Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-14T21:46:25.3148415Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-14T21:46:25.3149058Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-14T21:46:25.3149526Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-14T21:46:25.3149993Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-14T21:46:25.3150429Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-14T21:46:25.3150918Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-14T21:46:25.3151469Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-14T21:46:25.3152045Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-14T21:46:25.3152599Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-14T21:46:25.3153163Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-14T21:46:25.3155116Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-14T21:46:25.3157282Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-14T21:46:25.3159466Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-14T21:46:25.3167068Z Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.deregisterApplication(org.apache.flink.runtime.clusterframework.ApplicationStatus,java.lang.String) timed out.
2020-10-14T21:46:25.3173751Z 	at org.apache.flink.runtime.rpc.akka.$Proxy28.deregisterApplication(Unknown Source)
2020-10-14T21:46:25.3371159Z 	at org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.deregisterApplication(DispatcherResourceManagerComponent.java:124)
2020-10-14T21:46:25.3372499Z 	at org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.lambda$deregisterApplicationAndClose$0(DispatcherResourceManagerComponent.java:111)
2020-10-14T21:46:25.3373451Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$composeAfterwards$18(FutureUtils.java:598)
2020-10-14T21:46:25.3374185Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-14T21:46:25.3374864Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-10-14T21:46:25.3375645Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-10-14T21:46:25.3376288Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-10-14T21:46:25.3376972Z 	at org.apache.flink.runtime.rest.RestServerEndpoint.lambda$closeAsync$1(RestServerEndpoint.java:306)
2020-10-14T21:46:25.3377646Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-14T21:46:25.3378325Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-10-14T21:46:25.3379001Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-10-14T21:46:25.3379643Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-10-14T21:46:25.3380545Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$null$17(FutureUtils.java:607)
2020-10-14T21:46:25.3381211Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-14T21:46:25.3381884Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-10-14T21:46:25.3382535Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-10-14T21:46:25.3383176Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-10-14T21:46:25.3383872Z 	at org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.lambda$null$0(DispatcherRestEndpoint.java:153)
2020-10-14T21:46:25.3384683Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-14T21:46:25.3385494Z 	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792)
2020-10-14T21:46:25.3386252Z 	at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153)
2020-10-14T21:46:25.3386984Z 	at org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.lambda$shutDownInternal$1(DispatcherRestEndpoint.java:145)
2020-10-14T21:46:25.3387713Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-14T21:46:25.3388371Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-10-14T21:46:25.3389040Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-10-14T21:46:25.3389677Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-10-14T21:46:25.3390339Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$16(FutureUtils.java:573)
2020-10-14T21:46:25.3391041Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-14T21:46:25.3391789Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-10-14T21:46:25.3392449Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-10-14T21:46:25.3392968Z 	at java.lang.Thread.run(Thread.java:748)
2020-10-14T21:46:25.3394844Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/resourcemanager_2#-1642668643]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-10-14T21:46:25.3396483Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-10-14T21:46:25.3396982Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-10-14T21:46:25.3397478Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-10-14T21:46:25.3397950Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-10-14T21:46:25.3398439Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-10-14T21:46:25.3398967Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-10-14T21:46:25.3399483Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-10-14T21:46:25.3400054Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-10-14T21:46:25.3400662Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-10-14T21:46:25.3401274Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-10-14T21:46:25.3401859Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-10-14T21:46:25.3402237Z 	... 1 more
{code}",,aljoscha,dian.fu,guoyangze,kevin.cyj,kezhuw,pnowojski,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 16:54:58 UTC 2020,,,,,,,,,,"0|z0jprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/20 03:23;kevin.cyj;From the stack, we can see that the root cause is Akka RPC timeout. Maybe we can increase the timeout or just ignore it if the failure is rare.;;;","15/Oct/20 13:11;pnowojski;I've seen this test fail at least a couple of times, but I had no idea why is it failing, as the test was annoyingly swallowing the original exception (a couple of days ago I've added [a hotfix to reveal this exception|https://github.com/apache/flink/commit/bd3ae7bd3f933735dfb474eb70be80859050af34]).

I wouldn't be surprised that this  {{FlinkException: Could not close resource.}} is actually hiding the true exception [~kevin.cyj];;;","19/Oct/20 07:28;guoyangze;another instance: https://dev.azure.com/guoyangze/Flink/_build/results?buildId=193&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a;;;","19/Oct/20 08:35;kevin.cyj;Now that the failure reproduces frequently, I'd like to prepare a PR to fix it. As verification, I will run the test multiple times (> 100) both locally and on Azure.

[~pnowojski] Could you please assign this ticket to me?;;;","30/Oct/20 01:22;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8626&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","31/Oct/20 02:12;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8698&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","02/Nov/20 05:32;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8736&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","02/Nov/20 05:37;dian.fu;[~kevin.cyj] Any progress on this issue? It seems that this test is continuously failing in the recent nightly tests.;;;","02/Nov/20 06:15;rmetzger;Upgrading to Blocker;;;","02/Nov/20 11:40;aljoscha;Another one: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=374&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a&l=4179;;;","04/Nov/20 01:27;kevin.cyj;I will submit a PR soon.;;;","05/Nov/20 08:11;kevin.cyj;After some investigation, I didn't find any potential bug. The akka rpc timeout problem were caused by different rpc calls. I think the reason is purely because this test case is bit heavy. I tried to increase the akka timeout to 60s. The good news is that the problem never reproduce after running the test case over 2000 times on azure pipeline. If I roll back the timeout, the timeout issue can be reproduced.

I have opened a PR which just increases the akka timeout. Can anyone kindly review it? Let's see if it solves the problem.;;;","05/Nov/20 16:54;trohrmann;Fixed via 28110ed905c07df4ce0bdab40d34c84433ccac1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseConnectorITCase.testTableSourceSinkWithDDL is unstable with a result mismatch,FLINK-19635,13335349,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,rmetzger,rmetzger,14/Oct/20 06:21,28/May/21 07:08,13/Jul/23 08:12,15/Jan/21 02:12,1.12.0,,,,,1.12.0,1.13.0,,Connectors / HBase,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7562&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20
{code}
2020-10-14T04:35:36.9268975Z testTableSourceSinkWithDDL[planner = BLINK_PLANNER, legacy = false](org.apache.flink.connector.hbase2.HBaseConnectorITCase)  Time elapsed: 3.131 sec  <<< FAILURE!
2020-10-14T04:35:36.9276246Z java.lang.AssertionError: expected:<[1,10,Hello-1,100,1.01,false,Welt-1,2019-08-18T19:00,2019-08-18,19:00,12345678.0001, 2,20,Hello-2,200,2.02,true,Welt-2,2019-08-18T19:01,2019-08-18,19:01,12345678.0002, 3,30,Hello-3,300,3.03,false,Welt-3,2019-08-18T19:02,2019-08-18,19:02,12345678.0003, 4,40,null,400,4.04,true,Welt-4,2019-08-18T19:03,2019-08-18,19:03,12345678.0004, 5,50,Hello-5,500,5.05,false,Welt-5,2019-08-19T19:10,2019-08-19,19:10,12345678.0005, 6,60,Hello-6,600,6.06,true,Welt-6,2019-08-19T19:20,2019-08-19,19:20,12345678.0006, 7,70,Hello-7,700,7.07,false,Welt-7,2019-08-19T19:30,2019-08-19,19:30,12345678.0007, 8,80,null,800,8.08,true,Welt-8,2019-08-19T19:40,2019-08-19,19:40,12345678.0008]> but was:<[1,10,Hello-1,100,1.01,false,Welt-1,2019-08-18T19:00,2019-08-18,19:00,12345678.0001, 2,20,Hello-2,200,2.02,true,Welt-2,2019-08-18T19:01,2019-08-18,19:01,12345678.0002, 3,30,Hello-3,300,3.03,false,Welt-3,2019-08-18T19:02,2019-08-18,19:02,12345678.0003]>
2020-10-14T04:35:36.9281340Z 	at org.junit.Assert.fail(Assert.java:88)
2020-10-14T04:35:36.9282023Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-10-14T04:35:36.9328385Z 	at org.junit.Assert.assertEquals(Assert.java:118)
2020-10-14T04:35:36.9338939Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-10-14T04:35:36.9339880Z 	at org.apache.flink.connector.hbase2.HBaseConnectorITCase.testTableSourceSinkWithDDL(HBaseConnectorITCase.java:449)
2020-10-14T04:35:36.9341003Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}",,hxbks2ks,jark,kezhuw,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 02:12:30 UTC 2021,,,,,,,,,,"0|z0joqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/20 17:08;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7743&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","20/Oct/20 06:55;rmetzger;[~Leonard Xu] can you take a look at this test failure?;;;","20/Oct/20 06:55;rmetzger;This could be related: https://issues.apache.org/jira/browse/FLINK-19615;;;","20/Oct/20 07:08;leonard;Sure, I'll have a look;;;","09/Nov/20 07:20;leonard;I checked the failed tests in this ticket and   https://issues.apache.org/jira/browse/FLINK-19615,
both them happened in *hbase2* connector，thus I doubt the unstable tests may relate to hbase2 testing cluster(HBaseTestingClusterAutoStarter).

[~mgergely] Do you have any insight?  ;;;","17/Nov/20 14:35;leonard;
04:35:14,975 [hbase-upsert-sink-flusher-thread-1] ERROR org.apache.hadoop.hbase.client.AsyncProcess [] - Failed to get region location org.apache.hadoop.hbase.DoNotRetryIOException: hconnection-0x932344b closed at org.apache.hadoop.hbase.client.ConnectionImplementation.checkClosed(ConnectionImplementation.java:591) ~[hbase-client-2.2.3.jar:2.2.3] at org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:776) ~[hbase-client-2.2.3.jar:2.2.3] at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:273) ~[hbase-client-2.2.3.jar:2.2.3] at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:228) ~[hbase-client-2.2.3.jar:2.2.3] at org.apache.hadoop.hbase.client.BufferedMutatorImpl.doFlush(BufferedMutatorImpl.java:303) ~[hbase-client-2.2.3.jar:2.2.3] at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:280) ~[hbase-client-2.2.3.jar:2.2.3] at org.apache.flink.connector.hbase.sink.HBaseSinkFunction.flush(HBaseSinkFunction.java:189) ~[flink-connector-hbase-base_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.connector.hbase.sink.HBaseSinkFunction.lambda$open$0(HBaseSinkFunction.java:134) ~[flink-connector-hbase-base_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_242] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_242] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_242] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]04:35:14,975 [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: TableSourceScan(table=[[default_catalog, default_database, testTable1]], fields=[rowkey, family1, family2, family3, family4]) -> Sink: Sink(table=[default_catalog.default_database.testTable3], fields=[rowkey, family1, family2, family3, family4]) (1/32) 65f7b61a1994ee9b341a2a7f5c6cca3a_cbc357ccb763df2852fee8c4fc7d55f2_0_0.04:35:14,976 [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: TableSourceScan(table=[[default_catalog, default_database, testTable1]], fields=[rowkey, family1, family2, family3, family4]) -> Sink: Sink(table=[default_catalog.default_database.testTable3], fields=[rowkey, family1, family2, family3, family4]) (1/32) (65f7b61a1994ee9b341a2a7f5c6cca3a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FINISHED.04:35:14,986 [RS-EventLoopGroup-3-54] INFO SecurityLogger.org.apache.hadoop.hbase.Server 
 [] - Connection from 192.168.192.2:49496, version=2.2.3, sasl=false, ugi=agent01_azpcontainer (auth:SIMPLE), service=ClientService04:35:15,016 [ htable-pool146-t1] WARN org.apache.hadoop.hbase.client.AsyncRequestFutureImpl [] - id=175, table=testTable3, attempt=1/16, failureCount=1ops, last exception=org.apache.hadoop.hbase.DoNotRetryIOException: hconnection-0x932344b closed on 295934f72d17,36709,1602650080067, tracking started Wed Oct 14 04:35:14 UTC 2020; *NOT retrying, failed=1 – final attempt!*

 

from the failed log, I think the mismatch result is because one of the sink task failed to scan the data, we set the *hbase.client.retries.number* to ""*1*"" which means do not retry, but the default value in hbase-1.4 is ""*35*"" and in hbase-2.2 is ""*15*"", so we should at least allow the hbase client do retry.

 ;;;","18/Nov/20 08:48;jark;Fixed in master (1.12.0): 7ae2be5167b4b143d3aaef2b9bc58faadc2d3a26;;;","30/Dec/20 04:41;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11476&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20]

The failed case happened again. Do we reopen this issue?;;;","30/Dec/20 06:54;leonard;oops, let me reopen this;;;","07/Jan/21 10:23;hxbks2ks;another instance

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11714&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20];;;","15/Jan/21 02:12;jark;Fixed in master: 8a8e769228176d90678f58eac66bef63ec82fc7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comments of DecodingFormatFactory is not clear,FLINK-19631,13335335,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,lzljs3620320,lzljs3620320,14/Oct/20 03:23,15/Oct/20 07:43,13/Jul/23 08:12,15/Oct/20 07:43,,,,,,1.12.0,,,Table SQL / API,,,,,0,pull-request-available,starter,,,,"e.g. from \{@code key.format.ignore-errors} to \{@code format.ignore-errors}

Should be ""from \{@code format.ignore-errors} to \{@code ignore-errors}""",,jark,lzljs3620320,twalthr,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 07:43:25 UTC 2020,,,,,,,,,,"0|z0jons:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/20 08:37;ZhuShang;Hi [~lzljs3620320],I want to take this,can you assign it to me?;;;","14/Oct/20 08:43;lzljs3620320;Assigned to you~ [~ZhuShang];;;","15/Oct/20 07:43;twalthr;Fixed in 1.12.0: 9dee3489298d163cfb66fa85a388686d49b7675d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Fix NullPointException when deserializing map field with null value for Avro format,FLINK-19629,13335333,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tinny,tinny,tinny,14/Oct/20 03:18,22/Oct/20 12:31,13/Jul/23 08:12,22/Oct/20 12:31,1.11.2,,,,,1.11.3,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"create table tableA (
  name  STRING,
  hobly MAP<STRING, STRING>,
  phone STRING
) with (
  'connector' = 'kafka-0.11',
  'topic' = 'ShizcTest',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'ShizcTest',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'avro'
);
if hobly have an null value like this:

{""name"": ""shizc"", ""hobly"": {""key1"":null}, ""phone"": ""1104564""}

cause an NullPointException:

{code:java}
java.io.IOException: Failed to deserialize Avro record.
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:150)
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:75)
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:81)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56)
	at org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.runFetchLoop(Kafka010Fetcher.java:147)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)
Caused by: java.lang.NullPointerException: null
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createConverter$57e941b$5(AvroRowDataDeserializationSchema.java:252)
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createMapConverter$7941d275$1(AvroRowDataDeserializationSchema.java:315)
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createNullableConverter$c3bac5d8$1(AvroRowDataDeserializationSchema.java:221)
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.lambda$createRowConverter$80d8b6bd$1(AvroRowDataDeserializationSchema.java:206)
	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:148)
	... 8 common frames omitted
{code}
",,aljoscha,jark,libenchao,tinny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19622,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 02:57:02 UTC 2020,,,,,,,,,,"0|z0jonc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/20 03:22;tinny;[~jark], i can complete this work, could you assign it to me ?

 ;;;","14/Oct/20 03:30;jark;What's wrong with the example? Do you mean changing {{behavior STRING}} to {{behavior STRING NULL}}? However, datatypes are nullable by default and we don't support {{NULL}} AFAIK.;;;","14/Oct/20 05:59;tinny;[~jark] Thank you for reminding me that found this bug code: 

{code:java}
private static AvroToRowDataConverter createMapConverter(LogicalType type) {
		final AvroToRowDataConverter keyConverter = createConverter(DataTypes.STRING().getLogicalType());
		final AvroToRowDataConverter valueConverter = createConverter(extractValueTypeToAvroMap(type));

		return avroObject -> {
			final Map<?, ?> map = (Map<?, ?>) avroObject;
			Map<Object, Object> result = new HashMap<>();
			for (Map.Entry<?, ?> entry : map.entrySet()) {
				Object key = keyConverter.convert(entry.getKey());
				Object value = valueConverter.convert(entry.getValue());
				result.put(key, value);
			}
			return new GenericMapData(result);
		};
	}
{code}

 if you don’t have time to fix it, I can fix it and submit the code after verification.:D

;;;","14/Oct/20 06:58;tinny;This is a duplicate of FLINK-19622;;;","21/Oct/20 09:30;tinny;Hi, [~jark] Can you help me review the code, thank you very mutch.;;;","21/Oct/20 12:30;jark;Thanks for the reminder. Will review it. ;;;","22/Oct/20 02:57;jark;Fixed in
 - master: 3c661074b2a597db312ebaf6734c58eb66464ba4
 - release-1.11: 61a0760ee28795c1214efbd0a97ef02edc54b739;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test failed in Azure For EmulatedPubSubSourceTest,FLINK-19619,13335303,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,hailong wang,hailong wang,14/Oct/20 00:31,19/Oct/20 15:56,13/Jul/23 08:12,19/Oct/20 15:56,1.11.0,1.12.0,,,,1.12.0,,,Connectors / Google Cloud PubSub,,,,,0,pull-request-available,test-stability,,,," 

The link is [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/7545/logs/133|https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/7545/logs/133]
{code:java}

 [ERROR] Tests run: 3, Failures: 1, Errors: 2, Skipped: 0, Time elapsed: 1.705 s <<< FAILURE! - in org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest
2020-10-13T18:12:53.5967780Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest  Time elapsed: 1.703 s  <<< FAILURE!
2020-10-13T18:12:53.5973768Z java.lang.AssertionError: We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.5979530Z 	at org.junit.Assert.fail(Assert.java:88)
2020-10-13T18:12:53.5980372Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-10-13T18:12:53.5980722Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-10-13T18:12:53.5981575Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.launchDocker(GCloudEmulatorManager.java:141)
2020-10-13T18:12:53.5982596Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudUnitTestBase.launchGCloudEmulator(GCloudUnitTestBase.java:45)
2020-10-13T18:12:53.5983234Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-13T18:12:53.5983626Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-13T18:12:53.5984410Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-13T18:12:53.5985246Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-13T18:12:53.5985825Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-13T18:12:53.5986306Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-13T18:12:53.5986988Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-13T18:12:53.5987740Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-13T18:12:53.5988167Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-13T18:12:53.5988550Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-13T18:12:53.5988954Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-13T18:12:53.5989404Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-13T18:12:53.5989888Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-13T18:12:53.5990332Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-13T18:12:53.5990819Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-13T18:12:53.5991302Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-13T18:12:53.5991752Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-13T18:12:53.5992161Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-13T18:12:53.5992420Z 
2020-10-13T18:12:53.5992746Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest  Time elapsed: 1.704 s  <<< ERROR!
2020-10-13T18:12:53.5993127Z java.lang.NullPointerException
2020-10-13T18:12:53.5993502Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest.tearDown(EmulatedPubSubSinkTest.java:62)
2020-10-13T18:12:53.5993944Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-13T18:12:53.5994307Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-13T18:12:53.5994757Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-13T18:12:53.5995151Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-13T18:12:53.5995532Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-13T18:12:53.5995990Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-13T18:12:53.5996534Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-13T18:12:53.5996974Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2020-10-13T18:12:53.5997352Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-13T18:12:53.5997760Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-13T18:12:53.5998211Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-13T18:12:53.5998687Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-13T18:12:53.5999127Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-13T18:12:53.5999613Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-13T18:12:53.6000096Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-13T18:12:53.6000551Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-13T18:12:53.6001131Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-13T18:12:53.6001361Z 
2020-10-13T18:12:53.6001722Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest  Time elapsed: 1.705 s  <<< ERROR!
2020-10-13T18:12:53.6002100Z java.lang.NullPointerException
2020-10-13T18:12:53.6002531Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudUnitTestBase.terminateGCloudEmulator(GCloudUnitTestBase.java:50)
2020-10-13T18:12:53.6002970Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-13T18:12:53.6003334Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-13T18:12:53.6003932Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-13T18:12:53.6004505Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-13T18:12:53.6005045Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-13T18:12:53.6005741Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-13T18:12:53.6006340Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-13T18:12:53.6006990Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2020-10-13T18:12:53.6007557Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-13T18:12:53.6007987Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-13T18:12:53.6008614Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-13T18:12:53.6009099Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-13T18:12:53.6009906Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-13T18:12:53.6010569Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-13T18:12:53.6011069Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-13T18:12:53.6011754Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-13T18:12:53.6012190Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-13T18:12:53.6012431Z 
2020-10-13T18:12:53.9374060Z [INFO] 
2020-10-13T18:12:53.9374787Z [INFO] Results:
2020-10-13T18:12:53.9375080Z [INFO] 
2020-10-13T18:12:53.9375355Z [ERROR] Errors: 
2020-10-13T18:12:53.9375824Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest
2020-10-13T18:12:53.9376542Z [ERROR]   Run 1: CheckPubSubEmulatorTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9377425Z [ERROR]   Run 2: CheckPubSubEmulatorTest.tearDown:63 NullPointer
2020-10-13T18:12:53.9378871Z [ERROR]   Run 3: CheckPubSubEmulatorTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9379354Z [INFO] 
2020-10-13T18:12:53.9379804Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedFullTopologyTest.org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedFullTopologyTest
2020-10-13T18:12:53.9380731Z [ERROR]   Run 1: EmulatedFullTopologyTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9381341Z [ERROR]   Run 2: EmulatedFullTopologyTest.tearDown:80 Missing pubsubHelper.
2020-10-13T18:12:53.9382337Z [ERROR]   Run 3: EmulatedFullTopologyTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9383133Z [INFO] 
2020-10-13T18:12:53.9383578Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest.org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest
2020-10-13T18:12:53.9384435Z [ERROR]   Run 1: EmulatedPubSubSinkTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9384995Z [ERROR]   Run 2: EmulatedPubSubSinkTest.tearDown:62 NullPointer
2020-10-13T18:12:53.9385871Z [ERROR]   Run 3: EmulatedPubSubSinkTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9386348Z [INFO] 
2020-10-13T18:12:53.9386781Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSourceTest.org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSourceTest
2020-10-13T18:12:53.9387466Z [ERROR]   Run 1: EmulatedPubSubSourceTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9388032Z [ERROR]   Run 2: EmulatedPubSubSourceTest.tearDown:66 NullPointer
2020-10-13T18:12:53.9388799Z [ERROR]   Run 3: EmulatedPubSubSourceTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9389237Z [INFO] 
2020-10-13T18:12:53.9389484Z [INFO]
{code}
 ",,dian.fu,hailong wang,mapohl,rmetzger,yunta,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 19 15:56:08 UTC 2020,,,,,,,,,,"0|z0jogo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/20 02:11;ZhuShang;+1,my pr met the same azure failures.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7546&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","14/Oct/20 02:28;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7548&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c;;;","14/Oct/20 02:29;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7548&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7548&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7548&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","14/Oct/20 02:48;dian.fu;This issue has happened about two months ago: FLINK-18819. We have closed it because it has not occurred since then. 

[~nielsbasjes] Could you help to take a look at this issue?;;;","14/Oct/20 06:19;rmetzger;Since the test is permanently failing now, I propose a PR to disable it: https://github.com/apache/flink/pull/13627;;;","14/Oct/20 06:44;rmetzger;Disabled test here: https://github.com/apache/flink/commit/6f1eb7f4995ba2934d980fb7af69a365b2f205b7

Upgrading this to blocker to re-enable it before the release.;;;","19/Oct/20 09:43;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7571&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","19/Oct/20 10:01;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=87&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16;;;","19/Oct/20 10:03;dian.fu;[~yunta] [~mapohl] The tests have been disabled on master and so you need to rebase the PR to work around this issue.;;;","19/Oct/20 15:56;rmetzger;Merged to master in https://github.com/apache/flink/commit/5ffdc4e559666a1fa6cd70e2a2ea93e282750a7e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link in docs,FLINK-19618,13335255,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hailong wang,hailong wang,hailong wang,13/Oct/20 16:12,15/Oct/20 12:24,13/Jul/23 08:12,15/Oct/20 12:24,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Documentation,,,,,0,pull-request-available,,,,,"I run the `check_links` shell and found the following broken link,
{code:java}
ERROR `/api/java/' not found.
ERROR `/dev/python/table-api-users-guide/udfs.html' not found.
ERROR `/dev/python/user-guide/table/dependency_management.html' not found.
ERROR `/api/java/org/apache/flink/types/RowKind.html' not found.
{code}
1. `ERROR `/api/java/' not found` seems reachable in remote ['[{{ExecutionEnvironment}}|https://ci.apache.org/projects/flink/flink-docs-release-1.11/api/java/]'|[https://ci.apache.org/projects/flink/flink-docs-master/concepts/flink-architecture.html#flink-application-execution]] (PS, It only broken in my local, I have already built the javadoc in my local.)

2. For `ERROR `/dev/python/table-api-users-guide/udfs.html' not found.`, I did not found any documents use this.

3. It is really broken. We should use 
{code:java}
dev/python/table-apis-users-guide/dependency_management.html {code}
not 
{code:java}
dev/python/user-guide/table/dependency_management.html {code}
in
{code:java}
dev/python/user-guide/table/python_table_api_connectors.md{code}
4. It is same as the first case.

 ",,dian.fu,hailong wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 12:24:25 UTC 2020,,,,,,,,,,"0|z0jo68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/20 02:55;dian.fu;[~hailong wang]  Good catch! Would you like to submit a PR?;;;","14/Oct/20 03:00;hailong wang;[~dian.fu] Yes, Thank you for assgining to me~;;;","15/Oct/20 12:24;dian.fu;master: 26d20b06f9d25a292a1d63616a80d238d39d7f84
release-1.11: eaa2c98b2721c2c373c74bcb8242ae12272bad3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink : Formats : Parquet compilation failure,FLINK-19616,13335200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,pnowojski,pnowojski,13/Oct/20 10:26,15/Oct/20 09:26,13/Jul/23 08:12,15/Oct/20 09:26,1.12.0,,,,,1.12.0,,,Build System,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/pnowojski/Flink/_build/results?buildId=175&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa

{noformat}
[WARNING] [PROTOC] Unable to invoke protoc, will retry 1 time(s)
org.codehaus.plexus.util.cli.CommandLineException: Error while executing process.
	at org.codehaus.plexus.util.cli.Commandline.execute(Commandline.java:680)
	at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLineAsCallable(CommandLineUtils.java:136)
	at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLine(CommandLineUtils.java:106)
	at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLine(CommandLineUtils.java:89)
	at org.xolstice.maven.plugin.protobuf.Protoc.execute(Protoc.java:190)
	at org.xolstice.maven.plugin.protobuf.AbstractProtocMojo.execute(AbstractProtocMojo.java:529)
	at org.xolstice.maven.plugin.protobuf.AbstractProtocTestCompileMojo.execute(AbstractProtocTestCompileMojo.java:31)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: java.io.IOException: Cannot run program ""/__w/1/s/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.5.1-linux-x86_64.exe"": error=13, Permission denied
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at java.lang.Runtime.exec(Runtime.java:621)
	at org.codehaus.plexus.util.cli.Commandline.execute(Commandline.java:660)
	... 27 more
Caused by: java.io.IOException: error=13, Permission denied
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
{noformat}
",,aljoscha,gaoyunhaii,lzljs3620320,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 09:26:09 UTC 2020,,,,,,,,,,"0|z0jnu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/20 10:32;lzljs3620320;CC: [~gaoyunhaii];;;","13/Oct/20 10:33;gaoyunhaii;It should be introduced by me and I'll have a look at it asap, very sorry for the inconvenient brought. ;;;","14/Oct/20 12:40;gaoyunhaii;The root cause should be that azure pipeline do not keep the modified time for the files in the artifact. Current the azure pipeline per-compile the project and upload it as an artifact, then each module downloads the artifact and run the corresponding test. After downloading, the protoc executable downloaded by maven would lost the execution permission, which causes the error. Previously we set the <checkStaleness> property for protobuf generation maven plugin so that it could skip re-generating the java files after downloading to avoid the error. However, since the modification time is not kept, it might cause messy sometimes.

Therefore, we would like to explicitly touch the .proto files before touching the java files to avoid the re-generation. This should be the same method used to avoid re-compile the java to class. ;;;","15/Oct/20 09:26;aljoscha;master: c6e328c74a460b99a59c0e00e5ba460259802576;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-streaming-java module failed to compile,FLINK-19590,13335108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,dian.fu,dian.fu,13/Oct/20 02:18,13/Oct/20 02:35,13/Jul/23 08:12,13/Oct/20 02:35,1.12.0,,,,,1.12.0,,,API / DataStream,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7474&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-streaming-java_2.11: Compilation failure: Compilation failure:
[ERROR] /__w/1/s/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/StreamMultipleInputProcessorFactory.java:[138,45] method getManagedMemoryFractionOperatorUseCaseOfSlot in class org.apache.flink.streaming.api.graph.StreamConfig cannot be applied to given types;
[ERROR] required: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration,java.lang.ClassLoader
[ERROR] found: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] /__w/1/s/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/StreamTwoInputProcessorFactory.java:[109,45] method getManagedMemoryFractionOperatorUseCaseOfSlot in class org.apache.flink.streaming.api.graph.StreamConfig cannot be applied to given types;
[ERROR] required: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration,java.lang.ClassLoader
[ERROR] found: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] -> [Help 1]
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 02:35:06 UTC 2020,,,,,,,,,,"0|z0jn9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/20 02:35;dian.fu;Fixed via a8c7b50eaafa548b4cf71a7a04fd6e5311d2627f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error result when casting binary type as varchar,FLINK-19587,13335041,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hailong wang,hailong wang,hailong wang,12/Oct/20 15:15,29/Oct/20 04:22,13/Jul/23 08:12,29/Oct/20 04:22,1.11.0,,,,,1.11.3,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The result is error when casting binary type as varchar type.

For example,
{code:java}
@Test
def testCast1(): Unit = {
  testSqlApi(
    ""CAST(X'68656c6c6f' as varchar)"",
    ""hello"")
}
{code}
The result is
{code:java}
Expected :hello
Actual   :[B@57fae983
{code}
It is right as follow,
{code:java}
@Test
def testCast(): Unit = {
  testSqlApi(
    ""CAST(CAST(X'68656c6c6f' as varbinary) as varchar)"",
    ""hello"")
}
{code}
We just need to change 
{code:java}
case (VARBINARY, VARCHAR | CHAR){code}
to 
{code:java}
case (BINARY | VARBINARY, VARCHAR | CHAR) 
{code}
in ScalarOperatorGens#generateCast.

 ",,hailong wang,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 12:57:30 UTC 2020,,,,,,,,,,"0|z0jmuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/20 06:38;jark;Do you want to open a pull request for this? I can assign this issue to you [~hailong wang];;;","13/Oct/20 07:01;hailong wang;Yes [~jark] , Thank you for assigning to me.;;;","28/Oct/20 06:04;jark;Fixed in:
 - master: d9b0ac97ee4675aebdab1592af663b95fdc5051b
 - 1.11: 053f03c8ea1c7e1e93d41e87606799f4f844c719;;;","28/Oct/20 07:44;hailong wang;Hi [~jark], I open a pr for 1.11.

https://github.com/apache/flink/pull/13816;;;","28/Oct/20 07:46;hailong wang;I just saw you have did it. I closed it. :D;;;","28/Oct/20 07:47;jark;:D;;;","28/Oct/20 08:13;jark;Hi [~hailong wang], I found it's not compatible to just cherry pick the commit. Could you adapt your code in release-1.11 and create a new pull request?
I have closed mine. ;;;","28/Oct/20 08:53;hailong wang;[~jark] OK, I will open a new pr soon.;;;","28/Oct/20 12:57;hailong wang;Opened a new pull request,

https://github.com/apache/flink/pull/13826;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnalignedCheckpointCompatibilityITCase.test:97->runAndTakeSavepoint: ""Not all required tasks are currently running.""",FLINK-19585,13335032,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,rmetzger,rmetzger,12/Oct/20 14:46,22/Jun/21 14:05,13/Jul/23 08:12,23/Nov/20 10:22,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7419&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2020-10-12T10:27:51.7667213Z [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 13.146 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase
2020-10-12T10:27:51.7675454Z [ERROR] test[type: SAVEPOINT, startAligned: false](org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase)  Time elapsed: 2.168 s  <<< ERROR!
2020-10-12T10:27:51.7676759Z java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Not all required tasks are currently running.
2020-10-12T10:27:51.7686572Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-10-12T10:27:51.7688239Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-10-12T10:27:51.7689543Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.runAndTakeSavepoint(UnalignedCheckpointCompatibilityITCase.java:113)
2020-10-12T10:27:51.7690681Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.test(UnalignedCheckpointCompatibilityITCase.java:97)
2020-10-12T10:27:51.7691513Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-12T10:27:51.7692182Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-12T10:27:51.7692964Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-12T10:27:51.7693655Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-12T10:27:51.7694489Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-12T10:27:51.7707103Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-12T10:27:51.7729199Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-12T10:27:51.7730097Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-12T10:27:51.7730833Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-12T10:27:51.7731500Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-12T10:27:51.7732086Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-12T10:27:51.7732781Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-12T10:27:51.7733563Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-12T10:27:51.7734735Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-12T10:27:51.7735400Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-12T10:27:51.7736075Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-12T10:27:51.7736757Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-12T10:27:51.7737432Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-12T10:27:51.7738081Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-12T10:27:51.7739008Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-10-12T10:27:51.7739583Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-10-12T10:27:51.7740173Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-12T10:27:51.7740800Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-12T10:27:51.7741470Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-12T10:27:51.7742150Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-12T10:27:51.7742808Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-12T10:27:51.7743457Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-12T10:27:51.7768250Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-12T10:27:51.7769287Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-12T10:27:51.7770227Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-12T10:27:51.7771168Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-12T10:27:51.7772013Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-12T10:27:51.7772894Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-12T10:27:51.7773673Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-12T10:27:51.7774734Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-12T10:27:51.7775697Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Not all required tasks are currently running.
2020-10-12T10:27:51.7776658Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2020-10-12T10:27:51.7777468Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2020-10-12T10:27:51.7778379Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2020-10-12T10:27:51.7779152Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-10-12T10:27:51.7779888Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-10-12T10:27:51.7780806Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-10-12T10:27:51.7781692Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$null$0(CheckpointCoordinator.java:467)
2020-10-12T10:27:51.7782539Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-10-12T10:27:51.7783358Z 	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792)
2020-10-12T10:27:51.7784089Z 	at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153)
2020-10-12T10:27:51.7785057Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$triggerSavepointInternal$1(CheckpointCoordinator.java:463)
2020-10-12T10:27:51.7785854Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-10-12T10:27:51.7786452Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-10-12T10:27:51.7787161Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2020-10-12T10:27:51.7788496Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2020-10-12T10:27:51.7789333Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-10-12T10:27:51.7790043Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-10-12T10:27:51.7790770Z 	at java.lang.Thread.run(Thread.java:748)
2020-10-12T10:27:51.7791415Z Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Not all required tasks are currently running.
2020-10-12T10:27:51.7792516Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.getTriggerExecutions(CheckpointCoordinator.java:1724)
2020-10-12T10:27:51.7793448Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.startTriggeringCheckpoint(CheckpointCoordinator.java:510)
2020-10-12T10:27:51.7794766Z 	at java.util.Optional.ifPresent(Optional.java:159)
2020-10-12T10:27:51.7795546Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint(CheckpointCoordinator.java:500)
2020-10-12T10:27:51.7796558Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$triggerSavepointInternal$1(CheckpointCoordinator.java:458)
2020-10-12T10:27:51.7797253Z 	... 7 more
{code}",,AHeise,godfreyhe,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20065,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 10:22:26 UTC 2020,,,,,,,,,,"0|z0jmso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/20 19:14;arvid;Hard to debug as the logs are not telling much. Added `TestLogger` in a hotfix commit. Let's dig in if it pops up again.

 

Basic suspicions:
 * Job is stopped with a savepoint but a final checkpoint is triggered (maybe queued in coordinator) but the tasks have been completed already
 * Job is stopped with savepoint but somehow tasks are stopped before savepoint is actually started (ongoing UC?)
 * Job has not actually started (there seems to be a sleep which is ofc not very fool-proof).;;;","12/Oct/20 20:27;arvid;And the winner is... Option 3.
{noformat}
22481 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Checkpoint triggering task Source: Custom Source -> Sink: Unnamed (1/1) of job 5f9fb8ebb459427c7c15880245c4dbd0 is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.{noformat}
It was hard to reproduce, I needed to completely remove the sleep and had to put my laptop under heavy load and even then it was 1 out of 100 runs.;;;","13/Oct/20 05:40;arvid;I added a PR which explicitly waits for {{JobStatus}} to be {{RUNNING}}, but I could still reproduce the failure. Is there an easy way to ensure that all tasks are running?;;;","19/Oct/20 17:48;arvid;Merged into master as 02177a2b33a75e70925e29c8f268135adb85347c.;;;","09/Nov/20 13:53;rmetzger;Failure in release-1.11 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9339&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

Do we need to backport the fix?;;;","09/Nov/20 15:10;rmetzger;This is master: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8644&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a;;;","10/Nov/20 02:05;godfreyhe;https://dev.azure.com/godfreyhe/godfreyhe-flink/_build/results?buildId=70&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a;;;","11/Nov/20 07:21;arvid;The last two reports of master are FLINK-20065 (AskTimeout). I guess the reopened one is more about a backport to 1.11. I'm waiting for the other ticket to be resolved though as it may be related to the fix of this ticket.;;;","12/Nov/20 07:49;arvid;Backport will happen after [~trohrmann] fixed FLINK-20065 as I currently have no way of reliably testing it.;;;","18/Nov/20 07:15;arvid;Currently [~rmetzger] is investigating unexpected behavior of the mini cluster that hinders the backport.;;;","23/Nov/20 10:22;arvid;Merged into 1.11 as 001f952f38071cbb3a76b039e324d7a9f12bf419.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execution graph related tests are possibly broken due to registering duplicated ExecutionAttemptID,FLINK-19570,13334948,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,12/Oct/20 08:12,30/Nov/21 20:38,13/Jul/23 08:12,13/Oct/20 12:37,1.12.0,,,,,1.12.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,,,,,"Since FLINK-17295, many tests encounters unexpected global failure due to registering duplicated ExecutionAttemptID. Although these tests do not appear to be broken yet, they are potentially broken/unstable. And it further blocks to rework these tests to be based on the new scheduer (FLINK-17760). 

Below is a sample error which happens in ExecutionTest#testAllPreferredLocationCalculation():

{code:java}
2194 [main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job (unnamed job) (a22afb832b5f94b075d7ffb32fbc9023) switched from state CREATED to FAILING.
java.lang.Exception: Trying to register execution Attempt #0 (TestVertex (1/1)) @ (unassigned) - [CREATED] for already used ID a22afb832b5f94b075d7ffb32fbc9023_146968a4de2df0b2fef1e4b2e8297993_0_0
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.registerExecution(ExecutionGraph.java:1621) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.<init>(ExecutionVertex.java:181) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:211) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:139) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:448) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:419) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:411) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:452) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecution(ExecutionGraphTestUtils.java:477) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionTest.testAllPreferredLocationCalculation(ExecutionTest.java:298) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_261]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_261]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_261]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_261]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) [junit-4.12.jar:4.12]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) [junit-4.12.jar:4.12]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) [junit-4.12.jar:4.12]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) [junit-4.12.jar:4.12]
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.12.jar:4.12]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363) [junit-4.12.jar:4.12]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) [junit-4.12.jar:4.12]
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) [junit-rt.jar:?]
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) [junit-rt.jar:?]
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220) [junit-rt.jar:?]
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53) [junit-rt.jar:?]

{code}

This is because these tests improperly creates Execution/ExecutionVertex/ExecutionJobVertex from an existing ExecutionGraph which already contains the Execution/ExecutionVertex/ExecutionJobVertex. FLINK-17295 reveals this problem because it makes the attemptID no longer random. 

Below is an example of improper ExecutionJobVertex creation in ExecutionGraphTestUtils#getExecutionJobVertex():

{code:java}

		JobGraph jobGraph = new JobGraph(ajv);
		jobGraph.setScheduleMode(scheduleMode);

		ExecutionGraph graph = TestingExecutionGraphBuilder
			.newBuilder()
			.setJobGraph(jobGraph)
			.setIoExecutor(executor)
			.setFutureExecutor(executor)
			.build();

		graph.start(ComponentMainThreadExecutorServiceAdapter.forMainThread());

		return new ExecutionJobVertex(graph, ajv, 1, AkkaUtils.getDefaultTimeout());
{code}

We should get rid of such improper usages. 
Therefore, I would like to change these tests to get existing Execution/ExecutionVertex/ExecutionJobVertex from the generated ExecutionGraph, instead of invoking their constructors to create new ones.",,guoyangze,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 12:37:04 UTC 2020,,,,,,,,,,"0|z0jma0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/20 08:16;zhuzh;cc [~guoyangze] [~trohrmann] [~mapohl];;;","12/Oct/20 08:36;chesnay;[~zhuzh] Sounds good to me; I guess we are now discovering a few skeletons in the basement.;;;","12/Oct/20 09:21;guoyangze;[~zhuzh] Thanks for the analysis. The solution sounds good to me.;;;","12/Oct/20 12:51;trohrmann;Thanks for figuring this out [~zhuzh]. I agree with [~chesnay] and [~guoyangze]. I will review your PR.;;;","13/Oct/20 12:37;zhuzh;Fixed via
f2d70c85b659074342db93bd903b2c9e91cfb749
0c373822e31562c6f0d65505c0b95ea1b166c093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue retrieving leader after zookeeper session reconnect,FLINK-19557,13334673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,maxmzkr,maxmzkr,09/Oct/20 15:41,23/Oct/20 12:52,13/Jul/23 08:12,23/Oct/20 12:52,1.10.2,1.11.2,1.12.0,,,1.10.3,1.11.3,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"We have noticed an issue with leaders being retrieved after reconnecting to zookeeper. The steps to reproduce this issue are to break the connection between a job manager that is not the leader and zookeeper. Wait for the session to be lost between the two. At this point, flink notifies for a loss of leader. After the loss of leader has occured, reconnect the job manager to zookeeper. At this point, the leader will still be the same as it was before, but when trying to access the rest API, you will see this
{code}
$ curl -s localhost:8999/jobs
{""errors"":[""Service temporarily unavailable due to an ongoing leader election. Please refresh.""]}
{code}
I have been using `stress -t 60 -m 2048` (which spins up 2048 threads continuously alloc and freeing 256MB, to swap out the job manager and cause the connection loss.

I have done some amount of digging on this. The ZooKeeperLeaderRetrievalService has this code block for handling state changes
{code}
	protected void handleStateChange(ConnectionState newState) {
		switch (newState) {
			case CONNECTED:
				LOG.debug(""Connected to ZooKeeper quorum. Leader retrieval can start."");
				break;
			case SUSPENDED:
				LOG.warn(""Connection to ZooKeeper suspended. Can no longer retrieve the leader from "" +
						""ZooKeeper."");
				synchronized (lock) {
					notifyLeaderLoss();
				}
				break;
			case RECONNECTED:
				LOG.info(""Connection to ZooKeeper was reconnected. Leader retrieval can be restarted."");
				break;
			case LOST:
				LOG.warn(""Connection to ZooKeeper lost. Can no longer retrieve the leader from "" +
						""ZooKeeper."");
				synchronized (lock) {
					notifyLeaderLoss();
				}
				break;
		}
	}
{code}
It calls notifyLeaderLoss() when the connection is lost, but it doesn't do anything when the connection is reconnected. It appears that curator's NodeCache will retrieve the value of the leader znode after reconnect, but it won't notify the listeners if the value is the same as before the connection loss. So, unless a leader election happens after a zookeeper connection loss, the job managers that are not the leader will never know that there is a leader.

The method that is called for NodeCache when a new value is retrieved
{code}
    private void setNewData(ChildData newData) throws InterruptedException
    {
        ChildData   previousData = data.getAndSet(newData);
        if ( !Objects.equal(previousData, newData) )
        {
            listeners.forEach(listener -> {
                try
                {
                    listener.nodeChanged();
                }
                catch ( Exception e )
                {
                    ThreadUtils.checkInterrupted(e);
                    log.error(""Calling listener"", e);
                }
            });

            if ( rebuildTestExchanger != null )
            {
                try
                {
                    rebuildTestExchanger.exchange(new Object());
                }
                catch ( InterruptedException e )
                {
                    Thread.currentThread().interrupt();
                }
            }
        }
    }
{code}
note the
{code}
        if ( !Objects.equal(previousData, newData) )
{code}
seems to be preventing the job managers from getting the leader after a zookeeper connection loss.",,aitozi,kezhuw,klion26,maxmzkr,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 23 12:52:08 UTC 2020,,,,,,,,,,"0|z0jkkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 20:57;trohrmann;Thanks for reporting this issue and analyzing it [~maxmzkr]. I will take a look. Maybe we need to clear the leader data upon a connection suspension.;;;","20/Oct/20 21:21;trohrmann;Hmm I see. The problem is the caching of the {{NodeCache}}. So either we keep the last leader information in the {{ZooKeeperLeaderRetrievalService}} and call {{leaderListener.notifyLeaderAddress}} when we reconnect to ZooKeeper or we bind the lifecycle of a {{NodeCache}} to the lifecycle of an established ZooKeeper connection. I guess the former should be easier. The downside of the first approach is that we might send a stale leader address to the listener.;;;","21/Oct/20 12:11;maxmzkr;In our fork, we went with the second approach mostly because we didn't know what the ramifications of retrieving a stale leader would be. The second approach isn't super clean, however, because you have to completely close out the NodeCache and then open a new one. Both of the closing and opening can raise exceptions. The reconnect callback does't allow you to raise exceptions, so we had to add a big try catch. There's likely a case where we will still stall the leader election if we fail to open a new NodeCache but it's a smaller chance of stalling out. It's increased our stability a good amount now. (We're really good at swapping nodes out :P).;;;","21/Oct/20 12:30;trohrmann;The stale leader address should get resolved as soon as a new leader arrives. Hence, I believe that this should work. I have opened a PR with change. Maybe you can take a look or try it out in your setup [~maxmzkr].;;;","23/Oct/20 12:52;trohrmann;Fixed via

1.12.0: 594e6c03aa2942dcfe7d9ab37edacaaebf3af556
1.11.3: 751d42ca4a9642f53cb68f4b315b1a2cd000a42f
1.10.3: 933041b3d8d192391437e343857f1dec14c544d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager dies with IllegalStateException SharedSlot (physical request SlotRequestId{%}) has been released,FLINK-19552,13334575,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,rmetzger,rmetzger,09/Oct/20 07:13,03/Nov/20 14:21,13/Jul/23 08:12,27/Oct/20 12:00,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"While running some benchmarks that involve a lot of failures, I experienced fatal JobManager crashes, with the following log:
{code}
2020-10-09 09:01:45,001 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN Filter (Filter at main(TPCHQuery3.java:157)) -> Map (Map at appendMapper(KillerClientMapper.java:38)) (2/8) (cf993790aa641a2287b42939b3037f75_f25e1269f08c88185d8a3b9caad8d0c0_1_2) switched from CREATED to SCHEDULED.
2020-10-09 09:01:45,004 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-17' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.IllegalStateException: SharedSlot (physical request SlotRequestId{214e8e202b9b087388a6a17c6ba9bccf}) has been released
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:708) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:687) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_222]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
Caused by: java.lang.IllegalStateException: SharedSlot (physical request SlotRequestId{214e8e202b9b087388a6a17c6ba9bccf}) has been released
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:217) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.SharedSlot.allocateLogicalSlot(SharedSlot.java:126) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.allocateLogicalSlotsFromSharedSlots(SlotSharingExecutionSlotAllocator.java:165) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.allocateSlotsFor(SlotSharingExecutionSlotAllocator.java:127) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlots(DefaultScheduler.java:364) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:337) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegion(PipelinedRegionSchedulingStrategy.java:143) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegions(PipelinedRegionSchedulingStrategy.java:128) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.restartTasks(PipelinedRegionSchedulingStrategy.java:100) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$restartTasks$2(DefaultScheduler.java:290) ~[flip1-bench-jobs-1.0-SNAPSHOT.jar:?]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_222]
        ... 24 more
{code}",,azagrebin,hwanju,kezhuw,rmetzger,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19832,,,,,,,,,,,"09/Oct/20 08:07;rmetzger;flink-robert-standalonesession-0-RobertsibabaMac.localdomain.log-debug;https://issues.apache.org/jira/secure/attachment/13013288/flink-robert-standalonesession-0-RobertsibabaMac.localdomain.log-debug",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 12:00:07 UTC 2020,,,,,,,,,,"0|z0jjz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/20 08:01;zhuzh;Thanks for reporting this issue! [~rmetzger]
We will take a look. 
cc [~azagrebin];;;","09/Oct/20 08:05;chesnay;[~rmetzger] can you give us the jobmanager logs (preferably on debug)?;;;","09/Oct/20 08:07;rmetzger;I shared them with Andrey already, but I've now uploaded them to the ticket as well.;;;","14/Oct/20 07:24;zhuzh;The root cause is that `MergingSharedSlotProfileRetrieverFactory` incorrectly retrieves preferred locations for vertices which are not scheduled yet, while some of their producer vertex are canceled and so are their location futures.;;;","27/Oct/20 12:00;azagrebin;merged into master by 4b3150d7209f8efe97ff1c44f71cac13079cbcf2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMXReporterFactoryTest fails on Azure,FLINK-19539,13334465,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,roman,roman,08/Oct/20 17:17,30/Nov/21 20:38,13/Jul/23 08:12,12/Oct/20 08:22,1.12.0,,,,,1.12.0,,,Runtime / Metrics,,,,,0,test-stability,,,,,"The following build failed

[https://dev.azure.com/khachatryanroman/810e80cc-0656-4d3c-9d8c-186764456a01/_apis/build/builds/289/logs/106]

{code}
 [ERROR] testPortRangeArgument(org.apache.flink.metrics.jmx.JMXReporterFactoryTest)  Time elapsed: 0.02 s  <<< FAILURE!
 java.lang.AssertionError:
 
 Expected: (a value equal to or greater than <9000> and a value less than or equal to <9010>)
      but: a value less than or equal to <9010> <9040> was greater than <9010>
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:956)
    at org.junit.Assert.assertThat(Assert.java:923)
    at org.apache.flink.metrics.jmx.JMXReporterFactoryTest.testPortRangeArgument(JMXReporterFactoryTest.java:46)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}

 

I see the following problems in the code:

- tests in JMXReporterFactoryTest assumes JMXService.jmxServer wasn't started or was stopped

- JMXService.jmxServer is not volatile

 cc: [~chesnay], [~rongr]",,aljoscha,dian.fu,guoyangze,mapohl,pnowojski,rmetzger,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 08:22:49 UTC 2020,,,,,,,,,,"0|z0jjb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/20 06:55;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7307&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9;;;","09/Oct/20 08:42;aljoscha;Another one: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=343&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa&l=10791;;;","09/Oct/20 10:42;mapohl;And another one: https://dev.azure.com/mapohl/flink/_build/results?buildId=76&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa;;;","09/Oct/20 11:10;rmetzger;Okay, looks like something broke the test. Shall we ignore it and make this a blocker until somebody fixed it?

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8458&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa;;;","09/Oct/20 11:39;chesnay;We introduced the JMX server singleton weeks ago; it is quite odd that the tests start failing now.

From what I can tell we aren't shutting down the JMXServer in the JmxReporter(Factory)Test; so a simple {{@After}} might already do the trick.;;;","09/Oct/20 15:01;pnowojski;I've already seen the issue a couple of days ago on my WIP branch, so this failure is on the master since at least a couple of days [~chesnay]. I didn't report it back then as I was having quite a bit of other test failures that were caused by my changes, and only now I finally ended up with just this one often failure. Example instances:

https://dev.azure.com/pnowojski/Flink/_build/results?buildId=165&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa
https://dev.azure.com/pnowojski/Flink/_build/results?buildId=164&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa
https://dev.azure.com/pnowojski/Flink/_build/results?buildId=163&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f
https://dev.azure.com/pnowojski/Flink/_build/results?buildId=162&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f
https://dev.azure.com/pnowojski/Flink/_build/results?buildId=160&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa

and more... bumping to blocker (it happens almost every time on my branch);;;","10/Oct/20 07:02;guoyangze;Another one: https://dev.azure.com/guoyangze/Flink/_build/results?buildId=188&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa;;;","11/Oct/20 04:09;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7349&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9;;;","12/Oct/20 02:25;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7400&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9;;;","12/Oct/20 08:22;chesnay;master:
eb7e46777a28865b68c6ff543a7350d17ad46e12
cf4328db168fe5488514efdedb9773ca32ded9f4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinator should avoid fail job multiple times.,FLINK-19535,13334368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,becket_qin,becket_qin,08/Oct/20 07:56,12/Dec/20 11:57,13/Jul/23 08:12,24/Nov/20 12:20,1.11.2,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,,,,,Currently the {{SourceCoordinator}} may invoke {{SourceCoordinatorContext#failJob()}} multiple times from the same instance. This may cause the job to failover multiple times unnecessarily. The {{SourceCoordinator}} should instead just fail the job once.,,becket_qin,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 12:20:40 UTC 2020,,,,,,,,,,"0|z0jipk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/20 12:20;sewen;Fixed in 1.12.0 (master) via c3973d6915e26ed5aec2f5c6fb7688df12d8831a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duration of running job is shown as 0 in web UI,FLINK-19518,13334124,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,07/Oct/20 08:16,15/Oct/20 19:30,13/Jul/23 08:12,15/Oct/20 19:30,1.12.0,,,,,1.12.0,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"Most likely caused by FLINK-16866, the web UI is showing the ""Duration"" of a job as 0 in the overview. Once you open the detail page of a job, you see the correct duration.

",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 19:30:39 UTC 2020,,,,,,,,,,"0|z0jh7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/20 19:30;rmetzger;Merged in https://github.com/apache/flink/commit/9ae7d81f22b5a64257c7350924d3a4fdb9351aef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerJobMiniClusterFactoryTest.testJobClient(),FLINK-19516,13334032,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,sewen,sewen,06/Oct/20 17:20,09/Oct/20 13:45,13/Jul/23 08:12,08/Oct/20 07:47,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"*Log:*
https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/151/logs/137

*Exception:*
{code}
[ERROR] testJobClient(org.apache.flink.client.program.PerJobMiniClusterFactoryTest)  Time elapsed: 0.392 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: is <false>
     but: was <true>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.assertThatMiniClusterIsShutdown(PerJobMiniClusterFactoryTest.java:161)
	at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.testJobClient(PerJobMiniClusterFactoryTest.java:93)
{code}",,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19123,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 07:47:14 UTC 2020,,,,,,,,,,"0|z0jgmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/20 14:52;trohrmann;I believe that this problem has been introduced with FLINK-19123 because we no longer chain the result future after the callback which triggers the shutdown of the {{MiniCluster}}.;;;","07/Oct/20 15:34;trohrmann;[~sewen] if you post the link to the Azure pipeline run and not the directly logs, then it is a tad easier to retrieve the uploaded logs which are stored on Azure.;;;","08/Oct/20 07:02;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7275&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.358 s <<< FAILURE! - in org.apache.flink.client.program.PerJobMiniClusterFactoryTest
[ERROR] testMultipleExecutions(org.apache.flink.client.program.PerJobMiniClusterFactoryTest)  Time elapsed: 4.186 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: is <false>
     but: was <true>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.assertThatMiniClusterIsShutdown(PerJobMiniClusterFactoryTest.java:161)
	at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.testMultipleExecutions(PerJobMiniClusterFactoryTest.java:136)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code};;;","08/Oct/20 07:07;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7275&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392;;;","08/Oct/20 07:47;trohrmann;Fixed via bff791f833bd9e61796e7c9af2c88c1a2bfafa47;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Async RequestReply handler concurrency bug,FLINK-19515,13333951,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fransking,fransking,fransking,06/Oct/20 15:13,20/Oct/20 05:30,13/Jul/23 08:12,20/Oct/20 05:30,statefun-2.2.0,,,,,statefun-2.2.1,statefun-3.0.0,,,,,,,0,pull-request-available,,,,,"Async RequestReply handler implemented in https://issues.apache.org/jira/browse/FLINK-18518 has a concurrency problem.

 

Lines 151 to 152 of [https://github.com/apache/flink-statefun/blob/master/statefun-python-sdk/statefun/request_reply.py]

The coro is awaiting and may yield.  Another coro may continue that was yielding and call ic.complete() which sets the ic.context to None

 

In short:

 
{code:java}
ic.setup(request_bytes)        
await self.handle_invocation(ic)        
return ic.complete()
 
{code}
Needs to happen atomically.

 

I worked around this by creating an AsyncRequestReplyHandler for each request.

 

It should be possible to re-produce this by putting an await asyncio.sleep(5) in the greeter example and then run in gunicorn with a single asyncio thread/event loop (-w 1).  

 

 

 
{code:java}
    response_data = await handler(request_data)
  File ""/home/pi/.local/lib/python3.7/site-packages/statefun/request_reply.py"", line 152, in __call__
    return ic.complete()
  File ""/home/pi/.local/lib/python3.7/site-packages/statefun/request_reply.py"", line 57, in complete
    self.add_mutations(context, invocation_result)
  File ""/home/pi/.local/lib/python3.7/site-packages/statefun/request_reply.py"", line 82, in add_mutations
    for name, handle in context.states.items():
AttributeError: 'NoneType' object has no attribute 'states'
{code}
 ",,fransking,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 05:30:00 UTC 2020,,,,,,,,,,"0|z0jgko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 20:09;igal;Hi,
I think that you are correct, and it does seems like a bug.

The InvocationContext should be created in __call__ instead of the constructor. This also has to happen in the regular RequestRelplyHander.

Would you like to create a PR with the fix?
;;;","07/Oct/20 14:18;fransking;Sure.  Will submit a PR next week.  ;;;","19/Oct/20 09:20;fransking;Ok a bit delayed but here's the PR - https://github.com/apache/flink-statefun/pull/164;;;","20/Oct/20 05:30;tzulitai;2.2.1: e10cdd3218b56f75a40ae107994ca7463c55b5a9
2.3.0: b3a88dab20c56bc7cb1c1e2e2fe3237eda46144a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange times out,FLINK-19514,13333949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,sewen,sewen,06/Oct/20 15:03,20/Oct/20 13:19,13/Jul/23 08:12,20/Oct/20 13:19,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,test-stability,,,,,"Full logs:
https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/148/logs/115

Exception:
{code}
[ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 301.093 s  <<< ERROR!
java.util.concurrent.TimeoutException: Condition was not met in given timeout.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:132)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.getNextLeadingDispatcherGateway(ZooKeeperLeaderElectionITCase.java:140)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:122)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)

{code}",,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19458,,,,,,,,,,,"08/Oct/20 17:26;rmetzger;tests-1601994365.tgz;https://issues.apache.org/jira/secure/attachment/13013250/tests-1601994365.tgz",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 13:19:52 UTC 2020,,,,,,,,,,"0|z0jgk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/20 08:45;trohrmann;This issue could be related to FLINK-19458. cc [~rmetzger];;;","07/Oct/20 08:54;rmetzger;Yes, I will take a look.;;;","20/Oct/20 13:19;rmetzger;Fixed in FLINK-19458.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataGen source DECIMAL always returns null,FLINK-19496,13330617,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,02/Oct/20 18:27,09/Oct/20 13:50,13/Jul/23 08:12,08/Oct/20 14:17,,,,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,,,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-02 18:27:03.0,,,,,,,,,,"0|z0j4i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroSerializerSnapshot cannot handle large schema,FLINK-19491,13330532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,arvid,AHeise,02/Oct/20 10:52,22/Jun/21 14:06,13/Jul/23 08:12,09/Nov/20 13:09,1.10.2,1.11.2,1.12.0,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"Flink can only handle schemas up to a size of 64kb.

 
{noformat}
Caused by: java.io.UTFDataFormatException: encoded string too long: 223502 bytes
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshot.writeSnapshot(AvroSerializerSnapshot.java:75)
	at org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.writeVersionedSnapshot(TypeSerializerSnapshot.java:153)
	at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.writeNestedSerializerSnapshots(NestedSerializersSnapshotDelegate.java:159)
	at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.writeSnapshot(CompositeTypeSerializerSnapshot.java:148)
	at org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.writeVersionedSnapshot(TypeSerializerSnapshot.java:153)
	at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.write(TypeSerializerSnapshotSerializationUtil.java:138)
	at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.writeSerializerSnapshot(TypeSerializerSnapshotSerializationUtil.java:55)
	at org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotReadersWriters$CurrentWriterImpl.writeStateMetaInfoSnapshot(StateMetaInfoSnapshotReadersWriters.java:183)
	at org.apache.flink.runtime.state.KeyedBackendSerializationProxy.write(KeyedBackendSerializationProxy.java:126)
	at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy$1.callInternal(HeapSnapshotStrategy.java:171)
	at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy$1.callInternal(HeapSnapshotStrategy.java:158)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:75)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:510)
	... 5 common frames omitted{noformat}",,AHeise,aljoscha,dwysakowicz,fsk119,jark,jiangok,kezhuw,nicholasjiang,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 13:09:33 UTC 2020,,,,,,,,,,"0|z0j3zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 01:54;nicholasjiang;[~AHeise], the maximum length of the allowed string literal is 64KB, therefore encoded string of UTF is within 64KB. IMO, I couldn't get the bug of this issue.;;;","07/Oct/20 15:41;jiangok;Is it possible to support large schemas (>64KB) by having a new design working around the string/UTF 64KB limit? Thanks.;;;","07/Oct/20 18:53;arvid;Hi [~nicholasjiang] , there is no inherent reason to believe that Avro schemas are only 64kb. It's an unfortunate limitation of the current implementation that can be easily mended by added a V3 of the snapshot version that uses a different version to store the string, for example by using {{DataOutput#writeChars}} instead of {{#writeUTF}}.;;;","10/Oct/20 17:02;jiangok;Thanks [~nicholasjiang] and [~AHeise] for taking a look. Could this issue be fixed for 1.12? Appreciate sharing the plan.;;;","13/Oct/20 09:01;sewen;I would suggest to introduce a V3 that uses {{StringValue.writeString()}} and {{StringValue.readString()}} to handle the string encoding.;;;","16/Oct/20 04:07;nicholasjiang;[~AHeise], if you are not available, I would like to take this ticket and use V3 to handle the string encoding as [~sewen] suggested. Could you please assign this issue to me?;;;","16/Oct/20 06:53;dwysakowicz;I assigned the issue to you [~nicholasjiang];;;","16/Oct/20 07:33;sewen;[~nicholasjiang] Have a look at the existing serializer snapshot tests to see how to introduce a test for the new snapshot version.;;;","16/Oct/20 08:49;nicholasjiang;[~sewen],OK, thanks for the remind. I would like to take a look.;;;","09/Nov/20 13:09;aljoscha;master: 20aa5f9d91ba8e830810e9e31437f682feee07e3

I did not merge this as a fix for previous versions because it changes the {{TypeSerializer}} snapshot format, which would mean that, for example, snapshots from Flink 1.11.3 would not work on Flink 1.11.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent gets stuck,FLINK-19489,13330476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,dian.fu,dian.fu,02/Oct/20 00:51,13/Oct/20 09:45,13/Jul/23 08:12,13/Oct/20 09:45,1.12.0,,,,,1.12.0,,,Connectors / Common,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7158&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f

{code}
020-10-01T21:55:34.9982203Z ""main"" #1 prio=5 os_prio=0 cpu=1048.80ms elapsed=921.99s tid=0x00007f8c00015800 nid=0xf6e in Object.wait()  [0x00007f8c06648000]
2020-10-01T21:55:34.9982807Z    java.lang.Thread.State: WAITING (on object monitor)
2020-10-01T21:55:34.9983177Z 	at java.lang.Object.wait(java.base@11.0.7/Native Method)
2020-10-01T21:55:34.9983871Z 	- waiting on <0x000000008e0be190> (a org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread)
2020-10-01T21:55:34.9984581Z 	at java.lang.Thread.join(java.base@11.0.7/Thread.java:1305)
2020-10-01T21:55:34.9985433Z 	- waiting to re-lock in wait() <0x000000008e0be190> (a org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread)
2020-10-01T21:55:34.9985998Z 	at org.apache.flink.core.testutils.CheckedThread.trySync(CheckedThread.java:112)
2020-10-01T21:55:34.9986511Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:100)
2020-10-01T21:55:34.9987004Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:89)
2020-10-01T21:55:34.9987707Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread.shutdown(SplitFetcherTest.java:301)
2020-10-01T21:55:34.9988427Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent(SplitFetcherTest.java:131)
2020-10-01T21:55:34.9989025Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.7/Native Method)
2020-10-01T21:55:34.9989531Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@11.0.7/NativeMethodAccessorImpl.java:62)
2020-10-01T21:55:34.9990117Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.7/DelegatingMethodAccessorImpl.java:43)
2020-10-01T21:55:34.9990626Z 	at java.lang.reflect.Method.invoke(java.base@11.0.7/Method.java:566)
2020-10-01T21:55:34.9991078Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-01T21:55:34.9991602Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-01T21:55:34.9992119Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-01T21:55:34.9992749Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-01T21:55:34.9993229Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-01T21:55:34.9993700Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-01T21:55:34.9994202Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-01T21:55:34.9994670Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-01T21:55:34.9995098Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-01T21:55:34.9995524Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-01T21:55:34.9995965Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-01T21:55:34.9996403Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-01T21:55:34.9996816Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-01T21:55:34.9997268Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-10-01T21:55:34.9997695Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-10-01T21:55:34.9998077Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-01T21:55:34.9998510Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-01T21:55:34.9998941Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-01T21:55:34.9999380Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-01T21:55:34.9999815Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-01T21:55:35.0000226Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-01T21:55:35.0000662Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-10-01T21:55:35.0001190Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-10-01T21:55:35.0001746Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
2020-10-01T21:55:35.0002432Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
2020-10-01T21:55:35.0002974Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-10-01T21:55:35.0003797Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-10-01T21:55:35.0004349Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-01T21:55:35.0004913Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-01T21:55:35.0005427Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-01T21:55:35.0005900Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-01T21:55:35.0006196Z 
2020-10-01T21:55:35.0006585Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 cpu=0.79ms elapsed=921.95s tid=0x00007f8c0027e800 nid=0xf7b waiting on condition  [0x00007f8bd0af1000]
2020-10-01T21:55:35.0007132Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0007513Z 	at java.lang.ref.Reference.waitForReferencePendingList(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0008018Z 	at java.lang.ref.Reference.processPendingReferences(java.base@11.0.7/Reference.java:241)
2020-10-01T21:55:35.0008484Z 	at java.lang.ref.Reference$ReferenceHandler.run(java.base@11.0.7/Reference.java:213)
2020-10-01T21:55:35.0008769Z 
2020-10-01T21:55:35.0009134Z ""Finalizer"" #3 daemon prio=8 os_prio=0 cpu=0.48ms elapsed=921.95s tid=0x00007f8c00283000 nid=0xf7d in Object.wait()  [0x00007f8bd09f0000]
2020-10-01T21:55:35.0009630Z    java.lang.Thread.State: WAITING (on object monitor)
2020-10-01T21:55:35.0009980Z 	at java.lang.Object.wait(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0010612Z 	- waiting on <0x000000008e704170> (a java.lang.ref.ReferenceQueue$Lock)
2020-10-01T21:55:35.0011047Z 	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.7/ReferenceQueue.java:155)
2020-10-01T21:55:35.0011672Z 	- waiting to re-lock in wait() <0x000000008e704170> (a java.lang.ref.ReferenceQueue$Lock)
2020-10-01T21:55:35.0012130Z 	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.7/ReferenceQueue.java:176)
2020-10-01T21:55:35.0012672Z 	at java.lang.ref.Finalizer$FinalizerThread.run(java.base@11.0.7/Finalizer.java:170)
2020-10-01T21:55:35.0012943Z 
2020-10-01T21:55:35.0013322Z ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 cpu=0.52ms elapsed=921.94s tid=0x00007f8c00297800 nid=0xf7f runnable  [0x0000000000000000]
2020-10-01T21:55:35.0013773Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0013975Z 
2020-10-01T21:55:35.0014366Z ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 cpu=641.40ms elapsed=921.94s tid=0x00007f8c00299800 nid=0xf80 waiting on condition  [0x0000000000000000]
2020-10-01T21:55:35.0014854Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0015120Z    No compile task
2020-10-01T21:55:35.0015267Z 
2020-10-01T21:55:35.0015674Z ""C1 CompilerThread0"" #15 daemon prio=9 os_prio=0 cpu=491.81ms elapsed=921.94s tid=0x00007f8c0029c000 nid=0xf81 waiting on condition  [0x0000000000000000]
2020-10-01T21:55:35.0016145Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0016409Z    No compile task
2020-10-01T21:55:35.0016558Z 
2020-10-01T21:55:35.0016934Z ""Sweeper thread"" #20 daemon prio=9 os_prio=0 cpu=53.17ms elapsed=921.94s tid=0x00007f8c0029e000 nid=0xf82 runnable  [0x0000000000000000]
2020-10-01T21:55:35.0017450Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0017688Z 
2020-10-01T21:55:35.0018064Z ""Service Thread"" #21 daemon prio=9 os_prio=0 cpu=0.20ms elapsed=921.90s tid=0x00007f8c00321800 nid=0xf8a runnable  [0x0000000000000000]
2020-10-01T21:55:35.0018510Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0018710Z 
2020-10-01T21:55:35.0019343Z ""Common-Cleaner"" #22 daemon prio=8 os_prio=0 cpu=2.51ms elapsed=921.89s tid=0x00007f8c0032d000 nid=0xf90 in Object.wait()  [0x00007f8ba3ffe000]
2020-10-01T21:55:35.0019861Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-10-01T21:55:35.0020238Z 	at java.lang.Object.wait(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0020701Z 	- waiting on <no object reference available>
2020-10-01T21:55:35.0021178Z 	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.7/ReferenceQueue.java:155)
2020-10-01T21:55:35.0021880Z 	- waiting to re-lock in wait() <0x000000008e705d98> (a java.lang.ref.ReferenceQueue$Lock)
2020-10-01T21:55:35.0022450Z 	at jdk.internal.ref.CleanerImpl.run(java.base@11.0.7/CleanerImpl.java:148)
2020-10-01T21:55:35.0022860Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0023273Z 	at jdk.internal.misc.InnocuousThread.run(java.base@11.0.7/InnocuousThread.java:134)
2020-10-01T21:55:35.0023544Z 
2020-10-01T21:55:35.0024207Z ""surefire-forkedjvm-command-thread"" #23 daemon prio=5 os_prio=0 cpu=14.02ms elapsed=921.83s tid=0x00007f8c003de000 nid=0xf92 runnable  [0x00007f8ba3ad5000]
2020-10-01T21:55:35.0024689Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0025037Z 	at java.io.FileInputStream.readBytes(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0025453Z 	at java.io.FileInputStream.read(java.base@11.0.7/FileInputStream.java:279)
2020-10-01T21:55:35.0025895Z 	at java.io.BufferedInputStream.fill(java.base@11.0.7/BufferedInputStream.java:252)
2020-10-01T21:55:35.0026359Z 	at java.io.BufferedInputStream.read(java.base@11.0.7/BufferedInputStream.java:271)
2020-10-01T21:55:35.0026942Z 	- locked <0x000000008e707330> (a java.io.BufferedInputStream)
2020-10-01T21:55:35.0027426Z 	at java.io.DataInputStream.readInt(java.base@11.0.7/DataInputStream.java:392)
2020-10-01T21:55:35.0027979Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-10-01T21:55:35.0028623Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-10-01T21:55:35.0029053Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0029280Z 
2020-10-01T21:55:35.0029927Z ""surefire-forkedjvm-ping-30s"" #24 daemon prio=5 os_prio=0 cpu=847.16ms elapsed=921.75s tid=0x00007f8c00473800 nid=0xf93 waiting on condition  [0x00007f8ba35c8000]
2020-10-01T21:55:35.0030448Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-10-01T21:55:35.0030804Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0031430Z 	- parking to wait for  <0x000000008e702fe0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-10-01T21:55:35.0031930Z 	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@11.0.7/LockSupport.java:234)
2020-10-01T21:55:35.0032543Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(java.base@11.0.7/AbstractQueuedSynchronizer.java:2123)
2020-10-01T21:55:35.0033139Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.7/ScheduledThreadPoolExecutor.java:1182)
2020-10-01T21:55:35.0033730Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.7/ScheduledThreadPoolExecutor.java:899)
2020-10-01T21:55:35.0034270Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@11.0.7/ThreadPoolExecutor.java:1054)
2020-10-01T21:55:35.0034751Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.7/ThreadPoolExecutor.java:1114)
2020-10-01T21:55:35.0035246Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.7/ThreadPoolExecutor.java:628)
2020-10-01T21:55:35.0035671Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0035882Z 
2020-10-01T21:55:35.0036266Z ""process reaper"" #25 daemon prio=10 os_prio=0 cpu=278.14ms elapsed=921.73s tid=0x00007f8b94016000 nid=0xf97 waiting on condition  [0x00007f8c064d5000]
2020-10-01T21:55:35.0036747Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-10-01T21:55:35.0037150Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0037822Z 	- parking to wait for  <0x000000008e704488> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-10-01T21:55:35.0038282Z 	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@11.0.7/LockSupport.java:234)
2020-10-01T21:55:35.0038782Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(java.base@11.0.7/SynchronousQueue.java:462)
2020-10-01T21:55:35.0039467Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(java.base@11.0.7/SynchronousQueue.java:361)
2020-10-01T21:55:35.0039943Z 	at java.util.concurrent.SynchronousQueue.poll(java.base@11.0.7/SynchronousQueue.java:937)
2020-10-01T21:55:35.0040413Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@11.0.7/ThreadPoolExecutor.java:1053)
2020-10-01T21:55:35.0040907Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.7/ThreadPoolExecutor.java:1114)
2020-10-01T21:55:35.0041393Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.7/ThreadPoolExecutor.java:628)
2020-10-01T21:55:35.0041803Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0042026Z 
2020-10-01T21:55:35.0042516Z ""Queue Drainer"" #28 prio=10 os_prio=0 cpu=0.95ms elapsed=920.78s tid=0x00007f8c009d6800 nid=0xfb7 waiting on condition  [0x00007f8ba22bd000]
2020-10-01T21:55:35.0042993Z    java.lang.Thread.State: WAITING (parking)
2020-10-01T21:55:35.0043338Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0043949Z 	- parking to wait for  <0x000000008df00fa8> (a java.util.concurrent.CompletableFuture$Signaller)
2020-10-01T21:55:35.0044408Z 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.7/LockSupport.java:194)
2020-10-01T21:55:35.0044887Z 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.7/CompletableFuture.java:1796)
2020-10-01T21:55:35.0045357Z 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.7/ForkJoinPool.java:3128)
2020-10-01T21:55:35.0045830Z 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.7/CompletableFuture.java:1823)
2020-10-01T21:55:35.0046742Z 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.7/CompletableFuture.java:1998)
2020-10-01T21:55:35.0047375Z 	at org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.take(FutureCompletingBlockingQueue.java:235)
2020-10-01T21:55:35.0048103Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread.go(SplitFetcherTest.java:289)
2020-10-01T21:55:35.0048659Z 	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:74)
2020-10-01T21:55:35.0048929Z 
2020-10-01T21:55:35.0049311Z ""Attach Listener"" #29 daemon prio=9 os_prio=0 cpu=0.39ms elapsed=0.10s tid=0x00007f8bb4001000 nid=0x1922 waiting on condition  [0x0000000000000000]
2020-10-01T21:55:35.0049756Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0049952Z 
2020-10-01T21:55:35.0050238Z ""VM Thread"" os_prio=0 cpu=128.10ms elapsed=921.96s tid=0x00007f8c00276800 nid=0xf7a runnable  
2020-10-01T21:55:35.0050517Z 
2020-10-01T21:55:35.0050805Z ""GC Thread#0"" os_prio=0 cpu=31.75ms elapsed=921.98s tid=0x00007f8c00041000 nid=0xf72 runnable  
2020-10-01T21:55:35.0051086Z 
2020-10-01T21:55:35.0051375Z ""GC Thread#1"" os_prio=0 cpu=27.13ms elapsed=921.35s tid=0x00007f8bc8001000 nid=0xfaa runnable  
2020-10-01T21:55:35.0051656Z 
2020-10-01T21:55:35.0051947Z ""GC Thread#2"" os_prio=0 cpu=28.21ms elapsed=921.35s tid=0x00007f8bc8002800 nid=0xfab runnable  
2020-10-01T21:55:35.0052230Z 
2020-10-01T21:55:35.0052594Z ""GC Thread#3"" os_prio=0 cpu=29.39ms elapsed=921.35s tid=0x00007f8bc8004000 nid=0xfac runnable  
2020-10-01T21:55:35.0052871Z 
2020-10-01T21:55:35.0053157Z ""GC Thread#4"" os_prio=0 cpu=27.78ms elapsed=921.35s tid=0x00007f8bc8005800 nid=0xfad runnable  
2020-10-01T21:55:35.0053437Z 
2020-10-01T21:55:35.0053722Z ""GC Thread#5"" os_prio=0 cpu=28.21ms elapsed=921.35s tid=0x00007f8bc8007000 nid=0xfae runnable  
2020-10-01T21:55:35.0054000Z 
2020-10-01T21:55:35.0054290Z ""G1 Main Marker"" os_prio=0 cpu=0.88ms elapsed=921.98s tid=0x00007f8c00074800 nid=0xf73 runnable  
2020-10-01T21:55:35.0054572Z 
2020-10-01T21:55:35.0054851Z ""G1 Conc#0"" os_prio=0 cpu=0.10ms elapsed=921.98s tid=0x00007f8c00076800 nid=0xf74 runnable  
2020-10-01T21:55:35.0055127Z 
2020-10-01T21:55:35.0055518Z ""G1 Refine#0"" os_prio=0 cpu=0.83ms elapsed=921.98s tid=0x00007f8c001aa800 nid=0xf77 runnable  
2020-10-01T21:55:35.0055852Z 
2020-10-01T21:55:35.0056161Z ""G1 Young RemSet Sampling"" os_prio=0 cpu=257.38ms elapsed=921.98s tid=0x00007f8c001ac800 nid=0xf78 runnable  
2020-10-01T21:55:35.0056683Z ""VM Periodic Task Thread"" os_prio=0 cpu=657.91ms elapsed=921.90s tid=0x00007f8c00324000 nid=0xf8b waiting on condition  
2020-10-01T21:55:35.0056992Z 
2020-10-01T21:55:35.0057263Z JNI global refs: 29, weak refs: 0
{code}",,aljoscha,dian.fu,kezhuw,rmetzger,roman,sewen,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 09:45:13 UTC 2020,,,,,,,,,,"0|z0j3mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/20 00:22;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7184&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f;;;","05/Oct/20 07:09;kezhuw;After dived in, I think this could be a bug of {{CompletableFuture.get}} in openjdk 9 and all above. I suspect that {{CompletableFuture.get}} could swallow {{InterruptedException}} if the waiting future completes immediately after {{Thread.interrupt}} of parking thread.

[{{CompletableFuture.get}}|https://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/util/concurrent/CompletableFuture.java#l1984] and its complementary method [{{CompletableFuture.waitingGet}}|https://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/util/concurrent/CompletableFuture.java#l1805] have less than 40 lines of code, I copy them here for analysis.
{code:java}
    volatile Object result;       // Either the result or boxed AltResult

    public T get() throws InterruptedException, ExecutionException {
        Object r;
        if ((r = result) == null)
            r = waitingGet(true);
        return (T) reportGet(r);
    }

    /**
     * Returns raw result after waiting, or null if interruptible and
     * interrupted.
     */
    private Object waitingGet(boolean interruptible) {
        Signaller q = null;
        boolean queued = false;
        Object r;
        while ((r = result) == null) {
            if (q == null) {
                q = new Signaller(interruptible, 0L, 0L);
                if (Thread.currentThread() instanceof ForkJoinWorkerThread)
                    ForkJoinPool.helpAsyncBlocker(defaultExecutor(), q);
            }
            else if (!queued)
                queued = tryPushStack(q);
            else {
                try {
                    ForkJoinPool.managedBlock(q);
                } catch (InterruptedException ie) { // currently cannot happen
                    q.interrupted = true;
                }
                if (q.interrupted && interruptible)  // tag(INTERRUPTED): We are interrupted and about to break loop.
                    break;
            }
        }
        if (q != null && queued) {
            q.thread = null;
            if (!interruptible && q.interrupted)
                Thread.currentThread().interrupt();
            if (r == null)
                cleanStack();
        }
        if (r != null || (r = result) != null)     // tag(ASSIGNMENT): We could reach here because of interruption.
            postComplete();
        return r;
    }
{code}
I annotates {{CompletableFuture.waitingGet}} with tags {{INTERRUPTED}} and {{ASSIGNMENT}}. If {{CompletableFuture.complete}} occurs between these two tags, {{CompletableFuture.waitingGet}} will return {{result}} due to {{ASSIGNMENT}} statement, thus the interruption caught in {{INTERRUPTED}} lost.

{{CompletableFuture.complete}} is rather simple, it is merely compare-and-swap with completion posting.
{code:java}
    public boolean complete(T value) {
        boolean triggered = completeValue(value);
        postComplete();
        return triggered;
    }

    final boolean completeValue(T t) {
        return RESULT.compareAndSet(this, null, (t == null) ? NIL : t);
    }

    private static final VarHandle RESULT;

    static {
        RESULT = l.findVarHandle(CompletableFuture.class, ""result"", Object.class);
    }
{code}
In contrast to {{CompletableFuture.get}}, [{{ReentrantLock.lock}}|https://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/util/concurrent/locks/ReentrantLock.java#l252] and its complementary method [{{AbstractQueuedSynchronizer.acquire}}|https://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/util/concurrent/locks/AbstractQueuedSynchronizer.java#l1226] self-interrupt after blocking wait.

[~dian.fu] [~aljoscha] [~sewen] [~becket_qin] Does above sound sensible ? Glad to hear feedbacks.


I have also created a [github repository|https://github.com/kezhuw/openjdk-completablefuture-interruptedexception] to demonstrate this problem with ready-to-run maven project. I manually tested it with adoptopenjdk 8/9/10/11/12/13/14/15, seems that this problem exists in openjdk 9 and above, but not openjdk8. ;;;","06/Oct/20 13:07;kezhuw;Here are my imaginative execution flows for this issue, please point it out if there is something wrong.

 1. In testing thread, inside {{fetcher.runOnce}}, after putting one element to empty {{FutureCompletingBlockingQueue}}, the queue is available for pop.
 2. In drainer thread, {{QueueDrainerThread.go}} pops that element from the queue, and resets {{FutureCompletingBlockingQueue.currentFuture}} to new empty future.
 3. In drainer thread, {{QueueDrainerThread.go}} runs into next iteration, since {{FutureCompletingBlockingQueue.currentFuture}} is empty, it enters {{CompletableFuture.waitingGet}}.
 4. In testing thread, inside {{fetcher.runOnce}}, split fetcher calls {{FutureCompletingBlockingQueue.notifyAvailable}} due to idleness checking. {{FutureCompletingBlockingQueue.notifyAvailable}} will complete future step-3 waiting on. {{QueueDrainerThread}} is unparked, but does not get a chance to run.
 5. In testing thread, {{assertTrue(queue.getAvailabilityFuture().isDone())}} passes.
 6. In testing thread, {{QueueDrainerThread.shutdown}} interrupts {{QueueDrainerThread}}.
 7. In drainer thread, {{QueueDrainerThread.waitingGet.q.block}} gets its chance to run, and finds that it is interrupted. But that interrupt status will lose due to {{ASSIGNMENT}} statement in my previous comment.
 8. In drainer thread, {{QueueDrainerThread.go}}, {{FutureCompletingBlockingQueue.poll}} finds no element and resets {{FutureCompletingBlockingQueue.currentFuture}} to new empty future. {{FutureCompletingBlockingQueue.take}} falls into {{CompletableFuture.get}}. Since, interrupt status lost in step-7, {{CompletableFuture.get}} will block forever.
 9. In testing method, {{QueueDrainerThread.shutdown}} blocks forever in joining on will-not-terminated drainer thread.
 10. {{SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent}} gets stuck.

If my analysis in correct, {{SplitFetcherTest.testNotifiesOlderFutureWhenGoingIdleConcurrent}} should face this issue too, these two cases are almost same except assertion statement. But I didn't find more failed cases in [pipeline report|https://dev.azure.com/apache-flink/apache-flink/_pipeline/analytics/stageawareoutcome?definitionId=1].

Besides this, I have created two online repls ([openjdk8|https://repl.it/@kezhuw/openjdk8-completablefuture-interruptedexception#Main.java] and [openjdk11|https://repl.it/@kezhuw/openjdk11-completablefuture-interruptedexception#Main.java]) for easy evaluation. The two repls have identical code. The openjdk8 version probably will print {{Future get thread lost interrupt status after future.get}} if there is no {{Thread.sleep}} between {{futureGetThread.interrupt()}} and {{future.complete(null))}} while openjdk11 probably will print {{Future get thread lost interrupt status after future.get}} and exit with error code 1. I encountered cases that openjdk11 printed {{Future get thread lost interrupt status after future.get}}, I think it is caused by spuriously wake up from {{LockSupport.park}} before {{futureGetThread.interrupt}}.

Again, please point it out if there is something wrong. Anyway, glad to hear feedbacks.;;;","12/Oct/20 02:41;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7400&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f;;;","12/Oct/20 12:12;sewen;That sounds like a plausible explanation. Great debugging, [~kezhuw], thanks a lot.
A fix that I am applying for FLINK-19427 should also fix this here, please check my comment on that issue.;;;","12/Oct/20 16:45;kezhuw;Sorry for late update. I have reported it to openjdk, seems that it has been confirmed as [JDK-8254350|https://bugs.openjdk.java.net/browse/JDK-8254350]. I also think  [pr-13593|https://github.com/apache/flink/pull/13593] will fix this unstability.;;;","13/Oct/20 09:45;sewen;Fixed in 1.12 (master) via 401f56fe9d6b0271260edf9787cdcbfe4d03874d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint start delay is always zero for single channel tasks,FLINK-19487,13330387,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pnowojski,pnowojski,pnowojski,01/Oct/20 14:28,17/Sep/21 07:11,13/Jul/23 08:12,23/Apr/21 06:47,1.11.2,,,,,1.12.0,1.13.0,,Runtime / Checkpointing,Runtime / Metrics,,,,0,stale-assigned,,,,,"In {{CheckpointBarrierAligner}} we are marking start of the checkpoint only if the number of channels is more than 1. As a result of this, {{checkpointStartDelay}} is never updated.",,kezhuw,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 23 06:47:37 UTC 2021,,,,,,,,,,"0|z0j334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/21 10:52;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","23/Apr/21 06:47;pnowojski;
merged to master as 1d43462cae9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kubernetes pyflink application test failed with ""error executing jsonpath ""{range .items[*]}{.metadata.name}{\""\\n\""}{end}"": Error executing template: not in rang""",FLINK-19484,13330262,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,csq,dian.fu,dian.fu,01/Oct/20 00:27,13/Oct/20 12:15,13/Jul/23 08:12,13/Oct/20 12:15,1.12.0,,,,,1.12.0,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7139&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-09-30T21:14:41.0570715Z 2020-09-30 21:14:41,056 INFO  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create flink application cluster flink-native-k8s-pyflink-application-1 successfully, JobManager Web Interface: http://10.1.0.4:30141
2020-09-30T21:15:11.2323195Z error: timed out waiting for the condition on deployments/flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.2384760Z Stopping job timeout watchdog (with pid=111683)
2020-09-30T21:15:11.2386729Z Debugging failed Kubernetes test:
2020-09-30T21:15:11.2387191Z Currently existing Kubernetes resources
2020-09-30T21:15:11.3502610Z NAME                                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
2020-09-30T21:15:11.3506194Z service/flink-native-k8s-pyflink-application-1        ClusterIP   None           <none>        6123/TCP,6124/TCP   31s
2020-09-30T21:15:11.3507403Z service/flink-native-k8s-pyflink-application-1-rest   NodePort    10.104.215.1   <none>        8081:30141/TCP      31s
2020-09-30T21:15:11.3529743Z service/kubernetes                                    ClusterIP   10.96.0.1      <none>        443/TCP             18m
2020-09-30T21:15:11.3530391Z 
2020-09-30T21:15:11.3531349Z NAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE
2020-09-30T21:15:11.3532200Z deployment.apps/flink-native-k8s-pyflink-application-1   0/1     0            0           31s
2020-09-30T21:15:11.4887105Z Name:              flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.4887491Z Namespace:         default
2020-09-30T21:15:11.4888028Z Labels:            app=flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.4888534Z                    type=flink-native-kubernetes
2020-09-30T21:15:11.4888843Z Annotations:       <none>
2020-09-30T21:15:11.4890558Z Selector:          app=flink-native-k8s-pyflink-application-1,component=jobmanager,type=flink-native-kubernetes
2020-09-30T21:15:11.4900945Z Type:              ClusterIP
2020-09-30T21:15:11.4901267Z IP:                None
2020-09-30T21:15:11.4903601Z Port:              jobmanager-rpc  6123/TCP
2020-09-30T21:15:11.4903911Z TargetPort:        6123/TCP
2020-09-30T21:15:11.4904155Z Endpoints:         <none>
2020-09-30T21:15:11.4907175Z Port:              blobserver  6124/TCP
2020-09-30T21:15:11.4907586Z TargetPort:        6124/TCP
2020-09-30T21:15:11.4907842Z Endpoints:         <none>
2020-09-30T21:15:11.4908063Z Session Affinity:  None
2020-09-30T21:15:11.4908298Z Events:            <none>
2020-09-30T21:15:11.4970434Z 
2020-09-30T21:15:11.4970653Z 
2020-09-30T21:15:11.4971488Z Name:                     flink-native-k8s-pyflink-application-1-rest
2020-09-30T21:15:11.4971843Z Namespace:                default
2020-09-30T21:15:11.4972563Z Labels:                   app=flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.4973297Z                           type=flink-native-kubernetes
2020-09-30T21:15:11.4973749Z Annotations:              <none>
2020-09-30T21:15:11.4974568Z Selector:                 app=flink-native-k8s-pyflink-application-1,component=jobmanager,type=flink-native-kubernetes
2020-09-30T21:15:11.4974969Z Type:                     NodePort
2020-09-30T21:15:11.4976265Z IP:                       10.104.215.1
2020-09-30T21:15:11.4976620Z Port:                     rest  8081/TCP
2020-09-30T21:15:11.4976991Z TargetPort:               8081/TCP
2020-09-30T21:15:11.4977280Z NodePort:                 rest  30141/TCP
2020-09-30T21:15:11.4977860Z Endpoints:                <none>
2020-09-30T21:15:11.4978117Z Session Affinity:         None
2020-09-30T21:15:11.4978374Z External Traffic Policy:  Cluster
2020-09-30T21:15:11.4978719Z Events:                   <none>
2020-09-30T21:15:11.5048919Z 
2020-09-30T21:15:11.5049312Z 
2020-09-30T21:15:11.5049701Z Name:              kubernetes
2020-09-30T21:15:11.5050079Z Namespace:         default
2020-09-30T21:15:11.5050449Z Labels:            component=apiserver
2020-09-30T21:15:11.5050846Z                    provider=kubernetes
2020-09-30T21:15:11.5051223Z Annotations:       <none>
2020-09-30T21:15:11.5051570Z Selector:          <none>
2020-09-30T21:15:11.5051926Z Type:              ClusterIP
2020-09-30T21:15:11.5052303Z IP:                10.96.0.1
2020-09-30T21:15:11.5052663Z Port:              https  443/TCP
2020-09-30T21:15:11.5053040Z TargetPort:        8443/TCP
2020-09-30T21:15:11.5053426Z Endpoints:         10.1.0.4:8443
2020-09-30T21:15:11.5053791Z Session Affinity:  None
2020-09-30T21:15:11.5054148Z Events:            <none>
2020-09-30T21:15:11.5263442Z 
2020-09-30T21:15:11.5264696Z 
2020-09-30T21:15:11.5265861Z Name:                   flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.5266541Z Namespace:              default
2020-09-30T21:15:11.5267188Z CreationTimestamp:      Wed, 30 Sep 2020 21:14:40 +0000
2020-09-30T21:15:11.5268818Z Labels:                 app=flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.5269312Z                         component=jobmanager
2020-09-30T21:15:11.5269938Z                         type=flink-native-kubernetes
2020-09-30T21:15:11.5270236Z Annotations:            <none>
2020-09-30T21:15:11.5270925Z Selector:               app=flink-native-k8s-pyflink-application-1,component=jobmanager,type=flink-native-kubernetes
2020-09-30T21:15:11.5271468Z Replicas:               1 desired | 0 updated | 0 total | 0 available | 0 unavailable
2020-09-30T21:15:11.5271847Z StrategyType:           RollingUpdate
2020-09-30T21:15:11.5272112Z MinReadySeconds:        0
2020-09-30T21:15:11.5272434Z RollingUpdateStrategy:  25% max unavailable, 25% max surge
2020-09-30T21:15:11.5272869Z Pod Template:
2020-09-30T21:15:11.5273991Z   Labels:           app=flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.5274333Z                     component=jobmanager
2020-09-30T21:15:11.5274823Z                     type=flink-native-kubernetes
2020-09-30T21:15:11.5275116Z   Service Account:  default
2020-09-30T21:15:11.5275353Z   Containers:
2020-09-30T21:15:11.5275726Z    flink-job-manager:
2020-09-30T21:15:11.5276029Z     Image:       test_kubernetes_pyflink_application
2020-09-30T21:15:11.5276339Z     Ports:       8081/TCP, 6123/TCP, 6124/TCP
2020-09-30T21:15:11.5276646Z     Host Ports:  0/TCP, 0/TCP, 0/TCP
2020-09-30T21:15:11.5276891Z     Command:
2020-09-30T21:15:11.5277269Z       /docker-entrypoint.sh
2020-09-30T21:15:11.5277498Z     Args:
2020-09-30T21:15:11.5277849Z       native-k8s
2020-09-30T21:15:11.5279355Z       $JAVA_HOME/bin/java -classpath $FLINK_CLASSPATH -Xmx536870912 -Xms536870912 -XX:MaxMetaspaceSize=268435456 -Dlog.file=/opt/flink/log/jobmanager.log -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint
2020-09-30T21:15:11.5280350Z     Limits:
2020-09-30T21:15:11.5280749Z       cpu:     500m
2020-09-30T21:15:11.5280993Z       memory:  1088Mi
2020-09-30T21:15:11.5281211Z     Requests:
2020-09-30T21:15:11.5281434Z       cpu:     500m
2020-09-30T21:15:11.5281656Z       memory:  1088Mi
2020-09-30T21:15:11.5281889Z     Environment:
2020-09-30T21:15:11.5282359Z       _POD_IP_ADDRESS:   (v1:status.podIP)
2020-09-30T21:15:11.5283152Z     Mounts:
2020-09-30T21:15:11.5298836Z       /opt/flink/conf from flink-config-volume (rw)
2020-09-30T21:15:11.5300042Z   Volumes:
2020-09-30T21:15:11.5301257Z    flink-config-volume:
2020-09-30T21:15:11.5301588Z     Type:        ConfigMap (a volume populated by a ConfigMap)
2020-09-30T21:15:11.5302398Z     Name:        flink-config-flink-native-k8s-pyflink-application-1
2020-09-30T21:15:11.5302740Z     Optional:    false
2020-09-30T21:15:11.5302976Z OldReplicaSets:  <none>
2020-09-30T21:15:11.5303230Z NewReplicaSet:   <none>
2020-09-30T21:15:11.5303479Z Events:          <none>
2020-09-30T21:15:11.5303706Z Flink logs:
2020-09-30T21:15:11.6058156Z error: error executing jsonpath ""{range .items[*]}{.metadata.name}{\""\\n\""}{end}"": Error executing template: not in range, nothing to end. Printing more information for debugging the template:
2020-09-30T21:15:11.6058914Z 	template was:
2020-09-30T21:15:11.6059201Z 		{range .items[*]}{.metadata.name}{""\n""}{end}
2020-09-30T21:15:11.6059559Z 	object given to jsonpath engine was:
2020-09-30T21:15:11.6060319Z 		map[string]interface {}{""apiVersion"":""v1"", ""items"":[]interface {}{}, ""kind"":""List"", ""metadata"":map[string]interface {}{""resourceVersion"":"""", ""selfLink"":""""}}
2020-09-30T21:15:11.6060776Z 
2020-09-30T21:15:11.6060905Z 
2020-09-30T21:15:11.6857277Z deployment.apps ""flink-native-k8s-pyflink-application-1"" deleted
2020-09-30T21:15:11.7679475Z clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
2020-09-30T21:15:11.8457611Z error: no matching resources found
2020-09-30T21:15:11.8478156Z Stopping minikube ...
2020-09-30T21:15:12.2489617Z * Stopping ""minikube"" in none ...
2020-09-30T21:15:22.8782459Z * Node """" stopped.
2020-09-30T21:15:22.8842828Z [FAIL] Test script contains errors.
2020-09-30T21:15:22.8852045Z Checking for errors...
2020-09-30T21:15:22.9074940Z No errors in log files.
2020-09-30T21:15:22.9075558Z Checking for exceptions...
2020-09-30T21:15:22.9304666Z No exceptions in log files.
2020-09-30T21:15:22.9305787Z Checking for non-empty .out files...
2020-09-30T21:15:22.9328172Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/*.out: No such file or directory
2020-09-30T21:15:22.9330138Z No non-empty .out files.
2020-09-30T21:15:22.9331116Z 
2020-09-30T21:15:22.9331995Z [FAIL] 'Run kubernetes pyflink application test' failed after 10 minutes and 31 seconds! Test exited with exit code 1
{code}",,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 12:15:14 UTC 2020,,,,,,,,,,"0|z0j2bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/20 00:28;dian.fu;cc [~csq];;;","01/Oct/20 13:36;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7148&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","05/Oct/20 00:14;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7184&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","06/Oct/20 06:04;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7214&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","11/Oct/20 04:01;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7349&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","12/Oct/20 07:55;csq;[~dian.fu] Thank you for reporting this issue. It might be the matter of minikube resource allocation. The image size for native k8s pyflink application is a bit large, which might take some time to start up the pod. Currently, the timeout for waiting the job manager's availability are only 30s, increasing it to 120s might resolve this problem.;;;","12/Oct/20 11:52;dian.fu;[~csq] Thanks for the investigation. Could you submit a PR?;;;","13/Oct/20 04:42;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7478&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","13/Oct/20 12:15;dian.fu;Merged to master via e8ad5787355fc7bc55a81d901f9c627d83b3981b which sets the timeout to 60s.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PyFlink Table end-to-end test failed with ""FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs'""",FLINK-19483,13330261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,dian.fu,dian.fu,01/Oct/20 00:23,21/Oct/20 05:41,13/Jul/23 08:12,21/Oct/20 05:41,1.12.0,,,,,1.12.0,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7130&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-09-30T17:13:14.7489481Z Collecting package metadata (current_repodata.json): ...working... failed
2020-09-30T17:13:14.7699351Z 
2020-09-30T17:13:14.7699995Z # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<
2020-09-30T17:13:14.7700398Z 
2020-09-30T17:13:14.7700782Z     Traceback (most recent call last):
2020-09-30T17:13:14.7702095Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py"", line 107, in touch
2020-09-30T17:13:14.7702736Z         mkdir_p_sudo_safe(dirpath)
2020-09-30T17:13:14.7703608Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py"", line 84, in mkdir_p_sudo_safe
2020-09-30T17:13:14.7704221Z         os.mkdir(path)
2020-09-30T17:13:14.7704992Z     FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs'
2020-09-30T17:13:14.7705512Z     
2020-09-30T17:13:14.7705956Z     During handling of the above exception, another exception occurred:
2020-09-30T17:13:14.7706402Z     
2020-09-30T17:13:14.7706789Z     Traceback (most recent call last):
2020-09-30T17:13:14.7707615Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 185, in _load
2020-09-30T17:13:14.7708341Z         mtime = getmtime(self.cache_path_json)
2020-09-30T17:13:14.7709527Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 153, in cache_path_json
2020-09-30T17:13:14.7710340Z         return self.cache_path_base + '.json'
2020-09-30T17:13:14.7711227Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 144, in cache_path_base
2020-09-30T17:13:14.7711832Z         create_cache_dir(),
2020-09-30T17:13:14.7712821Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 645, in create_cache_dir
2020-09-30T17:13:14.7715308Z         cache_dir = join(PackageCacheData.first_writable(context.pkgs_dirs).pkgs_dir, 'cache')
2020-09-30T17:13:14.7715986Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/package_cache_data.py"", line 162, in first_writable
2020-09-30T17:13:14.7716407Z         created = create_package_cache_directory(package_cache.pkgs_dir)
2020-09-30T17:13:14.7717084Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/create.py"", line 435, in create_package_cache_directory
2020-09-30T17:13:14.7717522Z         touch(join(pkgs_dir, PACKAGE_CACHE_MAGIC_FILE), mkdir=True, sudo_safe=sudo_safe)
2020-09-30T17:13:14.7718150Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py"", line 125, in touch
2020-09-30T17:13:14.7718694Z         raise NotWritableError(path, e.errno, caused_by=e)
2020-09-30T17:13:14.7719040Z     conda.exceptions.NotWritableError: The current user does not have write permissions to a required path.
2020-09-30T17:13:14.7719797Z       path: /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt
2020-09-30T17:13:14.7720054Z       uid: 1001
2020-09-30T17:13:14.7720217Z       gid: 118
2020-09-30T17:13:14.7720375Z     
2020-09-30T17:13:14.7720625Z     If you feel that permissions on this path are set incorrectly, you can manually
2020-09-30T17:13:14.7720898Z     change them by executing
2020-09-30T17:13:14.7721072Z     
2020-09-30T17:13:14.7721513Z       $ sudo chown 1001:118 /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt
2020-09-30T17:13:14.7721778Z     
2020-09-30T17:13:14.7722334Z     In general, it's not advisable to use 'sudo conda'.
2020-09-30T17:13:14.7722539Z     
2020-09-30T17:13:14.7722680Z     
2020-09-30T17:13:14.7722946Z     During handling of the above exception, another exception occurred:
2020-09-30T17:13:14.7723200Z     
2020-09-30T17:13:14.7723407Z     Traceback (most recent call last):
2020-09-30T17:13:14.7724000Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/exceptions.py"", line 1062, in __call__
2020-09-30T17:13:14.7724516Z         return func(*args, **kwargs)
2020-09-30T17:13:14.7725075Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main.py"", line 84, in _main
2020-09-30T17:13:14.7725408Z         exit_code = do_call(args, p)
2020-09-30T17:13:14.7725983Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/conda_argparse.py"", line 82, in do_call
2020-09-30T17:13:14.7726556Z         exit_code = getattr(module, func_name)(args, parser)
2020-09-30T17:13:14.7727157Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main_install.py"", line 20, in execute
2020-09-30T17:13:14.7727688Z         install(args, parser, 'install')
2020-09-30T17:13:14.7728274Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/install.py"", line 256, in install
2020-09-30T17:13:14.7728663Z         force_reinstall=context.force_reinstall or context.force,
2020-09-30T17:13:14.7729601Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 112, in solve_for_transaction
2020-09-30T17:13:14.7729964Z         force_remove, force_reinstall)
2020-09-30T17:13:14.7730791Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 150, in solve_for_diff
2020-09-30T17:13:14.7731132Z         force_remove)
2020-09-30T17:13:14.7731878Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 249, in solve_final_state
2020-09-30T17:13:14.7732267Z         ssc = self._collect_all_metadata(ssc)
2020-09-30T17:13:14.7732844Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
2020-09-30T17:13:14.7733199Z         return f(*args, **kwds)
2020-09-30T17:13:14.7733780Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 389, in _collect_all_metadata
2020-09-30T17:13:14.7734177Z         index, r = self._prepare(prepared_specs)
2020-09-30T17:13:14.7734922Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 974, in _prepare
2020-09-30T17:13:14.7735307Z         self.subdirs, prepared_specs, self._repodata_fn)
2020-09-30T17:13:14.7736067Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/index.py"", line 214, in get_reduced_index
2020-09-30T17:13:14.7736435Z         repodata_fn=repodata_fn)
2020-09-30T17:13:14.7737001Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 91, in query_all
2020-09-30T17:13:14.7737494Z         result = tuple(concat(executor.map(subdir_query, channel_urls)))
2020-09-30T17:13:14.7738148Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py"", line 586, in result_iterator
2020-09-30T17:13:14.7738507Z         yield fs.pop().result()
2020-09-30T17:13:14.7739046Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py"", line 425, in result
2020-09-30T17:13:14.7739578Z         return self.__get_result()
2020-09-30T17:13:14.7740341Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py"", line 384, in __get_result
2020-09-30T17:13:14.7740685Z         raise self._exception
2020-09-30T17:13:14.7741217Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/thread.py"", line 57, in run
2020-09-30T17:13:14.7741767Z         result = self.fn(*self.args, **self.kwargs)
2020-09-30T17:13:14.7742581Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 87, in <lambda>
2020-09-30T17:13:14.7743099Z         package_ref_or_match_spec))
2020-09-30T17:13:14.7743680Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 96, in query
2020-09-30T17:13:14.7744019Z         self.load()
2020-09-30T17:13:14.7744563Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 160, in load
2020-09-30T17:13:14.7744932Z         _internal_state = self._load()
2020-09-30T17:13:14.7745837Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 188, in _load
2020-09-30T17:13:14.7746162Z         self.cache_path_json)
2020-09-30T17:13:14.7746724Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 153, in cache_path_json
2020-09-30T17:13:14.7747244Z         return self.cache_path_base + '.json'
2020-09-30T17:13:14.7748013Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 144, in cache_path_base
2020-09-30T17:13:14.7748381Z         create_cache_dir(),
2020-09-30T17:13:14.7749159Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 646, in create_cache_dir
2020-09-30T17:13:14.7749537Z         mkdir_p_sudo_safe(cache_dir)
2020-09-30T17:13:14.7750896Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py"", line 84, in mkdir_p_sudo_safe
2020-09-30T17:13:14.7751274Z         os.mkdir(path)
2020-09-30T17:13:14.7751795Z     FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs/cache'
{code}",,dian.fu,hxbks2ks,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 05:41:11 UTC 2020,,,,,,,,,,"0|z0j2bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/20 00:53;dian.fu;It seems because the directory ""/home/vsts/work/1/s/flink-python/dev/.conda/pkgs"" already exists and it failed to remove it because of permission issues. ;;;","13/Oct/20 14:03;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7327&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","20/Oct/20 08:51;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7903&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","20/Oct/20 09:00;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7895&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","20/Oct/20 09:19;dian.fu;cc [~hxbks2ks] Could you help to take a look at this issue?;;;","20/Oct/20 09:27;hxbks2ks;I will take a look soon.;;;","20/Oct/20 09:34;rmetzger;Thanks, I assigned you.;;;","21/Oct/20 05:41;dian.fu;Fixed in master via 2a9af7f6275166c1671bd495dc092b9845b960da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet and ORC reader reachEnd returns false after it has reached end,FLINK-19470,13330072,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,30/Sep/20 03:01,12/Oct/20 11:43,13/Jul/23 08:12,12/Oct/20 11:43,,,,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"After a {{ParquetColumnarRowSplitReader}} or {{OrcColumnarRowSplitReader}} has reached its end, calling {{reachEnd}} again gets false.",,leonard,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19365,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 11:43:44 UTC 2020,,,,,,,,,,"0|z0j15c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/20 11:43;lzljs3620320;master: b734454834d4daff80a38aa0e1925a1a8ba17a36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBase connector 2.2 failed to download dependencies ""org.glassfish:javax.el:jar:3.0.1-b06-SNAPSHOT"" ",FLINK-19469,13330068,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mgergely,dian.fu,dian.fu,30/Sep/20 01:32,29/Mar/23 11:13,13/Jul/23 08:12,22/Oct/20 01:10,1.12.0,,,,,1.12.0,,,Connectors / HBase,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7093&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20

{code}
2020-09-29T20:59:24.8085970Z [ERROR] Failed to execute goal on project flink-connector-hbase-2.2_2.11: Could not resolve dependencies for project org.apache.flink:flink-connector-hbase-2.2_2.11:jar:1.12-SNAPSHOT: Failed to collect dependencies at org.apache.hbase:hbase-server:jar:tests:2.2.3 -> org.glassfish.web:javax.servlet.jsp:jar:2.3.2 -> org.glassfish:javax.el:jar:3.0.1-b06-SNAPSHOT: Failed to read artifact descriptor for org.glassfish:javax.el:jar:3.0.1-b06-SNAPSHOT: Could not transfer artifact org.glassfish:javax.el:pom:3.0.1-b06-SNAPSHOT from/to jvnet-nexus-snapshots (https://maven.java.net/content/repositories/snapshots): Failed to transfer file: https://maven.java.net/content/repositories/snapshots/org/glassfish/javax.el/3.0.1-b06-SNAPSHOT/javax.el-3.0.1-b06-SNAPSHOT.pom. Return code is: 503 , ReasonPhrase:Service Unavailable: Back-end server is at capacity. -> [Help 1]
{code}",,dian.fu,gyfora,liyu,mapohl,mgergely,rmetzger,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31241,FLINK-31658,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 01:10:46 UTC 2020,,,,,,,,,,"0|z0j14g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/20 07:58;gyfora;Not sure what to do with this:
ReasonPhrase:Service Unavailable: Back-end server is at capacity;;;","01/Oct/20 00:32;dian.fu;[~gyfora] Agree with you. I think we can leave this issue as it for now and only handle it if it occurs frequently.;;;","03/Oct/20 00:53;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7174&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab;;;","05/Oct/20 06:43;rmetzger;Can we use a different mirror that hosts the same files?;;;","05/Oct/20 06:47;chesnay;Is that an _actual_ SNAPSHOT dependency?;;;","05/Oct/20 07:00;rmetzger;It looks a lot like it. Maybe we should consider banning SNAPSHOT dependencies?;;;","12/Oct/20 18:32;rmetzger;I observed this error now also locally.;;;","12/Oct/20 18:34;rmetzger;[~gyfora] or [~mgergely] can you see if we can upgrade (or downgrade) to a stable dependency for the Hbase2 connector? 
If it is a stable dependency, maybe define a different repository?;;;","13/Oct/20 14:22;rmetzger;Related failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7524&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb

{code}
[ERROR] Failed to execute goal on project flink-connector-hbase-2.2_2.11: Could not resolve dependencies for project org.apache.flink:flink-connector-hbase-2.2_2.11:jar:1.12-SNAPSHOT: Failed to collect dependencies at org.apache.hbase:hbase-server:jar:tests:2.2.3 -> org.glassfish.web:javax.servlet.jsp:jar:2.3.2 -> org.glassfish:javax.el:jar:3.0.1-b12: Failed to read artifact descriptor for org.glassfish:javax.el:jar:3.0.1-b12: Could not transfer artifact org.glassfish:javax.el:pom:3.0.1-b12 from/to java.net.Releases (https://maven.java.net/content/repositories/releases): Failed to transfer file: https://maven.java.net/content/repositories/releases/org/glassfish/javax.el/3.0.1-b12/javax.el-3.0.1-b12.pom. Return code is: 500 , ReasonPhrase:Internal Server Error. -> [Help 1]
{code};;;","15/Oct/20 08:01;roman;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7634&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee;;;","16/Oct/20 01:41;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7680&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","17/Oct/20 09:53;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7773&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","18/Oct/20 00:57;dian.fu;Fixed in master(1.12.0) via ce143f26b8236ddfca77c270fab31c98e906356a;;;","20/Oct/20 08:12;rmetzger;Mh, why is this still failing on master? https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7899&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3;;;","21/Oct/20 09:36;mgergely;It's still failing. hbase-mapreduce test dependency also requires it, although it is not showing up in the dependency tree. Excluding it from there too.;;;","21/Oct/20 11:56;rmetzger;Thank you!


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7987&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3;;;","21/Oct/20 14:29;mapohl;FYI: It failed on my branch as well: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8007&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20;;;","22/Oct/20 01:10;dian.fu;[~mgergely] Thanks a lot for the follow-up PR!

Merged the follow-up fix(Exclude the transitive dependency from hbase-mapreduce) via 215a12b01a5992724162b99d1991bb4e57a56670;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-dist won't build locally with newer (3.3+) maven versions,FLINK-19459,13329990,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,rmetzger,rmetzger,29/Sep/20 17:51,16/Oct/20 10:57,13/Jul/23 08:12,16/Oct/20 10:38,1.12.0,,,,,1.12.0,,,Build System,,,,,0,pull-request-available,,,,,"flink-dist will fail on non Maven 3.2.5 versions because of banned dependencies.

These are the messages you'll see:
{code}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) @ flink-dist_2.11 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: org.yaml:snakeyaml:jar:1.24
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Flink : 1.12-SNAPSHOT:

...

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) on project flink-dist_2.11: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
{code}",,hailong wang,klion26,libenchao,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19455,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 10:38:03 UTC 2020,,,,,,,,,,"0|z0j0n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/20 22:12;hailong wang;Hi, it also failed in flink-docs and flink-sql-connector-hive-2.3.6 module, https://issues.apache.org/jira/browse/FLINK-19455 ,mayble It could be the same problem.;;;","16/Oct/20 10:38;rmetzger;Resolved https://github.com/apache/flink/commit/18772b93612f748d7253a868c0d6a0801bbe74f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange: ZooKeeper unexpectedly modified,FLINK-19458,13329983,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,29/Sep/20 17:09,15/Apr/21 06:46,13/Jul/23 08:12,20/Oct/20 13:19,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8422&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a

{code}
2020-09-29T13:34:18.1803081Z [ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 23.524 s  <<< ERROR!
2020-09-29T13:34:18.1803707Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
2020-09-29T13:34:18.1804343Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-09-29T13:34:18.1804738Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-09-29T13:34:18.1805274Z 	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:117)
2020-09-29T13:34:18.1805772Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-29T13:34:18.1806136Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-29T13:34:18.1806555Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-29T13:34:18.1806936Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-29T13:34:18.1807313Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-29T13:34:18.1807731Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-29T13:34:18.1808341Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-29T13:34:18.1808973Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-29T13:34:18.1809376Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-29T13:34:18.1809851Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-29T13:34:18.1810201Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-29T13:34:18.1810632Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-29T13:34:18.1811035Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-29T13:34:18.1811700Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-29T13:34:18.1812082Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-29T13:34:18.1812447Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-29T13:34:18.1812824Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-29T13:34:18.1813190Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-29T13:34:18.1813565Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-29T13:34:18.1813964Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-29T13:34:18.1814364Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-29T13:34:18.1814752Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-29T13:34:18.1815298Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-29T13:34:18.1816096Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-29T13:34:18.1816552Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-29T13:34:18.1816984Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-29T13:34:18.1817421Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-29T13:34:18.1817894Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-29T13:34:18.1818318Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-29T13:34:18.1818888Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-29T13:34:18.1819294Z 	Suppressed: org.apache.flink.util.FlinkException: Could not close resource.
2020-09-29T13:34:18.1819698Z 		at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:42)
2020-09-29T13:34:18.1820260Z 		at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:136)
2020-09-29T13:34:18.1820678Z 		... 30 more
2020-09-29T13:34:18.1821326Z 	Caused by: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token not set: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(deregisterApplication(ApplicationStatus, String))) sent to akka://flink/user/rpc/resourcemanager_4 because the fencing token is null.
2020-09-29T13:34:18.1822143Z 		at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:63)
2020-09-29T13:34:18.1822621Z 		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-29T13:34:18.1823024Z 		at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-29T13:34:18.1823397Z 		at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-29T13:34:18.1823776Z 		at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-29T13:34:18.1824306Z 		at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-29T13:34:18.1824686Z 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-29T13:34:18.1825066Z 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-29T13:34:18.1825528Z 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-29T13:34:18.1825883Z 		at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-29T13:34:18.1826238Z 		at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-29T13:34:18.1826579Z 		at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-29T13:34:18.1826912Z 		at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-29T13:34:18.1827302Z 		at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-29T13:34:18.1827609Z 		at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-29T13:34:18.1827912Z 		at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-29T13:34:18.1828252Z 		at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-29T13:34:18.1828629Z 		at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-29T13:34:18.1829062Z 		at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-29T13:34:18.1829468Z 		at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-29T13:34:18.1829871Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
2020-09-29T13:34:18.1830321Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$2(Dispatcher.java:348)
2020-09-29T13:34:18.1830936Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-09-29T13:34:18.1831356Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-09-29T13:34:18.1831797Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-09-29T13:34:18.1832231Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-29T13:34:18.1832645Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-29T13:34:18.1833180Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-29T13:34:18.1833761Z Caused by: java.util.ConcurrentModificationException: ZooKeeper unexpectedly modified
2020-09-29T13:34:18.1834413Z 	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:158)
2020-09-29T13:34:18.1834954Z 	at org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.putJobGraph(ZooKeeperJobGraphStore.java:228)
2020-09-29T13:34:18.1835452Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.persistAndRunJob(Dispatcher.java:356)
2020-09-29T13:34:18.1835943Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$waitForTerminatingJob$28(Dispatcher.java:827)
2020-09-29T13:34:18.1836432Z 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedConsumer$3(FunctionUtils.java:94)
2020-09-29T13:34:18.1837422Z 	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
2020-09-29T13:34:18.1837857Z 	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
2020-09-29T13:34:18.1838282Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-09-29T13:34:18.1838730Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
2020-09-29T13:34:18.1839184Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
2020-09-29T13:34:18.1839647Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-09-29T13:34:18.1840123Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-29T13:34:18.1840531Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-29T13:34:18.1840895Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-29T13:34:18.1841272Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-29T13:34:18.1841659Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-29T13:34:18.1842029Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-29T13:34:18.1842490Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-29T13:34:18.1842878Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-29T13:34:18.1843223Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-29T13:34:18.1843742Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-29T13:34:18.1844151Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-29T13:34:18.1844467Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-29T13:34:18.1844793Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-29T13:34:18.1845114Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-29T13:34:18.1845399Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-29T13:34:18.1845735Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-29T13:34:18.1846125Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-29T13:34:18.1846508Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-29T13:34:18.1846905Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-29T13:34:18.1847397Z Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists
2020-09-29T13:34:18.1847933Z 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:122)
2020-09-29T13:34:18.1848418Z 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1015)
2020-09-29T13:34:18.1848889Z 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:919)
2020-09-29T13:34:18.1849421Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:197)
2020-09-29T13:34:18.1850005Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.access$000(CuratorTransactionImpl.java:37)
2020-09-29T13:34:18.1850598Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:130)
2020-09-29T13:34:18.1851818Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:126)
2020-09-29T13:34:18.1865959Z 	at org.apache.flink.shaded.curator4.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64)
2020-09-29T13:34:18.1866608Z 	at org.apache.flink.shaded.curator4.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)
2020-09-29T13:34:18.1867149Z 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:123)
2020-09-29T13:34:18.1867700Z 	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:152)
2020-09-29T13:34:18.1868034Z 	... 29 more
2020-09-29T13:34:18.1868164Z 
{code}",,dian.fu,maguowei,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19514,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 15 06:46:49 UTC 2021,,,,,,,,,,"0|z0j0lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/20 00:17;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7184&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","05/Oct/20 11:49;rmetzger;I'll have a look at this failure ...

The test failure seems to occur only on the azure provided VMs.


;;;","07/Oct/20 06:05;rmetzger;Again, Azure VM: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7253&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56;;;","13/Oct/20 09:07;rmetzger;I was able to reproduce the issue, even though it took me a lot of time to get there :(
This time, it failed with 
{code}
2020-10-07T09:56:42.2689556Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 80.636 s <<< FAILURE! - in org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase
2020-10-07T09:56:42.2690627Z [ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 75.698 s  <<< ERROR!
2020-10-07T09:56:42.2691316Z java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: JobMaster has been shut down.
2020-10-07T09:56:42.2691868Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-10-07T09:56:42.2692353Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-10-07T09:56:42.2693091Z 	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.lambda$testJobExecutionOnClusterWithLeaderChange$0(ZooKeeperLeaderElectionITCase.java:136)
2020-10-07T09:56:42.2694004Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:126)
2020-10-07T09:56:42.2694749Z 	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:136)
2020-10-07T09:56:42.2695392Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-07T09:56:42.2695844Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-07T09:56:42.2696413Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-07T09:56:42.2696912Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-07T09:56:42.2697387Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-07T09:56:42.2697955Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-07T09:56:42.2698513Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-07T09:56:42.2699052Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-07T09:56:42.2699579Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-07T09:56:42.2700064Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-10-07T09:56:42.2700486Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-07T09:56:42.2700926Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-07T09:56:42.2701431Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-07T09:56:42.2701968Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-07T09:56:42.2702468Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-07T09:56:42.2702934Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-07T09:56:42.2703391Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-07T09:56:42.2704224Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-07T09:56:42.2704701Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-07T09:56:42.2705251Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-10-07T09:56:42.2705784Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-07T09:56:42.2706269Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-07T09:56:42.2706751Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-07T09:56:42.2707328Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-07T09:56:42.2707915Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-07T09:56:42.2708468Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-07T09:56:42.2709062Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-07T09:56:42.2709679Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-07T09:56:42.2710308Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-07T09:56:42.2710821Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-07T09:56:42.2711329Z Caused by: org.apache.flink.util.FlinkException: JobMaster has been shut down.
2020-10-07T09:56:42.2711885Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.closeAsync(JobManagerRunnerImpl.java:184)
2020-10-07T09:56:42.2712462Z 	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
2020-10-07T09:56:42.2713069Z 	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137)
2020-10-07T09:56:42.2714022Z 	at org.apache.flink.runtime.dispatcher.DispatcherJob.lambda$closeAsync$7(DispatcherJob.java:243)
2020-10-07T09:56:42.2715078Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-10-07T09:56:42.2715652Z 	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848)
2020-10-07T09:56:42.2716197Z 	at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168)
2020-10-07T09:56:42.2716723Z 	at org.apache.flink.runtime.dispatcher.DispatcherJob.closeAsync(DispatcherJob.java:240)
2020-10-07T09:56:42.2717272Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.removeJob(Dispatcher.java:652)
2020-10-07T09:56:42.2717870Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.removeJobAndRegisterTerminationFuture(Dispatcher.java:624)
2020-10-07T09:56:42.2718467Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.jobNotFinished(Dispatcher.java:758)
2020-10-07T09:56:42.2719037Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.dispatcherJobFailed(Dispatcher.java:397)
2020-10-07T09:56:42.2719601Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$3(Dispatcher.java:386)
2020-10-07T09:56:42.2720133Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-10-07T09:56:42.2720680Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-10-07T09:56:42.2721240Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-10-07T09:56:42.2721800Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
2020-10-07T09:56:42.2722358Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
2020-10-07T09:56:42.2722965Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-10-07T09:56:42.2723566Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-10-07T09:56:42.2724068Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-10-07T09:56:42.2724534Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-10-07T09:56:42.2725013Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-10-07T09:56:42.2725484Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-10-07T09:56:42.2725979Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-10-07T09:56:42.2726469Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-10-07T09:56:42.2726938Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-10-07T09:56:42.2727395Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-10-07T09:56:42.2727853Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-10-07T09:56:42.2728297Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-10-07T09:56:42.2728734Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-10-07T09:56:42.2729160Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-10-07T09:56:42.2729557Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-10-07T09:56:42.2729949Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-10-07T09:56:42.2730383Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-10-07T09:56:42.2730967Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-10-07T09:56:42.2731480Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-10-07T09:56:42.2731996Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-10-07T09:56:42.2732298Z 
{code}
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8448&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=5129dea2-618b-5c74-1b8f-9ec63a37a8a6
;;;","19/Oct/20 13:11;rmetzger;I might have found first hint to go after. The logs are pretty full with messages like this

{code}
09:55:39,565 [        SyncThread:0] WARN  org.apache.zookeeper.server.persistence.FileTxnLog           [] - fsync-ing the write ahead log in SyncThread:0 took 6744ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide
09:55:44,114 [main-SendThread(localhost:45487)] WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Client session timed out, have not heard from server in 4000ms for sessionid 0x1000015fcfc0000
09:55:44,114 [main-SendThread(localhost:45487)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Client session timed out, have not heard from server in 4000ms for sessionid 0x1000015fcfc0000, closing socket connection and attempting reconnect
09:55:44,114 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:45487] WARN  org.apache.zookeeper.server.NIOServerCnxn                    [] - Unable to read additional data from client sessionid 0x1000015fcfc0000, likely client has closed socket
09:55:44,115 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:45487] INFO  org.apache.zookeeper.server.NIOServerCnxn                    [] - Closed socket connection for client /127.0.0.1:59498 which had sessionid 0x1000015fcfc0000
{code}

Which means: The disk is so slow, that writing the ZK txn log to it took 6744ms. The timeout is configured to 4000ms (2/3 of the session timeout).
There is a code path that sets the session timeout in a CI environment from 5000 to 30000. This check probably didn't work since we switched to Azure.
I will fix the check to set the timeout to 30000 on Azure as well.

;;;","20/Oct/20 13:19;rmetzger;Most likely resolved on master in https://github.com/apache/flink/commit/947b3d34daf01d15e6ad2e693e35e6637336e49f.;;;","15/Apr/21 06:46;maguowei;1.11
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16567&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323&l=3835;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LEAD/LAG cannot work correctly in streaming mode,FLINK-19449,13329893,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,libenchao,libenchao,29/Sep/20 09:03,28/May/21 11:05,13/Jul/23 08:12,28/Apr/21 09:24,1.10.2,1.11.2,,,,1.13.1,1.14.0,,Table SQL / Runtime,,,,,1,pull-request-available,stale-major,,,,,,fsk119,godfreyhe,jark,jingzhang,knaufk,laiyijie,leonard,libenchao,lzljs3620320,xiaodong.chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20100,,,,,,,,,,,,,,,,,FLINK-20405,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 28 09:24:12 UTC 2021,,,,,,,,,,"0|z0j01k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/21 08:00;laiyijie;在流式处理中遇到同样的问题

sql语句为

select *, lead(`at`) over ( partition by bond_id order by t_start ) as `lag_at`

lag_at 的值会和当前行的值一致，lead方法也同样有问题;;;","03/Mar/21 00:12;libenchao;[~laiyijie] Thanks for your comment, please use English in Jira issues.

And for your comment, yes, your observation is correct. For now, lead/lag is only correctly implemented in Batch SQL, and they are special from other aggregate functions. In Streaming SQL, I think we can support LAG firstly, and throw Exception for LEAD.;;;","07/Mar/21 07:00;laiyijie;Support LAG firstly is a good idea. I think LAG is a basic function. It's important to fixed thi bug. we can store the list of value . The size of the list is the lag's offset param. remove the first element when retract . return the first value when get value. ;;;","07/Mar/21 23:39;libenchao;[~laiyijie] IMO, a full-fledged LAG function implementation with `merge`, `retract` is complex. We cannot assume the records are retracted in the order of appending, hence just remove the first element when retract maybe not correct.

In our internal branch, I see a lot of usage of LAG without index. And I just implement a simplest one which does not support `retract`, `merge`, and the index either. What do you think?;;;","08/Mar/21 03:49;laiyijie;LI, Implementation  without the index may not be so good, but is a temporary solution. The `retract` and `merge` function is not so importand for LAG. I suggest to implement a LAG function with index. In the streaming mode , it could cover more situation than that without index .

I have tried to implement a UDF function for lag without `retract` and `merge` , but the UDF function has a restriction which cannot support different type of data in single one function name `LAG`. 

I test  the UDF in the streaming mode, and the result is correct.  And I think the `accumulate` function is called by order in streaming mode in over window with orderby.

finnaly what I wanna to resolve was the problem of cannot using lag in streaming mode.  Of course with index.

THX for your reading, I really wanna to fix this problem  and make flink better in basic functions.

 

Here is my UDF of `LAG` with index.

 

public class LagAggFunction<T> extends AggregateFunction<T, LagAggFunction.Acc<T>> {
 protected Integer offset = 1;

 public static class Acc<T> {
 LinkedList<T> window = new LinkedList<>();
 }

 @Override
 public Acc<T> createAccumulator() {
 return new Acc<T>();
 }

 @Override
 public T getValue(Acc<T> acc) {
 if (acc.window.size() - 1 < this.offset){
 return null;
 }
 return acc.window.getFirst();
 }

 public void accumulate(Acc<T> acc, T iValue, Integer offset) {
 this.offset = offset;
 if (this.offset == null) {
 this.offset = 1;
 }
 acc.window.add(iValue);
 if (acc.window.size() - 1 > this.offset) {
 acc.window.removeFirst();
 }
 }

 public void resetAccumulator(Acc<T> acc) {
 acc.window = new LinkedList<T>();
 }

 public static class FloatLag extends LagAggFunction<Float> {}
}

 

 

 ;;;","22/Apr/21 11:06;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","23/Apr/21 04:36;lzljs3620320;Hi [~laiyijie], do you want to fix this?;;;","25/Apr/21 02:55;lzljs3620320;I'll take this JIRA.;;;","26/Apr/21 03:29;lzljs3620320;Fixed via:

master:

362f4aea1eec38472d94fc8527d8e1275110b03c

9b9d4f08227570d670051e7249a10d4af19be3bc

bf8f998fbd6cacd14e24f71727aaf86d89c90643

075b0b22996e642e32bf581f8bff43eb69f5c5d4;;;","28/Apr/21 06:52;lzljs3620320;re-open for 1.13;;;","28/Apr/21 09:24;lzljs3620320;Fixed via:

release-1.13: c623db4dcf7549fed263ab6f92aac49a94a25897;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatedSourceITCase.testEnumeratorReaderCommunication hangs,FLINK-19448,13329865,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kezhuw,dian.fu,dian.fu,29/Sep/20 07:07,25/Dec/20 01:23,13/Jul/23 08:12,10/Nov/20 19:03,1.12.0,,,,,1.11.3,1.12.0,,Connectors / Common,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91

{code}
2020-09-28T21:40:41.0736918Z [INFO] Running org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase
2020-09-28T21:57:23.4590733Z ==============================================================================
2020-09-28T21:57:23.4591238Z Process produced no output for 900 seconds.
2020-09-28T21:57:23.4591593Z ==============================================================================
2020-09-28T21:57:23.4595995Z ==============================================================================
2020-09-28T21:57:23.4596439Z The following Java processes are running (JPS)
2020-09-28T21:57:23.4596789Z ==============================================================================
2020-09-28T21:57:23.4638075Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-09-28T21:57:23.6127853Z 21907 surefirebooter2023202237772619676.jar
2020-09-28T21:57:23.6128185Z 534 Launcher
2020-09-28T21:57:23.6128381Z 24630 Jps
2020-09-28T21:57:23.6159852Z ==============================================================================
2020-09-28T21:57:23.6160256Z Printing stack trace of Java process 21907
2020-09-28T21:57:23.6160806Z ==============================================================================
2020-09-28T21:57:23.6203860Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-09-28T21:57:23.9470219Z 2020-09-28 21:57:23
2020-09-28T21:57:23.9471512Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-09-28T21:57:23.9472274Z 
2020-09-28T21:57:23.9472805Z ""Attach Listener"" #215 daemon prio=9 os_prio=0 tid=0x00007f13c8074800 nid=0x6052 waiting on condition [0x0000000000000000]
2020-09-28T21:57:23.9473343Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9473660Z 
2020-09-28T21:57:23.9474554Z ""flink-akka.actor.default-dispatcher-103"" #214 prio=5 os_prio=0 tid=0x00007f13cc1b5000 nid=0x6018 waiting on condition [0x00007f13bb4f5000]
2020-09-28T21:57:23.9475189Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9475815Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9476662Z 	- parking to wait for  <0x0000000087a80408> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-09-28T21:57:23.9477295Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-09-28T21:57:23.9477871Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-28T21:57:23.9480210Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-28T21:57:23.9480723Z 
2020-09-28T21:57:23.9481669Z ""flink-taskexecutor-io-thread-8"" #125 daemon prio=5 os_prio=0 tid=0x00007f13e401d000 nid=0x571b waiting on condition [0x00007f13e84f8000]
2020-09-28T21:57:23.9482321Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9482727Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9483562Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9484241Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9484899Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9485585Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9486194Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9486818Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9487440Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9487970Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9488278Z 
2020-09-28T21:57:23.9489120Z ""flink-taskexecutor-io-thread-7"" #124 daemon prio=5 os_prio=0 tid=0x00007f13e401c800 nid=0x571a waiting on condition [0x00007f13b8f46000]
2020-09-28T21:57:23.9489760Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9490190Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9491003Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9491667Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9492821Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9493524Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9494139Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9494754Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9496451Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9496992Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9497201Z 
2020-09-28T21:57:23.9498016Z ""flink-taskexecutor-io-thread-6"" #123 daemon prio=5 os_prio=0 tid=0x00007f13e4019000 nid=0x5719 waiting on condition [0x00007f13e82f6000]
2020-09-28T21:57:23.9498481Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9498788Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9500245Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9500756Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9501473Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9502044Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9502519Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9503017Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9503595Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9503995Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9504209Z 
2020-09-28T21:57:23.9504942Z ""Flink-DispatcherRestEndpoint-thread-4"" #113 daemon prio=5 os_prio=0 tid=0x00007f13f801c800 nid=0x5637 waiting on condition [0x00007f13e88fa000]
2020-09-28T21:57:23.9505431Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9505743Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9506407Z 	- parking to wait for  <0x0000000087a80e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9506909Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9507460Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9508086Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-09-28T21:57:23.9508870Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9509439Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9509922Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9513273Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9513750Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9513951Z 
2020-09-28T21:57:23.9514797Z ""Flink-DispatcherRestEndpoint-thread-3"" #112 daemon prio=5 os_prio=0 tid=0x00007f13dc09c800 nid=0x562c waiting on condition [0x00007f13babf0000]
2020-09-28T21:57:23.9515275Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9515582Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9517202Z 	- parking to wait for  <0x0000000087a80e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9518111Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9518674Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9519312Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-09-28T21:57:23.9519922Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9520489Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9520990Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9521478Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9521891Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9522095Z 
2020-09-28T21:57:23.9522967Z ""Flink-DispatcherRestEndpoint-thread-2"" #110 daemon prio=5 os_prio=0 tid=0x00007f13dc024000 nid=0x561e waiting on condition [0x00007f13e87f9000]
2020-09-28T21:57:23.9523467Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9523759Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9524463Z 	- parking to wait for  <0x0000000087a80e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9525134Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9525671Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9526313Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-09-28T21:57:23.9526940Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9527561Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9528066Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9528570Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9528969Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9529182Z 
2020-09-28T21:57:23.9529954Z ""SourceCoordinator-Source: TestingSource -> Sink: Unnamed"" #107 prio=5 os_prio=0 tid=0x00007f13dc0a9000 nid=0x5611 waiting on condition [0x00007f13ba156000]
2020-09-28T21:57:23.9530505Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9530819Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9531489Z 	- parking to wait for  <0x0000000087a81460> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9532000Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9532555Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9533109Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9533600Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9534100Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9534591Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9535012Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9535207Z 
2020-09-28T21:57:23.9535569Z ""CloseableReaperThread"" #105 daemon prio=5 os_prio=0 tid=0x00007f13d0024800 nid=0x560f in Object.wait() [0x00007f13ba257000]
2020-09-28T21:57:23.9536032Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-28T21:57:23.9536342Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:23.9536920Z 	- waiting on <0x0000000087a816e8> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-28T21:57:23.9537348Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-09-28T21:57:23.9537954Z 	- locked <0x0000000087a816e8> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-28T21:57:23.9538370Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-09-28T21:57:23.9539931Z 	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:209)
2020-09-28T21:57:23.9540324Z 
2020-09-28T21:57:23.9541134Z ""Source: TestingSource -> Sink: Unnamed (2/4)"" #103 prio=5 os_prio=0 tid=0x00007f13d81db000 nid=0x560d waiting on condition [0x00007f13b8e45000]
2020-09-28T21:57:23.9542119Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9545357Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9547428Z 	- parking to wait for  <0x0000000087a81890> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9547946Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9548949Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9550987Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:146)
2020-09-28T21:57:23.9551574Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:298)
2020-09-28T21:57:23.9552333Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:183)
2020-09-28T21:57:23.9552915Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594)
2020-09-28T21:57:23.9556038Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558)
2020-09-28T21:57:23.9556541Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-09-28T21:57:23.9556973Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-09-28T21:57:23.9557477Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9557695Z 
2020-09-28T21:57:23.9561877Z ""AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1"" #97 daemon prio=5 os_prio=0 tid=0x00007f13e00dc000 nid=0x5607 waiting on condition [0x00007f13b944b000]
2020-09-28T21:57:23.9562627Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9565093Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9570829Z 	- parking to wait for  <0x0000000087a81d20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9575651Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9576427Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9594221Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9598708Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9599269Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9599778Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9600181Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9600393Z 
2020-09-28T21:57:23.9601273Z ""flink-taskexecutor-io-thread-5"" #96 daemon prio=5 os_prio=0 tid=0x00007f13d81da000 nid=0x5606 waiting on condition [0x00007f13b954c000]
2020-09-28T21:57:23.9601763Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9602069Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9602803Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9603323Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9603860Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9604439Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9604935Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9605438Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9605929Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9606352Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9606549Z 
2020-09-28T21:57:23.9607253Z ""flink-taskexecutor-io-thread-4"" #95 daemon prio=5 os_prio=0 tid=0x00007f13d817e800 nid=0x5605 waiting on condition [0x00007f13ba459000]
2020-09-28T21:57:23.9607730Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9608020Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9608696Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9609192Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9609749Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9610323Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9610800Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9611507Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9612018Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9612415Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9612629Z 
2020-09-28T21:57:23.9613327Z ""flink-taskexecutor-io-thread-3"" #94 daemon prio=5 os_prio=0 tid=0x00007f13d817d800 nid=0x5604 waiting on condition [0x00007f13b8d44000]
2020-09-28T21:57:23.9613912Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9614219Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9614908Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9615418Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9615968Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9616525Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9617016Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9617515Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9618002Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9618419Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9618620Z 
2020-09-28T21:57:23.9619297Z ""flink-taskexecutor-io-thread-2"" #93 daemon prio=5 os_prio=0 tid=0x00007f13d817c800 nid=0x5603 waiting on condition [0x00007f13ba65b000]
2020-09-28T21:57:23.9620126Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9620429Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9621171Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9621691Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9622227Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9622795Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9623289Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9623772Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9624276Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9624689Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9624887Z 
2020-09-28T21:57:23.9625567Z ""flink-taskexecutor-io-thread-1"" #92 daemon prio=5 os_prio=0 tid=0x00007f13d8179800 nid=0x5602 waiting on condition [0x00007f13ba75c000]
2020-09-28T21:57:23.9626029Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9626340Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9627016Z 	- parking to wait for  <0x0000000087a80860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9627506Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9628056Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9629291Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9629807Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9630310Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9630817Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9631218Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9631432Z 
2020-09-28T21:57:23.9632324Z ""Flink-MetricRegistry-thread-1"" #63 daemon prio=5 os_prio=0 tid=0x00007f13dc02a000 nid=0x55e3 waiting on condition [0x00007f13bacf1000]
2020-09-28T21:57:23.9632818Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:23.9633136Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9633803Z 	- parking to wait for  <0x0000000087a828a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9634320Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:23.9634992Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-28T21:57:23.9635629Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-28T21:57:23.9636253Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9636809Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9637299Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9637806Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9638220Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9638416Z 
2020-09-28T21:57:23.9639071Z ""pool-2-thread-1"" #60 prio=5 os_prio=0 tid=0x00007f13d8062000 nid=0x55e0 waiting on condition [0x00007f13bb3f4000]
2020-09-28T21:57:23.9639501Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9639806Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9642766Z 	- parking to wait for  <0x0000000087a82ae8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9646981Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9653370Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9661523Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-09-28T21:57:23.9677357Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9679468Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9682652Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9683181Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9691283Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9691503Z 
2020-09-28T21:57:23.9697420Z ""jobmanager-future-thread-2"" #57 daemon prio=5 os_prio=0 tid=0x00007f13d8057800 nid=0x55dd waiting on condition [0x00007f13bb2f3000]
2020-09-28T21:57:23.9697929Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9698233Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9698936Z 	- parking to wait for  <0x0000000087a82d48> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9699453Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9699993Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9700634Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-09-28T21:57:23.9706073Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9706660Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9708027Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9708690Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9709275Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9709496Z 
2020-09-28T21:57:23.9710354Z ""jobmanager-future-thread-1"" #52 daemon prio=5 os_prio=0 tid=0x00007f13fc006000 nid=0x55ce waiting on condition [0x00007f13bb1f2000]
2020-09-28T21:57:23.9711981Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:23.9712340Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9713125Z 	- parking to wait for  <0x0000000087a82d48> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9713788Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:23.9714363Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-28T21:57:23.9714998Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-28T21:57:23.9715632Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9716545Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9717049Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9717562Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9718752Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9718980Z 
2020-09-28T21:57:23.9719773Z ""mini-cluster-io-thread-8"" #47 daemon prio=5 os_prio=0 tid=0x00007f13f0006800 nid=0x55c9 waiting on condition [0x00007f13bb7f6000]
2020-09-28T21:57:23.9720529Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9720867Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9721624Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9722132Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9722688Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9723258Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9723734Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9724235Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9724748Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9725149Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9725990Z 
2020-09-28T21:57:23.9727058Z ""mini-cluster-io-thread-7"" #46 daemon prio=5 os_prio=0 tid=0x00007f13f0005000 nid=0x55c8 waiting on condition [0x00007f13bb8f7000]
2020-09-28T21:57:23.9727545Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9727864Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9728536Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9729060Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9729614Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9730172Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9730674Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9731176Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9731668Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9732083Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9732279Z 
2020-09-28T21:57:23.9733140Z ""mini-cluster-io-thread-6"" #45 daemon prio=5 os_prio=0 tid=0x00007f13e0006000 nid=0x55c7 waiting on condition [0x00007f13bb9f8000]
2020-09-28T21:57:23.9733609Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9733902Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9734576Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9735083Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9735712Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9749725Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9750270Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9750760Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9751282Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9751701Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9751899Z 
2020-09-28T21:57:23.9752776Z ""mini-cluster-io-thread-5"" #44 daemon prio=5 os_prio=0 tid=0x00007f13e4002800 nid=0x55c6 waiting on condition [0x00007f13bbaf9000]
2020-09-28T21:57:23.9753233Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9753540Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9754248Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9754743Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9755298Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9755868Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9756352Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9756854Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9757357Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9757757Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9757970Z 
2020-09-28T21:57:23.9758625Z ""mini-cluster-io-thread-4"" #43 daemon prio=5 os_prio=0 tid=0x00007f13bd54c000 nid=0x55c5 waiting on condition [0x00007f13bbbfa000]
2020-09-28T21:57:23.9759097Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9759406Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9760067Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9760573Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9761126Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9761689Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9762182Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9762683Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9763174Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9763595Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9763791Z 
2020-09-28T21:57:23.9764454Z ""mini-cluster-io-thread-3"" #42 daemon prio=5 os_prio=0 tid=0x00007f13d801c800 nid=0x55c4 waiting on condition [0x00007f13bbcfb000]
2020-09-28T21:57:23.9764922Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9765214Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9765885Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9766588Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9767126Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9767696Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9769412Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9770047Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9770555Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9770973Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9771170Z 
2020-09-28T21:57:23.9771943Z ""mini-cluster-io-thread-2"" #41 daemon prio=5 os_prio=0 tid=0x00007f13bd547000 nid=0x55c3 waiting on condition [0x00007f13bbdfc000]
2020-09-28T21:57:23.9772402Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9772708Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9773371Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9773881Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9774436Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9775011Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9775487Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9775988Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9776493Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9776892Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9777107Z 
2020-09-28T21:57:23.9777787Z ""Flink-DispatcherRestEndpoint-thread-1"" #39 daemon prio=5 os_prio=0 tid=0x00007f13bd529000 nid=0x55c1 waiting on condition [0x00007f13bbefd000]
2020-09-28T21:57:23.9778283Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:23.9778601Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9779258Z 	- parking to wait for  <0x0000000087a80e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9779777Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:23.9780336Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-28T21:57:23.9780990Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-28T21:57:23.9781613Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9782178Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9782664Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9783171Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9783581Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9783777Z 
2020-09-28T21:57:23.9784443Z ""mini-cluster-io-thread-1"" #38 daemon prio=5 os_prio=0 tid=0x00007f1414c2c000 nid=0x55c0 waiting on condition [0x00007f13bbffe000]
2020-09-28T21:57:23.9784895Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9785201Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9785857Z 	- parking to wait for  <0x0000000087a83170> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9786360Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9787010Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9787581Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9788059Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9788713Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9789309Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9789709Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9789923Z 
2020-09-28T21:57:23.9790639Z ""flink-rest-server-netty-boss-thread-1"" #37 daemon prio=5 os_prio=0 tid=0x00007f13bd523800 nid=0x55bf runnable [0x00007f13c48e6000]
2020-09-28T21:57:23.9791090Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9791576Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-09-28T21:57:23.9792121Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-09-28T21:57:23.9792575Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-09-28T21:57:23.9793006Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-09-28T21:57:23.9793799Z 	- locked <0x0000000087a842d0> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-09-28T21:57:23.9794485Z 	- locked <0x0000000087a842c0> (a java.util.Collections$UnmodifiableSet)
2020-09-28T21:57:23.9795075Z 	- locked <0x0000000087a84278> (a sun.nio.ch.EPollSelectorImpl)
2020-09-28T21:57:23.9795464Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-09-28T21:57:23.9796024Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-09-28T21:57:23.9796658Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-09-28T21:57:23.9797222Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-09-28T21:57:23.9798242Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-09-28T21:57:23.9798893Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-09-28T21:57:23.9799364Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9799578Z 
2020-09-28T21:57:23.9799949Z ""IOManager reader thread #1"" #31 daemon prio=5 os_prio=0 tid=0x00007f1415151800 nid=0x55bb waiting on condition [0x00007f13c47be000]
2020-09-28T21:57:23.9800407Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9800699Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9801928Z 	- parking to wait for  <0x0000000087a84538> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9803039Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9803965Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9804873Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9805730Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-09-28T21:57:23.9806066Z 
2020-09-28T21:57:23.9806445Z ""IOManager writer thread #1"" #30 daemon prio=5 os_prio=0 tid=0x00007f141514f000 nid=0x55ba waiting on condition [0x00007f13c4afa000]
2020-09-28T21:57:23.9806912Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9807204Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9807993Z 	- parking to wait for  <0x0000000087a84740> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9808490Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9809192Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:23.9809764Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:23.9810289Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-09-28T21:57:23.9810634Z 
2020-09-28T21:57:23.9811266Z ""Timer-2"" #28 daemon prio=5 os_prio=0 tid=0x00007f14150f9000 nid=0x55b9 in Object.wait() [0x00007f13c49e7000]
2020-09-28T21:57:23.9811814Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-09-28T21:57:23.9812154Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:23.9812694Z 	- waiting on <0x0000000087a84948> (a java.util.TaskQueue)
2020-09-28T21:57:23.9813061Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-09-28T21:57:23.9813613Z 	- locked <0x0000000087a84948> (a java.util.TaskQueue)
2020-09-28T21:57:23.9813947Z 	at java.util.TimerThread.run(Timer.java:505)
2020-09-28T21:57:23.9814173Z 
2020-09-28T21:57:23.9814750Z ""Timer-1"" #26 daemon prio=5 os_prio=0 tid=0x00007f14150f6000 nid=0x55b8 in Object.wait() [0x00007f13c4bfb000]
2020-09-28T21:57:23.9815208Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-09-28T21:57:23.9815545Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:23.9816051Z 	- waiting on <0x0000000087a84b28> (a java.util.TaskQueue)
2020-09-28T21:57:23.9816415Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-09-28T21:57:23.9817018Z 	- locked <0x0000000087a84b28> (a java.util.TaskQueue)
2020-09-28T21:57:23.9817360Z 	at java.util.TimerThread.run(Timer.java:505)
2020-09-28T21:57:23.9817578Z 
2020-09-28T21:57:23.9817929Z ""BLOB Server listener at 46483"" #22 daemon prio=5 os_prio=0 tid=0x00007f14150f3000 nid=0x55b7 runnable [0x00007f13c4cfc000]
2020-09-28T21:57:23.9818357Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9818684Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-09-28T21:57:23.9819087Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-09-28T21:57:23.9819551Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-09-28T21:57:23.9819936Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-09-28T21:57:23.9820364Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)
2020-09-28T21:57:23.9820621Z 
2020-09-28T21:57:23.9821232Z ""Timer-0"" #23 daemon prio=5 os_prio=0 tid=0x00007f14150a1800 nid=0x55b6 in Object.wait() [0x00007f13c4ffd000]
2020-09-28T21:57:23.9821695Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-09-28T21:57:23.9822017Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:23.9822540Z 	- waiting on <0x0000000087a85168> (a java.util.TaskQueue)
2020-09-28T21:57:23.9822906Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-09-28T21:57:23.9823440Z 	- locked <0x0000000087a85168> (a java.util.TaskQueue)
2020-09-28T21:57:23.9823789Z 	at java.util.TimerThread.run(Timer.java:505)
2020-09-28T21:57:23.9823999Z 
2020-09-28T21:57:23.9824635Z ""flink-metrics-scheduler-1"" #18 prio=5 os_prio=0 tid=0x00007f1415064800 nid=0x55b2 waiting on condition [0x00007f13e83f7000]
2020-09-28T21:57:23.9825085Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-09-28T21:57:23.9825404Z 	at java.lang.Thread.sleep(Native Method)
2020-09-28T21:57:23.9825820Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-09-28T21:57:23.9826362Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-09-28T21:57:23.9827122Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-09-28T21:57:23.9827547Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9827758Z 
2020-09-28T21:57:23.9829451Z ""flink-scheduler-1"" #14 prio=5 os_prio=0 tid=0x00007f1414c85000 nid=0x55ae waiting on condition [0x00007f13e89fb000]
2020-09-28T21:57:23.9829947Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-09-28T21:57:23.9830402Z 	at java.lang.Thread.sleep(Native Method)
2020-09-28T21:57:23.9830792Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-09-28T21:57:23.9831345Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-09-28T21:57:23.9831901Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-09-28T21:57:23.9832320Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9832599Z 
2020-09-28T21:57:23.9832943Z ""process reaper"" #11 daemon prio=10 os_prio=0 tid=0x00007f13c8030000 nid=0x55ab waiting on condition [0x00007f13e9439000]
2020-09-28T21:57:23.9833394Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:23.9833709Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9834451Z 	- parking to wait for  <0x000000008006e918> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-09-28T21:57:23.9834943Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:23.9835465Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-09-28T21:57:23.9836000Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-09-28T21:57:23.9836502Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-09-28T21:57:23.9836982Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-09-28T21:57:23.9837474Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9837981Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9838391Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9838588Z 
2020-09-28T21:57:23.9839272Z ""surefire-forkedjvm-ping-30s"" #10 daemon prio=5 os_prio=0 tid=0x00007f141434a800 nid=0x55a8 waiting on condition [0x00007f13e99c9000]
2020-09-28T21:57:23.9839743Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:23.9840063Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9840727Z 	- parking to wait for  <0x000000008006eb78> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:23.9841241Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:23.9841815Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-28T21:57:23.9842470Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-28T21:57:23.9843078Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:23.9843632Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:23.9844245Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:23.9844753Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:23.9845164Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9845360Z 
2020-09-28T21:57:23.9846040Z ""surefire-forkedjvm-command-thread"" #9 daemon prio=5 os_prio=0 tid=0x00007f1414333800 nid=0x55a7 runnable [0x00007f14041f8000]
2020-09-28T21:57:23.9846483Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9846787Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-09-28T21:57:23.9847158Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-09-28T21:57:23.9847566Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-09-28T21:57:23.9848006Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-09-28T21:57:23.9848623Z 	- locked <0x000000008006ee18> (a java.io.BufferedInputStream)
2020-09-28T21:57:23.9849002Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-09-28T21:57:23.9849489Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-09-28T21:57:23.9850148Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-09-28T21:57:23.9850574Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:23.9850786Z 
2020-09-28T21:57:23.9851108Z ""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007f1414202800 nid=0x55a5 runnable [0x0000000000000000]
2020-09-28T21:57:23.9851509Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9851751Z 
2020-09-28T21:57:23.9852108Z ""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f14141ff800 nid=0x55a4 waiting on condition [0x0000000000000000]
2020-09-28T21:57:23.9852538Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9852718Z 
2020-09-28T21:57:23.9853073Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f14141fd000 nid=0x55a3 waiting on condition [0x0000000000000000]
2020-09-28T21:57:23.9853482Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9853684Z 
2020-09-28T21:57:23.9854005Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f14141fb000 nid=0x55a2 runnable [0x0000000000000000]
2020-09-28T21:57:23.9854406Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9854585Z 
2020-09-28T21:57:23.9854979Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f14141f9800 nid=0x55a1 waiting on condition [0x0000000000000000]
2020-09-28T21:57:23.9855438Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:23.9855621Z 
2020-09-28T21:57:23.9855962Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f14141c9800 nid=0x55a0 in Object.wait() [0x00007f14182f8000]
2020-09-28T21:57:23.9856393Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-28T21:57:23.9856718Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:23.9857330Z 	- waiting on <0x000000008006f798> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-28T21:57:23.9857741Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-09-28T21:57:23.9858369Z 	- locked <0x000000008006f798> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-28T21:57:23.9858783Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-09-28T21:57:23.9859202Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-09-28T21:57:23.9859468Z 
2020-09-28T21:57:23.9859810Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f14141c5000 nid=0x559f in Object.wait() [0x00007f14183f9000]
2020-09-28T21:57:23.9860267Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-28T21:57:23.9860604Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:23.9860900Z 	at java.lang.Object.wait(Object.java:502)
2020-09-28T21:57:23.9861280Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-09-28T21:57:23.9861895Z 	- locked <0x000000008006f950> (a java.lang.ref.Reference$Lock)
2020-09-28T21:57:23.9862297Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-09-28T21:57:23.9862564Z 
2020-09-28T21:57:23.9862875Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f141400b800 nid=0x5596 waiting on condition [0x00007f141de0b000]
2020-09-28T21:57:23.9879457Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:23.9879924Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:23.9880710Z 	- parking to wait for  <0x0000000087a855c8> (a java.util.concurrent.CompletableFuture$Signaller)
2020-09-28T21:57:23.9881191Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:23.9881680Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-09-28T21:57:23.9882170Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-09-28T21:57:23.9882650Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-09-28T21:57:23.9883135Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-09-28T21:57:23.9883627Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:671)
2020-09-28T21:57:23.9884338Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-09-28T21:57:23.9884947Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1706)
2020-09-28T21:57:23.9885589Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1688)
2020-09-28T21:57:23.9886253Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.executeAndVerify(CoordinatedSourceITCase.java:84)
2020-09-28T21:57:23.9887031Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.testEnumeratorReaderCommunication(CoordinatedSourceITCase.java:52)
2020-09-28T21:57:23.9887586Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-28T21:57:23.9888010Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-28T21:57:23.9888533Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-28T21:57:23.9888992Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-28T21:57:23.9889427Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-28T21:57:23.9889947Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-28T21:57:23.9890464Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-28T21:57:23.9890962Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-28T21:57:23.9891430Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-28T21:57:23.9891821Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-28T21:57:23.9892230Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-28T21:57:23.9892701Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-28T21:57:23.9893193Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-28T21:57:23.9893659Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-28T21:57:23.9894085Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-28T21:57:23.9894512Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-28T21:57:23.9894951Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-28T21:57:23.9895388Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-28T21:57:23.9895827Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-28T21:57:23.9896286Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-28T21:57:23.9896709Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-28T21:57:23.9897093Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-28T21:57:23.9897553Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-28T21:57:23.9898083Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-28T21:57:23.9898609Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-28T21:57:23.9899132Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-28T21:57:23.9899683Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-28T21:57:23.9900239Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-28T21:57:23.9900748Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-28T21:57:23.9901234Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-28T21:57:23.9901507Z 
2020-09-28T21:57:23.9901776Z ""VM Thread"" os_prio=0 tid=0x00007f14141bb800 nid=0x559e runnable 
2020-09-28T21:57:23.9901994Z 
2020-09-28T21:57:23.9902370Z ""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007f1414020800 nid=0x5597 runnable 
2020-09-28T21:57:23.9902632Z 
2020-09-28T21:57:23.9902940Z ""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007f1414022000 nid=0x5598 runnable 
2020-09-28T21:57:23.9903193Z 
2020-09-28T21:57:23.9903492Z ""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007f1414046000 nid=0x559c runnable 
2020-09-28T21:57:23.9903745Z 
2020-09-28T21:57:23.9904062Z ""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f1414047800 nid=0x559d runnable 
2020-09-28T21:57:23.9904410Z 
2020-09-28T21:57:23.9904710Z ""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007f1414028000 nid=0x559b runnable 
2020-09-28T21:57:23.9904964Z 
2020-09-28T21:57:23.9905265Z ""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007f1414026000 nid=0x559a runnable 
2020-09-28T21:57:23.9905518Z 
2020-09-28T21:57:23.9905816Z ""G1 Concurrent Refinement Thread#2"" os_prio=0 tid=0x00007f1414024800 nid=0x5599 runnable 
2020-09-28T21:57:23.9906077Z 
2020-09-28T21:57:23.9906381Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f1414205800 nid=0x55a6 waiting on condition 
2020-09-28T21:57:23.9906642Z 
2020-09-28T21:57:23.9906849Z JNI global references: 1426
2020-09-28T21:57:23.9907016Z 
2020-09-28T21:57:23.9907295Z ==============================================================================
2020-09-28T21:57:23.9907632Z Printing stack trace of Java process 534
2020-09-28T21:57:23.9907981Z ==============================================================================
2020-09-28T21:57:23.9909557Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-09-28T21:57:24.2975554Z 2020-09-28 21:57:24
2020-09-28T21:57:24.2976240Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-09-28T21:57:24.2976578Z 
2020-09-28T21:57:24.2983040Z ""Attach Listener"" #321 daemon prio=9 os_prio=0 tid=0x00007f12fc005800 nid=0x6060 waiting on condition [0x0000000000000000]
2020-09-28T21:57:24.2983527Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.2983816Z 
2020-09-28T21:57:24.2984551Z ""Thread-127"" #319 daemon prio=5 os_prio=0 tid=0x00007f130c01d800 nid=0x5595 runnable [0x00007f12e7596000]
2020-09-28T21:57:24.2984969Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.2985285Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-09-28T21:57:24.2985638Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-09-28T21:57:24.2986064Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-09-28T21:57:24.2986504Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-09-28T21:57:24.2987176Z 	- locked <0x00000000def7c180> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-09-28T21:57:24.2987606Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-09-28T21:57:24.2988006Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-09-28T21:57:24.2988412Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-09-28T21:57:24.2989286Z 	- locked <0x00000000def81350> (a java.io.InputStreamReader)
2020-09-28T21:57:24.2989683Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-09-28T21:57:24.2990090Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-09-28T21:57:24.2990494Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-09-28T21:57:24.2991075Z 	- locked <0x00000000def81350> (a java.io.InputStreamReader)
2020-09-28T21:57:24.2991463Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-09-28T21:57:24.2991970Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-09-28T21:57:24.2992319Z 
2020-09-28T21:57:24.2992910Z ""Thread-126"" #318 daemon prio=5 os_prio=0 tid=0x00007f130c01c800 nid=0x5594 runnable [0x00007f12e6f20000]
2020-09-28T21:57:24.2993306Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.2993623Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-09-28T21:57:24.2993991Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-09-28T21:57:24.2994639Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-09-28T21:57:24.2995083Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-09-28T21:57:24.2995782Z 	- locked <0x00000000def7a0c0> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-09-28T21:57:24.2996197Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-09-28T21:57:24.2996610Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-09-28T21:57:24.2997012Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-09-28T21:57:24.2997729Z 	- locked <0x00000000def7e748> (a java.io.InputStreamReader)
2020-09-28T21:57:24.2998126Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-09-28T21:57:24.2998515Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-09-28T21:57:24.2998920Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-09-28T21:57:24.2999510Z 	- locked <0x00000000def7e748> (a java.io.InputStreamReader)
2020-09-28T21:57:24.2999891Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-09-28T21:57:24.3000399Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-09-28T21:57:24.3000741Z 
2020-09-28T21:57:24.3001366Z ""Thread-125"" #317 daemon prio=5 os_prio=0 tid=0x00007f130c01b800 nid=0x5592 waiting on condition [0x00007f12e7495000]
2020-09-28T21:57:24.3001814Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3002105Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3002716Z 	- parking to wait for  <0x00000000dee62550> (a java.util.concurrent.Semaphore$NonfairSync)
2020-09-28T21:57:24.3003178Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3003714Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
2020-09-28T21:57:24.3004379Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
2020-09-28T21:57:24.3005061Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
2020-09-28T21:57:24.3005589Z 	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
2020-09-28T21:57:24.3006158Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestLessInputStream.awaitNextCommand(TestLessInputStream.java:165)
2020-09-28T21:57:24.3006882Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestLessInputStream.beforeNextCommand(TestLessInputStream.java:136)
2020-09-28T21:57:24.3007574Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.AbstractCommandStream.read(AbstractCommandStream.java:100)
2020-09-28T21:57:24.3012904Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.feed(StreamFeeder.java:123)
2020-09-28T21:57:24.3014525Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.run(StreamFeeder.java:60)
2020-09-28T21:57:24.3015881Z 
2020-09-28T21:57:24.3017173Z ""ThreadedStreamConsumer"" #315 daemon prio=5 os_prio=0 tid=0x00007f130c01a800 nid=0x5590 waiting on condition [0x00007f12e7798000]
2020-09-28T21:57:24.3019315Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3021382Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3022314Z 	- parking to wait for  <0x00000000def45660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3022827Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3023379Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:24.3024066Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-09-28T21:57:24.3024632Z 	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:83)
2020-09-28T21:57:24.3025279Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3025479Z 
2020-09-28T21:57:24.3026200Z ""surefire-fork-starter"" #314 daemon prio=5 os_prio=0 tid=0x00007f12d17af800 nid=0x558f in Object.wait() [0x00007f12e789c000]
2020-09-28T21:57:24.3026677Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-28T21:57:24.3026990Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:24.3027525Z 	- waiting on <0x00000000def77e48> (a java.lang.UNIXProcess)
2020-09-28T21:57:24.3027875Z 	at java.lang.Object.wait(Object.java:502)
2020-09-28T21:57:24.3028358Z 	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
2020-09-28T21:57:24.3029123Z 	- locked <0x00000000def77e48> (a java.lang.UNIXProcess)
2020-09-28T21:57:24.3029639Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.CommandLineUtils$1.call(CommandLineUtils.java:260)
2020-09-28T21:57:24.3030222Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:614)
2020-09-28T21:57:24.3030775Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-09-28T21:57:24.3031318Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-09-28T21:57:24.3031840Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-09-28T21:57:24.3032609Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-28T21:57:24.3033081Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-28T21:57:24.3033580Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3033994Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3034191Z 
2020-09-28T21:57:24.3034882Z ""ping-timer-10s"" #312 daemon prio=5 os_prio=0 tid=0x00007f132e852000 nid=0x558e waiting on condition [0x00007f12e7697000]
2020-09-28T21:57:24.3035344Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:24.3035645Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3036329Z 	- parking to wait for  <0x00000000de6d84d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3036845Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:24.3037402Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-28T21:57:24.3038051Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-28T21:57:24.3038675Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:24.3039223Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3039721Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3040225Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3040629Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3040840Z 
2020-09-28T21:57:24.3041480Z ""timeout-check-timer"" #311 daemon prio=5 os_prio=0 tid=0x00007f132e851000 nid=0x558d waiting on condition [0x00007f12e6c1d000]
2020-09-28T21:57:24.3041950Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-28T21:57:24.3042265Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3042922Z 	- parking to wait for  <0x00000000de6d8db8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3043442Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-28T21:57:24.3044008Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-28T21:57:24.3044639Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-28T21:57:24.3045263Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-28T21:57:24.3045942Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3046428Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3046930Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3047341Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3047602Z 
2020-09-28T21:57:24.3047942Z ""process reaper"" #289 daemon prio=10 os_prio=0 tid=0x00007f12e001a000 nid=0x54b7 runnable [0x00007f12e705a000]
2020-09-28T21:57:24.3048333Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.3048666Z 	at java.lang.UNIXProcess.waitForProcessExit(Native Method)
2020-09-28T21:57:24.3049066Z 	at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)
2020-09-28T21:57:24.3049461Z 	at java.lang.UNIXProcess$$Lambda$7/59217951.run(Unknown Source)
2020-09-28T21:57:24.3049901Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-28T21:57:24.3050408Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3050806Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3051017Z 
2020-09-28T21:57:24.3051669Z ""resolver-5"" #16 daemon prio=5 os_prio=0 tid=0x00007f132df24800 nid=0x230 waiting on condition [0x00007f12e79b5000]
2020-09-28T21:57:24.3054788Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3055141Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3055926Z 	- parking to wait for  <0x0000000094fd64f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3056437Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3056977Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:24.3057552Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:24.3058045Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3058542Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3059031Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3059447Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3059648Z 
2020-09-28T21:57:24.3060278Z ""resolver-4"" #15 daemon prio=5 os_prio=0 tid=0x00007f132df24000 nid=0x22f waiting on condition [0x00007f12e7cfd000]
2020-09-28T21:57:24.3060704Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3061011Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3061728Z 	- parking to wait for  <0x0000000094fd64f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3062221Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3062780Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:24.3063349Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:24.3063826Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3064326Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3064834Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3065235Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3065444Z 
2020-09-28T21:57:24.3066063Z ""resolver-3"" #13 daemon prio=5 os_prio=0 tid=0x00007f132dc18000 nid=0x22d waiting on condition [0x00007f12e7dfe000]
2020-09-28T21:57:24.3066506Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3066811Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3067648Z 	- parking to wait for  <0x0000000094fd64f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3068156Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3069049Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:24.3069624Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:24.3070243Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3070744Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3071237Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3071652Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3071850Z 
2020-09-28T21:57:24.3072543Z ""resolver-2"" #12 daemon prio=5 os_prio=0 tid=0x00007f132dac7000 nid=0x22c waiting on condition [0x00007f12f04e5000]
2020-09-28T21:57:24.3072997Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3073289Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3073967Z 	- parking to wait for  <0x0000000094fd64f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3074477Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3075016Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:24.3075591Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:24.3076083Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3076568Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3077069Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3077483Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3077681Z 
2020-09-28T21:57:24.3078303Z ""resolver-1"" #11 daemon prio=5 os_prio=0 tid=0x00007f132dc04800 nid=0x22b waiting on condition [0x00007f12f05e6000]
2020-09-28T21:57:24.3078733Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3079039Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3079710Z 	- parking to wait for  <0x0000000094fd64f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-28T21:57:24.3080208Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3080757Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-09-28T21:57:24.3081323Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-09-28T21:57:24.3081799Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-28T21:57:24.3082300Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-28T21:57:24.3082801Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-28T21:57:24.3083200Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-28T21:57:24.3083410Z 
2020-09-28T21:57:24.3083732Z ""Service Thread"" #7 daemon prio=9 os_prio=0 tid=0x00007f132c0bc000 nid=0x227 runnable [0x0000000000000000]
2020-09-28T21:57:24.3084136Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.3084322Z 
2020-09-28T21:57:24.3084678Z ""C1 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f132c0b9000 nid=0x226 waiting on condition [0x0000000000000000]
2020-09-28T21:57:24.3085104Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.3085287Z 
2020-09-28T21:57:24.3085639Z ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f132c0b6000 nid=0x225 waiting on condition [0x0000000000000000]
2020-09-28T21:57:24.3086046Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.3086350Z 
2020-09-28T21:57:24.3086673Z ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f132c0b4800 nid=0x224 runnable [0x0000000000000000]
2020-09-28T21:57:24.3087079Z    java.lang.Thread.State: RUNNABLE
2020-09-28T21:57:24.3087258Z 
2020-09-28T21:57:24.3087594Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f132c084800 nid=0x223 in Object.wait() [0x00007f131c5f4000]
2020-09-28T21:57:24.3088033Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-28T21:57:24.3088347Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:24.3089023Z 	- waiting on <0x00000000951fb148> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-28T21:57:24.3089446Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-09-28T21:57:24.3090057Z 	- locked <0x00000000951fb148> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-28T21:57:24.3090471Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-09-28T21:57:24.3090890Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-09-28T21:57:24.3091164Z 
2020-09-28T21:57:24.3092959Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f132c080000 nid=0x222 in Object.wait() [0x00007f131c6f5000]
2020-09-28T21:57:24.3094988Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-28T21:57:24.3095317Z 	at java.lang.Object.wait(Native Method)
2020-09-28T21:57:24.3095981Z 	- waiting on <0x00000000951fb188> (a java.lang.ref.Reference$Lock)
2020-09-28T21:57:24.3096349Z 	at java.lang.Object.wait(Object.java:502)
2020-09-28T21:57:24.3096743Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-09-28T21:57:24.3097345Z 	- locked <0x00000000951fb188> (a java.lang.ref.Reference$Lock)
2020-09-28T21:57:24.3097756Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-09-28T21:57:24.3098015Z 
2020-09-28T21:57:24.3098337Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f132c00c800 nid=0x21e waiting on condition [0x00007f13351b9000]
2020-09-28T21:57:24.3098744Z    java.lang.Thread.State: WAITING (parking)
2020-09-28T21:57:24.3099042Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-28T21:57:24.3099626Z 	- parking to wait for  <0x00000000dea8eb00> (a java.util.concurrent.FutureTask)
2020-09-28T21:57:24.3100069Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-09-28T21:57:24.3100498Z 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
2020-09-28T21:57:24.3100920Z 	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
2020-09-28T21:57:24.3101412Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:476)
2020-09-28T21:57:24.3101997Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-09-28T21:57:24.3102558Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-09-28T21:57:24.3103084Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-09-28T21:57:24.3103631Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-09-28T21:57:24.3104266Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-09-28T21:57:24.3104884Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-09-28T21:57:24.3105442Z 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-09-28T21:57:24.3105989Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-09-28T21:57:24.3106492Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-09-28T21:57:24.3106984Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-09-28T21:57:24.3107543Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-09-28T21:57:24.3108148Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-09-28T21:57:24.3109086Z 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-09-28T21:57:24.3110696Z 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-09-28T21:57:24.3111201Z 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-09-28T21:57:24.3111622Z 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-09-28T21:57:24.3112176Z 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-09-28T21:57:24.3112582Z 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-09-28T21:57:24.3112967Z 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-09-28T21:57:24.3113351Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-28T21:57:24.3113783Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-28T21:57:24.3114293Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-28T21:57:24.3114747Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-28T21:57:24.3115196Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-09-28T21:57:24.3115685Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-09-28T21:57:24.3116188Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-09-28T21:57:24.3116695Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-09-28T21:57:24.3116964Z 
2020-09-28T21:57:24.3117226Z ""VM Thread"" os_prio=0 tid=0x00007f132c076000 nid=0x221 runnable 
2020-09-28T21:57:24.3117440Z 
2020-09-28T21:57:24.3117732Z ""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f132c022000 nid=0x21f runnable 
2020-09-28T21:57:24.3117979Z 
2020-09-28T21:57:24.3118270Z ""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f132c023800 nid=0x220 runnable 
2020-09-28T21:57:24.3118524Z 
2020-09-28T21:57:24.3118824Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f132c0be800 nid=0x228 waiting on condition 
2020-09-28T21:57:24.3119083Z 
2020-09-28T21:57:24.3119286Z JNI global references: 853
{code}",,aljoscha,dian.fu,kezhuw,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 19:03:25 UTC 2020,,,,,,,,,,"0|z0izvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/20 15:47;sewen;I looked through the logs and the code, and I cannot spot this bug.

From the logs, one can see that splits are processed and all fetchers shut down. Still, one task does not exit, which means one of the following:
  - Either the wrong status is returned ({{NOTHING_AVAILABLE}} instead of {{END_OF_INPUT}} preventing the source from exiting.
  - The right status is returned, but the mailbox does not handle it correctly.

I am expecting that the first case is happening, to confirm this, I will push a temporary patch to enable TRACE logging for this component. Then, the next time this hangs, we can confirm the issue.
;;;","12/Oct/20 17:34;kezhuw;Neither did I. But I found {{SourceReaderBase}} may shadowed exception from {{SplitReader.fetch}} as {{InputStatus.END_OF_INPUT}}. I haven't verify it using test code, post it for early evaluation. Here are my imaginative execution flows:
1. In mailbox thread, we enters {{SourceReaderBase.getNextFetch}}. After executes {{splitFetcherManager.checkErrors()}} but before {{elementsQueue.poll()}}, {{SplitFetcher}} gets its chance to run.
2. {{SplitFetcher}} terminates due to exception from {{SplitReader.fetch}}. {{SplitFetcher.shutdownHook}} will removes this exceptional fetcher from {{SplitFetcherManager}}.
3. In mailbox thread,  {{elementsQueue.poll()}} executes. If there is no elements in queue, {{elementsQueue}} will be reset to unavailable.
4. After getting no elements from {{SourceReaderBase.getNextFetch}}, we will enter {{SourceReaderBase.finishedOrAvailableLater}}. If the exceptional fetcher is last alive fetcher, then {{SourceReaderBase.finishedOrAvailableLater}} may evaluate to {{InputStatus.END_OF_INPUT}}
5. {{StreamTask}} will terminate itself due to {{InputStatus.END_OF_INPUT}}.

Does it sound sensible? I think this could be verified by mocking if correct.

;;;","13/Oct/20 09:35;sewen;[~kezhuw] Good observation. If you can reproduce this in a test, that would be great.
Maybe the fix would be to always check for errors before returning {{NOTHING_AVAILABLE}}.;;;","13/Oct/20 09:36;sewen;Better invariant condition guarding changes in 68c5c2ff779d82a1ff81ffaf60d8a1b283797db1
Temporarily activating trace logging in d0e41a04b8bec19aedf5ebe63950bc24958dcb9d;;;","03/Nov/20 00:24;kezhuw;[~sewen] Seems that [68c5c2ff779d82a1ff81ffaf60d8a1b283797db1|https://github.com/apache/flink/commit/68c5c2ff779d82a1ff81ffaf60d8a1b283797db1] assume {{elementsQueue}} must be empty when {{noMoreSplitsAssignment && allFetchersHaveShutdown}} meet. I think this will may not hold in multiple splits case since {{SourceReaderBase.pollNext}} may fall into {{finishedOrAvailableLater}} due to pure finished records from {{elementsQueue.poll}} in {{getNextFetch}}. I pushed commit [e72c3c61bf7bf255cbb788d5d8c428903f8a889d|https://github.com/kezhuw/flink/commit/e72c3c61bf7bf255cbb788d5d8c428903f8a889d] in my local branch to demonstrate this. This commit fails constantly in recently master, but succeeds in original code.;;;","03/Nov/20 16:01;kezhuw;I think I may find the cause.

First, let's see what javadoc say about {{size()}} and {{isEmpty()}}:
{quote}Bear in mind that the results of aggregate status methods including size, isEmpty, and containsValue are typically useful only when a map is not undergoing concurrent updates in other threads. Otherwise the results of these methods reflect transient states that may be adequate for monitoring or estimation purposes, but not for program control.
{quote}
Second, let's see simplified source code of {{ConcurrentHashMap.remove}} and its complementary method {{ConcurrentHashMap.replaceNode}}:
{code:java}
```java
    public V remove(Object key) {
        return replaceNode(key, null, null);
    }

    /**
     * Implementation for the four public remove/replace methods:
     * Replaces node value with v, conditional upon match of cv if
     * non-null.  If resulting value is null, delete.
     */
    final V replaceNode(Object key, V value, Object cv) {
        int hash = spread(key.hashCode());
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0 ||
                (f = tabAt(tab, i = (n - 1) & hash)) == null)
                break;                         // tag(absent)
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);
            else {
                V oldVal = null;
                boolean validated = false;
                synchronized (f) {      // tag(removing)
                    if (tabAt(tab, i) == f) {
                        // ... replacing code omitted here
                    }
                }
                if (validated) {        // tag(size-counting-synchronization)
                    if (oldVal != null) {
                        if (value == null)
                            addCount(-1L, -1);
                        return oldVal;
                    }
                    break;
                }
            }
        }
        return null;
    }
```
{code}

There is no synchronization around key-removing and size-counting. Seems that key-removing happens-before size-counting in concurrent sense.

Finally, I have published two online repls to demonstrate this for [openjdk8|https://repl.it/@kezhuw/openjdk8-concurrent-hashmap-unsynchronized-size] and [openjdk11|https://repl.it/@kezhuw/openjdk11-concurrent-hashmap-unsynchronized-size]. Both will fail after a number of runs which may exceed one thousand in extreme cases.

I am kind of straggling about whether this is a bug or concurrent nature of {{ConcurrentHashMap}}. I think it is normal to not rely on {{size}} to detect concurrent key removing. But after {{ConcurrentHashMap.remove}}, sequential {{ConcurrentHashMap.size()}} or {{ConcurrentHashMap.isEmpty()}} should reflect that removing, otherwise it is somewhat counterintuitive from my perspective. I will report this to OpenJDK in next few days if no further inspiration.

Back to Flink side, in {{SplitFetcherManager.maybeShutdownFinishedFetchers}} idle-fetcher-removing runs concurrently with {{SplitFetcher.shutdownHook}}. {{fetchers.isEmpty()}} may return {{false}} even if last entry removed. This will make {{SourceReaderBase.finishedOrAvailableLater}} erroneously returns {{InputStatus.NOTHING_AVAILABLE}} instead of {{InputStatus.END_OF_INPUT}}.;;;","06/Nov/20 15:46;kezhuw;I think we can fix this by adding {{elementsQueue.notifyAvailable()}} after {{fetchers.remove(fetcherId)}}. This way we are guaranteed that:
 * If above availability is consumed by {{elementsQueue.poll()}}, {{fetchers.isEmpty}} will get latest size-counting info since updating of size-counting info happens-before above availability.
 * Otherwise, source reader will resume polling due to above availability.

 Besides this, I think it doesn't change any internal api semantics and small enough.

[~sewen] Could you assign this ticket to me if above make sense to you ? I will open pr in next two days with following changes:
 1. Add test case in {{SourceReaderBaseTest}} to test multiple splits with separating finished splits. This will fail current code.
 2. Revert [68c5c2ff779d82a1ff81ffaf60d8a1b283797db1|https://github.com/apache/flink/commit/68c5c2ff779d82a1ff81ffaf60d8a1b283797db1] to pass above test case.
 3. Add {{elementsQueue.notifyAvailable}} after {{fetchers.remove(fetcherId)}} in shutdown hook.;;;","08/Nov/20 15:45;sewen;[~kezhuw] Sounds good, I am assigning this to you.

Should we adjust the change [68c5c2ff779d82a1ff81ffaf60d8a1b283797db1|https://github.com/apache/flink/commit/68c5c2ff779d82a1ff81ffaf60d8a1b283797db1] to return {{MORE_AVAILABLE}} in the case that currently throws an exception?
;;;","08/Nov/20 18:37;kezhuw;[~sewen] Yes. Returning {{MORE_AVAILABLE}} is better than reverting.;;;","10/Nov/20 19:03;sewen;Fixed in 1.12.0 (master) via aa84aa7ac0cdf1076f16d71d9c7b41be04321a84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBaseConnectorITCase.HBaseTestingClusterAutoStarter failed with ""Master not initialized after 200000ms""",FLINK-19447,13329859,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mgergely,dian.fu,dian.fu,29/Sep/20 06:43,19/Feb/21 07:32,13/Jul/23 08:12,01/Oct/20 13:29,1.12.0,,,,,1.12.0,,,Connectors / HBase,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407

{code}
2020-09-28T21:52:21.2146147Z org.apache.flink.connector.hbase2.HBaseConnectorITCase  Time elapsed: 208.382 sec  <<< ERROR!
2020-09-28T21:52:21.2146638Z java.io.IOException: Shutting down
2020-09-28T21:52:21.2147004Z 	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:266)
2020-09-28T21:52:21.2147637Z 	at org.apache.hadoop.hbase.MiniHBaseCluster.<init>(MiniHBaseCluster.java:116)
2020-09-28T21:52:21.2148120Z 	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1142)
2020-09-28T21:52:21.2148831Z 	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1107)
2020-09-28T21:52:21.2149347Z 	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1061)
2020-09-28T21:52:21.2149896Z 	at org.apache.flink.connector.hbase2.util.HBaseTestingClusterAutoStarter.setUp(HBaseTestingClusterAutoStarter.java:122)
2020-09-28T21:52:21.2150721Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-28T21:52:21.2151136Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-28T21:52:21.2151609Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-28T21:52:21.2152039Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-28T21:52:21.2152462Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-28T21:52:21.2152941Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-28T21:52:21.2153489Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-28T21:52:21.2153962Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-09-28T21:52:21.2154406Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-28T21:52:21.2154828Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-28T21:52:21.2155381Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
2020-09-28T21:52:21.2155864Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
2020-09-28T21:52:21.2156378Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-28T21:52:21.2156865Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
2020-09-28T21:52:21.2157458Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
2020-09-28T21:52:21.2157993Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
2020-09-28T21:52:21.2158470Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2020-09-28T21:52:21.2158890Z Caused by: java.lang.RuntimeException: Master not initialized after 200000ms
2020-09-28T21:52:21.2159350Z 	at org.apache.hadoop.hbase.util.JVMClusterUtil.waitForEvent(JVMClusterUtil.java:229)
2020-09-28T21:52:21.2159823Z 	at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:197)
2020-09-28T21:52:21.2160270Z 	at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:413)
2020-09-28T21:52:21.2160800Z 	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:259)
2020-09-28T21:52:21.2161096Z 	... 22 more
{code}",,dian.fu,leonard,mgergely,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-25140,,,,,,,,,,,"30/Sep/20 10:23;dian.fu;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13012334/screenshot-1.png","30/Sep/20 23:36;dian.fu;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13012368/screenshot-2.png","30/Sep/20 23:37;dian.fu;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13012369/screenshot-3.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 14:32:17 UTC 2020,,,,,,,,,,"0|z0izu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/20 06:44;dian.fu;cc [~mgergely] [~gyfora]  Could you help to take a look at if this is related to FLINK-18795?;;;","29/Sep/20 12:12;mgergely;[~dian.fu] it is related. This test produces the same result for me on my computer, but it was running fine during the CI tests on azure at every run. My conclusion was that the lack of resources causes it to fail. It seems that the nightly tests are also running on a computer that lacks the resources to start up a MiniHBaseCluster. I'm trying to figure out how to solve this involving some HBase experts.;;;","29/Sep/20 12:38;mgergely;[~dian.fu] do you know by any chance where could I take a look at the descriptor of those CI tests / nightly tests?;;;","29/Sep/20 14:47;dian.fu;[~mgergely] I found the following exception in the log file. Could you take a look at if it would help?
{code}
21:49:04,691 [RS_CLOSE_META-regionserver/71af2d647bb3:0-0] WARN  org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler [] - Fatal error occurred while opening region hbase:meta,,1.1588230740
, aborting...
java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper
        at org.apache.hadoop.hbase.io.asyncfs.AsyncFSOutputHelper.createOutput(AsyncFSOutputHelper.java:51) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter.initOutput(AsyncProtobufLogWriter.java:169) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter.init(AbstractProtobufLogWriter.java:166) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.AsyncFSWALProvider.createAsyncWriter(AsyncFSWALProvider.java:113) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createWriterInstance(AsyncFSWAL.java:643) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createWriterInstance(AsyncFSWAL.java:126) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.rollWriter(AbstractFSWAL.java:767) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.rollWriter(AbstractFSWAL.java:501) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.init(AbstractFSWAL.java:442) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getWAL(AbstractFSWALProvider.java:156) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getWAL(AbstractFSWALProvider.java:61) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.WALFactory.getWAL(WALFactory.java:284) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getWAL(HRegionServer.java:2181) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler.process(AssignRegionHandler.java:133) [hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104) [hbase-server-2.2.3.jar:2.2.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
21:49:04,692 [RS_CLOSE_META-regionserver/71af2d647bb3:0-0] ERROR org.apache.hadoop.hbase.regionserver.HRegionServer           [] - ***** ABORTING region server 71af2d647bb3,35715,1601329739543: Failed to open region hbase:meta,,1.1588230740 and can not recover *****
java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper
        at org.apache.hadoop.hbase.io.asyncfs.AsyncFSOutputHelper.createOutput(AsyncFSOutputHelper.java:51) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter.initOutput(AsyncProtobufLogWriter.java:169) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter.init(AbstractProtobufLogWriter.java:166) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.AsyncFSWALProvider.createAsyncWriter(AsyncFSWALProvider.java:113) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createWriterInstance(AsyncFSWAL.java:643) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createWriterInstance(AsyncFSWAL.java:126) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.rollWriter(AbstractFSWAL.java:767) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.rollWriter(AbstractFSWAL.java:501) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.init(AbstractFSWAL.java:442) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getWAL(AbstractFSWALProvider.java:156) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getWAL(AbstractFSWALProvider.java:61) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.wal.WALFactory.getWAL(WALFactory.java:284) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getWAL(HRegionServer.java:2181) ~[hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler.process(AssignRegionHandler.java:133) [hbase-server-2.2.3.jar:2.2.3]
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104) [hbase-server-2.2.3.jar:2.2.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
{code};;;","29/Sep/20 15:50;mgergely;Thank you [~dian.fu]. Could you please point me to the log where you found it?;;;","30/Sep/20 00:59;dian.fu;All the logs could be found here: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=artifacts&type=publishedArtifacts

The log for this specific failed tests:
https://artprodsu6weu.artifacts.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/artifact/cGlwZWxpbmVhcnRpZmFjdDovL2FwYWNoZS1mbGluay9wcm9qZWN0SWQvOTg0NjM0OTYtMWFmMi00NjIwLThlYWItYTJlY2MxYTJlNmZlL2J1aWxkSWQvNzA0Mi9hcnRpZmFjdE5hbWUvbG9ncy1jcm9uX2hhZG9vcDI0MS1jb25uZWN0b3JzLTE2MDEzMjc5ODk1/content?format=zip;;;","30/Sep/20 01:34;dian.fu;Another instance: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7093&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7093&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","30/Sep/20 01:36;dian.fu;Upgrade it to ""Blocker"" as it seems that this test is continuously failing.;;;","30/Sep/20 09:32;mgergely;Thank you [~dian.fu], I'm investigating. Just out of curiosity, how can I navigate from [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=results] to [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=artifacts&type=publishedArtifacts] via clicking on links? I was trying to locate the logs, but couldn't find them.;;;","30/Sep/20 10:24;dian.fu;I have attached a screenshot which shows how to find the download link. :);;;","30/Sep/20 10:27;mgergely;[~dian.fu] I could find the download link on the ""Artifacts"" page, what I don't know how to navigate to the ""Artifacts"" page from the main page.;;;","30/Sep/20 14:33;mgergely;[~dian.fu] , [~gyfora] , [~bamrabi] it seems that the HBase 2.2.3 mini cluster test utility works only with hadoop 2.8.0 - 3.0.3. I've created a PR, that runs it only if the hadoop.version is within the range. As a result it will be skipped in both the 2.4.1 and 3.1.3 nightly build.;;;","30/Sep/20 23:38;dian.fu;[~mgergely] Have attached another two screenshots. Hope that could help! :) 

Thanks a lot for the PR!;;;","01/Oct/20 08:08;mgergely;[~dian.fu] thanks for the screenshots, never thought that it was a link. Also thank you for your review.;;;","01/Oct/20 13:29;dian.fu;Fixed in master via c05027370b33c178d8b5d597b80a98857c941ba0;;;","01/Oct/20 14:32;mgergely;Thank you [~dian.fu]. Created https://issues.apache.org/jira/browse/HBASE-25140 to track the issue of the test mini cluster compatibility.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Canal-json format parse UPDATE record with null value will get wrong result,FLINK-19446,13329855,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,tinny,tinny,29/Sep/20 06:30,28/May/21 07:12,13/Jul/23 08:12,22/Jan/21 05:53,1.11.1,,,,,1.13.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"line 118 in CanalJsonDeserializationSchema#deserialize method:

{code:java}
GenericRowData after = (GenericRowData) data.getRow(i, fieldCount);
GenericRowData before = (GenericRowData) old.getRow(i, fieldCount);
for (int f = 0; f < fieldCount; f++) {
	if (before.isNullAt(f)) {
		// not null fields in ""old"" (before) means the fields are changed
		// null/empty fields in ""old"" (before) means the fields are not changed
		// so we just copy the not changed fields into before
		before.setField(f, after.getField(f));
	}
}
before.setRowKind(RowKind.UPDATE_BEFORE);
after.setRowKind(RowKind.UPDATE_AFTER);
{code}

if a field is null before update,it will cause -U and +U to be equal",,Balro,fsk119,jark,libenchao,michael ran,nicholasjiang,tinny,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19432,,,,,,,,,,,,,,,,,,FLINK-20763,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 05:53:39 UTC 2021,,,,,,,,,,"0|z0izt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/20 04:03;jark;That's true. I think we need to know whether this is a ""changed"" field. Depending on whether field is null is not enough. Do you have any idea [~tinny]?;;;","30/Sep/20 09:28;tinny;currently, i think it can be solved in the following steps: 

# the old(before) shuld be ""DataTypes.ARRAY(DataTypes.MAP(DataTypes.STRING(), DataTypes.STRING()))"" ， not ""DataTypes.ARRAY(databaseSchema)"", then the old is a ""MapData"" only contains the ""changed"" field
# find the index of the updated field in MapData
;;;","30/Sep/20 14:26;jark;This may not work and complex if the {{databaseSchema}} contains nested fields. ;;;","09/Oct/20 08:04;tinny;In addition, there is no good way for the time being.;;;","25/Dec/20 04:06;Balro;Is this issue fixed on version 1.13?;;;","25/Dec/20 05:59;jark;This is not fixed yet. [~Balro].;;;","14/Jan/21 05:12;nicholasjiang;[~jark], if no one is available to work for this issue, I would like to take. Could you please assign this to me?;;;","20/Jan/21 03:12;tinny;[~Balro]，i think your patch would be work well.;;;","22/Jan/21 05:53;jark;Fixed in master: 6ee32ee4a4cc24f2e519545f6c032f06849f08d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Several tests for HBase connector 1.4 failed with ""NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V""",FLINK-19445,13329854,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,dian.fu,dian.fu,29/Sep/20 06:24,19/Feb/21 07:30,13/Jul/23 08:12,09/Feb/21 08:10,1.12.0,1.13.0,,,,1.12.0,,,Connectors / HBase,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51

{code}
2020-09-28T21:28:29.4171075Z Running org.apache.flink.connector.hbase1.HBaseTablePlanTest
2020-09-28T21:28:31.0367584Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.62 sec <<< FAILURE! - in org.apache.flink.connector.hbase1.HBaseTablePlanTest
2020-09-28T21:28:31.0368925Z testProjectionPushDown(org.apache.flink.connector.hbase1.HBaseTablePlanTest)  Time elapsed: 0.031 sec  <<< ERROR!
2020-09-28T21:28:31.0369805Z org.apache.flink.table.api.ValidationException: 
2020-09-28T21:28:31.0370409Z Unable to create a source for reading table 'default_catalog.default_database.hTable'.
2020-09-28T21:28:31.0370707Z 
2020-09-28T21:28:31.0370976Z Table options are:
2020-09-28T21:28:31.0371204Z 
2020-09-28T21:28:31.0371528Z 'connector'='hbase-1.4'
2020-09-28T21:28:31.0371871Z 'table-name'='my_table'
2020-09-28T21:28:31.0372255Z 'zookeeper.quorum'='localhost:2021'
2020-09-28T21:28:31.0372812Z 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)
2020-09-28T21:28:31.0373359Z 	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)
2020-09-28T21:28:31.0373905Z 	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)
2020-09-28T21:28:31.0374390Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)
2020-09-28T21:28:31.0375224Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)
2020-09-28T21:28:31.0375867Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)
2020-09-28T21:28:31.0376479Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)
2020-09-28T21:28:31.0377077Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
2020-09-28T21:28:31.0377593Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)
2020-09-28T21:28:31.0378114Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)
2020-09-28T21:28:31.0378622Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)
2020-09-28T21:28:31.0379132Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)
2020-09-28T21:28:31.0379872Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)
2020-09-28T21:28:31.0380477Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:196)
2020-09-28T21:28:31.0381128Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:154)
2020-09-28T21:28:31.0381666Z 	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:823)
2020-09-28T21:28:31.0382264Z 	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:795)
2020-09-28T21:28:31.0382968Z 	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)
2020-09-28T21:28:31.0383550Z 	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
2020-09-28T21:28:31.0384172Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)
2020-09-28T21:28:31.0384700Z 	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:346)
2020-09-28T21:28:31.0385201Z 	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)
2020-09-28T21:28:31.0385717Z 	at org.apache.flink.connector.hbase1.HBaseTablePlanTest.testProjectionPushDown(HBaseTablePlanTest.java:124)
2020-09-28T21:28:31.0386166Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-28T21:28:31.0386575Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-28T21:28:31.0387257Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-28T21:28:31.0387822Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-28T21:28:31.0388229Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-28T21:28:31.0388718Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-28T21:28:31.0389198Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-28T21:28:31.0389745Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-28T21:28:31.0390262Z 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
2020-09-28T21:28:31.0390732Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-28T21:28:31.0391179Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-28T21:28:31.0391582Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-28T21:28:31.0391964Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-28T21:28:31.0392382Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-28T21:28:31.0393053Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-28T21:28:31.0393617Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-28T21:28:31.0393997Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-28T21:28:31.0394407Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-28T21:28:31.0394817Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-28T21:28:31.0395211Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-28T21:28:31.0395608Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-28T21:28:31.0396041Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
2020-09-28T21:28:31.0396517Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
2020-09-28T21:28:31.0397026Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-28T21:28:31.0397512Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
2020-09-28T21:28:31.0398245Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
2020-09-28T21:28:31.0398778Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
2020-09-28T21:28:31.0399251Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2020-09-28T21:28:31.0399838Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
2020-09-28T21:28:31.0400340Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
2020-09-28T21:28:31.0400756Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
2020-09-28T21:28:31.0401304Z 	at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113)
2020-09-28T21:28:31.0401869Z 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)
2020-09-28T21:28:31.0402307Z 	... 50 more
2020-09-28T21:28:31.0402624Z 
2020-09-28T21:28:31.0402949Z Running org.apache.flink.connector.hbase1.HBaseDescriptorTest
2020-09-28T21:28:31.0416116Z Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.connector.hbase1.HBaseDescriptorTest
2020-09-28T21:28:31.4448287Z 
2020-09-28T21:28:31.4448950Z Results :
2020-09-28T21:28:31.4449082Z 
2020-09-28T21:28:31.4449270Z Tests in error: 
2020-09-28T21:28:31.4450556Z   HBaseDynamicTableFactoryTest.testTableSourceFactory:104->createTableSource:332 Â» Validation
2020-09-28T21:28:31.4451232Z   HBaseTableFactoryTest.testTableSourceFactory:101 Â» NoSuchMethod com.google.com...
2020-09-28T21:28:31.4451851Z   HBaseTablePlanTest.testProjectionPushDown:124 Â» Validation Unable to create a ...
{code}",,dian.fu,gyfora,maguowei,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18795,,,,FLINK-21006,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 08:10:34 UTC 2021,,,,,,,,,,"0|z0izsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/20 06:30;dian.fu;cc [~mgergely] [~gyfora] ;;;","29/Sep/20 06:45;gyfora;I wonder why these tests did not run for the PR. [~dian.fu] any ideas?;;;","29/Sep/20 06:49;dian.fu;This is from the nightly test which will run more kinds of tests than the PR tests.;;;","29/Sep/20 12:31;gyfora;Master: f30caf87879a013920622039c0707e0c6d7cf08c;;;","20/Jan/21 02:53;maguowei;These tests fails again in the master.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12254&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","21/Jan/21 03:47;maguowei;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12299&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","21/Jan/21 06:43;rmetzger;This seems very similar, and fixed recently: FLINK-21006. Maybe it's not fixed?;;;","25/Jan/21 02:40;maguowei;another instance

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12414&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12429&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","27/Jan/21 03:59;maguowei;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12532&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12534&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","09/Feb/21 08:10;rmetzger;Resolved in FLINK-21006;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FileSourceTextLinesITCase.testContinuousTextFileSource failed with ""SimpleStreamFormat is not splittable, but found split end (0) different from file length (198)""",FLINK-19437,13329665,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,dian.fu,dian.fu,28/Sep/20 01:04,30/Sep/20 01:02,13/Jul/23 08:12,29/Sep/20 17:50,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,Tests,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7008&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code}
2020-09-27T21:58:38.9199090Z [ERROR] testContinuousTextFileSource(org.apache.flink.connector.file.src.FileSourceTextLinesITCase)  Time elapsed: 0.517 s  <<< ERROR!
2020-09-27T21:58:38.9199619Z java.lang.RuntimeException: Failed to fetch next result
2020-09-27T21:58:38.9200118Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2020-09-27T21:58:38.9200722Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77)
2020-09-27T21:58:38.9201290Z 	at org.apache.flink.streaming.api.datastream.DataStreamUtils.collectRecordsFromUnboundedStream(DataStreamUtils.java:150)
2020-09-27T21:58:38.9201920Z 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSource(FileSourceTextLinesITCase.java:136)
2020-09-27T21:58:38.9202570Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-27T21:58:38.9203054Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-27T21:58:38.9203539Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-27T21:58:38.9203968Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-27T21:58:38.9204369Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-27T21:58:38.9204844Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-27T21:58:38.9205359Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-27T21:58:38.9205814Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-27T21:58:38.9206240Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-27T21:58:38.9206611Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-27T21:58:38.9206971Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-27T21:58:38.9207404Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-27T21:58:38.9207971Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-27T21:58:38.9208404Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-27T21:58:38.9208877Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-27T21:58:38.9209279Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-27T21:58:38.9209680Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-27T21:58:38.9210064Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-27T21:58:38.9210476Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-27T21:58:38.9210881Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-27T21:58:38.9211272Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-27T21:58:38.9211638Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-27T21:58:38.9212305Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-27T21:58:38.9213157Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-27T21:58:38.9213663Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-27T21:58:38.9214123Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-27T21:58:38.9214620Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-27T21:58:38.9215148Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-27T21:58:38.9215650Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-27T21:58:38.9216095Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-27T21:58:38.9216516Z Caused by: java.io.IOException: Failed to fetch job execution result
2020-09-27T21:58:38.9217004Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
2020-09-27T21:58:38.9217595Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126)
2020-09-27T21:58:38.9218182Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103)
2020-09-27T21:58:38.9218585Z 	... 33 more
2020-09-27T21:58:38.9219037Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-09-27T21:58:38.9219563Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-09-27T21:58:38.9219987Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-09-27T21:58:38.9220511Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172)
2020-09-27T21:58:38.9220915Z 	... 35 more
2020-09-27T21:58:38.9221225Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-09-27T21:58:38.9221680Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-09-27T21:58:38.9222277Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:196)
2020-09-27T21:58:38.9223032Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-09-27T21:58:38.9223479Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2020-09-27T21:58:38.9223938Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2020-09-27T21:58:38.9224503Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.getJobExecutionResult(PerJobMiniClusterFactory.java:194)
2020-09-27T21:58:38.9224926Z 	... 36 more
2020-09-27T21:58:38.9225315Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-09-27T21:58:38.9225889Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-09-27T21:58:38.9226529Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-09-27T21:58:38.9227128Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217)
2020-09-27T21:58:38.9227658Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210)
2020-09-27T21:58:38.9228200Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204)
2020-09-27T21:58:38.9228806Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:527)
2020-09-27T21:58:38.9229416Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:421)
2020-09-27T21:58:38.9229885Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-27T21:58:38.9230283Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-27T21:58:38.9230755Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-27T21:58:38.9231160Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-27T21:58:38.9231589Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-09-27T21:58:38.9232086Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-09-27T21:58:38.9232792Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-09-27T21:58:38.9233305Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-27T21:58:38.9233750Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-27T21:58:38.9234127Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-27T21:58:38.9234533Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-27T21:58:38.9234949Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-27T21:58:38.9235391Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-27T21:58:38.9235808Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-27T21:58:38.9236218Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-27T21:58:38.9236590Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-27T21:58:38.9236981Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-27T21:58:38.9237371Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-27T21:58:38.9237728Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-27T21:58:38.9238087Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-27T21:58:38.9238421Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-27T21:58:38.9238819Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-27T21:58:38.9239191Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-27T21:58:38.9239605Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-27T21:58:38.9240040Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-27T21:58:38.9240480Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-27T21:58:38.9240913Z Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2020-09-27T21:58:38.9241431Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:178)
2020-09-27T21:58:38.9242009Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:155)
2020-09-27T21:58:38.9242688Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
2020-09-27T21:58:38.9243221Z 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:199)
2020-09-27T21:58:38.9243738Z 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:64)
2020-09-27T21:58:38.9244271Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
2020-09-27T21:58:38.9244798Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:368)
2020-09-27T21:58:38.9245369Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:185)
2020-09-27T21:58:38.9245853Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594)
2020-09-27T21:58:38.9246417Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558)
2020-09-27T21:58:38.9246841Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-09-27T21:58:38.9247288Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-09-27T21:58:38.9247629Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-27T21:58:38.9248047Z Caused by: java.lang.RuntimeException: SplitFetcher thread 1 received unexpected exception while polling the records
2020-09-27T21:58:38.9248558Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:123)
2020-09-27T21:58:38.9249151Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:91)
2020-09-27T21:58:38.9249647Z 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
2020-09-27T21:58:38.9250090Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-09-27T21:58:38.9250506Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-27T21:58:38.9250920Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-27T21:58:38.9251369Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-27T21:58:38.9251694Z 	... 1 more
2020-09-27T21:58:38.9252087Z Caused by: java.lang.IllegalArgumentException: SimpleStreamFormat is not splittable, but found split end (0) different from file length (198)
2020-09-27T21:58:38.9252875Z 	at org.apache.flink.connector.file.src.reader.SimpleStreamFormat.checkNotSplit(SimpleStreamFormat.java:110)
2020-09-27T21:58:38.9253430Z 	at org.apache.flink.connector.file.src.reader.SimpleStreamFormat.createReader(SimpleStreamFormat.java:85)
2020-09-27T21:58:38.9253999Z 	at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.lambda$createReader$0(StreamFormatAdapter.java:68)
2020-09-27T21:58:38.9254516Z 	at org.apache.flink.connector.file.src.util.Utils.doWithCleanupOnException(Utils.java:45)
2020-09-27T21:58:38.9255034Z 	at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.createReader(StreamFormatAdapter.java:67)
2020-09-27T21:58:38.9255655Z 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:103)
2020-09-27T21:58:38.9256225Z 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:68)
2020-09-27T21:58:38.9256723Z 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2020-09-27T21:58:38.9257235Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:117)
2020-09-27T21:58:38.9257586Z 	... 7 more
{code}",,aljoscha,dian.fu,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 17:50:04 UTC 2020,,,,,,,,,,"0|z0iymw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/20 01:04;dian.fu;cc [~sewen];;;","29/Sep/20 15:08;sewen;Fixing this. As suggested by [~lzljs3620320] in FLINK-19370, the non-atomic file writing can make the streaming file source pick up partial files in some cases.

This can be fixed by writing to a hidden temp file first, and then renaming this to the final file.;;;","29/Sep/20 17:50;sewen;Fixed via d9991269d38feaef7165534fc29563e6ce7f4bce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS end-to-end test (Blink planner) failed during shutdown,FLINK-19436,13329664,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,dian.fu,dian.fu,28/Sep/20 00:59,28/May/21 07:13,13/Jul/23 08:12,25/Jan/21 14:00,1.11.0,1.12.0,,,,1.11.4,1.12.0,,Table SQL / Planner,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7009&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9

{code}
2020-09-27T22:37:53.2236467Z Stopping taskexecutor daemon (pid: 2992) on host fv-az655.
2020-09-27T22:37:53.4450715Z Stopping standalonesession daemon (pid: 2699) on host fv-az655.
2020-09-27T22:37:53.8014537Z Skipping taskexecutor daemon (pid: 11173), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8019740Z Skipping taskexecutor daemon (pid: 11561), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8022857Z Skipping taskexecutor daemon (pid: 11849), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8023616Z Skipping taskexecutor daemon (pid: 12180), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8024327Z Skipping taskexecutor daemon (pid: 12950), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8025027Z Skipping taskexecutor daemon (pid: 13472), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8025727Z Skipping taskexecutor daemon (pid: 16577), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8026417Z Skipping taskexecutor daemon (pid: 16959), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8027086Z Skipping taskexecutor daemon (pid: 17250), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8027770Z Skipping taskexecutor daemon (pid: 17601), because it is not running anymore on fv-az655.
2020-09-27T22:37:53.8028400Z Stopping taskexecutor daemon (pid: 18438) on host fv-az655.
2020-09-27T22:37:53.8029314Z /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/bin/taskmanager.sh: line 99: 18438 Terminated              ""${FLINK_BIN_DIR}""/flink-daemon.sh $STARTSTOP $ENTRYPOINT ""${ARGS[@]}""
2020-09-27T22:37:53.8029895Z [FAIL] Test script contains errors.
2020-09-27T22:37:53.8032092Z Checking for errors...
2020-09-27T22:37:55.3713368Z No errors in log files.
2020-09-27T22:37:55.3713935Z Checking for exceptions...
2020-09-27T22:37:56.9046391Z No exceptions in log files.
2020-09-27T22:37:56.9047333Z Checking for non-empty .out files...
2020-09-27T22:37:56.9064402Z No non-empty .out files.
2020-09-27T22:37:56.9064859Z 
2020-09-27T22:37:56.9065588Z [FAIL] 'TPC-DS end-to-end test (Blink planner)' failed after 16 minutes and 54 seconds! Test exited with exit code 1
{code}",,dian.fu,dwysakowicz,jark,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/20 03:08;leonard;image-2020-11-10-11-08-53-199.png;https://issues.apache.org/jira/secure/attachment/13014938/image-2020-11-10-11-08-53-199.png","10/Nov/20 03:09;leonard;image-2020-11-10-11-09-20-534.png;https://issues.apache.org/jira/secure/attachment/13014939/image-2020-11-10-11-09-20-534.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 14:00:41 UTC 2021,,,,,,,,,,"0|z0iymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 01:17;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7814&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","20/Oct/20 06:26;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7884&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","20/Oct/20 06:53;rmetzger;[~Leonard Xu] can you take a look at this failure?;;;","20/Oct/20 07:08;leonard;Sure, I'll have a look:);;;","26/Oct/20 01:43;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8232&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9;;;","09/Nov/20 13:52;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9345&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","09/Nov/20 13:53;leonard;I think the reason is the previous e2e tests didn't close flink cluster properly, which lead the e2e test call taskmanager.sh stop-all failed ;;;","09/Nov/20 13:57;rmetzger;Are you sure there was a previous TaskManager running?

From the logs, you can see that no port was allocated by a JVM before the Blink planner test started:

{code}
Nov 09 12:53:55 Allocated ports
Nov 09 12:53:55 Active Internet connections (only servers)
Nov 09 12:53:55 Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
Nov 09 12:53:55 tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1637/sshd       
Nov 09 12:53:55 tcp        0      0 127.0.0.1:44381         0.0.0.0:*               LISTEN      1588/containerd 
Nov 09 12:53:55 tcp6       0      0 :::22                   :::*                    LISTEN      1637/sshd       
Nov 09 12:53:55 udp        0      0 0.0.0.0:68              0.0.0.0:*                           1142/dhclient   
{code};;;","10/Nov/20 03:24;leonard;Hi, [~rmetzger] I'm pretty sure that, all failed log showed there're more than one TMS, but the TPC-DS test only starts 1 TM.

!image-2020-11-10-11-08-53-199.png!

And from my PR, you can see the only one TM is closing. 

!image-2020-11-10-11-09-20-534.png!

 

If e2e bash script doesn't `call clean_up()` the JM and TMs it started, the running JM and TMs will be closed in `test-runner-common.sh#shutdown_all()`,  `test-runner-common.sh#shutdown_all()` using `kill -9 ` to stop the process without any check which may fail, I think this is why the test is unstable.;;;","10/Nov/20 04:10;leonard;And in these failed tests, all failed TM pid(e.g. *4267*) can be found in previous e2e log,

[PASS] 'Netty shuffle direct memory consumption end-to-end test' passed after 2 minutes and 23 seconds! Test exited with exit code 0.
2020-10-18T22:26:05.8214649Z Stopping taskexecutor daemon (pid: 5306) on host fv-az679.
2020-10-18T22:26:06.0204996Z Stopping standalonesession daemon (pid: 3673) on host fv-az679.
.... 
2020-10-18T22:26:06.6608513Z Jps
2020-10-18T22:26:06.7766551Z *{color:#de350b}4627{color}* TaskManagerRunner
2020-10-18T22:26:06.7911919Z 3962 TaskManagerRunner
2020-10-18T22:26:06.8175144Z 6875 Jps
2020-10-18T22:26:06.8219362Z Disk information
....
2020-10-18T22:58:21.1330389Z [INFO] Validation succeeded for file: 99.ans (103/103)
2020-10-18T22:58:21.4205488Z Stopping taskexecutor daemon (pid: 5407) on host fv-az679.
2020-10-18T22:58:21.6132761Z Stopping standalonesession daemon (pid: 5120) on host fv-az679.
....
2020-10-18T22:58:21.9654801Z Stopping taskexecutor daemon (pid: 4627) on host fv-az679.
2020-10-18T22:58:21.9670548Z /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test-runner-common.sh: line 47: *{color:#de350b}4627{color}* Terminated ${command}
2020-10-18T22:58:21.9671129Z [FAIL] Test script contains errors.

 

I tend  to stop all TMs in common.shutdown_all before `kill -9` them, How do you think? [~rmetzger];;;","10/Nov/20 06:15;rmetzger;I see. Thanks a lot for the detailed explanation. It seems that the TPC-DS tests are ""stopping"" all the leftover pids from previous tests.
I wonder why this error is occurring now. Maybe some other change has broken a cleanup mechanism.

Thanks a lot for this good analysis. I'll take a look at the PR.;;;","10/Nov/20 13:44;rmetzger;Resolved in https://github.com/apache/flink/commit/9b12c1553de7a896d2d80600fab8366e70d6747e.;;;","25/Jan/21 07:34;dwysakowicz;This reappeared on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12397&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873;;;","25/Jan/21 07:39;rmetzger;I'll backport the fix to 1.11.;;;","25/Jan/21 07:43;leonard;Thanks for the report [~dwysakowicz] 
IIRC, we didn't pick the PR to 1.11， let me open a PR for 1.11 branch. ;;;","25/Jan/21 14:00;jark;Fixed in release-1.11: cece43b93b94d5e424583d69717c7ab55f8d45b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock when loading different driver classes concurrently using Class.forName,FLINK-19435,13329622,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kezhuw,xiaodao,xiaodao,27/Sep/20 12:25,13/Jan/21 12:28,13/Jul/23 08:12,22/Dec/20 02:55,1.10.2,,,,,1.12.1,1.13.0,,Connectors / JDBC,,,,,0,pull-request-available,,,,,"when we sink data to multi jdbc outputformat , 

{code}
protected void establishConnection() throws SQLException, ClassNotFoundException {
 Class.forName(drivername);
 if (username == null) {
 connection = DriverManager.getConnection(dbURL);
 } else {
 connection = DriverManager.getConnection(dbURL, username, password);
 }
}
{code}

may cause jdbc driver deadlock. it need to change to synchronized function.",,jark,kezhuw,libenchao,lzljs3620320,roman,xiaodao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/20 12:48;xiaodao;image-2020-10-09-20-48-48-261.png;https://issues.apache.org/jira/secure/attachment/13013314/image-2020-10-09-20-48-48-261.png","09/Oct/20 12:49;xiaodao;image-2020-10-09-20-49-23-644.png;https://issues.apache.org/jira/secure/attachment/13013315/image-2020-10-09-20-49-23-644.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 21 02:44:23 UTC 2020,,,,,,,,,,"0|z0iydc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/20 03:44;jark;You mean {{Class.forName()}} is not thread safe? ;;;","09/Oct/20 12:49;xiaodao;[~jark]  yes.

this is my test program.

!image-2020-10-09-20-48-48-261.png|width=1015,height=392!

and this is jstack info:

!image-2020-10-09-20-49-23-644.png!;;;","06/Dec/20 05:13;kezhuw;Seems that this problem still exist. I have pushed [a branch|https://github.com/kezhuw/flink/commits/flink-19435-sql-driver-class-loading-deadlock-test-case] with [hang test case|https://github.com/kezhuw/flink/commit/a732d30093073e1e5bcdbed13290cef3f9a00d69#diff-0dafd910621d60887021215d320c2a9fc5b12305825c6776de8a307483920a52R110] in my repository to demonstrate this. One could use command {{mvn -Dcheckstyle.skip=true -Dtest=JdbcDriverClassConcurrentLoadingTest -DfailIfNoTests=false -pl ""flink-connectors/flink-connector-jdbc"" -am test -Pskip-webui-build}} or IDE to reproduce this hang. Please rerun if it does not, it is near 100% in my local environment. After hang, you will see similar threads in thread stack dumps:
{code:java}
""Sink: Postgres Jdbc Sink (1/1)#0"" #102 prio=5 os_prio=31 tid=0x00007f8fae050800 nid=0xd703 in Object.wait() [0x000070000fb6c000]
   java.lang.Thread.State: RUNNABLE
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at java.lang.Class.newInstance(Class.java:442)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at java.sql.DriverManager$2.run(DriverManager.java:603)
        at java.sql.DriverManager$2.run(DriverManager.java:583)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.sql.DriverManager.loadInitialDrivers(DriverManager.java:583)
        at java.sql.DriverManager.<clinit>(DriverManager.java:101)
        at org.postgresql.Driver.register(Driver.java:721)
        at org.postgresql.Driver.<clinit>(Driver.java:73)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getConnection(SimpleJdbcConnectionProvider.java:52)
        - locked <0x000000074cf7c3e8> (a org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider)
        at org.apache.flink.connector.jdbc.internal.AbstractJdbcOutputFormat.establishConnection(AbstractJdbcOutputFormat.java:66)
        at org.apache.flink.connector.jdbc.internal.AbstractJdbcOutputFormat.open(AbstractJdbcOutputFormat.java:59)
        at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.open(JdbcBatchingOutputFormat.java:114)
        at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:50)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
        at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:401)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$641/1400119541.run(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
        at java.lang.Thread.run(Thread.java:748)

""Sink: MySQL Jdbc Sink (1/1)#0"" #101 prio=5 os_prio=31 tid=0x00007f8fae37b000 nid=0x12603 in Object.wait() [0x000070000fa6a000]
   java.lang.Thread.State: RUNNABLE
        at com.mysql.cj.jdbc.Driver.<clinit>(Driver.java:55)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.getConnection(SimpleJdbcConnectionProvider.java:52)
        - locked <0x000000074d0a5368> (a org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider)
        at org.apache.flink.connector.jdbc.internal.AbstractJdbcOutputFormat.establishConnection(AbstractJdbcOutputFormat.java:66)
        at org.apache.flink.connector.jdbc.internal.AbstractJdbcOutputFormat.open(AbstractJdbcOutputFormat.java:59)
        at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.open(JdbcBatchingOutputFormat.java:114)
        at org.apache.flink.connector.jdbc.internal.GenericJdbcSinkFunction.open(GenericJdbcSinkFunction.java:50)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
        at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:401)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$641/1400119541.run(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
        at java.lang.Thread.run(Thread.java:748)
{code}

Two threads are deadlock due to [wait on each other's initializing static block in JVM|https://github.com/openjdk/jdk/blob/jdk8-b120/hotspot/src/share/vm/oops/instanceKlass.cpp#L794]. Restricted to above dump, thread {{Sink: Postgres Jdbc Sink (1/1)#0}} is waiting on {{com.mysql.cj.jdbc.Driver}}'s static block in {{java.sql.DriverManager}}'s static block, thread {{Sink: MySQL Jdbc Sink (1/1)#0}} is waiting on {{java.sql.DriverManager}}'s static block in {{com.mysql.cj.jdbc.Driver}}'s static block.

To trigger this deadlock, one could load different driver classes concurrently using {{Class.forName}} before {{java.sql.DriverManager}} is loaded. Here is [a relatively detailed article|https://medium.com/@priyaaggarwal24/avoiding-deadlock-when-using-multiple-jdbc-drivers-in-an-application-ce0b9464ecdf] on this problem, there are should be more on internet. This is a JDK 8 specific problem, it has been solved in jdk 9 and above with [JDK-8060068|https://github.com/openjdk/jdk/commit/7e500d532492437658e6736d6279809f3fa40406] and [JDK-8067904|https://github.com/openjdk/jdk/commit/ddcbf6138900d2f3a6c550ceea5a01e096819323] by moving driver loading out of {{java.sql.DriverManager}}'s static block.

There are probably several ways to circumvent this, such as ensuring {{java.sql.DriverManager}} is loaded before {{Class.forName}} and locking on a global enough object before {{Class.forName}}. But I think a more elegant way is not using {{Class.forName}} for driver class loading. {{DriverManager.getConnection}} will do this automatically for us. It is value of {{java.sql.Driver}} and {{java.util.ServiceLoader}}: care about promise not implementation. I think we should drop required field {{driverName}} from {{JdbcConnectionOptions}} in long term as it force clients care about driver class in addition to driver jar.

[~jark] [~twalthr] [~roman_khachatryan] [~Leonard Xu] What do you think ?;;;","07/Dec/20 03:12;jark;Thanks [~kezhuw] for the detailed anaylisis. I reproduced the problem using your code. Your proposal makes sense to me. Currently, {{driverName}} is optional, I think we can drop it if necessary. 
Do you want to take this issue?;;;","07/Dec/20 14:48;kezhuw;[~jark] I am willing to take over this.

Currently, {{driverName}} plays nothing except class loading, I think it is ok to omit {{Class.forName}} without breaking existing code.

But I saw {{JdbcDialect.defaultDriverName}}, I am wondering whether we proposed to provide a strong guarantee to clients that a driver with class name {{driverName}} will be used to handle jdbc connection if provided. If this is the case, simply dropping {{driverName}} may cause rollback in future. This could be useful if clients want to switch between different drivers for one database. But this will also complicate things much because of reasons: 1), all connection-establishing code will need to take care of this, for example {{PostgresCatalog}} may need handle this well; 2), jdbc connector need be driver-agnostic(or support all possible drivers) to achieve this. I am not sure whether this is an reasonable use case to keep {{driverName}}. If this driver level customization is not desired, then {{JdbcDialect.defaultDriverName}} is useless.

Besides this, though {{driverName}} is not direct accessible from sql clients, but it is an required field in {{JdbcConnectionOptions}} which is {{PublicEvolving}}. And if we want make that field optional, we need change signature of {{getDriverName}} according to [coding style|https://flink.apache.org/contributing/code-style-and-quality-java.html#java-optional] which is an API breaking operation.;;;","07/Dec/20 16:13;jark;Hi [~kezhuw]. 

""This could be useful if clients want to switch between different drivers for one database.""
Do you have use cases that show switching different drivers is useful?

""but it is an required field in JdbcConnectionOptions which is PublicEvolving.""
I just noticed this. Well, this is sad. 
cc [~roman_khachatryan] do you know why we require the driverName in JdbcConnectionOptions? Is there any strong use cases depends on this?;;;","07/Dec/20 18:10;roman;I guess Class.forName is there only for historical reasons ([~lzljs3620320] might know better). I agree that DriverManager should be able to find the driver (the same class loaders are used).

 

However, I see that SimpleJdbcConnectionProvider calls DriverManager.getConnection which internally can also call Class.forName.

So simply removing Class.forName from Flink code might not solve the problem.

 ;;;","08/Dec/20 00:50;kezhuw;{quote}
Do you have use cases that show switching different drivers is useful?
{quote}

No, just a paranoid hypothesis that client may use different drivers for one dialect in one stream, I am not sure whether this is an reasonable use case. Personally, I tend to drop {{driverName}} as the de-facto one-to-one binding among dialect and driver implementation, it will also simplify things. But I am kind of worry about whether driver class customization was desired in current design as [~lzljs3620320] checked in.;;;","08/Dec/20 00:52;kezhuw;[~roman_khachatryan] Inside {{DriverManager.getConnection}}, {{DriverManager}} has been loaded, no one will block on its static block.;;;","08/Dec/20 02:32;jark;What do you think about this [~lzljs3620320]? I found the original discussion about the driverName: https://github.com/apache/flink/pull/8867#discussion_r298877518;;;","08/Dec/20 03:02;lzljs3620320;Thanks all for your discussions.

I prefer to keep the driver class customization. I think customized driver is needed since JDBC has exposed the ability of driver.

There is another way to solve this deadlock problem, we can initialize DriverManager first before loading JDBC Drivers like Storm and Spark do:

https://issues.apache.org/jira/browse/STORM-2527

https://issues.apache.org/jira/browse/SPARK-23186

What do you think?;;;","08/Dec/20 03:41;jark;I'm also fine with the Spark way: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala#L41

which looks like not change a lot. ;;;","08/Dec/20 13:55;kezhuw;[~lzljs3620320] From capability side, it is nice to have driver class customization. But current implementation has margin to support this:
1. {{driverName}} is not used to filter {{Driver}} now.
2. {{PostgresCatalog}} and {{JdbcCatalog}} does not take {{driverName}} into account.
3. {{PostgresDialect.getRowConverter}} is tied to {{org.postgresql:postgresql}}.
4. Which driver should be default for jdbc dialect/connection if no {{driverName}} specified ? I prefer to driver selected by {{DriverManager.getConnection}}, it make {{JdbcDialect}} a step to driver-agnostic and free clients from caring about concrete driver class. But we may meet api compatibility issue as {{JdbcConnectionOptions}} has its own exposing path besides sql.

[~lzljs3620320] [~jark] I think we may need steps to solve all discussions in this jira:
1. Fix deadlock issue by ensuring {{DriverManager}} is loaded before {{Class.forName(driverName)}}. We need touch {{JdbcInputFormat}}, {{JdbcLookupFunction}}, {{JdbcRowDataLookupFunction}} and {{SimpleJdbcConnectionProvider}}.
2. Migrate {{JdbcInputFormat}}, {{JdbcLookupFunction}}, {{JdbcRowDataLookupFunction}} to {{SimpleJdbcConnectionProvider}}.
3. Support driver class filter in {{SimpleJdbcConnectionProvider}}. We could assert this support in test with fake driver classes.
4. Solve rest concerns, such as: should {{JdbcConnectionOptions.driverName}} be optional from {{JdbcSink}} and sql ? 

What do you think ?;;;","09/Dec/20 06:34;jark;[~kezhuw], you are right. The current {{PostgresDialect}} implementation is tied to the specific driver and I think we have to do this if we want to have better integration with external databases. 

Your steps sound good to me. I have assigned this issue to you. Feel free to create other related issues if needed. 

> should JdbcConnectionOptions.driverName be optional from JdbcSink and sql?

I think we can make them optional, at least it is optional for SQL connectors (by using the default driver name defined in {{JdbcDialect}}).;;;","21/Dec/20 02:44;jark;Fixed in 
 - master: 7ca2dd311a2e5dc3b30a99ce56ad384f267b6c4f, 84ed65356fe61e6ba74a7e4c4aef0dbd3c63f44a
 - release-1.12: 6ed1a28cb196042c5ba229821cd63cdf71633c22,39b53f672d7fa745c331012643a8de28b02b2821;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An Error example of FROM_UNIXTIME function in document,FLINK-19433,13329603,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,KyleZhang,KyleZhang,KyleZhang,27/Sep/20 09:01,19/Feb/21 07:30,13/Jul/23 08:12,29/Sep/20 03:26,,,,,,1.11.3,1.12.0,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"In the documentation:[https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/systemFunctions.html#temporal-functions]

There is an example in FROM_UNIXTIME function
{code:java}
E.g., FROM_UNIXTIME(44) returns '1970-01-01 09:00:44' if in UTC time zone, but returns '1970-01-01 09:00:44' if in 'Asia/Tokyo' time zone.
{code}
However, the correct result should be 1970-01-01 00:00:44 in UTC time zone

 ",,hailong wang,jark,KyleZhang,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20101,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 13:59:46 UTC 2020,,,,,,,,,,"0|z0iy94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/20 09:04;KyleZhang;Hi [~jark], if it is a problem, could you assign it to me?;;;","27/Sep/20 12:22;KyleZhang;Hi [~libenchao], if you are watching is issue, could you assign it to me? thx :);;;","27/Sep/20 13:02;libenchao;[~KyleZhang] Thanks for reporting this, assigned to you.;;;","29/Sep/20 03:26;jark;Fixed in master : 83b429bbc7fbbbbde4b958cfc6d6726fbbe6f590;;;","12/Nov/20 15:02;hailong wang;Cherry pick to release-1.11,
https://github.com/apache/flink/pull/14050;;;","13/Nov/20 13:59;jark;Fixed in 1.11.3: 10ad46bb061ac9cd5460bbad5f92d6f11a5eb389;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent is instable,FLINK-19427,13329568,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,dian.fu,dian.fu,27/Sep/20 01:12,13/Oct/20 09:38,13/Jul/23 08:12,13/Oct/20 09:38,1.12.0,,,,,1.12.0,,,Connectors / Common,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6983&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab

{code}
2020-09-26T21:27:46.6223579Z [ERROR] testNotifiesWhenGoingIdleConcurrent(org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest)  Time elapsed: 0.602 s  <<< FAILURE!
2020-09-26T21:27:46.6224448Z java.lang.AssertionError
2020-09-26T21:27:46.6224804Z 	at org.junit.Assert.fail(Assert.java:86)
2020-09-26T21:27:46.6225136Z 	at org.junit.Assert.assertTrue(Assert.java:41)
2020-09-26T21:27:46.6225498Z 	at org.junit.Assert.assertTrue(Assert.java:52)
2020-09-26T21:27:46.6225984Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent(SplitFetcherTest.java:129)
{code}",,dian.fu,kezhuw,roman,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 09:38:07 UTC 2020,,,,,,,,,,"0|z0iy1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 06:35;roman;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7213&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf
 ;;;","06/Oct/20 10:17;kezhuw;I could image the execution steps for this failure:

1. After {{fetcher.runOnce()}}, the queue is available for pop.
 2. Then {{QueueDrainerThread.go}} pop the last element from the queue and move the queue to un-available.
 3. {{assertTrue(queue.getAvailabilityFuture().isDone())}} fails.

The assertion in {{SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent}} is similar to {{SourceReaderBase.finishedOrAvailableLater}}, they are all some kind of end-of-input checking. {{FutureCompletingBlockingQueue}} has no builtin end-of-input checking, {{SourceReaderBase}} uses external knowledge to check end-of-input. After reaching end-of-input, {{SourceReaderBase.elementsQueue.getAvailabilityFuture().isDone()}} is indeterministic. It could be {{false}} due to last poll, or {{true}} due to no atomic concurrent {{SplitFetcher.checkAndSetIdle()}}.

I think we can pass this test by run {{SplitFetcher.runOnce}} in separate thread and merge {{QueueDrainerThread}} to main testing method, this is actually the way {{SourceReaderBase}} taken. This way, after {{SplitFetcher.runOnce}} joined, we can safely assert {{SplitFetcher.isIdle()}} and {{FutureCompletingBlockingQueue.getAvailabilityFuture().isDone()}}. But then, this test looks almost same to its no concurrent counterpart.

Seems that we are testing wrong assumption, may be we can just delete this test ?

[~roman_khachatryan] [~dian.fu] [~sewen] [~becket_qin] Any thoughts ?;;;","12/Oct/20 12:15;sewen;You are right. The test is somehow not correctly written.

What we really need to check here is the following: We must have either of the following conditions being true:
  - Either the fetcher was already set to idle when the thread pulled the fetch from the queue, because that is when the reader thread is supposed to check for fetcher being idle and shutting it down (which signals end of input)
  - Or the fetcher was not yet marked idle, because the fetch was pulled before that flag was set. In that case, the future must be complete, so that the reader thread goes back to checking the fetcher.

I prepared a PR that changes this. As a side effect, this should also fix FLINK-19489.;;;","12/Oct/20 12:24;sewen;Here is the PR: https://github.com/apache/flink/pull/13593;;;","13/Oct/20 09:38;sewen;Fixed in 1.12 (master) via 401f56fe9d6b0271260edf9787cdcbfe4d03874d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ArrayIndexOutOfBoundsException when executing DELETE statement in JDBC upsert sink,FLINK-19423,13329522,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,limbo,limbo,26/Sep/20 09:29,13/Oct/20 10:45,13/Jul/23 08:12,13/Oct/20 10:45,1.11.1,,,,,1.11.3,,,Connectors / JDBC,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"We found  that the primary key position can cause  ArrayIndexOutOfBoundsException

the sink like that( the primary key select the position of 1, 3):
{code:java}
CREATE TABLE `test`(
  col1 STRING, 
  col2 STRING, 
  col3 STRING, 
  PRIMARY KEY (col1, col3) NOT ENFORCED ) WITH (
  'connector' = 'jdbc',
  ...
){code}
when the DELETE (cdc message) come , it will raise ArrayIndexOutOfBoundsException:
{code:java}
Caused by: java.lang.RuntimeException: Writing records to JDBC failed.    ... 10 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 2    at org.apache.flink.table.data.GenericRowData.getString(GenericRowData.java:169)    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$1(RowData.java:310)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.getPrimaryKey(JdbcDynamicOutputFormatBuilder.java:216)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.lambda$createRowKeyExtractor$7(JdbcDynamicOutputFormatBuilder.java:193)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.lambda$createKeyedRowExecutor$3fd497bb$1(JdbcDynamicOutputFormatBuilder.java:128)    at org.apache.flink.connector.jdbc.internal.executor.KeyedBatchStatementExecutor.executeBatch(KeyedBatchStatementExecutor.java:71)    at org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor.executeBatch(BufferReduceStatementExecutor.java:99)    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:200)    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:171)    ... 8 more
{code}
 ",,jark,leonard,libenchao,limbo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 10:45:25 UTC 2020,,,,,,,,,,"0|z0ixr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/20 09:47;limbo;KeyedBatchStatementExecutor.java
{code:java}
@Override
public void addToBatch(T record) {
   batch.add(keyExtractor.apply(record));
}

@Override
public void executeBatch() throws SQLException {
   if (!batch.isEmpty()) {
      for (K entry : batch) {
         parameterSetter.accept(st, entry);
         st.addBatch();
      }
      st.executeBatch();
      batch.clear();
   }
}
{code}
It looks like the addToBatch select the key and save as <col1, col3>,  and the executeBatch select the position \{0, 2}, so raise the exception;;;","11/Oct/20 06:54;limbo;I'm trying to fix the code, and do some test, but find that the table API not support DELETE , is there any function to test this case;;;","12/Oct/20 02:23;jark;The following SQL will produce DELETE/UPDATE_BEFORE to the sink. 
{code:sql}
select * 
from (
  select key, count(*) as cnt
  from T
  group by key
) where cnt < 10;
{code}
;;;","13/Oct/20 02:18;limbo;Good job, Thanks to pull request;;;","13/Oct/20 06:19;jark;I cound't reproduce the exception in master branch. I guess it has been fixed in master by FLINK-15728. So we would only fix this in 1.11 branch. ;;;","13/Oct/20 10:45;jark;Fixed in release-1.11 branch: 42320ef205cb6afd98ffc9c2756a4abc7dc042e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Avro Confluent Schema Registry nightly end-to-end test failed with ""Register operation timed out; error code: 50002""",FLINK-19422,13329510,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dian.fu,dian.fu,26/Sep/20 01:44,20/Oct/20 06:58,13/Jul/23 08:12,20/Oct/20 06:58,1.12.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6955&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-09-25T14:09:18.2560779Z Caused by: java.io.IOException: Could not register schema in registry
2020-09-25T14:09:18.2561395Z 	at org.apache.flink.formats.avro.registry.confluent.ConfluentSchemaRegistryCoder.writeSchema(ConfluentSchemaRegistryCoder.java:91) ~[?:?]
2020-09-25T14:09:18.2562127Z 	at org.apache.flink.formats.avro.RegistryAvroSerializationSchema.serialize(RegistryAvroSerializationSchema.java:82) ~[?:?]
2020-09-25T14:09:18.2562883Z 	at org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper.serialize(KafkaSerializationSchemaWrapper.java:71) ~[?:?]
2020-09-25T14:09:18.2563622Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:866) ~[?:?]
2020-09-25T14:09:18.2564255Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99) ~[?:?]
2020-09-25T14:09:18.2565375Z 	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2566540Z 	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2567692Z 	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2568852Z 	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2570022Z 	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2571315Z 	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:76) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2572586Z 	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2573736Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2574824Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2576019Z 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2577309Z 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collectWithTimestamp(StreamSourceContexts.java:111) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2578256Z 	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:352) ~[?:?]
2020-09-25T14:09:18.2579003Z 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:185) ~[?:?]
2020-09-25T14:09:18.2579687Z 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141) ~[?:?]
2020-09-25T14:09:18.2580733Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755) ~[?:?]
2020-09-25T14:09:18.2582441Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2583534Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2584695Z 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2585556Z Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Register operation timed out; error code: 50002
2020-09-25T14:09:18.2586258Z 	at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:275) ~[?:?]
2020-09-25T14:09:18.2586871Z 	at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:334) ~[?:?]
2020-09-25T14:09:18.2587494Z 	at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:434) ~[?:?]
2020-09-25T14:09:18.2588119Z 	at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:426) ~[?:?]
2020-09-25T14:09:18.2588734Z 	at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:412) ~[?:?]
2020-09-25T14:09:18.2589423Z 	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.registerAndGetId(CachedSchemaRegistryClient.java:140) ~[?:?]
2020-09-25T14:09:18.2590396Z 	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.register(CachedSchemaRegistryClient.java:196) ~[?:?]
2020-09-25T14:09:18.2591103Z 	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.register(CachedSchemaRegistryClient.java:172) ~[?:?]
2020-09-25T14:09:18.2591838Z 	at org.apache.flink.formats.avro.registry.confluent.ConfluentSchemaRegistryCoder.writeSchema(ConfluentSchemaRegistryCoder.java:86) ~[?:?]
2020-09-25T14:09:18.2592587Z 	at org.apache.flink.formats.avro.RegistryAvroSerializationSchema.serialize(RegistryAvroSerializationSchema.java:82) ~[?:?]
2020-09-25T14:09:18.2593352Z 	at org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper.serialize(KafkaSerializationSchemaWrapper.java:71) ~[?:?]
2020-09-25T14:09:18.2594156Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:866) ~[?:?]
2020-09-25T14:09:18.2594805Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99) ~[?:?]
2020-09-25T14:09:18.2595953Z 	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2597091Z 	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2598379Z 	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2600143Z 	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2601355Z 	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2602660Z 	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:76) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2603895Z 	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2605041Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2606110Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2607310Z 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2608642Z 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collectWithTimestamp(StreamSourceContexts.java:111) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2609528Z 	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:352) ~[?:?]
2020-09-25T14:09:18.2610261Z 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:185) ~[?:?]
2020-09-25T14:09:18.2610968Z 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141) ~[?:?]
2020-09-25T14:09:18.2611631Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755) ~[?:?]
2020-09-25T14:09:18.2612635Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2613693Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-09-25T14:09:18.2614875Z 	at org.apache.flink.streaming.runtime.task
{code}",,dian.fu,dwysakowicz,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 06:58:05 UTC 2020,,,,,,,,,,"0|z0ixog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/20 00:16;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7184&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","05/Oct/20 12:34;dwysakowicz;I think this is a problem  in the confluent schema registry. I find some log entries in the registry initialization process, which if searched for lead to similar reports. There are not many helpful answers there though. I observed though we are using very old versions of kafka (and transiently zookeeper) and registry. I'd suggest upgrading the versions and see if the issue still occurs. I opened a PR with upgrade versions. ;;;","06/Oct/20 08:19;dwysakowicz;Upgraded components in b2d2af45bda167d376f723d8c08177b6a821c4c1;;;","06/Oct/20 08:20;dwysakowicz;I will keep the issue open for another week. If it does not reappear I will close it.;;;","20/Oct/20 06:58;rmetzger;Closing as fixed for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of the method from_data_stream in table_environement,FLINK-19417,13329389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,hxbks2ks,hxbks2ks,25/Sep/20 12:15,19/Feb/21 07:28,13/Jul/23 08:12,28/Sep/20 05:00,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,"The parameter fields should be str or expression *, not the current list [str]. And the table_env object passed to the Table object should be Python's TableEnvironment, not Java's TableEnvironment",,dian.fu,hxbks2ks,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 28 05:00:34 UTC 2020,,,,,,,,,,"0|z0iwxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/20 13:38;nicholasjiang;[~hxbks2ks], could you please assign to me for fix? I could fix the bug of the method from_data_stream in table_environement.;;;","26/Sep/20 14:19;dian.fu;[~nicholasjiang] Thanks a lot! I have assigned this JIRA to you!;;;","28/Sep/20 05:00;dian.fu;master: 900071bb7a9073f67b8af1097ee858e59626593c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleInputStreamTask fails with RuntimeException when its input contains union,FLINK-19411,13329356,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,TsReaper,TsReaper,25/Sep/20 07:40,19/Feb/21 07:28,13/Jul/23 08:12,25/Sep/20 13:54,1.12.0,,,,,1.12.0,,,API / DataStream,,,,,0,pull-request-available,,,,,"MultipleInputStreamTask fails with the following exception stack when its input contains union.

{code}
Caused by: java.lang.RuntimeException: No such input gate.
	at org.apache.flink.streaming.runtime.io.InputGateUtil.createInputGate(InputGateUtil.java:37)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545)
	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438)
	at org.apache.flink.streaming.runtime.io.InputProcessorUtil.createCheckpointedMultipleInputGate(InputProcessorUtil.java:104)
	at org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.createInputProcessor(MultipleInputStreamTask.java:120)
	at org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.init(MultipleInputStreamTask.java:98)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:480)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:550)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
{code}

Add the following test case to {{MultipleInputITCase}} will reproduce this bug.

{code:java}
@Test
public void testUnion() throws Exception {
	StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
	env.setParallelism(1);

	TestListResultSink<Long> resultSink = new TestListResultSink<>();

	DataStream<Long> source1 = env.fromElements(1L, 10L);
	DataStream<Long> source2 = env.fromElements(2L, 11L);
	DataStream<String> source3 = env.fromElements(""42"", ""44"");

	UnionTransformation<Long> union = new UnionTransformation<>(
		Arrays.asList(source1.getTransformation(), source2.getTransformation()));

	MultipleInputTransformation<Long> multipleInput = new MultipleInputTransformation<>(
		""My Operator"",
		new SumAllInputOperatorFactory(),
		BasicTypeInfo.LONG_TYPE_INFO,
		1);

	env.addOperator(multipleInput
		.addInput(union)
		.addInput(source3.getTransformation()));

	new MultipleConnectedStreams(env)
		.transform(multipleInput)
		.addSink(resultSink);

	env.execute();

	List<Long> result = resultSink.getResult();
	Collections.sort(result);
	long actualSum = result.get(result.size() - 1);
	assertEquals(1 + 10 + 2 + 11 + 42 + 44, actualSum);
}
{code}",,pnowojski,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 13:54:05 UTC 2020,,,,,,,,,,"0|z0iwqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/20 07:41;TsReaper;cc [~pnowojski];;;","25/Sep/20 13:54;pnowojski;Merged to master as 7bfd8ec1e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestAPIStabilityTest does not assert on enum changes,FLINK-19410,13329352,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,mapohl,mapohl,25/Sep/20 07:17,06/Oct/20 07:51,13/Jul/23 08:12,06/Oct/20 07:51,1.10.2,1.11.2,,,,1.12.0,,,Runtime / REST,Runtime / Web Frontend,,,,0,starter,,,,,"{{org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest}} does not assert on {{enum}} changes as happened in FLINK-16866.

The consequence is that the test is not failing even though there was a REST API change.",,klion26,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16866,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 07:51:50 UTC 2020,,,,,,,,,,"0|z0iwpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 07:51;chesnay;master: 623cfdb1dac999729f73f790a02bd8de3828130d ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting row time to timestamp loses nullability info,FLINK-19406,13329326,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,25/Sep/20 03:16,10/Oct/20 02:40,13/Jul/23 08:12,10/Oct/20 02:40,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,jark,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 10 02:40:37 UTC 2020,,,,,,,,,,"0|z0iwjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/20 02:40;lzljs3620320;master: a77f4a566629f2aa8d830a42964948344ec17afd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job stuck in restart loop due to excessive checkpoint recoveries which block the JobMaster,FLINK-19401,13329255,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,stevenz3wu,stevenz3wu,24/Sep/20 16:21,05/Aug/21 13:24,13/Jul/23 08:12,22/Oct/20 08:11,1.10.1,1.11.2,,,,1.10.3,1.11.3,1.12.0,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"Flink job sometimes got into a restart loop for many hours and can't recover until redeployed. We had some issue with Kafka that initially caused the job to restart. 

Below is the first of the many exceptions for ""ResourceManagerException: Could not find registered job manager"" error.
{code}
2020-09-19 00:03:31,614 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [flink-akka.actor.default-dispatcher-35973]  - Requesting new slot [SlotRequestId{171f1df017dab3a42c032abd07908b9b}] and profile ResourceP
rofile{UNKNOWN} from resource manager.
2020-09-19 00:03:31,615 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [flink-akka.actor.default-dispatcher-35973]  - Requesting new slot [SlotRequestId{cc7d136c4ce1f32285edd4928e3ab2e2}] and profile ResourceProfile{UNKNOWN} from resource manager.
2020-09-19 00:03:31,615 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [flink-akka.actor.default-dispatcher-35973]  - Requesting new slot [SlotRequestId{024c8a48dafaf8f07c49dd4320d5cc94}] and profile ResourceProfile{UNKNOWN} from resource manager.
2020-09-19 00:03:31,615 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [flink-akka.actor.default-dispatcher-35973]  - Requesting new slot [SlotRequestId{a591eda805b3081ad2767f5641d0db06}] and profile ResourceProfile{UNKNOWN} from resource manager.
2020-09-19 00:03:31,620 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [flink-akka.actor.default-dispatcher-35973]  - Source: k2-csevpc -> k2-csevpcRaw -> (vhsPlaybackEvents -> Flat Map, merchImpressionsClientLog -> Flat Map) (56/640) (1b0d3dd1f19890886ff373a3f08809e8) switched from SCHEDULED to FAILED.
java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed
        at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:433)
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forward$21(FutureUtils.java:1065)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792)
        at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153)
        at org.apache.flink.runtime.concurrent.FutureUtils.forward(FutureUtils.java:1063)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.createRootSlot(SlotSharingManager.java:155)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateMultiTaskSlot(SchedulerImpl.java:511)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateSharedSlot(SchedulerImpl.java:311)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.internalAllocateSlot(SchedulerImpl.java:160)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateSlotInternal(SchedulerImpl.java:143)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateSlot(SchedulerImpl.java:113)
        at org.apache.flink.runtime.executiongraph.SlotProviderStrategy$NormalSlotProviderStrategy.allocateSlot(SlotProviderStrategy.java:115)
        at org.apache.flink.runtime.scheduler.DefaultExecutionSlotAllocator.lambda$allocateSlotsFor$0(DefaultExecutionSlotAllocator.java:104)
        at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
        at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137)
        at org.apache.flink.runtime.scheduler.DefaultExecutionSlotAllocator.allocateSlotsFor(DefaultExecutionSlotAllocator.java:102)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlots(DefaultScheduler.java:342)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:311)
        at org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy.allocateSlotsAndDeploy(EagerSchedulingStrategy.java:76)
        at org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy.restartTasks(EagerSchedulingStrategy.java:57)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$restartTasks$1(DefaultScheduler.java:268)
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719)
        at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:701)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
        at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
        at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestNewAllocatedSlot(SlotPoolImpl.java:438)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.requestNewAllocatedSlot(SchedulerImpl.java:236)
        at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.allocateMultiTaskSlot(SchedulerImpl.java:506)
        ... 39 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.slotRequestToResourceManagerFailed(SlotPoolImpl.java:360)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.lambda$requestSlotFromResourceManager$1(SlotPoolImpl.java:348)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792)
        at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153)
        at org.apache.flink.runtime.concurrent.FutureUtils.whenCompleteAsyncIfNotDone(FutureUtils.java:941)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestSlotFromResourceManager(SlotPoolImpl.java:342)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestNewAllocatedSlotInternal(SlotPoolImpl.java:309)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestNewAllocatedSlot(SlotPoolImpl.java:437)
        ... 41 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not find registered job manager for job 70216adbeed914b35d77717c4b7b13ea.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
        at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
        at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
        at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:214)
        at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:129)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.invoke(FencedAkkaInvocationHandler.java:78)
        at com.sun.proxy.$Proxy94.requestSlot(Unknown Source)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.requestSlotFromResourceManager(SlotPoolImpl.java:337)
        ... 43 more
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not find registered job manager for job 70216adbeed914b35d77717c4b7b13ea.
        at org.apache.flink.runtime.resourcemanager.ResourceManager.requestSlot(ResourceManager.java:443)
        at sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
        ... 20 more
{code}


Grepped through our log.  job manager registration happened once when the job was deployed a few days ago.
{code}
2020-09-16 17:23:28,081 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-60]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
{code}

Then there were a flurry of 9 registrations in the same milli-seconds that happened ~30 mins after the first error of "" Could not find registered job manager"". The issue persisted many hours after this. Because this is a pre-prod job, so we didn't have alert on it.
{code}
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,827 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager [flink-akka.actor.default-dispatcher-35963]  - Registered job manager a7263fdf0cd75d4d6481858f89894876@akka.tcp://flink@100.118.253.133:43917/user/jobmanager_0 for job 70216adbeed914b35d77717c4b7b13ea.
2020-09-19 00:33:07,828 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [flink-akka.actor.default-dispatcher-35968]  - JobManager successfully registered at ResourceManager, leader id: bf239dac186bc8ba901a8702f4bb42e3.
{code}

I have the job manager logs for the hour with INFO level that I can share offline if needed.
 ",,AHeise,klion26,nira-david,rmetzger,roman,stevenz3wu,trohrmann,wind_ljy,ym,yuyang08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19596,,,,,,,FLINK-19596,,,,FLINK-13698,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 08:11:39 UTC 2020,,,,,,,,,,"0|z0iw40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/20 14:05;trohrmann;Thanks for reporting this issue [~stevenz3wu]. Could you share the JM logs with me privately? They might show us what's going on on the JM side during this time.

Is the problem reproducible? If yes, then the DEBUG logs would be very helpful as well.;;;","28/Sep/20 15:29;stevenz3wu;[~trohrmann] emailed you the JM logs with INFO level logging.

It is not easily reproducible, maybe it happens once every 1-2 weeks for a high-parallelism (~1,500) and large state (TBs) job. If DEBUG is required, we can try to enable the DEBUG level logging and wait for it to happen again.;;;","29/Sep/20 13:06;trohrmann;Thanks for providing me with the logs [~stevenz3wu]. I think the debug logs are not necessary. What I found is the following:

First of all the {{ResourceManager}} behaviour seems to be justified. What happens is that the {{JobMaster}} connects to the {{ResourceManager}} but then fails to respond to the heartbeats (for the reason see further below). Due to this timeout, the {{ResourceManager}} disconnects from the {{JobMaster}} and rejects future slot requests.

The reason why the {{JobMaster}} fails to respond to the heartbeats is because of an excessive amount of checkpoint recoveries which are happening in the {{JobMaster's}} main thread and are blocking it. The excessive amount of checkpoint recoveries is caused by the embarrassingly parallel nature of the job. After the first failure, I believe that each embarrassingly parallel component of the job will be recovered individually by the {{RestartPipelinedRegionFailoverStrategy}}. Each of these pipelined regions will then ask the {{CheckpointCoordinator}} to restore its state. This operation, however, triggers a recovery of the latest completed checkpoints which causes over and over again the same reads to s3.

I think there are two problems here:

1) Checkpoint recovery operations are being executed from within the {{JobMaster's}} main thread. Since these operations can be blocking, we should not do it. I think this problem should be solved with FLINK-13698.
2) Each restore operation {{CheckpointCoordinator.restoreLatestCheckpointedStateInternal}} will try to recover the latest completed checkpoints. For successive failovers, this is prohibitively expensive and should be avoided.

Since 1) will be dealt with in FLINK-13698, I think that this issue can concentrate on 2). [~pnowojski] and [~roman_khachatryan] are you aware of this problem?;;;","29/Sep/20 13:07;trohrmann;Since the problem seems to be caused by the {{CheckpointCoordintor}}, I will update the component accordingly.;;;","29/Sep/20 15:27;stevenz3wu;[~trohrmann] thanks a lot for looking into the problem. Just a small clarification on the job, it is a stream join job with keyBy and co-process functions.;;;","30/Sep/20 07:12;trohrmann;In the logs it looks as if you also have an embarrassingly parallel part: Several sources which are chained with a flat map and then two filters which are chained with a process function and sinks. I believe that this part causes the repeated checkpoint recoveries.;;;","30/Sep/20 13:27;stevenz3wu;[~trohrmann] just to clarify, do you mean operator chain caused the repeated checkpoint recoveries? i.e.  if an operator chain has 3 operators, we are repeating checkpoint recoveries 3 times no matter the 3 operators has state or not.

In this job, only two Kafka sources and the co-process functions have state. It has 3 operator chains as Till described (two operator chains for source, one operator chain for co-process+sink).

 ;;;","30/Sep/20 15:43;trohrmann;No, the 3 operator chains are recovered together. However, we do this operation for every subtask index which means that we read the checkpoints {{parallelism}} times.;;;","05/Oct/20 21:28;roman;I do see that the same data is downloaded over and over, but I'm not sure whether it's the root cause.
 From the logs I see that heartbeats *on JM* from TMs are expired first, and heartbeats *on TMs* from JM expire two minutes later:
{code:java}
flink-19401 $ grep -ah -B1 'Heartbeat of Task' *.log | grep '^2020' | cut -c-16 | sort | uniq -c
     66 2020-09-18 23:57
     11 2020-09-19 01:06
flink-19401 $ grep -ah -B1 'heartbeat of Job' *.log | grep '^2020' | cut -c-16 | sort | uniq -c
    509 2020-09-18 23:59
      2 2020-09-19 00:03
      2 2020-09-19 00:32
      2 2020-09-19 00:37
      2 2020-09-19 01:06
{code}
(77 vs 517 is due to parallelism level I think)

 

Furthermore, the first 8 recoveries start after failures in Kafka: NPE in NetworkClient and ""SSLProtocolException: Handshake message sequence violation, 2"".

Then they happen after Kafka failures and the aforementioned TM heartbeat timeouts on JM.

 

So it looks like that the root cause is misconfiguration and/or network.

WDYT [~trohrmann], [~stevenz3wu] ?

 

Steven, can you please also explain what Titus is doing here?
 (looks like it'is adding and then removing the task every minute - does it ring any bell?)
{code:java}
$ grep 72cc604b-dcd5-4c75-a466-0321d7c51c3e *.log

2020-09-18 23:55:43,804 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60067 ms.
2020-09-18 23:56:11,256 - Deploying Source: ee_clevent_presentation -> ee_clevent_presentationRaw -> cleventPresentation -> Flat Map (510/640) (attempt #1) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:56:11,256 - Deploying Source: k2-logtracevpc -> k2-logtracevpcRaw -> (Filter -> cybertronLogReplicated, Filter -> cybertronLogReplicatedCrossRegion, gpsRequestPivotAudit, gpsRequestPivotAuditCrossRegion) (510/640) (attempt #1) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:56:11,261 - Deploying Source: k2-ee_clevent -> k2-ee_cleventRaw -> (cleventAddToPlaylistCommand -> Flat Map, cleventThumbRating -> Flat Map) (510/640) (attempt #1) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:56:11,261 - Deploying Source: k2-defaultvpc -> k2-defaultvpcRaw -> (PssiPlaybackEvents -> Flat Map, Filter -> cybertronLogUnreplicated) (510/640) (attempt #1) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:56:11,261 - Deploying Filter -> Process -> (Sink: cybertron_joiner_client_input_0, Sink: cybertron_joiner_client_input_1) (510/640) (attempt #1) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:56:11,261 - Deploying Filter -> Process -> (Sink: cybertron_joiner_server_input_0, Sink: cybertron_joiner_server_input_1) (510/640) (attempt #1) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:56:43,872 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
2020-09-18 23:57:43,940 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60068 ms.
2020-09-18 23:58:44,008 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
2020-09-18 23:59:11,794 - Deploying Source: k2-defaultvpc -> k2-defaultvpcRaw -> (PssiPlaybackEvents -> Flat Map, Filter -> cybertronLogUnreplicated) (488/640) (attempt #3) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:59:27,101 - Deploying Source: k2-defaultvpc -> k2-defaultvpcRaw -> (PssiPlaybackEvents -> Flat Map, Filter -> cybertronLogUnreplicated) (478/640) (attempt #2) to 1273fbc6edbab45548807742b2db6c4e @ 72cc604b-dcd5-4c75-a466-0321d7c51c3e (dataPort=39833)
2020-09-18 23:59:44,075 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60066 ms.
2020-09-19 00:00:44,144 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
2020-09-19 00:01:44,634 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60489 ms.
2020-09-19 00:02:44,710 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
2020-09-19 00:03:44,799 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60088 ms.
2020-09-19 00:04:44,871 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
2020-09-19 00:05:44,938 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60067 ms.
2020-09-19 00:06:45,023 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
2020-09-19 00:07:45,427 - Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e is being removed from TM termination pool after 60404 ms.
2020-09-19 00:08:45,506 - Adding Titus task 72cc604b-dcd5-4c75-a466-0321d7c51c3e to TM termination pool because TM is not registered.
{code};;;","06/Oct/20 00:42;stevenz3wu;[~roman_khachatryan] I don't know if repeated checkpoint recovery is the root/main cause or not. [~trohrmann] identified two problems during his investigation. This is one of the identified problem. 

Regarding the logs related to Titus, please ignore them. They are just noise. We haven't cleaned up our logs yet.;;;","06/Oct/20 06:19;roman;Thanks for the clarification [~stevenz3wu].

 

I looked at the code again and I think [~trohrmann] is right. The order of heartbeat timeouts is because TMs don't request heartbeats, only respond.;;;","21/Oct/20 19:35;arvid;Added to master as be448ecbd97ff9f79daac9ec7b6ec022d8c4e18d.;;;","22/Oct/20 08:11;arvid;Merged into release-1.11 as bff79f5efffccd1793e09a5e08d0ceb9fe90cf2
Merged into release-1.10 as 88c06e38d4eff06ac09e4141b988f9a561f286a4
Closing as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removed unused BufferPoolOwner,FLINK-19400,13329238,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,AHeise,24/Sep/20 15:32,22/Jun/21 14:07,13/Jul/23 08:12,01/Oct/20 06:43,1.12.0,,,,,,,,Runtime / Network,,,,,0,,,,,,{{BufferPoolOwner}} does not have any production usages and just complicates a few tests.,,AHeise,nicholasjiang,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16972,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 06:43:33 UTC 2020,,,,,,,,,,"0|z0iw08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/20 07:59;nicholasjiang;[~AHeise], I agree with you. IMO, BufferPoolOwner only works for releasing memory buffers when taking up too much memory, and releaseInternal of ResultPartition could also release the memory buffers, no need to implement BufferPoolOwner to release memory separately which doesn't have any usages. I think that release much memory operation could be involved in releaseInternal.
[~zjwang],[~kevin.cyj] what do you think about?;;;","25/Sep/20 08:16;arvid;I think Zhijiang already gave green light.

[~sewen], [~pnowojski] also did not object, so I'd start with it. I'm waiting a few days until actually going through with it.;;;","27/Sep/20 07:40;zjwang;+1 to remove, it is indeed no actual usages in core codes, as `ResultPartition#releaseMemory` is empty and we have to pass `nullable` owner in related constructors and maintain the related logics.;;;","01/Oct/20 06:43;arvid;Merged into master as part of FLINK-16972.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive connector fails with IllegalAccessError if submitted as usercode,FLINK-19398,13329211,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,fhueske,fhueske,24/Sep/20 13:02,04/Dec/20 04:59,13/Jul/23 08:12,03/Dec/20 08:11,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Using Flink's Hive connector fails if the dependency is loaded with the user code classloader with the following exception.


{code:java}
java.lang.IllegalAccessError: tried to access method org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.<init>(Lorg/apache/flink/core/fs/Path;Lorg/apache/flink/streaming/api/functions/sink/filesystem/BucketAssigner;Lorg/apache/flink/streaming/api/functions/sink/filesystem/BucketFactory;Lorg/apache/flink/streaming/api/functions/sink/filesystem/BucketWriter;Lorg/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicy;ILorg/apache/flink/streaming/api/functions/sink/filesystem/OutputFileConfig;)V from class org.apache.flink.streaming.api.functions.sink.filesystem.HadoopPathBasedBulkFormatBuilder
	at org.apache.flink.streaming.api.functions.sink.filesystem.HadoopPathBasedBulkFormatBuilder.createBuckets(HadoopPathBasedBulkFormatBuilder.java:127) ~[flink-connector-hive_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.table.filesystem.stream.StreamingFileWriter.initializeState(StreamingFileWriter.java:81) ~[flink-table-blink_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:106) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:258) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:479) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:475) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:528) ~[flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.12-1.11.2-stream2-SNAPSHOT.jar:1.11.2-stream2-SNAPSHOT]
{code}

The problem is the constructor of {{Buckets}} with default visibility which is called from {{HadoopPathBasedBulkFormatBuilder}} . This works as long as both classes are loaded with the same classloader but when they are loaded in different classloaders, the access fails.

{{Buckets}} is loaded with the system CL because it is part of flink-streaming-java. 

 

To solve this issue, we should change the visibility of the {{Buckets}} constructor to {{public}}.

 ",,dian.fu,dwysakowicz,fhueske,gaoyunhaii,gengmao,kezhuw,klion26,leonard,lzljs3620320,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20151,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 08:11:50 UTC 2020,,,,,,,,,,"0|z0ivu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/20 03:50;lzljs3620320;CC: [~gaoyunhaii];;;","19/Oct/20 03:54;gaoyunhaii;I'll have a look at this issue. Very sorry for the inconvenience brought. ;;;","17/Nov/20 08:21;gaoyunhaii;Hi I opened a PR for this issue. Currently I leave _HadoopPathBasedBulkFormatBuilder_ in the original package and add a specialized test to verify that it could be used normally in the user classloader. I did so since logically  _HadoopPathBasedBulkFormatBuilder_  could be logically used by other users and it might be better to keep compatible, although theoretically it should not be frequent. ;;;","18/Nov/20 03:01;xtsong;I'm deferring the fix version to 1.11.4 for now. Will change it back if it makes the 1.11.3 release in the end.;;;","24/Nov/20 01:29;dian.fu;[~lzljs3620320] Could you help to review the PR submitted by [~gaoyunhaii]?;;;","25/Nov/20 02:11;lzljs3620320;master (1.12): cb79fd88c85480e1932218c82f2532e9b0dd652c

Needs PR for 1.11.;;;","01/Dec/20 15:43;leonard;[~gaoyunhaii] Could you raise a PR for 1.11 branch and then we can close this clocker after fixed in 1.11 branch ?;;;","02/Dec/20 03:10;gaoyunhaii;OK, I'll open the PR, very sorry for not noticed the message.;;;","03/Dec/20 08:11;lzljs3620320;release-1.11: 4616f7d954275081a9a97c5745d5f55072241d17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock during partition update,FLINK-19391,13329150,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,arvid,AHeise,24/Sep/20 07:23,22/Jun/21 14:07,13/Jul/23 08:12,24/Sep/20 14:17,1.12.0,,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,,,"Master cron job is currently failing because of a deadlock introduced in FLINK-19026.
{noformat}
2020-09-23T21:50:39.2444176Z Found one Java-level deadlock:
2020-09-23T21:50:39.2444633Z =============================
2020-09-23T21:50:39.2445001Z ""Temp writer"":
2020-09-23T21:50:39.2445484Z   waiting to lock monitor 0x00007f4e14004ca8 (object 0x0000000086501948, a java.lang.Object),
2020-09-23T21:50:39.2446418Z   which is held by ""flink-akka.actor.default-dispatcher-2""
2020-09-23T21:50:39.2447193Z ""flink-akka.actor.default-dispatcher-2"":
2020-09-23T21:50:39.2447903Z   waiting to lock monitor 0x00007f4e14004bf8 (object 0x0000000086501930, a org.apache.flink.runtime.io.network.partition.PrioritizedDeque),
2020-09-23T21:50:39.2448703Z   which is held by ""Temp writer""
2020-09-23T21:50:39.2448965Z 
2020-09-23T21:50:39.2449384Z Java stack information for the threads listed above:
2020-09-23T21:50:39.2449900Z ===================================================
2020-09-23T21:50:39.2450325Z ""Temp writer"":
2020-09-23T21:50:39.2451050Z 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.checkAndWaitForSubpartitionView(LocalInputChannel.java:244)
2020-09-23T21:50:39.2452264Z 	- waiting to lock <0x0000000086501948> (a java.lang.Object)
2020-09-23T21:50:39.2453183Z 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.getNextBuffer(LocalInputChannel.java:205)
2020-09-23T21:50:39.2454173Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:642)
2020-09-23T21:50:39.2455422Z 	- locked <0x0000000086501930> (a org.apache.flink.runtime.io.network.partition.PrioritizedDeque)
2020-09-23T21:50:39.2456310Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:619)
2020-09-23T21:50:39.2457311Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNext(SingleInputGate.java:602)
2020-09-23T21:50:39.2458205Z 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.getNext(InputGateWithMetrics.java:105)
2020-09-23T21:50:39.2459258Z 	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:100)
2020-09-23T21:50:39.2460465Z 	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47)
2020-09-23T21:50:39.2461344Z 	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59)
2020-09-23T21:50:39.2462164Z 	at org.apache.flink.runtime.operators.TempBarrier$TempWritingThread.run(TempBarrier.java:178)
2020-09-23T21:50:39.2463418Z ""flink-akka.actor.default-dispatcher-2"":
2020-09-23T21:50:39.2464109Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.queueChannel(SingleInputGate.java:825)
2020-09-23T21:50:39.2465336Z 	- waiting to lock <0x0000000086501930> (a org.apache.flink.runtime.io.network.partition.PrioritizedDeque)
2020-09-23T21:50:39.2466228Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.notifyChannelNonEmpty(SingleInputGate.java:791)
2020-09-23T21:50:39.2467222Z 	at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.notifyChannelNonEmpty(InputChannel.java:154)
2020-09-23T21:50:39.2468212Z 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.notifyDataAvailable(LocalInputChannel.java:236)
2020-09-23T21:50:39.2469577Z 	at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.createSubpartitionView(ResultPartitionManager.java:76)
2020-09-23T21:50:39.2470607Z 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.requestSubpartition(LocalInputChannel.java:133)
2020-09-23T21:50:39.2471765Z 	- locked <0x0000000086501948> (a java.lang.Object)
2020-09-23T21:50:39.2472685Z 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.updateInputChannel(SingleInputGate.java:489)
2020-09-23T21:50:39.2473727Z 	- locked <0x0000000086532500> (a java.lang.Object)
2020-09-23T21:50:39.2474449Z 	at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.updatePartitionInfo(NettyShuffleEnvironment.java:279)
2020-09-23T21:50:39.2475394Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$updatePartitions$12(TaskExecutor.java:758)
2020-09-23T21:50:39.2476235Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutor$$Lambda$406/1860601696.run(Unknown Source)
2020-09-23T21:50:39.2476973Z 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2020-09-23T21:50:39.2477714Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-09-23T21:50:39.2478698Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-09-23T21:50:39.2479506Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-23T21:50:39.2480263Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-23T21:50:39.2481018Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-23T21:50:39.2481727Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-23T21:50:39.2482192Z 
{noformat}
 

The deadlock was introduced by the alternative fix for FLINK-12510 in FLINK-19026 .

The alternative fix avoided double lock acquisition in {{SingleInputGate#waitAndGetNextData}} by moving {{notifyDataAvailable}} from {{ResultSubpartition#createReadView}} to {{ResultPartitionManager#createSubpartitionView}}, which solved the circular deadlock of FLINK-12510 by not acquiring the {{buffers}} lock of {{PipelinedSubpartition}}.

However, that fix didn't go far enough. For local channels, it is still possible to create a similar deadlock. While {{SingleInputGate}} reads from {{LocalInputChannel}}, it holds the lock {{inputChannelsWithData}}. The channel may start requesting the partition and tries to acquire {{requestLock}}. 
At the same time there is an update of partition info, which acquires the {{requestLock}} and notifies the {{SingleInputGate}}, which needs to lock on {{inputChannelsWithData}}.

The solution is to first acquire the partition and release {{requestLock}} before notifying {{SingleInputGate}}.",,AHeise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19387,,,,,,,,FLINK-19026,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 24 14:17:02 UTC 2020,,,,,,,,,,"0|z0ivgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/20 14:17;arvid;Merged into master as https://github.com/apache/flink/commit/c3fab5173c1127c970ee992e8bde948abd22dced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Streaming bucketing end-to-end test failed with ""Number of running task managers has not reached 4 within a timeout of 40 sec""",FLINK-19388,13329125,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,dian.fu,dian.fu,24/Sep/20 03:56,19/Feb/21 07:32,13/Jul/23 08:12,30/Sep/20 13:26,1.12.0,,,,,1.12.0,,,Connectors / FileSystem,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6876&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-09-24T03:13:12.1370987Z Starting taskexecutor daemon on host fv-az661.
2020-09-24T03:13:12.1773280Z Number of running task managers 2 is not yet 4.
2020-09-24T03:13:16.4342638Z Number of running task managers 3 is not yet 4.
2020-09-24T03:13:20.4570976Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:24.4762428Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:28.4955622Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:32.5110079Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:36.5272551Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:40.5672343Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:44.5857760Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:48.6039181Z Number of running task managers 0 is not yet 4.
2020-09-24T03:13:52.6056222Z Number of running task managers has not reached 4 within a timeout of 40 sec
2020-09-24T03:13:52.9275629Z Stopping taskexecutor daemon (pid: 13694) on host fv-az661.
2020-09-24T03:13:53.1753734Z No standalonesession daemon (pid: 10610) is running anymore on fv-az661.
2020-09-24T03:13:53.5812242Z Skipping taskexecutor daemon (pid: 10912), because it is not running anymore on fv-az661.
2020-09-24T03:13:53.5813449Z Stopping taskexecutor daemon (pid: 11330) on host fv-az661.
2020-09-24T03:13:53.5818053Z Stopping taskexecutor daemon (pid: 11632) on host fv-az661.
2020-09-24T03:13:53.5819341Z Skipping taskexecutor daemon (pid: 11965), because it is not running anymore on fv-az661.
2020-09-24T03:13:53.5820870Z Skipping taskexecutor daemon (pid: 12906), because it is not running anymore on fv-az661.
2020-09-24T03:13:53.5821698Z Stopping taskexecutor daemon (pid: 13392) on host fv-az661.
2020-09-24T03:13:53.5839544Z [FAIL] Test script contains errors.
{code}",,azagrebin,dian.fu,guoyangze,klion26,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19426,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 00:36:13 UTC 2020,,,,,,,,,,"0|z0ivb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/20 11:10;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6882&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","25/Sep/20 00:56;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6921&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","25/Sep/20 02:25;dian.fu;I found the following exception in the log:
{code}
2020-09-24 03:13:06,860 ERROR org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor         [] - Error while executing remote procedure call public void org.apache.flink.runtime.jobmaster.JobMaster.notifyAllocationFailure(org.apache.flink.runtime.clusterframework.types.AllocationID,java.lang.Exception).
java.lang.reflect.InvocationTargetException: null
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_265]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_265]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_265]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_265]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: java.util.ConcurrentModificationException
        at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719) ~[?:1.8.0_265]
        at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742) ~[?:1.8.0_265]
        at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:218) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:154) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFailingAllocatedSlot(SlotPoolImpl.java:774) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.failAllocation(SlotPoolImpl.java:754) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.internalFailAllocation(JobMaster.java:597) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.notifyAllocationFailure(JobMaster.java:735) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code};;;","25/Sep/20 02:26;dian.fu;Upgrade it to ""Blocker""!;;;","25/Sep/20 02:27;dian.fu;cc [~azagrebin] Could you help to take a look at this issue to see if it's related to FLINK-18957?;;;","25/Sep/20 09:01;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6942&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","26/Sep/20 01:26;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6962&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","26/Sep/20 01:27;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6969&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","27/Sep/20 01:08;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6983&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","28/Sep/20 07:37;azagrebin;[~dian.fu] 
 This looks like a cluster resource management problem before starting the job.

FLINK-18957 changed the job resource management which happens when the task managers are already up in the cluster.

but the exception looks related to FLINK-18689, we should check it;;;","30/Sep/20 13:26;azagrebin;merged into master by 8d50daa7e13e00cffbd09efac73e6bea0e48ee5c;;;","01/Oct/20 00:36;dian.fu;[~azagrebin] Thanks a lot for the quick fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source API exception signatures are inconsistent,FLINK-19384,13329025,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,23/Sep/20 15:55,22/Jun/21 13:55,13/Jul/23 08:12,06/Nov/20 16:44,1.11.0,1.11.1,1.11.2,,,1.12.0,,,API / Core,,,,,0,pull-request-available,,,,,"The methods in {{org.apache.flink.api.connector.source.Source}} have inconsistent exception signatures:
  - the methods to create reader and enumerator do not throw a checked exception
  - the method to restore an enumerator throws an {{IOException}}.

Either all methods should allow throwing checked IOExceptions or all methods should not allo any checked exceptions.",,AHeise,becket_qin,dian.fu,klion26,rmetzger,sewen,yongsheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 16:44:04 UTC 2020,,,,,,,,,,"0|z0iup4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 15:56;sewen;Given the practices in the remaining Flink code base, I would lean towards allowing checked Exceptions {{IOException}} or even {{Exception}} in all of the three above-mentioned methods.

[~jqin] what do you think?;;;","13/Oct/20 01:53;becket_qin;[~sewen] Thanks for catching this. Throwing `Exception` in all the three cases sounds a good idea. It gives the source developer a clear indication on what to do if something goes wrong.;;;","13/Oct/20 09:30;sewen;Yes, signaling to the developer to just let exceptions bubble up seems a good idea here.;;;","14/Oct/20 06:15;arvid;Please don't go the {{Exception}} route: it's impossible to react appropriately except by resorting to Pokemon exception handling (gonna catch them all). Either list all the possible checked exceptions explicitly or go with runtime exceptions.

Also read Joshua Bloch's take of exception handling in [Effective Java|https://ahdak.github.io/blog/effective-java-part-9/].

{noformat}
Item 70: ""Use checked exceptions for conditions from which the caller can reasonably be expected to recover. You force the caller to handle the exception in a catch block or to propagate it.""
{noformat}
I'd argue it's impossible to recover in a reasonable way if you catch all {{Exception}}s, since that also includes all {{RuntimeException}}s, which are by this definition not recoverable.

{noformat}
Item 71: ""Ask yourself how the programmer will handle the exception ? If the programmer can’t do better than e.printStackTrace() or throw new AssertionError(), then, an unchecked exception is called for.""
Without any way to detect which exception is thrown, you are pretty much forced to just propagate it.

{noformat}
Item 72: ""Do not reuse Exception, RuntimeException , Throwable or Error directly. Treat these classes as if they were abstract.""
{noformat}
This is more about not throwing an {{Exception}}, but at the same time it also means that if your API simply declares {{throws Exception}}, it is well possible in the API to use it in this way.;;;","14/Oct/20 06:26;chesnay;[~AHeise] I don't think these principles really apply here; there's no way to recover from any exception thrown from user-code anyway. All we'd be doing is forcing users to wrap everything in a RuntimeException, which is just inconvenient. And don't we have to catch everything from user-code in any case?;;;","14/Oct/20 11:26;arvid;Sorry I rechecked the code and I completely agree [~chesnay]. It's very similar to {{MapFunction}} and co, where we also do it. 
;;;","14/Oct/20 20:05;sewen;The Josh Bloch guide is what I try to follow in Flink's internal implementations.

For user functions and the like it is as you and Chesnay said - just spare them the inconvenience to wrap into some permitted exceptions (or RuntimeException).;;;","27/Oct/20 08:37;rmetzger;[~sewen] [~becket_qin] what is the status of this discussion? We need to merge a fix for this this week before the feature freeze.;;;","27/Oct/20 10:13;sewen;[~becket_qin] I can take this unless you have already started on this one.;;;","05/Nov/20 15:56;rmetzger;Is this blocker expected to be merged before the feature freeze on Sunday?
Since this is an API breaking change, I would prefer to have it resolved rather soon.;;;","05/Nov/20 17:05;sewen;PR is ready: https://github.com/apache/flink/pull/13953;;;","06/Nov/20 12:21;rmetzger;Great, thanks a lot!;;;","06/Nov/20 16:44;sewen;Fixed in 1.12.0 via c965d3b25984bf97d1e8de8f1440537774c5a49a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix docs about relocatable savepoints,FLINK-19381,13328992,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,klion26,nkruber,nkruber,23/Sep/20 13:18,28/May/21 08:06,13/Jul/23 08:12,26/Jan/21 03:06,1.11.2,1.12.0,,,,1.13.0,,,Documentation,Runtime / Checkpointing,,,,0,pull-request-available,,,,,"Although savepoints are relocatable since Flink 1.11, the docs still state otherwise, for example in [https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/savepoints.html#triggering-savepoints]

The warning there, as well as the other changes from FLINK-15863, should be removed again and potentially replaced with new constraints.

One known constraint is that if taskowned state is used ({{GenericWriteAhreadLog}} sink), savepoints are currently not relocatable yet.",,alpinegizmo,klion26,liyu,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 03:06:24 UTC 2021,,,,,,,,,,"0|z0iuhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/20 12:07;klion26;Ah, the doc wasn't updated when FLINK-5763 implemented, I'd like to fix the doc, could you please assign this ticket to me? [~NicoK];;;","26/Jan/21 03:06;klion26;Merged into master with a9d2b766b2a04b5dc6532381c1f0c60bf56c4e74;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EventTimeWindowCheckpointingITCase setup can hide problems when changing parameters,FLINK-19377,13328956,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,23/Sep/20 09:33,22/Jun/21 13:55,13/Jul/23 08:12,25/Sep/20 12:26,1.10.2,1.11.2,1.8.3,1.9.3,,1.12.0,,,API / DataStream,Tests,,,,0,,,,,,"I noticed this while debugging {{EventTimeWindowCheckpointingITCase}}. If you change {{testSlidingWindows()}} to use a tumbling window instead of the sliding window the test should fail. Interestingly, the tests only fail when using the RocksDB backend but not for the heap-based backends.

You can follow the flow by setting a breakpoint on the {{AssertionError}} being thrown in {{ValidatingSink}}. Somewhere in the Task innards it will be suppressed/swallowed and the test will succeed.",,AHeise,aljoscha,alpinegizmo,rmetzger,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 12:26:29 UTC 2020,,,,,,,,,,"0|z0iu9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 09:34;aljoscha;I observed this behaviour as far back as Flink 1.8, then I stopped going back further.;;;","23/Sep/20 10:19;roman;I see in the debugger that with memory backend the error is ignored because the task is being canceled.

With RocksDb it's the same, but then the error is thrown again and this time it fails the test.;;;","23/Sep/20 11:10;pnowojski;Yes, we are ignoring exceptions during cancelation. This is a feature, not a bug :)

Have one of you checked/know what is the reason behind the cancelation? Is there something else happening and the test is broken, or is the `AssertionError` causing the task to cancel and it's being ignored because task was cancelled? ;;;","23/Sep/20 13:20;arvid;I dug into it and see it as a non-issue. Here is what happens:

* You have a failing source that fails once.
* JM cancels job.
* During cleanup, sink verifies and fails as well.
* That failure is hidden because task is failing already.
* Restart happens (restarts = 1 in strategy).
* Job finishes normally.
* During cleanup, sink verifies but does not fail in heap backends.
* Test succeeds.

Not sure why your test succeeds in the second attempt, but that should be easy to debug.;;;","23/Sep/20 15:10;aljoscha;Oh boy, I finally figured it out. The window size/slide etc. parameters are parameterized based on the state backend. The parameters used for the heap backend happen to also work when you use tumbling windows. That’s why you get different failure behaviour for different state backend types…;;;","25/Sep/20 12:26;aljoscha;master: add08a6aa5905a17fce80a9b697ffed0e33dbcca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobClientTest.testGetFailsDuringStreamingForJobPermanentBlob hangs,FLINK-19369,13328924,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,dian.fu,dian.fu,23/Sep/20 06:25,21/Dec/20 10:09,13/Jul/23 08:12,21/Dec/20 10:09,1.11.0,1.12.0,,,,1.11.4,1.12.1,1.13.0,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6803&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708

{code}
2020-09-22T21:40:57.5304615Z ""main"" #1 prio=5 os_prio=0 cpu=18407.84ms elapsed=1969.42s tid=0x00007f0730015800 nid=0x79bd waiting for monitor entry  [0x00007f07389fb000]
2020-09-22T21:40:57.5305080Z    java.lang.Thread.State: BLOCKED (on object monitor)
2020-09-22T21:40:57.5305487Z 	at sun.security.ssl.SSLSocketImpl.duplexCloseOutput(java.base@11.0.7/SSLSocketImpl.java:541)
2020-09-22T21:40:57.5306159Z 	- waiting to lock <0x000000008661a560> (a sun.security.ssl.SSLSocketOutputRecord)
2020-09-22T21:40:57.5306545Z 	at sun.security.ssl.SSLSocketImpl.close(java.base@11.0.7/SSLSocketImpl.java:472)
2020-09-22T21:40:57.5307045Z 	at org.apache.flink.runtime.blob.BlobUtils.closeSilently(BlobUtils.java:367)
2020-09-22T21:40:57.5307605Z 	at org.apache.flink.runtime.blob.BlobServerConnection.close(BlobServerConnection.java:141)
2020-09-22T21:40:57.5308337Z 	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreaming(BlobClientTest.java:443)
2020-09-22T21:40:57.5308904Z 	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreamingForJobPermanentBlob(BlobClientTest.java:408)
{code}",,dian.fu,mapohl,nkruber,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 21 10:09:45 UTC 2020,,,,,,,,,,"0|z0iu2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/20 16:26;mapohl;Not sure whether that's related but \{{BlobClientTest#testGetFailsDuringStreamingForJobPermanentBlob}} fails (using Java 8 and Java 11) running it for multiple times (e.g. after 2008 tries):
{quote}java.io.IOException: PUT operation failed: Broken pipe (Write failed)

at org.apache.flink.runtime.blob.BlobClient.putBuffer(BlobClient.java:345)
 at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreaming(BlobClientTest.java:431)
 at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreamingForJobPermanentBlob(BlobClientTest.java:409)
 at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
 at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:52)
 at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
 at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
 Caused by: java.net.SocketException: Broken pipe (Write failed)
 at java.net.SocketOutputStream.socketWrite0(Native Method)
 at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
 at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
 at org.apache.flink.runtime.blob.BlobUtils.writeLength(BlobUtils.java:282)
 at org.apache.flink.runtime.blob.BlobOutputStream.write(BlobOutputStream.java:86)
 at org.apache.flink.runtime.blob.BlobClient.putBuffer(BlobClient.java:340)
 ... 29 more
{quote}

This can be also reproduced with {{BlobClientTest#testGetFailsDuringStreaming}}.;;;","29/Oct/20 16:28;mapohl;One time I ran into the following error:
{quote}Scala Compile Server
 java.lang.NegativeArraySizeException
 at com.martiansoftware.nailgun.NGSession.run(NGSession.java:204)
{quote}
It caused the test to run infinitely.
 This ran on a AdoptOpenJDK 1.8u265 Hotspot VM (8.0.265.hs-adpt).;;;","29/Oct/20 16:57;mapohl;And a different probably infinitely running test having the following Thread stack:

 {quote}
2020-10-29 17:54:30
Full thread dump OpenJDK 64-Bit Server VM (11.0.8+10 mixed mode):

Threads class SMR info:
_java_thread_list=0x00007fc0af74b890, length=13, elements={
0x00007fc0b000b800, 0x00007fc0b5808800, 0x00007fc0b580d800, 0x00007fc0b581e000,
0x00007fc0b5811800, 0x00007fc0b581f000, 0x00007fc0b3008800, 0x00007fc0b00f0800,
0x00007fc0b580a000, 0x00007fc0b58cd000, 0x00007fc0b0264000, 0x00007fc0b0a16800,
0x00007fc0b0a75000
}

""main"" #1 prio=5 os_prio=31 cpu=3402.11ms elapsed=124.79s tid=0x00007fc0b000b800 nid=0x1803 runnable  [0x000070000763f000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(java.base@11.0.8/Native Method)
	at java.net.SocketInputStream.socketRead(java.base@11.0.8/SocketInputStream.java:115)
	at java.net.SocketInputStream.read(java.base@11.0.8/SocketInputStream.java:168)
	at java.net.SocketInputStream.read(java.base@11.0.8/SocketInputStream.java:140)
	at java.net.SocketInputStream.read(java.base@11.0.8/SocketInputStream.java:200)
	at org.apache.flink.runtime.blob.BlobOutputStream.receiveAndCheckPutResponse(BlobOutputStream.java:151)
	at org.apache.flink.runtime.blob.BlobOutputStream.finish(BlobOutputStream.java:104)
	at org.apache.flink.runtime.blob.BlobClient.putBuffer(BlobClient.java:342)
	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreaming(BlobClientTest.java:431)
	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreamingForJobTransientBlob(BlobClientTest.java:404)
	at jdk.internal.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.8/DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(java.base@11.0.8/Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:52)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)

""Reference Handler"" #2 daemon prio=10 os_prio=31 cpu=1.13ms elapsed=124.76s tid=0x00007fc0b5808800 nid=0x3b03 waiting on condition  [0x0000700007d55000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.ref.Reference.waitForReferencePendingList(java.base@11.0.8/Native Method)
	at java.lang.ref.Reference.processPendingReferences(java.base@11.0.8/Reference.java:241)
	at java.lang.ref.Reference$ReferenceHandler.run(java.base@11.0.8/Reference.java:213)

""Finalizer"" #3 daemon prio=8 os_prio=31 cpu=0.81ms elapsed=124.76s tid=0x00007fc0b580d800 nid=0x3d03 in Object.wait()  [0x0000700007e58000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.8/Native Method)
	- waiting on <0x000000078084cf08> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.8/ReferenceQueue.java:155)
	- waiting to re-lock in wait() <0x000000078084cf08> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.8/ReferenceQueue.java:176)
	at java.lang.ref.Finalizer$FinalizerThread.run(java.base@11.0.8/Finalizer.java:170)

""Signal Dispatcher"" #4 daemon prio=9 os_prio=31 cpu=0.26ms elapsed=124.74s tid=0x00007fc0b581e000 nid=0x4203 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=31 cpu=1064.32ms elapsed=124.74s tid=0x00007fc0b5811800 nid=0xa903 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

""C1 CompilerThread0"" #8 daemon prio=9 os_prio=31 cpu=671.99ms elapsed=124.74s tid=0x00007fc0b581f000 nid=0x5703 waiting on condition  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
   No compile task

""Sweeper thread"" #9 daemon prio=9 os_prio=31 cpu=20.64ms elapsed=124.74s tid=0x00007fc0b3008800 nid=0xa503 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Common-Cleaner"" #10 daemon prio=8 os_prio=31 cpu=1.18ms elapsed=124.67s tid=0x00007fc0b00f0800 nid=0xa303 in Object.wait()  [0x000070000846d000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.8/Native Method)
	- waiting on <0x000000078084daf8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.8/ReferenceQueue.java:155)
	- waiting to re-lock in wait() <0x000000078084daf8> (a java.lang.ref.ReferenceQueue$Lock)
	at jdk.internal.ref.CleanerImpl.run(java.base@11.0.8/CleanerImpl.java:148)
	at java.lang.Thread.run(java.base@11.0.8/Thread.java:834)
	at jdk.internal.misc.InnocuousThread.run(java.base@11.0.8/InnocuousThread.java:134)

""Monitor Ctrl-Break"" #11 daemon prio=5 os_prio=31 cpu=16.32ms elapsed=124.62s tid=0x00007fc0b580a000 nid=0x5b03 runnable  [0x0000700008570000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(java.base@11.0.8/Native Method)
	at java.net.SocketInputStream.socketRead(java.base@11.0.8/SocketInputStream.java:115)
	at java.net.SocketInputStream.read(java.base@11.0.8/SocketInputStream.java:168)
	at java.net.SocketInputStream.read(java.base@11.0.8/SocketInputStream.java:140)
	at sun.nio.cs.StreamDecoder.readBytes(java.base@11.0.8/StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(java.base@11.0.8/StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(java.base@11.0.8/StreamDecoder.java:178)
	- locked <0x000000078084d568> (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(java.base@11.0.8/InputStreamReader.java:185)
	at java.io.BufferedReader.fill(java.base@11.0.8/BufferedReader.java:161)
	at java.io.BufferedReader.readLine(java.base@11.0.8/BufferedReader.java:326)
	- locked <0x000000078084d568> (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(java.base@11.0.8/BufferedReader.java:392)
	at com.intellij.rt.execution.application.AppMainV2$1.run(AppMainV2.java:61)

""Service Thread"" #12 daemon prio=9 os_prio=31 cpu=0.04ms elapsed=124.62s tid=0x00007fc0b58cd000 nid=0x5c03 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Attach Listener"" #14 daemon prio=9 os_prio=31 cpu=9.18ms elapsed=123.76s tid=0x00007fc0b0264000 nid=0x6103 runnable  [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Timer-44"" #192 daemon prio=5 os_prio=31 cpu=0.10ms elapsed=120.84s tid=0x00007fc0b0a16800 nid=0x8e77 in Object.wait()  [0x0000700008879000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(java.base@11.0.8/Native Method)
	- waiting on <0x000000078fe57b50> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(java.base@11.0.8/Timer.java:553)
	- waiting to re-lock in wait() <0x000000078fe57b50> (a java.util.TaskQueue)
	at java.util.TimerThread.run(java.base@11.0.8/Timer.java:506)

""BLOB Server listener at 65030"" #191 daemon prio=5 os_prio=31 cpu=0.12ms elapsed=120.84s tid=0x00007fc0b0a75000 nid=0x91b3 runnable  [0x0000700008f8e000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(java.base@11.0.8/Native Method)
	at java.net.AbstractPlainSocketImpl.accept(java.base@11.0.8/AbstractPlainSocketImpl.java:458)
	at java.net.ServerSocket.implAccept(java.base@11.0.8/ServerSocket.java:565)
	at java.net.ServerSocket.accept(java.base@11.0.8/ServerSocket.java:533)
	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)

""VM Thread"" os_prio=31 cpu=31.07ms elapsed=124.77s tid=0x00007fc0b080e000 nid=0x3903 runnable  

""GC Thread#0"" os_prio=31 cpu=24.78ms elapsed=124.79s tid=0x00007fc0b101b800 nid=0x2e03 runnable  

""GC Thread#1"" os_prio=31 cpu=23.25ms elapsed=124.28s tid=0x00007fc0b2812000 nid=0x9b03 runnable  

""GC Thread#2"" os_prio=31 cpu=28.46ms elapsed=124.28s tid=0x00007fc0b2813000 nid=0x9903 runnable  

""GC Thread#3"" os_prio=31 cpu=21.70ms elapsed=124.28s tid=0x00007fc0afb06800 nid=0x5f03 runnable  

""GC Thread#4"" os_prio=31 cpu=22.83ms elapsed=124.28s tid=0x00007fc0afb07000 nid=0x9603 runnable  

""GC Thread#5"" os_prio=31 cpu=22.38ms elapsed=124.28s tid=0x00007fc0afb08000 nid=0x9503 runnable  

""G1 Main Marker"" os_prio=31 cpu=1.48ms elapsed=124.79s tid=0x00007fc0b0035000 nid=0x3003 runnable  

""G1 Conc#0"" os_prio=31 cpu=17.64ms elapsed=124.79s tid=0x00007fc0b0035800 nid=0x3303 runnable  

""G1 Conc#1"" os_prio=31 cpu=20.39ms elapsed=122.77s tid=0x00007fc0b600b000 nid=0x9c07 runnable  

""G1 Conc#2"" os_prio=31 cpu=18.86ms elapsed=122.77s tid=0x00007fc0b687e800 nid=0x9d07 runnable  

""G1 Refine#0"" os_prio=31 cpu=0.52ms elapsed=124.78s tid=0x00007fc0b00de800 nid=0x3503 runnable  

""G1 Young RemSet Sampling"" os_prio=31 cpu=21.50ms elapsed=124.78s tid=0x00007fc0b00df000 nid=0x5103 runnable  
""VM Periodic Task Thread"" os_prio=31 cpu=86.53ms elapsed=124.62s tid=0x00007fc0b108c800 nid=0x9e03 waiting on condition  

JNI global refs: 20, weak refs: 0

Heap
 garbage-first heap   total 262144K, used 65749K [0x0000000780000000, 0x0000000800000000)
  region size 1024K, 9 young (9216K), 3 survivors (3072K)
 Metaspace       used 18385K, capacity 18778K, committed 19072K, reserved 1067008K
  class space    used 1965K, capacity 2144K, committed 2176K, reserved 1048576K
{quote}

The single test failed after 5mins:
{quote}
java.io.IOException: PUT operation failed: Read timed out

	at org.apache.flink.runtime.blob.BlobClient.putBuffer(BlobClient.java:345)
	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreaming(BlobClientTest.java:431)
	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreamingForJobTransientBlob(BlobClientTest.java:404)
	at jdk.internal.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:52)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.base/java.net.SocketInputStream.socketRead0(Native Method)
	at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
	at java.base/java.net.SocketInputStream.read(SocketInputStream.java:200)
	at org.apache.flink.runtime.blob.BlobOutputStream.receiveAndCheckPutResponse(BlobOutputStream.java:151)
	at org.apache.flink.runtime.blob.BlobOutputStream.finish(BlobOutputStream.java:104)
	at org.apache.flink.runtime.blob.BlobClient.putBuffer(BlobClient.java:342)
	... 29 more
{quote};;;","30/Oct/20 15:43;mapohl;Ok, the stacktraces I mentioned in the comments seem to be unrelated a probably due to heavy load on the Blob storage.

But I came across [JDK-8241239|https://bugs.openjdk.java.net/browse/JDK-8241239] which might explain this failure. I'm going to close the issue for now considering that it only appeared once more than 1 month ago. We might have to reopen the issue in case we experience failures like that again.;;;","27/Nov/20 01:27;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10212&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","18/Dec/20 10:06;nkruber;It looks like the following code from {{BlobClientTest}} could be problematic:

{code}
			InputStream is = client.getInternal(jobId, key); // (1)
// ...
			BlobUtils.readFully(is, receiveBuffer, 0, firstChunkLen, null);
			BlobUtils.readFully(is, receiveBuffer, firstChunkLen, firstChunkLen, null); // (2)
// ...
			for (BlobServerConnection conn : getBlobServer().getCurrentActiveConnections()) {
				conn.close(); // (3)
			}
// ...
				BlobUtils.readFully(is, receiveBuffer, 2 * firstChunkLen, data.length - 2 * firstChunkLen, null); // (4)
// ...
{code}

I'm not 100% sure, but from the latest instance at https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10212&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0, the {{BlobServer}}'s connection thread seems to have acquired ""locked <0x000000008d873170> (a sun.security.ssl.SSLSocketOutputRecord)"" while writing data out to the socket that was created from the client (1). The client only read a few bytes in (2) and then didn't continue (yet) but the socket is still open. Now, the test wants to introduce a connection failure and tries to close the connection in (3) by letting the {{BlobServer}} close it. However, this cannot be done because that requires the same lock inside the JDK's SSL stack (0x000000008d873170).

This may lead to a situation where the {{BlobServer}} is blocked and waits to write data to the socket because the socket is already full (not sure this would also result in a RUNNABLE thread, but {{BlobServerConnection}} is not interruptable in this code and {{outputStream.write}} must be a blocking operation) while at the same time, the client does not continue reading because we want to introduce the failure first and only then continue reading in (4).

I'm not really sure how to fix this without writing code into the {{BlobServerConnection}} to introduce the failure there (not really the right way for code used only during testing), or rewriting it to use some non-blocking IO. If this is just happening with SSL (due to this additional lock), then maybe we should just disable these tests ( {{testGetFailsDuringStreamingNoJobTransientBlob}}, {{testGetFailsDuringStreamingForJobTransientBlob}}, {{testGetFailsDuringStreamingForJobPermanentBlob}} ) for {{BlobClientSslTest}} and trust that this isn't different with SSL and covered by {{BlobClientTest}} already.;;;","18/Dec/20 10:58;trohrmann;I think you are right with your analysis that this is a problem of the test setup [~NicoK]. I will disable the tests for {{BlobClientSslTest}}.;;;","21/Dec/20 10:09;trohrmann;Fixed via

1.13.0: 912539b67f0d11589684c5846e7f457fa8018299
1.12.1: c35cdbc1e31573b664e6343bb29df98970dd92c1
1.11.4: 6154610efd99f7506588c854f4abc07eab017fd7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make HiveCatalog thread safe,FLINK-19361,13328779,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,22/Sep/20 13:14,19/Feb/21 07:26,13/Jul/23 08:12,24/Sep/20 10:14,,,,,,1.11.3,1.12.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Currently the metastore client we create is not thread safe, which causes issues if the {{HiveCatalog}} instance is used concurrently. We should use synchronized client with a remote HMS. This is also inline with how Hive creates metastore client.",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 24 10:14:17 UTC 2020,,,,,,,,,,"0|z0it6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 03:46;lzljs3620320;master: c909f845ffc7d85f4b9be34ee33a68f875b8980d;;;","24/Sep/20 10:14;lzljs3620320;release-1.11: 2ca2f9cddfda4141a9bd2327fc3753e3ef14e54f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink fails if JAVA_HOME contains spaces,FLINK-19360,13328765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,raghavgautam,chesnay,chesnay,22/Sep/20 11:41,28/May/21 08:16,13/Jul/23 08:12,03/Feb/21 10:32,1.10.0,,,,,1.13.0,,,Deployment / Scripts,,,,,0,pull-request-available,,,,,"While we do account for the possibility of the path to the Flink binary containing spaces, we do not do this for JAVA_HOME.",,kezhuw,raghav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 10:32:14 UTC 2021,,,,,,,,,,"0|z0it3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/21 01:06;raghav;I have opened a pull request for this issue.
https://github.com/apache/flink/pull/14724;;;","22/Jan/21 01:07;raghav;[~chesnay] Can you please review my patch ?;;;","22/Jan/21 01:09;raghav;I have tested the patch on my laptop.;;;","03/Feb/21 10:32;chesnay;master: affbd597fa25df6de783de1522502f66ea2faaa3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherResourceCleanupTest#testJobSubmissionUnderSameJobId is unstable on Azure Pipeline,FLINK-19344,13328729,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,kevin.cyj,kevin.cyj,22/Sep/20 09:27,07/Oct/20 06:18,13/Jul/23 08:12,07/Oct/20 06:18,1.12.0,,,,,1.12.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,Here is the log and stack: https://dev.azure.com/kevin-flink/3f520f11-5170-4153-99d0-2ade0d99b911/_apis/build/builds/88/logs/102,,dian.fu,kevin.cyj,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 07 06:17:57 UTC 2020,,,,,,,,,,"0|z0isvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/20 01:32;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6870&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392;;;","25/Sep/20 16:18;pnowojski;another instance:
https://dev.azure.com/pnowojski/Flink/_build/results?buildId=154&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374;;;","28/Sep/20 12:24;rmetzger;For making this better discoverable, this is the error:

{code}
2020-09-22T06:44:57.9688388Z [ERROR] testJobSubmissionUnderSameJobId(org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest)  Time elapsed: 0.035 s  <<< ERROR!
2020-09-22T06:44:57.9689378Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.DuplicateJobSubmissionException: Job has already been submitted.
2020-09-22T06:44:57.9690304Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-09-22T06:44:57.9690744Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-09-22T06:44:57.9691294Z 	at org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.testJobSubmissionUnderSameJobId(DispatcherResourceCleanupTest.java:356)
2020-09-22T06:44:57.9691804Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-22T06:44:57.9692202Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-22T06:44:57.9692683Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-22T06:44:57.9693114Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-22T06:44:57.9693528Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-22T06:44:57.9700215Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-22T06:44:57.9701122Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-22T06:44:57.9701787Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-22T06:44:57.9702277Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-22T06:44:57.9702733Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-22T06:44:57.9703235Z 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
2020-09-22T06:44:57.9703834Z 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.evaluate(TestingFatalErrorHandlerResource.java:87)
2020-09-22T06:44:57.9704484Z 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.access$200(TestingFatalErrorHandlerResource.java:80)
2020-09-22T06:44:57.9705285Z 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:56)
2020-09-22T06:44:57.9705980Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-22T06:44:57.9706768Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-22T06:44:57.9707473Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-22T06:44:57.9708406Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-22T06:44:57.9708997Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-22T06:44:57.9709532Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-22T06:44:57.9710810Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-22T06:44:57.9711285Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-22T06:44:57.9711789Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-22T06:44:57.9712582Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-22T06:44:57.9713220Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-22T06:44:57.9713785Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-22T06:44:57.9714428Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-22T06:44:57.9714830Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-22T06:44:57.9715225Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-22T06:44:57.9715666Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-22T06:44:57.9716551Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-22T06:44:57.9717098Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-22T06:44:57.9717638Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-22T06:44:57.9718179Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-22T06:44:57.9718748Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-22T06:44:57.9719276Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-22T06:44:57.9720084Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-22T06:44:57.9721035Z Caused by: org.apache.flink.runtime.client.DuplicateJobSubmissionException: Job has already been submitted.
2020-09-22T06:44:57.9721790Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.submitJob(Dispatcher.java:282)
2020-09-22T06:44:57.9722251Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-22T06:44:57.9723950Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-22T06:44:57.9724523Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-22T06:44:57.9725479Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-22T06:44:57.9726466Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-09-22T06:44:57.9727241Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-09-22T06:44:57.9727800Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-09-22T06:44:57.9728366Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-22T06:44:57.9728874Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-22T06:44:57.9729328Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-22T06:44:57.9730134Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-22T06:44:57.9730689Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-22T06:44:57.9731137Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-22T06:44:57.9731579Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-22T06:44:57.9732004Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-22T06:44:57.9732428Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-22T06:44:57.9732849Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-22T06:44:57.9733259Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-22T06:44:57.9740822Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-22T06:44:57.9741476Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-22T06:44:57.9742718Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-22T06:44:57.9743155Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-22T06:44:57.9743603Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-22T06:44:57.9744081Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-22T06:44:57.9744803Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-22T06:44:57.9745513Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

I will take a look into the issue.;;;","07/Oct/20 06:17;rmetzger;Resolved on master: https://github.com/apache/flink/commit/238d94d3798a55feee0e5d86168a088b568c8b2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AggregateITCase.testListAggWithDistinct failed with ""expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#A, 4,EF#EF)>""",FLINK-19340,13328712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,dian.fu,dian.fu,22/Sep/20 08:14,12/Nov/20 11:10,13/Jul/23 08:12,12/Nov/20 11:10,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6734&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

{code}
2020-09-22T04:59:30.1229430Z [ERROR] testListAggWithDistinct[LocalGlobal=ON, MiniBatch=ON, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase)  Time elapsed: 0.346 s  <<< FAILURE!
2020-09-22T04:59:30.1230120Z java.lang.AssertionError: expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#A, 4,EF#EF)>
2020-09-22T04:59:30.1232835Z 	at org.junit.Assert.fail(Assert.java:88)
2020-09-22T04:59:30.1233314Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-09-22T04:59:30.1233688Z 	at org.junit.Assert.assertEquals(Assert.java:118)
2020-09-22T04:59:30.1234034Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-09-22T04:59:30.1234528Z 	at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testListAggWithDistinct(AggregateITCase.scala:667)
{code}",,dian.fu,jark,rmetzger,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 11:10:23 UTC 2020,,,,,,,,,,"0|z0isrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/20 12:09;twalthr;This might be related to the recent changes of updating aggregate function to the new type system. I will assign it to me. Please let me know if it happens again.;;;","29/Sep/20 14:15;dian.fu;[~twalthr] Thanks a lot!;;;","03/Nov/20 01:25;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8828&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

It failed with the following error:
{code}
[ERROR] testListAggWithDistinct[LocalGlobal=ON, MiniBatch=ON, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase)  Time elapsed: 0.469 s  <<< FAILURE!
java.lang.AssertionError: expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#C#A, 4,EF)>
{code};;;","03/Nov/20 15:32;twalthr;Thanks for letting me know [~dian.fu]. I was hoping that this was an old error and got fixed. But in this case I will upgrade the issue to a blocker.;;;","05/Nov/20 15:57;rmetzger;Is this blocker expected to be merged before the feature freeze of 1.12 on Sunday?
;;;","05/Nov/20 16:35;twalthr;[~rmetzger] If it is urgent, I can allocate some time for it tomorrow. Otherwise I would have investigated it next week after feature freeze. It seems to not occur very often but definitely needs a fix.;;;","06/Nov/20 12:22;rmetzger;I'd say it's up to you. I can not decide to what extend this affects the component's stability.
I guess it's fine to fix it next week, given that this doesn't change any APIs and is rarely happening.;;;","09/Nov/20 16:57;twalthr;Update: I can reproduce it locally. It occurs once every ~2000 runs. I'm working on a fix.;;;","12/Nov/20 11:10;twalthr;Fixed in 1.12.0: b48104f5f40fb46384fd900e04a61dcc39d3d2a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New source interface cannot unregister unregistered source,FLINK-19338,13328691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,AHeise,22/Sep/20 05:17,22/Jun/21 14:06,13/Jul/23 08:12,25/Oct/20 19:01,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,,,,,,"When a source subtask fails before the respective reader is registered in {{SourceCoordinatorContext}}, unregistering fails as it explicitly verifies that the reader has been registered before.

The solution is to drop the assumption that all readers are registered when cleaning up subtasks.",,AHeise,kezhuw,sewen,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 25 22:39:48 UTC 2020,,,,,,,,,,"0|z0ismw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/20 14:00;sewen;[~AHeise] are you still working on this?
[~jqin] Do you think this is a valid issue?;;;","25/Oct/20 19:01;arvid;I'm deeply sorry for the confusion. I already added a fix for that in https://github.com/apache/flink/pull/13228 .;;;","25/Oct/20 19:02;arvid;Merged into master as c34b5a2121deed7475f172d7fa7cca6c7f6b976e .;;;","25/Oct/20 22:39;sewen;Fix looks good, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EncodingUtils#encodeObjectToString should propagate inner exception,FLINK-19336,13328675,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,libenchao,libenchao,22/Sep/20 03:03,13/Oct/20 02:52,13/Jul/23 08:12,13/Oct/20 02:52,1.10.2,1.11.2,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Currently {{EncodingUtils#encodeObjectToString}} swallows the inner exception, which makes it hard to track the root cause.",,klion26,libenchao,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 02:51:42 UTC 2020,,,,,,,,,,"0|z0isjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/20 12:34;ZhuShang;hi [~libenchao],I want to take this,plz assign to me.;;;","23/Sep/20 02:50;libenchao;sure, assigned to you [~ZhuShang];;;","13/Oct/20 02:51;libenchao;fixed via 29a5135180cd2bda70eb9cce427dfeccd1444be6 (1.12.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State processor api has native resouce leak when working with RocksDB,FLINK-19331,13328642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,21/Sep/20 21:16,19/Feb/21 07:24,13/Jul/23 08:12,05/Oct/20 06:01,1.12.0,,,,,1.12.0,,,API / State Processor,,,,,0,pull-request-available,,,,,"State processor api uses AbstractStateBackend#getKeys and AbstractStateBackend#getKeysAndNamespaces to iterate over keys and namespaces in a savepoint. These methods return java.util.stream.Stream. The RocksDBKeyedStateBackend implemention of these methods use streams onClose callback to free native resources. 

However, spa eagerly turns this stream into an iterator. This causes the onClose method to be discarded leading to a native resource leak. This can lead to a segmentation fault when multiple spa jobs are submitted to the same session cluster. ",,klion26,rmetzger,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 05 06:01:41 UTC 2020,,,,,,,,,,"0|z0isc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/20 06:01;rmetzger;This has been merged to master in https://github.com/apache/flink/commit/dad729752ee43d95d47ba079b5ea2cfb6f6ca22c.
Closing ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recovery with async operations fails due to uninitialized runtimeContext,FLINK-19330,13328625,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,igal,igal,igal,21/Sep/20 18:57,23/Sep/20 08:25,13/Jul/23 08:12,23/Sep/20 08:25,,,,,,statefun-2.2.0,,,Stateful Functions,,,,,0,pull-request-available,,,,,"In Flink 1.11, the AbstractStreamOperator's runtimeContext is not fully initialized when executing
{code:java}
 AbstractStreamOperator#intializeState(){code}
in particular KeyedStateStore is set after intializeState was finished.
 See: 
 [https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java#L258,L259]
 This behaviour was changed from Flink 1.10->Flink 1.11.

StateFun's FunctionGroupOperator performs its initialization logic at initalizeState, and it requires an already initialized runtimeContext.

This situation causes the following failure after recovery:
{code:java}
java.lang.RuntimeException: java.lang.NullPointerException: Keyed state can only be used on a 'keyed stream', i.e., after a 'keyBy()' operation.
	at org.apache.flink.runtime.state.AbstractKeyedStateBackend.lambda$applyToAllKeys$0(AbstractKeyedStateBackend.java:256) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_265]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_265]
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647) ~[?:1.8.0_265]
	at org.apache.flink.runtime.state.AbstractKeyedStateBackend.applyToAllKeys(AbstractKeyedStateBackend.java:249) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.statefun.flink.core.functions.AsyncOperationFailureNotifier.fireExpiredAsyncOperations(AsyncOperationFailureNotifier.java:42) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.FunctionGroupOperator.initializeState(FunctionGroupOperator.java:160) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:106) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:258) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
Caused by: java.lang.NullPointerException: Keyed state can only be used on a 'keyed stream', i.e., after a 'keyBy()' operation.
	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:75) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.checkPreconditionsAndGetKeyedStateStore(StreamingRuntimeContext.java:223) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:188) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	at org.apache.flink.statefun.flink.core.state.FlinkState.createFlinkStateAccessor(FlinkState.java:69) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.state.FlinkStateBinder.bindValue(FlinkStateBinder.java:48) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.sdk.state.StateBinder.bind(StateBinder.java:30) ~[statefun-flink-distribution.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.state.PersistedStates.findReflectivelyAndBind(PersistedStates.java:46) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.StatefulFunctionRepository.load(StatefulFunctionRepository.java:74) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.StatefulFunctionRepository.get(StatefulFunctionRepository.java:59) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.LocalFunctionGroup.newActivation(LocalFunctionGroup.java:73) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.LocalFunctionGroup.enqueue(LocalFunctionGroup.java:50) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.Reductions.enqueue(Reductions.java:148) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.Reductions.enqueueAsyncOperationAfterRestore(Reductions.java:154) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.AsyncOperationFailureNotifier.process(AsyncOperationFailureNotifier.java:66) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.statefun.flink.core.functions.AsyncOperationFailureNotifier.process(AsyncOperationFailureNotifier.java:30) ~[statefun-flink-core.jar:2.3-SNAPSHOT]
	at org.apache.flink.runtime.state.AbstractKeyedStateBackend.lambda$applyToAllKeys$0(AbstractKeyedStateBackend.java:252) ~[flink-dist_2.12-1.11.1.jar:1.11.1]
	... 16 more
{code}",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 08:25:57 UTC 2020,,,,,,,,,,"0|z0is88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 08:25;tzulitai;flink-statefun/master: 0d85a2c7b8bf6b1e0568d6e7ffcc8bbd0d5562ad
flink-statefun/release-2.2: 22fc4d34e2e6f3792cd041e447c400a2fd39e486;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FunctionGroupOperator#dispose might throw an NPE during operator dispose,FLINK-19329,13328570,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,igal,igal,igal,21/Sep/20 14:12,23/Sep/20 08:26,13/Jul/23 08:12,23/Sep/20 08:26,,,,,,statefun-2.2.0,,,Stateful Functions,,,,,0,pull-request-available,,,,,"The dispose method of an operator can be called without a successful call to initalizeState.

(for example a failure to load the checkpoint data, or any user exception during initializeState) 

This doesn't cause a real issue, since it happens within dispose() and in a try {} finally \{ super.dispose()} block,  but still might be confusing to users to see an NPE.

 ",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 08:26:21 UTC 2020,,,,,,,,,,"0|z0irw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 08:26;tzulitai;flink-statefun/master: a08141e58f2918d8659cf52f035d2660df4063f2
flink-statefun/release-2.2: 80d4c70d799abf1bde8d2195b27bc6b9555a4694;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Helm charts specify a too low value for the job manager's heap size,FLINK-19327,13328554,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,21/Sep/20 12:48,23/Sep/20 08:27,13/Jul/23 08:12,23/Sep/20 08:27,,,,,,statefun-2.2.0,,,Stateful Functions,,,,,0,pull-request-available,,,,,"Currently our Helm chart specify the value of 

jobmanager.memory.process.size to be 500mb which causes the JobManager to crush on start with:
{code:java}
INFO  [] - Loading configuration property: jobmanager.memory.process.size, 500m
INFO  [] - Loading configuration property: taskmanager.memory.process.size, 4g
INFO  [] - Loading configuration property: parallelism.default, 3
INFO  [] - The derived from fraction jvm overhead memory (50.000mb (52428800 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: The configured Total Flink Memory (52.000mb (54525952 bytes)) is less than the configured Off-heap Memory (128.000mb (134217728 bytes)).
	at org.apache.flink.runtime.util.config.memory.jobmanager.JobManagerFlinkMemoryUtils.deriveFromTotalFlinkMemory(JobManagerFlinkMemoryUtils.java:107)
	at org.apache.flink.runtime.util.config.memory.jobmanager.JobManagerFlinkMemoryUtils.deriveFromTotalFlinkMemory(JobManagerFlinkMemoryUtils.java:36)
	at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveProcessSpecWithTotalProcessMemory(ProcessMemoryUtils.java:105)
	at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:79)
	at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:76)
	at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:71)
	at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:102)
	at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:73)
	at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:61)

{code}
 ",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 08:27:04 UTC 2020,,,,,,,,,,"0|z0irsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/20 08:27;tzulitai;flink-statefun/master: 9070d297d6221d87195b1ee478b2383a0b2afc28
flink-statefun/release-2.2: 875ef5f73f9bc073a5955db94d1d7968d9695fa6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map requested/allocated containers with priority on YARN,FLINK-19324,13328526,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,21/Sep/20 09:59,16/Oct/20 10:59,13/Jul/23 08:12,16/Oct/20 10:59,,,,,,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,,,,,"In the design doc of FLINK-14106, there was a [discussion|https://docs.google.com/document/d/1f8imSus3QwKEUPAldzR8CSMjZ-2a9O17-rn4oeKGtqw/edit?disco=AAAAGPX_tmg] on how we map allocated containers with the requested ones on YARN. We rejected the design option that uses container priorities for mapping containers of different resources, because we do not want to priorities different container requests (which is the original purpose for this field). As a result, we have to interpret how the requested container request would be normalized by Yarn, and map the allocated/requested containers accordingly, which is complicated and fragile. See also FLINK-19151.

Recently in our POC for fine grained resource management, we surprisingly discovered that Yarn actually doesn't work with container requests same priority and different resources. I do not find this described as an official protocol in any Yarn's documents. The issue has been raised in early Yarn versions (YARN-314) and has not been fixed util Hadoop 2.9 when {{allocationRequestId}} is introduced. In Hadoop 2.8, Yarn scheduler is still internally using priority as the key of a container request (see [AppSchedulingInfo#updateResourceRequests |https://github.com/apache/hadoop/blob/eb818cdc64336ade273a960ba3b9b5a5d0c4d4ec/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java#L341]), thus requests same priority and different resources would overwrite each other.

The new discovery suggests that, if we want to support containers with different resources on Hadoop 2.8 and earlier versions, we have to give them different priorities anyway. Thus, I would suggest to get rid of the container normalization simulation and go back to the previously rejected priority based design option.",,guoyangze,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19151,FLINK-14106,,FLINK-19556,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 10:59:51 UTC 2020,,,,,,,,,,"0|z0irm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/20 10:59;xtsong;Fixed via
* master: a8893324ffcc7c8785c0c8a1d09b953f5d7e8d15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectSinkFunction does not define serialVersionUID,FLINK-19321,13328518,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,fhueske,fhueske,21/Sep/20 09:28,19/Feb/21 07:22,13/Jul/23 08:12,22/Sep/20 04:29,1.11.2,1.12.0,,,,1.11.3,1.12.0,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"The {{org.apache.flink.streaming.api.operators.collect.CollectSinkFunction}} function does not define a {{serialVersionUID}}.

Function objects are serialized using Java Serialization and should define a {{serialVersionUID.}}
 

If no o serialVersionUID is defined, Java automatically generates IDs to check compatibility of objects during deserialization. However, the generation depends on the environment (JVM, class version, etc.) and can hence lead to {{[java.io|http://java.io/].InvalidClassException}} even if the classes are compatible.",,fhueske,jark,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 04:29:26 UTC 2020,,,,,,,,,,"0|z0irkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 12:39;ZhuShang;[~fhueske],I want to take this.
 BTW,is there any requirement for the serialVersionUID? whether it is generated by IDEA will be ok.;;;","21/Sep/20 12:46;fhueske;Hi [~ZhuShang] , Thanks for helping here!
As far as I know, most Serializable classes in Flink just define it as {{1L}}.;;;","21/Sep/20 12:56;ZhuShang;i got it,could you assign to me.[~fhueske];;;","22/Sep/20 04:29;jark;Fixed in 
 - master: 1b0cba3cfd113cb4c33aab4915fbedeff3d1916f
 - release-1.11: 42ac4337ee9b5c293e2626874f00a7473e68aa3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timer loss after restoring from savepoint,FLINK-19300,13328449,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xianggao,xianggao,xianggao,21/Sep/20 04:09,08/Dec/21 09:03,13/Jul/23 08:12,17/Nov/20 04:56,1.8.0,,,,,1.11.3,1.12.0,,Runtime / State Backends,,,,,1,pull-request-available,,,,,"While using heap-based timers, we are seeing occasional timer loss after restoring program from savepoint, especially when using a remote savepoint storage (s3). 

After some investigation, the issue seems to be related to [this line in deserialization|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/core/io/PostVersionedIOReadableWritable.java#L65]. When trying to check the VERSIONED_IDENTIFIER, the input stream may not guarantee filling the byte array, causing timers to be dropped for the affected key group.

Should keep reading until expected number of bytes are actually read or if end of the stream has been reached. ",,aljoscha,davidhw,dian.fu,hwanju,kezhuw,klion26,liyu,rmetzger,tzulitai,xianggao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 04:56:55 UTC 2020,,,,,,,,,,"0|z0ir54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 11:12;klion26;[~xianggao] thanks for reporting the issue, I think you analysis is right, {{InputStream#read}} can't guarantee to fully read the byte array.

As this may loss the timer I'm not sure this need to be a blocker or not?;;;","28/Sep/20 20:20;xianggao;[~klion26] Thanks for taking a look! I'm not sure about the criteria for ""blocker"" issues. This issue seems to only exist in heap based timers, but losing timer is a bit dangerous.

Wonder what's our process on patching this kind of issue. May I submit a PR for this?;;;","11/Nov/20 06:42;rmetzger;Thanks a lot for reporting this. Which Flink version are you using?;;;","11/Nov/20 06:51;davidhw;We're using Flink version 1.8.;;;","11/Nov/20 07:03;tzulitai;This looks like a real issue, the typical {{read}} v.s. {{readFully}} mistake.

This read path should only occur for the case where users are using RocksDB backends + heap-based timers (using the heap backend should be fine, would not bump into this).

[~xianggao] to help me understand the full problem: in your scenarios, I'm assuming that the timer losses are caused by somehow the {{InternalTimerServiceSerializationProxy}} silently skipping reading the timers, instead of some {{IOException}} due to incorrect read attempts (and eventually fails the restore, instead of a timer loss). Could you clarify if my assumption is correct?

As for the fix, I would suggest to try to reuse the {{java.io.DataInput#readFully}} method instead or re-implementing it:
{code}
byte[] tmp = new byte[VERSIONED_IDENTIFIER.length];

DataInputView inputView = new DataInputViewStreamWrapper(inputStream);
inputView.readFully(tmp);
{code}

You can catch {{EOFException}} to determine if end of stream is reached / not enough bytes available.

;;;","11/Nov/20 07:04;tzulitai;As for priority of this issue:

The bug looks like it's been there for quite a while already across many versions, but I'd suggest to list it as a blocker for 1.12.0 and 1.11.3.

[~xianggao] Please do submit a PR for this, I'll try to review it as soon as possible.;;;","11/Nov/20 07:08;tzulitai;Just a comment on the severity of the issue:

It looks like timer loss is only possible if somehow, the key groups contain a {{0}} at the very beginning of the stream. This seems to be the only possible case that would lead to the {{InternalTimerServiceSerializationProxy}} silently skipping the rest of the reads, instead of failing the restore with some {{IOException}}.;;;","11/Nov/20 18:49;xianggao;Hi [~tzulitai], 

Yeah, your assumption is right on the behavior of the issue. The InternalTimerServiceSerializationProxy will silently skip timers instead of throwing error. 

Will submit a PR asap.

Thanks,;;;","12/Nov/20 00:32;xianggao;Created a PR. Seems like in case of non-versioned payload, we would push back those read bytes. We might not know the correct number of bytes to push back if we use DataInputView.readFully() while catching EOF. Did something similar to DataInputView.readFully(), but keep the number of bytes so that we can push back when necessary.;;;","17/Nov/20 04:56;tzulitai;flink/master: 4bd0ad19e6ac056106ba0d5c3a3440bd5054f89f
flink/release-1.11: f33c30feed2cdd36c04b373ef36f6c11a5f5d504

Thanks for the contribution [~xianggao]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyShuffleEnvironmentBuilder#setBufferSize does not take effect,FLINK-19299,13328448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,21/Sep/20 04:04,19/Feb/21 07:21,13/Jul/23 08:12,21/Sep/20 08:48,,,,,,1.12.0,,,Tests,,,,,0,pull-request-available,,,,,"Currently, NettyShuffleEnvironmentBuilder#setBufferSize does not take effect because the set value is never used when building the NettyShuffleEnvironment.",,kevin.cyj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 08:48:22 UTC 2020,,,,,,,,,,"0|z0ir4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 08:48;kevin.cyj;Fix on master via f30699cbaad17498482e5e62fbaed2fffb40d2aa.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RetryingCallback is not aware of task cancaltion ,FLINK-19296,13328428,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,igal,igal,igal,20/Sep/20 20:06,22/Sep/20 03:32,13/Jul/23 08:12,21/Sep/20 05:18,,,,,,statefun-2.2.0,,,Stateful Functions,,,,,0,pull-request-available,,,,,"Currently RetryingCallback would be retrying until the maximum timeout elapses, unaware of rather or not the task was canceled.",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 05:18:24 UTC 2020,,,,,,,,,,"0|z0ir0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 05:18;tzulitai;flink-statefun/master: e9a20a765bec5418ac2e8870e9541505013ced9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOITCase.checkForProhibitedLogContents found a log with prohibited string,FLINK-19295,13328411,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,dian.fu,dian.fu,20/Sep/20 13:36,05/Oct/20 10:24,13/Jul/23 08:12,05/Oct/20 10:24,1.11.3,1.12.0,,,,1.11.3,1.12.0,,Deployment / YARN,Runtime / Coordination,Tests,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6661&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354]

{code}
2020-09-19T22:08:13.5364974Z [ERROR]   YARNSessionFIFOITCase.checkForProhibitedLogContents:83->YarnTestBase.ensureNoProhibitedStringInLogFiles:476 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-1_0/application_1600553154281_0001/container_1600553154281_0001_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14347,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 05 10:24:18 UTC 2020,,,,,,,,,,"0|z0iqwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/20 13:42;dian.fu;This issue is similar to FLINK-14347. However, as FLINK-14347 has been fixed for almost one year, maybe this issue is caused by recent changes and so created a new ticket to track this issue.;;;","21/Sep/20 06:47;rmetzger;The prohibited string on the TaskManager is this:

{code}
2020-09-19 22:06:19,428 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [null] failed with java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352
2020-09-19 22:06:19,458 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@e466f3e261f3:42352] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@e466f3e261f3:42352]] Caused by: [java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352]
2020-09-19 22:06:19,507 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Keyed Aggregation -> Sink: Print to Std. Out (1/1) (b5c8d46f3e7b141acf271f12622e752b_0_0) switched from RUNNING to FINISHED.
2020-09-19 22:06:19,507 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Keyed Aggregation -> Sink: Print to Std. Out (1/1) (b5c8d46f3e7b141acf271f12622e752b_0_0).
2020-09-19 22:06:19,507 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Keyed Aggregation -> Sink: Print to Std. Out (1/1) b5c8d46f3e7b141acf271f12622e752b_0_0.
2020-09-19 22:06:19,580 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [null] failed with java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352
2020-09-19 22:06:19,594 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://flink@e466f3e261f3:42352] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink@e466f3e261f3:42352]] Caused by: [java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352]
{code}

It seems that netty (the RPC netty) is trying to connect to itself (same host) during the shutdown procedure.;;;","21/Sep/20 09:48;chesnay;This one should be whitelisted:
{{Remote connection to [null] failed with java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352}}
It is virtually identical to this one, which likely caused the failure:
{{Association with remote system [akka.tcp://flink@e466f3e261f3:***@e466f3e261f3:42352]] Caused by: [java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352]}}

It looks like we just changed how errors are reported; it would be good to figure out where that happened, but it should be fine to whitelist this one as well.;;;","21/Sep/20 10:58;rmetzger;Thanks a lot. {{Remote connection to [null] failed with java.net.ConnectException: Connection refused: e466f3e261f3/192.168.224.2:42352}} is already whitelisted. I added a PR for the other string.
For the reason why this happens now: we didn't bump the netty version yet, and I'm not aware of any recent akka changes in general? I'm happy to look into this more, but I need some initial lead.;;;","22/Sep/20 01:06;dian.fu;Another instance on 1.11: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6720&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=6f8201e9-1579-595a-9d2b-7158b26b4c57];;;","05/Oct/20 00:20;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7184&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab;;;","05/Oct/20 00:24;dian.fu;Instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7193&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=61b612c5-1c7d-5289-27c2-71f332ae98d7;;;","05/Oct/20 07:34;rmetzger;Resolved on master in https://github.com/apache/flink/commit/6c7b195d57c3bad5bc1f2251de75ac744dbbe4a7
Preparing backport to 1.11 ...;;;","05/Oct/20 10:24;rmetzger;Resolved on 1.11 in https://github.com/apache/flink/commit/c9f3cc6b6c25963eec648bd06d70a6e9a0e004e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix exception for AvroSchemaConverter#convertToSchema when RowType contains multiple row fields,FLINK-19291,13328205,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaozilong,xiaozilong,xiaozilong,18/Sep/20 08:56,11/Oct/20 05:14,13/Jul/23 08:12,11/Oct/20 05:14,1.11.0,,,,,1.11.3,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"Hi, I use `AvroSchemaConverter` to converts flink logical type into an avro schema will be throw `SchemaParseException` when there are multiple row type fields and the field names in each row type are different

 

This is my simplified code
{code:java}
RowType rowType = (RowType) TableSchema.builder()
 .field(""row1"", DataTypes.ROW(DataTypes.FIELD(""a"", DataTypes.STRING())))
 .field(""row2"", DataTypes.ROW(DataTypes.FIELD(""b"", DataTypes.STRING())))
 .build().toRowDataType().getLogicalType();
Schema schema = AvroSchemaConverter.convertToSchema(rowType);
System.out.println(schema.toString(true));
{code}
 

it will throw
{code:java}
org.apache.avro.SchemaParseException: Can't redefine: row_1

at org.apache.avro.Schema$Names.put(Schema.java:1128) 
at org.apache.avro.Schema$NamedSchema.writeNameRef(Schema.java:562) 
at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:690) 
at org.apache.avro.Schema$ArraySchema.toJson(Schema.java:805) 
at org.apache.avro.Schema$UnionSchema.toJson(Schema.java:882) 
at org.apache.avro.Schema$RecordSchema.fieldsToJson(Schema.java:716) 
at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:701) 
at org.apache.avro.Schema.toString(Schema.java:324) 
at org.apache.avro.Schema.toString(Schema.java:314) 
at java.lang.String.valueOf(String.java:2994) 
at java.io.PrintStream.println(PrintStream.java:821) 
at org.apache.flink.formats.avro.typeutils.AvroSchemaConverterTest.testRowTypeAvroSchemaConversion(AvroSchemaConverterTest.java:99) 
at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
 {code}
Thanks!
  ",,jark,kezhuw,libenchao,xiaozilong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/20 07:16;xiaozilong;image-2020-09-22-15-16-50-130.png;https://issues.apache.org/jira/secure/attachment/13011899/image-2020-09-22-15-16-50-130.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 10 10:56:43 UTC 2020,,,,,,,,,,"0|z0ipmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/20 09:01;xiaozilong;if it determinated, can assign this issue to me? I‘m resolved it.
 ;;;","21/Sep/20 10:19;xiaozilong;[~libenchao] [~jark] hi Jark,Benchao, can you help me to review this issue?
  ;;;","22/Sep/20 04:37;jark;Thanks for reporting this [~xiaozilong]. This is a bug in {{AvroSchemaConverter.convertToSchema}}, the {{rowTypeCounter}} is still possible to be duplicate which causes this problem. ;;;","22/Sep/20 11:20;xiaozilong;Thanks for review this issue [~jark] , I think we can replace rowTypeCounter with fieldName, it works fine. Do you have any suggestions? !image-2020-09-22-15-16-50-130.png!  ;;;","23/Sep/20 03:31;jark;[~xiaozilong], I think this is a good idea. ;;;","23/Sep/20 06:37;xiaozilong;okay:D;;;","10/Oct/20 10:56;jark;Fixed in 
 - master: de87a2debde8546e6741390a81f43c032521c3c0
 - release-1.11: 0a5893db7c38c529acf81fd76d0a5f634ceb7678;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove pods terminated during JM failover,FLINK-19289,13328201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,yittg,yittg,18/Sep/20 08:17,23/Sep/20 04:48,13/Jul/23 08:12,22/Sep/20 11:28,1.12.0,,,,,1.12.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"For a senario,

During JM is down (no JM is running), a TM down with error (for reasons from the node or TM inner), then an Error pod present there. After one JM recover, it will receive a ADDED event about this pod and do nothing.

We should deal with this case in `onAdded` callback properly, I think.

cc [~xintongsong].",,guoyangze,xtsong,yittg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 11:28:53 UTC 2020,,,,,,,,,,"0|z0ipm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/20 09:06;xtsong;Hi [~yittg],

Thanks for pulling me in.

I'm not sure whether I have understand the problem you described correctly. By ""deal with this case properly"", do you mean that Flink should remove the error pod from K8s?

If that is what you mean, I think Flink should be able to remove such pods. For such pods terminated during JM failover, Flink should receive not only a ADDED event, but also the MODIFIED/ERROR events, thus triggering removal of the pods.

Have you verified that the error pod is not removed even after the JM successfully recovered for a while?;;;","18/Sep/20 09:19;yittg;[~xintongsong]

bq. Flink should receive not only a ADDED event, but also the MODIFIED/ERROR events, thus triggering removal of the pods.

I believe only an ADDED event will be received, it's about the k8s resource watching logic.

bq. Have you verified that the error pod is not removed even after the JM successfully recovered for a while?

Yeah, the JM do nothing with the error pod,
i try to update some not important fields of the error pod, then it can be removed right now.;;;","18/Sep/20 09:51;xtsong;Double checked the k8s watching logic. I think you are right, only ADDED event will be received for the terminated pods. This is indeed a valid bug, thanks for reporting this [~yittg].

Regarding the solution, I would suggest to check the pod status in {{recoverWorkerNodesFromPreviousAttempts}} and remove pods that are already terminated, rather than handle this in {{onAdded}}. If I understand correctly, only pods that are recovered from previous attempt have this problem. To that end, removing the pods in {{recoverWorkerNodesFromPreviousAttempt}} might be better since this method only affects recovered pods, unlike {{onAdded}} affects both recovered and new pods.

What do you think?;;;","18/Sep/20 13:59;yittg;Exactly, it's good to filter and remove terminated pods before starting to watch.

But, there is a gap (maybe very short) btw listing and watching pods. So just filter before is not *absolutely* right. What do you think?;;;","21/Sep/20 02:00;xtsong;Good point.
I think we can create the watcher before listing pods in `initializeInternal`. Both calls are synchronized, which guarantees resource version for the watch is <= that of listing.;;;","21/Sep/20 03:27;yittg;Then, a terminated pod may be processed twice, in listing / onUpdated, which should be noted;;;","21/Sep/20 03:48;xtsong;That is exactly the protocol between {{ResourceManagerDriver}} and {{ActiveResourceManager}}. The {{ResourceManagerDriver}}s do not maintain status for the living workers, they simply forward events received from external systems (K8s, Yarn, Mesos). The {{ActiveResourceManager}} should deal with the duplicated events.;;;","21/Sep/20 04:03;yittg;That's great.;;;","22/Sep/20 04:05;xtsong;[~yittg],
I've opened a PR to fix this problem. Would you like to take a look?;;;","22/Sep/20 04:12;yittg;[~xintongsong] Looks good.;;;","22/Sep/20 11:28;xtsong;Fixed via
* master: c884aee70555fab70a80fac84b3812a2b74b17fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix minor version in flink docs,FLINK-19287,13328183,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,18/Sep/20 06:45,21/Sep/20 11:58,13/Jul/23 08:12,21/Sep/20 11:58,1.10.2,1.11.2,,,,,,,Documentation,,,,,0,,,,,,"The minor version in docs of 1.10/1.11 still points to 1.10.0/1.11.0, which can mislead users to use old version artifacts.
It should be corrected to 1.10.2/1.11.2.",,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 11:58:47 UTC 2020,,,,,,,,,,"0|z0ipi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 11:58;zhuzh;Fixed via
release-1.11: a2ab22e771ddd5feedb873d901a047e0bcb9e297
release-1.10: 6c49a8e54be78001aeffc7ec81a19ed5f281234d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIKE cannot recognize full table path,FLINK-19281,13328049,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,libenchao,libenchao,17/Sep/20 12:35,23/Sep/20 04:08,13/Jul/23 08:12,22/Sep/20 08:12,1.11.2,,,,,1.11.3,1.12.0,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,,,"for example, if we have a table whose full path is {{default_catalog.default_database.my_table1}}, and the following DDL will fail
{code:sql}
create table my_table2 
like default_catalog.default_database.my_table1
{code}
it will throw
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Source table '`default_catalog`.`default_database`.`default_catalog.default_database.my_table1`' of the LIKE clause not found in the catalog, at line 11, column 6
	at org.apache.flink.table.planner.operations.SqlCreateTableConverter.lambda$lookupLikeSourceTable$1(SqlCreateTableConverter.java:207)
	at java.util.Optional.orElseThrow(Optional.java:290)
	at org.apache.flink.table.planner.operations.SqlCreateTableConverter.lookupLikeSourceTable(SqlCreateTableConverter.java:207)
	at org.apache.flink.table.planner.operations.SqlCreateTableConverter.createCatalogTable(SqlCreateTableConverter.java:103)
	at org.apache.flink.table.planner.operations.SqlCreateTableConverter.convertCreateTable(SqlCreateTableConverter.java:83)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:188)
	at org.apache.flink.table.planner.delegation.ParserImpl.convertSqlNodeToOperation(ParserImpl.java:92)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlInternal(TableEnvironmentImpl.java:724)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sql(TableEnvironmentImpl.java:704)
{code}
We can fix it in {{SqlCreateTableConverter#lookupLikeSourceTable}}, via using `SqlTableLike`'s full name.",,godfreyhe,jark,kaibo.zhou,kezhuw,libenchao,lsy,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 08:11:31 UTC 2020,,,,,,,,,,"0|z0ioo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/20 03:57;ZhuShang;I think ,we should use `sqlTableLike.getSourceTable().names` instead of `sqlTableLike.getSourceTable().toString()`.
WDYT,[~libenchao].If i'm right ,i can do this.;;;","18/Sep/20 05:51;libenchao;[~ZhuShang] yes, you're right. assigned to you.;;;","22/Sep/20 08:11;libenchao;Fixed via:

383991adc7ca68a0e0e4b595e06e8c5edd853eb1 (1.11)

d0226d4c06cd6fd6f0c4f0d68a668edc16cb6b02 (1.12);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The option ""sink.buffer-flush.max-rows"" for JDBC can't be disabled by set to zero",FLINK-19280,13328046,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,jark,jark,17/Sep/20 12:28,19/Feb/21 07:21,13/Jul/23 08:12,21/Sep/20 11:11,1.11.2,,,,,1.11.3,1.12.0,,Connectors / JDBC,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"According to the documentation, when set ""sink.buffer-flush.max-rows=0"", it should be disabled to flush buffered data by number of rows, however, it still flushes when received each row. ",,godfreyhe,jark,leonard,libenchao,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 03:43:12 UTC 2020,,,,,,,,,,"0|z0ionk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/20 05:37;lsy;I want to try fix it, can you assigne it to me.;;;","18/Sep/20 06:15;jark;Assigned to you [~lsy];;;","21/Sep/20 03:43;jark;Fixed in:
- master (1.12.0): 9565099050d1c1b86023695973b9b5740c5fe7f1
- 1.11.3: 3768dcaf7c2937ec03589747e6cd14734ceddee0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove StatefulFunctionUnvierse cache,FLINK-19279,13328035,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,igal,igal,17/Sep/20 11:12,18/Sep/20 07:15,13/Jul/23 08:12,18/Sep/20 07:15,,,,,,statefun-2.2.0,,,Stateful Functions,,,,,0,pull-request-available,,,,,"Currently we cache the provided universe, per JVM. The cache exists to avoid scanning the class path, whenever accessing the universe across different operators executing at the same JVM, however:
 # Each operator access the universe during its open phase.
 # Having such a cache result in a shared universe across attempts, and this is undesirable. For example accessing previously released resources.

 

 

 ",,igal,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 07:15:53 UTC 2020,,,,,,,,,,"0|z0iol4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/20 07:15;tzulitai;statefun-master: a0ef9deba769320d922efb42cf9e3c05e72edaba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobs with identical graph shapes cannot be run concurrently,FLINK-19264,13327870,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,aljoscha,aljoscha,16/Sep/20 14:22,30/Nov/21 20:38,13/Jul/23 08:12,01/Oct/20 13:33,1.12.0,,,,,1.12.0,,,,,,,,1,pull-request-available,,,,,"While working on FLINK-19123 I noticed that jobs often fail on {{MiniCluster}} when multiple jobs are running at the same time. 

I created a reproducer here: https://github.com/aljoscha/flink/tree/flink-19123-fix-test-stream-env-alternative. You can run {{MiniClusterConcurrencyITCase}} to see the problem in action. Sometimes the test will succeed, sometimes it will fail with
{code}
java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.

	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.example.MiniClusterConcurrencyITCase.submitConcurrently(MiniClusterConcurrencyITCase.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:107)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:926)
	at akka.dispatch.OnComplete.internal(Future.scala:264)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:215)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:208)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:202)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523)
	at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:237)
	at org.apache.flink.runtime.jobmaster.DefaultExecutionDeploymentReconciler.reconcileExecutionDeployments(DefaultExecutionDeploymentReconciler.java:55)
	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.reportPayload(JobMaster.java:1229)
	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.reportPayload(JobMaster.java:1216)
	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.receiveHeartbeat(HeartbeatManagerImpl.java:199)
	at org.apache.flink.runtime.jobmaster.JobMaster.heartbeatFromTaskManager(JobMaster.java:674)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: org.apache.flink.util.FlinkException: Execution cbc357ccb763df2852fee8c4fc7d55f2_0_0 is unexpectedly no longer running on task executor 0a6edd8c-b1f4-48da-a228-c395b2aff92d.
	at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:241)
	... 31 more
{code}

If you reduce the number of submitted jobs the test will always pass.

Am important piece of the puzzle seems to be this message that you will also find in the log output:
{code}
4594 [mini-cluster-io-thread-25] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - JobManager for job 580f78b9224c2079ebe0c2f6b08fb4ba with leader id b0ca0700de4f8e8a2555bfe7596741ac lost leadership.
{code}

It seems that the JobMasters of the concurrent jobs somehow clash with each other.",,aljoscha,dian.fu,guoyangze,kezhuw,kkrugler,klion26,sewen,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19123,,,,,,,,,,,,,,FLINK-17295,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 13:33:10 UTC 2020,,,,,,,,,,"0|z0inkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/20 14:52;kkrugler;I often ran into this same issue, and eventually had to bail on async job submission with the MiniCluster. But this was with an older version of Flink (1.8/1.9) where I'd hacked up the code to support async submission of jobs, so I thought it was my bug.;;;","16/Sep/20 15:50;chesnay;In FLINK-17295 the {{ExecutionAttemptID}} was reworked to be based on the {{VertexID}} and attempt number. Since {{VertexIDs}} are deterministic for a given job graph, this results in {{ExecutionAttemptID}} being identical if the same job is submitted multiple times.

That ExecutionAttemptIDs are unique is a core assumption that is relied upon in multiple places; for example they are the only identifier used by the TaskExecutor to identify tasks. With this assumption being broken the TE can no longer differentiate between these tasks, causing all sorts of failure cases.;;;","16/Sep/20 15:52;chesnay;As such, I'd propose to revert FLINK-17295 and go back to random attempt IDs for the time being, while we figure out a way to make this change (or decide to discard the idea) that does not break such assumptions (e.g., by scoping every single affected data structure to a job ID).;;;","16/Sep/20 15:58;sewen;Good point, the execution attempt ID needs to be unique. At the very least, it should include the jobId.

That should make it unique on all sanely configured setups. If you submit the identical job graph (same Job ID) twice you probably already clash on the JobManager.
;;;","16/Sep/20 16:34;kkrugler;Hmm, since FLINK-17295 was a recent change, either I did have a bug in my async MiniCluster mods, or there are other bugs lurking (at least in 1.8/1.9 code base).;;;","17/Sep/20 02:11;guoyangze;Thanks for the analysis and suggestion, [~chesnay], and [~sewen]. I think including the JobID will be good enough. Another solution could be adding a short random suffix to the ExecutionAttemptID, just like what we do in FLINK-17579.

I'd like to take this ticket when we figure out there is no other bugs that cause this issue.;;;","17/Sep/20 03:56;zhuzh;Thanks for reporting and investigating this issue!
I had wrongly assumed that a {{JobVertexID}} is generated randomly. But it is not. Instead, it is generated from the hash of StreamNode, which is always the same in the same topology. That's why in [~aljoscha]'s case {{JobVertexID}} from different jobs can be the same.
Including the {{JobID}} in {{ExecutionAttemptID}} sounds a valid solution to me, given that a Flink cluster will not accept a job with duplicated {{JobID}}.

I'm also thinking whether we can further {{JobVertexID}} and {{IntermediateDataSetID}} to consist of {{JobID}} in the future. In this way, the composite {{ExecutionAttemptID}} will be naturally unique across jobs.;;;","17/Sep/20 09:24;aljoscha;Btw, the jobs are only identical in the example I provided. The jobs where I originally found this were not identical but their ""shape"" was the same, i.e. the same number of vertices, parallelism, etc.;;;","17/Sep/20 09:47;sewen;[~kkrugler] Maybe you are running into FLINK-19123 ?;;;","18/Sep/20 03:28;zhuzh;[~aljoscha] yes the {{StreamNode}} hash is, if not specified by users, generated in a deterministic way with regard to:
* node id
* chained output nodes hashes
* input nodes hashes

So identical stream graph shape will result in the same set of {{StreamNode}} hashes, and then {{JobVertexID}}s.

Hi [~guoyangze], I'm still a bit concerned about the size of {{TaskDeploymentDescriptor}} if we enlarge each {{ExecutionAttemptID}} with a {{JobID}}. So before making this change, could you do an experiment on whether there would be a significant TDD size increase? ;;;","21/Sep/20 04:05;guoyangze;[~zhuzh] I think your concern is probably valid. I'll try to figure out the impact and return here.;;;","22/Sep/20 07:56;guoyangze;Hi, there. I did a quick experiment, run WordCount with 2000 parallelism. The TDD size after serialization does increase (238.3 KB to 248.3 KB). Since FLINK-17295 has decreased the TDD size, I think the overall change might be acceptable. WDYT?;;;","22/Sep/20 11:39;zhuzh;Thanks for doing the experiment [~guoyangze]

I checked your previous experiment result [1] in the FLIP discussion and yes 
there was a nice side effect of ~5% decreasing TDD size by reworking the IDs.
Therefore I think it's acceptable to add a JobID to {{ExecutionAttemptID}} though
it increases TDD size ~5%.

I have assigned you this ticket and just go ahead.

[1] https://docs.google.com/spreadsheets/d/13lt6J29uikWoux79047lkvbGi2fS4ioWHSbvT1kTL2M/edit#gid=0.;;;","01/Oct/20 13:33;zhuzh;Fixed via 6e5240f8ac24974826a07b5370908a85bd874ce1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not setParallelism for FLIP-27 source,FLINK-19262,13327815,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,liufangliang,lzljs3620320,lzljs3620320,16/Sep/20 08:39,18/Sep/20 03:58,13/Jul/23 08:12,18/Sep/20 03:58,,,,,,1.12.0,,,API / DataStream,,,,,0,pull-request-available,,,,,"The constructor for new Sources in {{DataStreamSource}} does not set isParallel to true. So if we setParallelism for FLIP-27 source, there will be a validation exception from validateParallelism.",,liufangliang,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 03:58:17 UTC 2020,,,,,,,,,,"0|z0in8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/20 09:16;liufangliang;Hi [~lzljs3620320] ,

master 
{code:java}
/**
 * Constructor for new Sources (FLIP-27).
 */
public DataStreamSource(
      StreamExecutionEnvironment environment,
      Source<T, ?, ?> source,
      WatermarkStrategy<T> timestampsAndWatermarks,
      TypeInformation<T> outTypeInfo,
      String sourceName) {
   super(environment,
         new SourceTransformation<>(
               sourceName,
               new SourceOperatorFactory<>(source, timestampsAndWatermarks),
               outTypeInfo,
               environment.getParallelism()));
}
{code}
I want to change to the following
{code:java}
/**
 * Constructor for new Sources (FLIP-27).
 */
public DataStreamSource(
      StreamExecutionEnvironment environment,
      Source<T, ?, ?> source,
      WatermarkStrategy<T> timestampsAndWatermarks,
      TypeInformation<T> outTypeInfo,
      String sourceName) {
   super(environment,
         new SourceTransformation<>(
               sourceName,
               new SourceOperatorFactory<>(source, timestampsAndWatermarks),
               outTypeInfo,
               environment.getParallelism()));
   this.isParallel = true;
}
{code}
What do you thank of ?

 ;;;","16/Sep/20 09:23;lzljs3620320;[~liufangliang] I think it is good.;;;","16/Sep/20 09:32;liufangliang;[~lzljs3620320] Can you assign this issue to me? I will finish it.;;;","16/Sep/20 09:54;lzljs3620320;Assigned to you [~liufangliang];;;","16/Sep/20 10:07;liufangliang;[~lzljs3620320] Thanks ;;;","18/Sep/20 03:58;lzljs3620320;master: 49e3ff72e7fb06360c02578e30fa03cce24ca04f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix the wrong example of ""csv.line-delimiter"" in CSV documentation",FLINK-19258,13327800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,jark,jark,16/Sep/20 07:26,19/Feb/21 07:32,13/Jul/23 08:12,30/Sep/20 14:31,,,,,,1.11.3,1.12.0,,Documentation,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,0,pull-request-available,,,,,"In the documentation [1], it says:

{code}
'csv.line-delimiter' = U&'\\000D' specifies the to use carriage return \r as line delimiter. 
'csv.line-delimiter' = U&'\\000A'  specifies the to use line feed \n as line delimiter.
{code}

This is wrong. They should be U&'\000D' and U&'\000A'.



[1]: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/csv.html#csv-line-delimiter",,jark,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 07:34:15 UTC 2020,,,,,,,,,,"0|z0in5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/20 11:05;lsy;Hi, [~jark] I can help do it.;;;","30/Sep/20 14:31;jark;Fixed in master : 237b5e02ba2ee43d8d33b6f5c0fa04710b3088be;;;","04/Nov/20 07:34;jark;Fixed in 1.11.3: be1aea76b88b156d23dc68517086bda578faecac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceReaderTestBase.testAddSplitToExistingFetcher hangs,FLINK-19253,13327765,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,dian.fu,dian.fu,16/Sep/20 02:53,08/Dec/20 09:46,13/Jul/23 08:12,08/Dec/20 09:45,1.11.2,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6521&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code}
2020-09-15T10:51:35.5236837Z ""SourceFetcher"" #39 prio=5 os_prio=0 tid=0x00007f70d0a57000 nid=0x858 in Object.wait() [0x00007f6fd81f0000]
2020-09-15T10:51:35.5237447Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-15T10:51:35.5237962Z 	at java.lang.Object.wait(Native Method)
2020-09-15T10:51:35.5238886Z 	- waiting on <0x00000000c27f5be8> (a java.util.ArrayDeque)
2020-09-15T10:51:35.5239380Z 	at java.lang.Object.wait(Object.java:502)
2020-09-15T10:51:35.5240401Z 	at org.apache.flink.connector.base.source.reader.mocks.TestingSplitReader.fetch(TestingSplitReader.java:52)
2020-09-15T10:51:35.5241471Z 	- locked <0x00000000c27f5be8> (a java.util.ArrayDeque)
2020-09-15T10:51:35.5242180Z 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2020-09-15T10:51:35.5243245Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:128)
2020-09-15T10:51:35.5244263Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:95)
2020-09-15T10:51:35.5245128Z 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
2020-09-15T10:51:35.5245973Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-09-15T10:51:35.5247081Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-15T10:51:35.5247816Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-15T10:51:35.5248809Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-15T10:51:35.5249463Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-15T10:51:35.5249827Z 
2020-09-15T10:51:35.5250383Z ""SourceFetcher"" #37 prio=5 os_prio=0 tid=0x00007f70d0a4b000 nid=0x856 in Object.wait() [0x00007f6f80cfa000]
2020-09-15T10:51:35.5251124Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-15T10:51:35.5251636Z 	at java.lang.Object.wait(Native Method)
2020-09-15T10:51:35.5252767Z 	- waiting on <0x00000000c298d0b8> (a java.util.ArrayDeque)
2020-09-15T10:51:35.5253336Z 	at java.lang.Object.wait(Object.java:502)
2020-09-15T10:51:35.5254184Z 	at org.apache.flink.connector.base.source.reader.mocks.TestingSplitReader.fetch(TestingSplitReader.java:52)
2020-09-15T10:51:35.5255220Z 	- locked <0x00000000c298d0b8> (a java.util.ArrayDeque)
2020-09-15T10:51:35.5255678Z 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2020-09-15T10:51:35.5256235Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:128)
2020-09-15T10:51:35.5256803Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:95)
2020-09-15T10:51:35.5257351Z 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
2020-09-15T10:51:35.5257838Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-09-15T10:51:35.5258284Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-15T10:51:35.5258856Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-15T10:51:35.5259350Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-15T10:51:35.5260011Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-15T10:51:35.5260211Z 
2020-09-15T10:51:35.5260574Z ""process reaper"" #24 daemon prio=10 os_prio=0 tid=0x00007f6f70042000 nid=0x844 waiting on condition [0x00007f6fd832a000]
2020-09-15T10:51:35.5261036Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-15T10:51:35.5261342Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-15T10:51:35.5261972Z 	- parking to wait for  <0x00000000815d0810> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-09-15T10:51:35.5262456Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-15T10:51:35.5263067Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-09-15T10:51:35.5263611Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-09-15T10:51:35.5264176Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-09-15T10:51:35.5264649Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-09-15T10:51:35.5265143Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-15T10:51:35.5265750Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-15T10:51:35.5266152Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-15T10:51:35.5266369Z 
2020-09-15T10:51:35.5267000Z ""surefire-forkedjvm-ping-30s"" #23 daemon prio=5 os_prio=0 tid=0x00007f70d03eb800 nid=0x83e waiting on condition [0x00007f6fd8942000]
2020-09-15T10:51:35.5267483Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-09-15T10:51:35.5267801Z 	at sun.misc.Unsafe.park(Native Method)
2020-09-15T10:51:35.5268418Z 	- parking to wait for  <0x0000000081595d18> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-09-15T10:51:35.5269030Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-09-15T10:51:35.5269600Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-09-15T10:51:35.5270234Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-09-15T10:51:35.5270850Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-09-15T10:51:35.5271405Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-09-15T10:51:35.5271891Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-09-15T10:51:35.5272397Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-15T10:51:35.5272908Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-15T10:51:35.5273106Z 
2020-09-15T10:51:35.5273722Z ""surefire-forkedjvm-command-thread"" #22 daemon prio=5 os_prio=0 tid=0x00007f70d03d4000 nid=0x83d runnable [0x00007f6fd8c4d000]
2020-09-15T10:51:35.5274193Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5274508Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-09-15T10:51:35.5274878Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-09-15T10:51:35.5275292Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-09-15T10:51:35.5275737Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-09-15T10:51:35.5276314Z 	- locked <0x0000000081597fc8> (a java.io.BufferedInputStream)
2020-09-15T10:51:35.5276692Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-09-15T10:51:35.5277173Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-09-15T10:51:35.5277719Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-09-15T10:51:35.5278143Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-15T10:51:35.5278358Z 
2020-09-15T10:51:35.5278764Z ""Service Thread"" #21 daemon prio=9 os_prio=0 tid=0x00007f70d02cb800 nid=0x839 runnable [0x0000000000000000]
2020-09-15T10:51:35.5279262Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5279447Z 
2020-09-15T10:51:35.5279807Z ""C1 CompilerThread14"" #20 daemon prio=9 os_prio=0 tid=0x00007f70d02c8000 nid=0x838 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5280243Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5280425Z 
2020-09-15T10:51:35.5280788Z ""C1 CompilerThread13"" #19 daemon prio=9 os_prio=0 tid=0x00007f70d02c6000 nid=0x837 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5281203Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5281398Z 
2020-09-15T10:51:35.5281748Z ""C1 CompilerThread12"" #18 daemon prio=9 os_prio=0 tid=0x00007f70d02c4000 nid=0x836 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5282180Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5282362Z 
2020-09-15T10:51:35.5282804Z ""C1 CompilerThread11"" #17 daemon prio=9 os_prio=0 tid=0x00007f70d02c1800 nid=0x835 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5283239Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5283423Z 
2020-09-15T10:51:35.5283768Z ""C1 CompilerThread10"" #16 daemon prio=9 os_prio=0 tid=0x00007f70d02c0000 nid=0x834 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5284331Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5284529Z 
2020-09-15T10:51:35.5284870Z ""C2 CompilerThread9"" #15 daemon prio=9 os_prio=0 tid=0x00007f70d02bd000 nid=0x833 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5285299Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5285481Z 
2020-09-15T10:51:35.5285838Z ""C2 CompilerThread8"" #14 daemon prio=9 os_prio=0 tid=0x00007f70d02bb000 nid=0x832 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5286249Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5286446Z 
2020-09-15T10:51:35.5286784Z ""C2 CompilerThread7"" #13 daemon prio=9 os_prio=0 tid=0x00007f70d02b9000 nid=0x831 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5287211Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5287410Z 
2020-09-15T10:51:35.5287752Z ""C2 CompilerThread6"" #12 daemon prio=9 os_prio=0 tid=0x00007f70d02b7800 nid=0x830 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5288182Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5288362Z 
2020-09-15T10:51:35.5288804Z ""C2 CompilerThread5"" #11 daemon prio=9 os_prio=0 tid=0x00007f70d02b5000 nid=0x82f waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5289213Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5289409Z 
2020-09-15T10:51:35.5289753Z ""C2 CompilerThread4"" #10 daemon prio=9 os_prio=0 tid=0x00007f70d02b3000 nid=0x82e waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5290168Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5290352Z 
2020-09-15T10:51:35.5290706Z ""C2 CompilerThread3"" #9 daemon prio=9 os_prio=0 tid=0x00007f70d02a9000 nid=0x82d waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5291125Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5291310Z 
2020-09-15T10:51:35.5291667Z ""C2 CompilerThread2"" #8 daemon prio=9 os_prio=0 tid=0x00007f70d02a6800 nid=0x82c waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5292082Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5292276Z 
2020-09-15T10:51:35.5292692Z ""C2 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f70d02a4800 nid=0x82b waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5293115Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5293296Z 
2020-09-15T10:51:35.5293655Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f70d02a2800 nid=0x82a waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5294127Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5294310Z 
2020-09-15T10:51:35.5294647Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f70d02a0800 nid=0x829 runnable [0x0000000000000000]
2020-09-15T10:51:35.5295037Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5295304Z 
2020-09-15T10:51:35.5295679Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f70d029f000 nid=0x828 waiting on condition [0x0000000000000000]
2020-09-15T10:51:35.5296135Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5296319Z 
2020-09-15T10:51:35.5296658Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f70d026e800 nid=0x827 in Object.wait() [0x00007f6fda624000]
2020-09-15T10:51:35.5297103Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-15T10:51:35.5297418Z 	at java.lang.Object.wait(Native Method)
2020-09-15T10:51:35.5297994Z 	- waiting on <0x00000000815d17f0> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-15T10:51:35.5298413Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-09-15T10:51:35.5299073Z 	- locked <0x00000000815d17f0> (a java.lang.ref.ReferenceQueue$Lock)
2020-09-15T10:51:35.5299486Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-09-15T10:51:35.5299917Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-09-15T10:51:35.5300174Z 
2020-09-15T10:51:35.5300529Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f70d026a000 nid=0x813 in Object.wait() [0x00007f6fda725000]
2020-09-15T10:51:35.5301046Z    java.lang.Thread.State: WAITING (on object monitor)
2020-09-15T10:51:35.5301372Z 	at java.lang.Object.wait(Native Method)
2020-09-15T10:51:35.5301884Z 	- waiting on <0x00000000815d1d80> (a java.lang.ref.Reference$Lock)
2020-09-15T10:51:35.5302228Z 	at java.lang.Object.wait(Object.java:502)
2020-09-15T10:51:35.5302701Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-09-15T10:51:35.5303254Z 	- locked <0x00000000815d1d80> (a java.lang.ref.Reference$Lock)
2020-09-15T10:51:35.5303668Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-09-15T10:51:35.5303989Z 
2020-09-15T10:51:35.5304284Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f70d000b800 nid=0x7a1 runnable [0x00007f70d7e66000]
2020-09-15T10:51:35.5304658Z    java.lang.Thread.State: RUNNABLE
2020-09-15T10:51:35.5305148Z 	at org.apache.flink.connector.base.source.reader.SourceReaderTestBase.testAddSplitToExistingFetcher(SourceReaderTestBase.java:98)
2020-09-15T10:51:35.5305688Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-15T10:51:35.5306130Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-15T10:51:35.5306631Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-15T10:51:35.5307087Z 	at java.lang.reflect.Method.invoke(Method.java:498)
{code}",,becket_qin,dian.fu,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20083,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 09:45:47 UTC 2020,,,,,,,,,,"0|z0imxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/20 02:54;dian.fu;cc [~jqin];;;","10/Nov/20 04:03;xuannan;I think it is a potential race condition in SplitFetcher.

It occurs when one thread executing the SplitFetcher#checkAndSetIdle method and the other thread adding split to the SplitFetcher with SplitFetcher#addSplits. i.e., the checkAndSetIdle first check if it should go to idle and set the isIdle flag. Between these two steps, another thread could call the addSplits, which put a new task into the taskQueue and set the isIdle flag to false. Then, the first thread set the isIdle flag to true.

We need to sychronize the thread that modifying the isIdle flag.

cc [~becket_qin]
;;;","10/Nov/20 06:46;becket_qin;[~xuannan] Good catch. Will you submit a fix?;;;","10/Nov/20 06:50;xuannan;[~becket_qin] I'd love to submit a fix. Could you assign the ticket to me?;;;","11/Nov/20 09:30;becket_qin;Looking at the code, I think there might be another race in the {{SourceReaderBase}} that may cause {{finishOrAvailableLater()}} to throw {{IllegalStateException}} incorrectly. The sequence is following:
 # The {{SourceReader}} has received {{NoMoreSplitsAssignedEvent}}.
 # The main thread calls {{SourceReader.pollNext()}} but there is no record in the elementQueue. So it invokes {{finishedOrAvailableLater()}}
 # In {{SplitFetcher.runOnce()}}, the {{SplitFetcher}} thread runs the {{FetchTask}}. The {{FetchTask}} finishes enqueuing the last {{RecordsWithSplitIds}} to the elementQueue, remove the last split from the {{assignedSplits}}, and then returns. At this point the {{assignedSplits}} becomes empty.
 # The {{SplitFetcher}} thread then invoke {{checkAndSetIdle()}} and set isIdle to true.
 # The main thread invokes {{splitFetcherManager.maybeShutdownFinishedFetchers()}}, sees the {{SplitFetcher}} is idle then shuts it down. At this point there is no alive fetchers any more.
 # Since {{noMoreSplitsAssignment && allFetchersHaveShutdown}} is met, the main thread will go and check to see if the elementQueue is empty. And in this case, the element queue has the last RecordsWithSplitIds from step 3. The SourceReaderBase will then throw {{IllegalStateException}} incorrectly.

Since this is also related to the SplitFetcher idleness, I am thinking that we can fix this problem in this patch as well.;;;","11/Nov/20 13:32;xuannan;[~becket_qin] You are right. This indeed can cause IllegalStateException to be thrown. After closely looking at the code, I think it made more sense to return MORE_AVAILABLE instead of throwing the IllegalStateException at that point. The rationale is shown in the comment of the code below. And we might want to change the name of the method from `finishedOrAvailableLater` to something like `getInputStatus`. What do you think?

{code:java}
	private InputStatus finishedOrAvailableLater() {
		// assumption: elementQueue is empty (might not be valid already)
		final boolean allFetchersHaveShutdown = splitFetcherManager.maybeShutdownFinishedFetchers();
		if (!(noMoreSplitsAssignment && allFetchersHaveShutdown)) {
			return InputStatus.NOTHING_AVAILABLE;
		}
		// at this point we are guarantee that no more split will be assigned
		// and all the SplitFetchers are idle and has been shutdown
		if (elementsQueue.isEmpty()) {
			splitFetcherManager.checkErrors();
			return InputStatus.END_OF_INPUT;
		} else {
			// if we see a non-empty elementsQueue here, it means that our assumption that the
			// elementsQueue is empty is violated. This indicates that after our last check on
			// the elementsQueue, all SplitFetchers fetch all the elements, put them in the
			// element queue and closed, which means there are element available before
			// we can return END_OF_INPUT. Therefore, we return MORE_AVAILABLE 
			// instead of throwing exception.
			return InputStatus.MORE_AVAILABLE;
//			throw new IllegalStateException(""Called 'finishedOrAvailableLater()' with shut-down fetchers but non-empty queue"");
		}
	}
{code}

;;;","11/Nov/20 14:33;becket_qin;[~xuannan] Thanks for digging into this. Yes, I agree that returning {{MORE_AVAILABLE}} makes sense. Can you update the patch? Ideally, we would also like to have a unit test to detect such issues. Thanks.;;;","12/Nov/20 06:29;xuannan;[~becket_qin] I think https://issues.apache.org/jira/browse/FLINK-19448 has fixed the problem. Can you take a look?;;;","12/Nov/20 14:56;becket_qin;Yes, that has been fixed. Thanks for adding the tests.

The patch has been merged to master.
e131a474fbfefddcede2600027dcf883ad3aeffd


I am leaving this issue open because it has not been backported to 1.11.3. I am not doing that right away because there seems some other bug fixes prior to this fix that is still not backported. Ideally we want to do that in order so that we can avoid unnecessary rebase.;;;","12/Nov/20 16:30;xuannan;Thanks for letting me know. I will stay tuned until we can backport. ;;;","08/Dec/20 09:45;becket_qin;Merged to release-1.11.

69e9d3c2913fb1f923d71ab0f9d0eed43af11ff3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitFetcherManager does not propagate errors correctly,FLINK-19250,13327725,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,15/Sep/20 20:27,15/Nov/20 22:11,13/Jul/23 08:12,16/Sep/20 08:07,1.11.2,,,,,1.11.3,1.12.0,,Connectors / Common,,,,,0,,,,,,"The first exception that is reported does not lead to a notification, only successive exceptions do. ",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 15 22:11:05 UTC 2020,,,,,,,,,,"0|z0imoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/20 08:07;sewen;Fixed in 1.12.0 (master) via 5abef56b2bf85bcac786f6b16b6899b6cced7176;;;","15/Nov/20 22:11;sewen;Fixed in 1.11.3 (release-1.11) via f220c2486181e4af19b246e9745a4990cd6aa9ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableSourceITCase.testStreamScanParallelism fails on private Azure accounts,FLINK-19246,13327691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,rmetzger,rmetzger,15/Sep/20 14:17,18/Sep/20 02:32,13/Jul/23 08:12,18/Sep/20 02:32,1.12.0,,,,,1.12.0,,,Table SQL / Legacy Planner,,,,,0,pull-request-available,test-stability,,,,"Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8381&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac
or
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8379&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac
or
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8369&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac
(this change is already merged to master, so it is unlikely to cause the error)
{code}
2020-09-15T13:51:34.6773312Z org.apache.flink.api.common.InvalidProgramException: The implementation of the CollectionInputFormat is not serializable. The object probably contains or references non serializable fields.
2020-09-15T13:51:34.6774140Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:151)
2020-09-15T13:51:34.6774634Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:126)
2020-09-15T13:51:34.6775136Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:71)
2020-09-15T13:51:34.6775728Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.clean(StreamExecutionEnvironment.java:1913)
2020-09-15T13:51:34.6776617Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.addSource(StreamExecutionEnvironment.java:1601)
2020-09-15T13:51:34.6777322Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1493)
2020-09-15T13:51:34.6778029Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1483)
2020-09-15T13:51:34.6778887Z 	at org.apache.flink.table.factories.utils.TestCollectionTableFactory$CollectionTableSource.getDataStream(TestCollectionTableFactory.scala:159)
2020-09-15T13:51:34.6779659Z 	at org.apache.flink.table.factories.utils.TestCollectionTableFactory$CollectionTableSource.getDataStream(TestCollectionTableFactory.scala:134)
2020-09-15T13:51:34.6780394Z 	at org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.translateToPlan(StreamTableSourceScan.scala:105)
2020-09-15T13:51:34.6781058Z 	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.translateInput(DataStreamSink.scala:189)
2020-09-15T13:51:34.6781655Z 	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.writeToSink(DataStreamSink.scala:84)
2020-09-15T13:51:34.6782266Z 	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.translateToPlan(DataStreamSink.scala:59)
2020-09-15T13:51:34.6782951Z 	at org.apache.flink.table.planner.StreamPlanner.org$apache$flink$table$planner$StreamPlanner$$translateToCRow(StreamPlanner.scala:274)
2020-09-15T13:51:34.6783640Z 	at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:119)
2020-09-15T13:51:34.6784227Z 	at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:116)
2020-09-15T13:51:34.6784799Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-09-15T13:51:34.6785345Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-09-15T13:51:34.6785828Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-09-15T13:51:34.6786285Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-09-15T13:51:34.6786760Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-09-15T13:51:34.6787210Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-09-15T13:51:34.6787681Z 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
2020-09-15T13:51:34.6788168Z 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2020-09-15T13:51:34.6788648Z 	at org.apache.flink.table.planner.StreamPlanner.translate(StreamPlanner.scala:116)
2020-09-15T13:51:34.6789286Z 	at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:178)
2020-09-15T13:51:34.6790031Z 	at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:103)
2020-09-15T13:51:34.6790705Z 	at org.apache.flink.table.api.bridge.scala.TableConversions.toAppendStream(TableConversions.scala:78)
2020-09-15T13:51:34.6791362Z 	at org.apache.flink.table.runtime.stream.table.TableSourceITCase.testStreamScanParallelism(TableSourceITCase.scala:118)
2020-09-15T13:51:34.6791907Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-15T13:51:34.6792350Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-15T13:51:34.6792906Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-15T13:51:34.6793391Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-15T13:51:34.6793869Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-15T13:51:34.6794406Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-15T13:51:34.6795396Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-15T13:51:34.6796052Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-15T13:51:34.6796565Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-15T13:51:34.6797051Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-15T13:51:34.6797481Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-15T13:51:34.6797896Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-15T13:51:34.6798465Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-15T13:51:34.6799768Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-15T13:51:34.6800273Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-15T13:51:34.6800728Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-15T13:51:34.6801197Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-15T13:51:34.6801656Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-15T13:51:34.6802119Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-15T13:51:34.6802599Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-15T13:51:34.6803183Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-15T13:51:34.6803687Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-15T13:51:34.6804117Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-15T13:51:34.6804589Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-15T13:51:34.6805148Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-15T13:51:34.6805721Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-15T13:51:34.6806253Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-15T13:51:34.6806844Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-15T13:51:34.6807445Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-15T13:51:34.6807972Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-15T13:51:34.6808487Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-15T13:51:34.6809041Z Caused by: java.lang.RuntimeException: Row arity of record (3) does not match this serializers field length (1).
2020-09-15T13:51:34.6809610Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:181)
2020-09-15T13:51:34.6810199Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:58)
2020-09-15T13:51:34.6810784Z 	at org.apache.flink.api.java.io.CollectionInputFormat.writeObject(CollectionInputFormat.java:90)
2020-09-15T13:51:34.6811287Z 	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
2020-09-15T13:51:34.6811746Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-15T13:51:34.6814374Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-15T13:51:34.6814842Z 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1140)
2020-09-15T13:51:34.6815329Z 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
2020-09-15T13:51:34.6815850Z 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2020-09-15T13:51:34.6816355Z 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2020-09-15T13:51:34.6816826Z 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
2020-09-15T13:51:34.6817345Z 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:586)
2020-09-15T13:51:34.6817871Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:133)
2020-09-15T13:51:34.6818350Z 	... 59 more
{code}

I don't understand why this is failing only in my personal azure account (on branches with different changes).",,aljoscha,dian.fu,dwysakowicz,jark,rmetzger,sewen,twalthr,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 02:32:24 UTC 2020,,,,,,,,,,"0|z0imhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 14:37;aljoscha;Also saw this: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=294&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac&l=7310.

But I had at least one later build (on my personal Azure) where it didn't fail.;;;","15/Sep/20 20:58;sewen;Have the same problem. The test also passes on my local machine, but fails on Azure private account.

Are there some environment variables or Locale settings that can affect this test?;;;","16/Sep/20 03:01;jark;I think the reason is that {{TableSourceITCase.testStreamScanParallelism}} uses {{COLLECTION}} connector, but forgot to call {{TestCollectionTableFactory.reset()}}. The COLLECTION connector uses a global dataset, so it may be affected by other tests if {{reset()}} is not called. This is a design flaw in the COLLECTION connector. We have a better design in the new VALUES connector to have per-table dataset. 

I will take this issue. ;;;","16/Sep/20 03:07;dian.fu;Another instance: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6528&view=logs&j=508fb1b1-538d-50d3-f3b5-2c91da394c53&t=66b5b08b-49c1-51bc-2227-987658d900a8;;;","16/Sep/20 05:01;xtsong;Another instance:
https://dev.azure.com/tonysong820/Flink/_build/results?buildId=69&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac;;;","17/Sep/20 07:08;dwysakowicz;https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=414&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac

It happens quite often. Would be good to tackle it soonish. Do we know what change triggered those failures?;;;","17/Sep/20 09:35;jark;Hi [~dwysakowicz], I think this is introduced by FLINK-18852 which was merged recently. I have opened a pull request for this. Would be great if you can take a look. ;;;","18/Sep/20 01:42;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6615&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=508fb1b1-538d-50d3-f3b5-2c91da394c53;;;","18/Sep/20 02:32;jark;Fixed in master (1.12.0): b930423c5a7f1f30ce0ff837d28bde15a40d6fb3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV format can't deserialize null ROW field,FLINK-19244,13327678,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yingz,yingz,yingz,15/Sep/20 13:21,19/Feb/21 07:21,13/Jul/23 08:12,22/Sep/20 03:48,1.11.2,,,,,1.11.3,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"CREATE TABLE csv_table (
 f0 ROW<f0c0 VARCHAR, f0c1 VARCHAR>,
 f1 ROW<f1c0 INT, f1c1 VARCHAR>
 )

If f0 is null and f1c0=123, f1c1=456, the serialized data will be: ,123;456
When deserialize the data, the jsonNode of f0 would be [], then throws cast exception: Row length mismatch. 2 fields expected but was 0.

In the real scene, I set two streams:
 First, read json_table, sink to csv_table, which has the schema above.
 Then, read csv_table, do sth.

if json is \{""f0"": null, ""f1"": {""f1c0"": 123, ""f1c1"": 456}}, the second streams failed with the exception.

If this is a bug, I want to help to fix this and unittests.

 

here is the  test code:
{code:java}
// code placeholder
val subDataType0 = ROW(
  FIELD(""f0c0"", STRING()),
  FIELD(""f0c1"", STRING())
)
val subDataType1 = ROW(
  FIELD(""f1c0"", INT()),
  FIELD(""f1c1"", INT())
)
val datatype = ROW(
  FIELD(""f0"", subDataType0),
  FIELD(""f1"", subDataType1))
val rowType = datatype.getLogicalType.asInstanceOf[RowType]

val serSchema = new CsvRowDataSerializationSchema.Builder(rowType).build()
val deserSchema = new CsvRowDataDeserializationSchema.Builder(rowType, new RowDataTypeInfo(rowType)).build()
def foo(r: RowData): Unit = {
  val serData = new String(serSchema.serialize(r))
  print(s""${serData}"")
  val deserRow = deserSchema.deserialize(serData.getBytes)
  println(s""${deserRow}"")
}

val normalRowData = GenericRowData.of(
  GenericRowData.of(BinaryStringData.fromString(""hello""), BinaryStringData.fromString(""world"")),
  GenericRowData.of(Integer.valueOf(123), Integer.valueOf(456))
)
// correct.
foo(normalRowData)

val nullRowData = GenericRowData.of(
  null,
  GenericRowData.of(Integer.valueOf(123), Integer.valueOf(456))
)
/*
Exception in thread ""main"" java.io.IOException: Failed to deserialize CSV row ',123;456
...
Caused by: java.lang.RuntimeException: Row length mismatch. 2 fields expected but was 0.
 */
foo(nullRowData)
{code}",,jark,libenchao,yingz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 03:48:24 UTC 2020,,,,,,,,,,"0|z0imeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/20 02:36;jark;You can try to set {{ignoreParseErrors}} to {{true}}.;;;","16/Sep/20 02:37;jark;Btw, CSV is not a good format to handle nested types. ;;;","16/Sep/20 02:59;yingz;[~jark] ignoreParseErrors=true will skip this record

x = serialize( y) means y = deserialize( x), so is deserialize(',123;456') = originalRowData maybe better?

 

I totally agree with 'CSV is not a good format to handle nested types'. I apply this to compat with the legacy code, which uses csv format for less space and more visible.;;;","16/Sep/20 03:19;jark;I think we can add a check in {{org.apache.flink.formats.csv.CsvRowDataDeserializationSchema#createRowConverter}}, when the {{jsonNode.size()}} is 0, we can return a null RowData directly. ;;;","16/Sep/20 04:31;yingz;[~jark]

Thanks for your advice, this will lead the user to use format like json/arvo/protobuf for nested types too:);;;","19/Sep/20 15:41;yingz;[~jark]
I try to pull a request, and cloud you help to assign this ticket? Thanks.;;;","22/Sep/20 03:48;jark;Fixed in
 - master: 83795aaeaa2519627a403715b904c4c0c1c90710
 - release-1.11: 1ba6c7e591d838beaf74e932e919575a78ef9302;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.table.api.ValidationException: Cannot resolve field,FLINK-19242,13327658,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,henvealf,henvealf,15/Sep/20 11:59,15/Sep/20 12:13,13/Jul/23 08:12,15/Sep/20 12:13,1.11.1,,,,,,,,Table SQL / API,,,,,0,,,,,,"Hello, 

Planner: Blink

The Code:
{code:java}
val countResult = eventTable
  .select($""name"", $""product"", $""id"", $""_event_time"")
  .window(Tumble over 10.second() on $""_event_time"" as ""w"")
  .groupBy( $""id"", $""w"")
  .select(
    $""id"".count() as(""c""),  $""id"", $""w""
  )
{code}
Exception:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Cannot resolve field [w], input field list:[id, EXPR$0].Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Cannot resolve field [w], input field list:[id, EXPR$0]. at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule$ExpressionResolverVisitor.failForField(ReferenceResolverRule.java:80) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule$ExpressionResolverVisitor.lambda$null$3(ReferenceResolverRule.java:75) at java.util.Optional.orElseThrow(Optional.java:290) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule$ExpressionResolverVisitor.lambda$null$4(ReferenceResolverRule.java:75) at java.util.Optional.orElseGet(Optional.java:267) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule$ExpressionResolverVisitor.lambda$visit$5(ReferenceResolverRule.java:74) at java.util.Optional.orElseGet(Optional.java:267) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule$ExpressionResolverVisitor.visit(ReferenceResolverRule.java:71) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule$ExpressionResolverVisitor.visit(ReferenceResolverRule.java:51) at org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:31) at org.apache.flink.table.expressions.UnresolvedReferenceExpression.accept(UnresolvedReferenceExpression.java:60) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule.lambda$apply$0(ReferenceResolverRule.java:47) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule.apply(ReferenceResolverRule.java:48) at org.apache.flink.table.expressions.resolver.ExpressionResolver.lambda$null$1(ExpressionResolver.java:211) at java.util.function.Function.lambda$andThen$1(Function.java:88) at java.util.function.Function.lambda$andThen$1(Function.java:88) at java.util.function.Function.lambda$andThen$1(Function.java:88) at org.apache.flink.table.expressions.resolver.ExpressionResolver.resolve(ExpressionResolver.java:178) at org.apache.flink.table.operations.utils.OperationTreeBuilder.projectInternal(OperationTreeBuilder.java:191) at org.apache.flink.table.operations.utils.OperationTreeBuilder.project(OperationTreeBuilder.java:160) at org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:792) at 
...
Process finished with exit code 1
{code}
Why?

Thanks!",,henvealf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 12:13:26 UTC 2020,,,,,,,,,,"0|z0ima0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 12:13;henvealf;My problem. It worked after Changed the following code
{code:java}
val countResult = eventTable
  .select($""name"", $""product"", $""id"", $""_event_time"")
  .window(Tumble over 10.second() on $""_event_time"" as ""w"")
  .groupBy( $""id"", $""w"")
  .select(
    $""id"".count() as(""c""),  $""id"", $""w"".end()
  ){code}
change $""w"" to $""w"".end() in select;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"LeaderChangeClusterComponentsTest.testReelectionOfJobMaster failed with ""NoResourceAvailableException: Could not allocate the required slot within slot request timeout""",FLINK-19237,13327576,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,dian.fu,dian.fu,15/Sep/20 03:19,09/Nov/20 09:11,13/Jul/23 08:12,09/Nov/20 09:11,1.11.2,1.12.0,,,,1.10.3,1.11.3,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6499&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6]

{code}
2020-09-14T21:11:02.8200203Z [ERROR] testReelectionOfJobMaster(org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest)  Time elapsed: 300.14 s  <<< FAILURE!
2020-09-14T21:11:02.8201761Z java.lang.AssertionError: Job failed.
2020-09-14T21:11:02.8202749Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.throwAssertionErrorOnFailedResult(JobResultUtils.java:54)
2020-09-14T21:11:02.8203794Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.assertSuccess(JobResultUtils.java:30)
2020-09-14T21:11:02.8205177Z 	at org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.testReelectionOfJobMaster(LeaderChangeClusterComponentsTest.java:152)
2020-09-14T21:11:02.8206191Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-14T21:11:02.8206985Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-14T21:11:02.8207930Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-14T21:11:02.8208927Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-14T21:11:02.8209753Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-14T21:11:02.8210710Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-14T21:11:02.8211608Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-14T21:11:02.8214473Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-14T21:11:02.8215398Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-14T21:11:02.8216199Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-14T21:11:02.8216947Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-14T21:11:02.8217695Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-14T21:11:02.8218635Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-14T21:11:02.8219499Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-14T21:11:02.8220313Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-14T21:11:02.8221060Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-14T21:11:02.8222171Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-14T21:11:02.8222937Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-14T21:11:02.8223688Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-14T21:11:02.8225191Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-14T21:11:02.8226086Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-14T21:11:02.8226761Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-14T21:11:02.8227453Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-14T21:11:02.8228392Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-14T21:11:02.8229256Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-14T21:11:02.8235798Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-14T21:11:02.8237650Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-14T21:11:02.8239039Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-14T21:11:02.8239894Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-14T21:11:02.8240591Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-14T21:11:02.8241325Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-09-14T21:11:02.8242225Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-09-14T21:11:02.8243358Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-09-14T21:11:02.8244425Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:215)
2020-09-14T21:11:02.8245291Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:208)
2020-09-14T21:11:02.8246150Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:202)
2020-09-14T21:11:02.8247006Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523)
2020-09-14T21:11:02.8247960Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)
2020-09-14T21:11:02.8249102Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1722)
2020-09-14T21:11:02.8249971Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1283)
2020-09-14T21:11:02.8250675Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1251)
2020-09-14T21:11:02.8251369Z 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1082)
2020-09-14T21:11:02.8252104Z 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)
2020-09-14T21:11:02.8253060Z 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)
2020-09-14T21:11:02.8253956Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:458)
2020-09-14T21:11:02.8254967Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:445)
2020-09-14T21:11:02.8393562Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-09-14T21:11:02.8394920Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-09-14T21:11:02.8396122Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8397194Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8398150Z 	at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:169)
2020-09-14T21:11:02.8399234Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-09-14T21:11:02.8400048Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-09-14T21:11:02.8401048Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8402025Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8403171Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:731)
2020-09-14T21:11:02.8404708Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)
2020-09-14T21:11:02.8405751Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)
2020-09-14T21:11:02.8406633Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-09-14T21:11:02.8407378Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-09-14T21:11:02.8408120Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8408948Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8409748Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1168)
2020-09-14T21:11:02.8410511Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-09-14T21:11:02.8411543Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-09-14T21:11:02.8412553Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8413340Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8414204Z 	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1072)
2020-09-14T21:11:02.8415364Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
2020-09-14T21:11:02.8416128Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
2020-09-14T21:11:02.8417172Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-09-14T21:11:02.8417995Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-14T21:11:02.8418997Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-14T21:11:02.8419692Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-14T21:11:02.8420336Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-14T21:11:02.8421055Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-14T21:11:02.8421655Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-14T21:11:02.8422336Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-14T21:11:02.8423049Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-14T21:11:02.8423681Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-14T21:11:02.8424505Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-14T21:11:02.8425209Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-14T21:11:02.8425760Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-14T21:11:02.8426376Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-14T21:11:02.8427252Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-14T21:11:02.8427931Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-14T21:11:02.8428684Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-14T21:11:02.8429375Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-14T21:11:02.8430118Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-14T21:11:02.8430853Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-14T21:11:02.8431971Z Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.
2020-09-14T21:11:02.8433179Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:464)
2020-09-14T21:11:02.8434082Z 	... 45 more
2020-09-14T21:11:02.8434809Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException
2020-09-14T21:11:02.8435611Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2020-09-14T21:11:02.8436379Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2020-09-14T21:11:02.8437159Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2020-09-14T21:11:02.8437976Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-09-14T21:11:02.8438658Z 	... 25 more
2020-09-14T21:11:02.8439085Z Caused by: java.util.concurrent.TimeoutException
2020-09-14T21:11:02.8439476Z 	... 23 more
{code}",,azagrebin,dian.fu,mapohl,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16866,,,,,,,,,,FLINK-19210,FLINK-20033,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 09:11:12 UTC 2020,,,,,,,,,,"0|z0ils0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 03:22;dian.fu;This issue and FLINK-19210 seems caused by the same problem.;;;","22/Sep/20 05:48;mapohl;The problem seems to be caused by [65ed0393|https://github.com/apache/flink/commit/65ed0393] which is related to FLINK-16866. We were able to reproduce it by running the test LeaderChangeClusterComponentsTest#testReelectionOfJobMaster multiple times until the failure happened (5825 and even less in other tries).
We failed to reproduce it on the 65ed0393's parent commit [7da74dc2|https://github.com/apache/flink/commit/7da74dc2795abfb8806f14768a95327c8c204dc8] after 20000 runs.

The runtime of the test increases by a factor of 3 with the change introduce in 65ed0393.;;;","28/Sep/20 14:39;rmetzger;Since I've worked on FLINK-16866, I'll take a look at the issue.;;;","03/Oct/20 00:47;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7169&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","05/Oct/20 07:36;rmetzger;It seems that this test instability is reproducible on CI https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8423&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374 (for me it is quite difficult reproducing it locally. I needed 27238 (~50 minutes) locally).;;;","06/Oct/20 09:10;rmetzger;In the normal case, it seems that:
a) the job gets submitted
b) operators get scheduled, but slot requests cannot be served yet (no RM connected),
c) RM registration starts, but doesn't finish
d) JM loses leadership, job gets suspended
e) JM regains leadership
f) job starts running, test succeeds.

In the failure case:
1. Job gets submitted
2. Operators get scheduled, but slot requests cannot be served yet (no RM connected)
3. RM registration succeeds, slots get allocated and activated, operators switch to DEPLOYING
4. JM loses leadership, job gets suspended
5. JM regains leadership
6. TaskManager reports: slots get rejected by the job manager
7. TaskExecutors close JM connection: no more allocated slots
8. Slot allocation times out. 
;;;","09/Oct/20 06:56;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7307&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6;;;","12/Oct/20 12:28;xtsong;Another instance:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7429&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","15/Oct/20 19:32;rmetzger;Fixed in https://github.com/apache/flink/commit/ff026787b1e461eae7520a9a52eb33b210b78c2d;;;","06/Nov/20 14:40;trohrmann;I think this issue also affects earlier Flink releases (e.g. 1.11.x). I think we should backport the fix.;;;","06/Nov/20 17:41;rmetzger;Good catch! I'll backport the fix.;;;","07/Nov/20 15:11;trohrmann;[~rmetzger] I will do it because I also have to add a test for FLINK-20033.;;;","07/Nov/20 16:49;trohrmann;The backport PRs are

1.11: https://github.com/apache/flink/pull/13979
1.10: https://github.com/apache/flink/pull/13980;;;","09/Nov/20 09:11;trohrmann;Backports:

1.11.3: d9be3188e49fe93e3bf56b3077aca3e53177807a
1.10.3: 260cf3b7b9f5bd6bf6a8755c698e119ae45fe9fd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The catalog is still created after opening failed in catalog registering,FLINK-19227,13327553,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhuShang,lzljs3620320,lzljs3620320,15/Sep/20 01:43,19/Feb/21 07:23,13/Jul/23 08:12,23/Sep/20 02:23,,,,,,1.11.3,,,Table SQL / API,,,,,0,pull-request-available,starter,,,,"> When I create the HiveCatalog and Flink is not able to connect to the HiveMetastore, the statement can not be executed, but the catalog is still created. Subsequent attempts to query the tables result in a NPE.

In CatalogManager.registerCatalog.
 Consider open is a relatively easy operation to fail, we should put catalog into catalog manager after its open.",,jark,lsy,lzljs3620320,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 02:23:49 UTC 2020,,,,,,,,,,"0|z0ilmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 10:00;ZhuShang;I think ,we shoud have a double check of HiveMetastoreClientWrapper,if client is created failed,we need to thorw a CatalogException. And then put the catalog into catalog manager only if it is opened successfully.

WDYT,[~lzljs3620320].BTW, can i taka this?;;;","15/Sep/20 10:53;lzljs3620320;Good, It is.

Assigned to you [~ZhuShang].;;;","23/Sep/20 02:23;lzljs3620320;master: 37f90c1ef94ba47a0b688f3663395f8f2b33d6cb

release-1.11: 7129b18ef7ab0a331bd8503088247f0c345ee571;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove inconsistent host logic for LocalFileSystem,FLINK-19218,13327451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,14/Sep/20 14:20,15/Sep/20 20:17,13/Jul/23 08:12,15/Sep/20 20:17,1.11.1,,,,,1.12.0,,,API / Core,,,,,0,,,,,,"The {{LocalFileSystem}} returns file splits with a host information as returned via
{code}
InetAddress.getLocalHost().getHostName();
{code}

This might be different, though, from the host name that the TaskManager is configured to use, which results in incorrect location matching if this information is used.

It is also incorrect in cases where the file system is in fact not local, but a mounted NAS.

Since this information is anyways not useful (there no good way to support locality-aware file access for the LocalFileSystem) I would suggest to remove this code. That would be better than having code in place that tries to suggest locality information that is frequently incorrect.
",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 20:17:09 UTC 2020,,,,,,,,,,"0|z0il08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 20:17;sewen;Fixed in 1.12.0 via 5bcdd7de67bf21bc19c8b5b84f617dacaf7f0c99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Resuming Savepoint (rocks, scale down, rocks timers) end-to-end test"" failed with ""Dispatcher REST endpoint has not started within a timeout of 20 sec""",FLINK-19215,13327432,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,dian.fu,dian.fu,14/Sep/20 12:28,23/Oct/20 06:47,13/Jul/23 08:12,29/Sep/20 08:07,1.11.0,,,,,1.12.0,,,Runtime / State Backends,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6476&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994

{code}
2020-09-13T21:26:23.3646770Z Running 'Resuming Savepoint (rocks, scale down, rocks timers) end-to-end test'
2020-09-13T21:26:23.3647852Z ==============================================================================
2020-09-13T21:26:23.3689605Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-23367497881
2020-09-13T21:26:23.7122791Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-09-13T21:26:23.9988115Z Starting cluster.
2020-09-13T21:26:27.3702750Z Starting standalonesession daemon on host fv-az655.
2020-09-13T21:26:35.1213853Z Starting taskexecutor daemon on host fv-az655.
2020-09-13T21:26:35.2756714Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:36.4111928Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:37.5358508Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:38.7156039Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:39.8602294Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:41.0399056Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:42.1680966Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:43.2520250Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:44.3833552Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:45.5204296Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:46.6730448Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:47.8274365Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:49.0147447Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:51.5463623Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:52.7366058Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:53.8867521Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:55.0469025Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:56.1901349Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:57.3124935Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:58.4596457Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:59.4828675Z Dispatcher REST endpoint has not started within a timeout of 20 sec
2020-09-13T21:26:59.4831446Z [FAIL] Test script contains errors.
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19014,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 23 06:47:15 UTC 2020,,,,,,,,,,"0|z0ikw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/20 07:25;rmetzger;I could not find any reason for the failure. It seems that the startup of the dispatcher usually takes quite some time. In the previous test, it took 19 seconds.
This makes my effort related to FLINK-19014.;;;","21/Sep/20 10:13;rmetzger;After looking a bit through the numbers, my conclusion is: our setup of starting Flink jobs through the scripts is slow
Examples:
- ""jobmanager.sh:parseJmJvmArgsAndExportLogs"": takes roughly 0.5 seconds on modern hardware -- it starts a JVM
- sourcing the ""config.sh"" script takes 0.1 seconds (it is sourced multiple times
- getting the java version takes another 0.15 seconds
- submitting a Flink job takes ~5 seconds
- waiting till the job is running ~2 seconds

Suggestion: fix low hanging fruits & increase timeout.;;;","29/Sep/20 08:07;rmetzger;Increased timeout in https://github.com/apache/flink/commit/6f3453e86f4553ae784652b86fe2da1784963230.;;;","23/Oct/20 01:58;dian.fu;Instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8131&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=7d4f7375-52df-5ce0-457f-b2ffbb2289a4

[~rmetzger] What about also backport this fix to 1.11?;;;","23/Oct/20 05:31;rmetzger;Backported to 1.11 in: 0c01563a220a35db266322a83951aa0ede47cdc8;;;","23/Oct/20 06:47;dian.fu;[~rmetzger] Thanks a lot! :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PyFlink e2e tests is instable and failed with ""Connection broken: OSError""",FLINK-19201,13327030,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,dian.fu,dian.fu,11/Sep/20 07:58,28/Oct/20 02:27,13/Jul/23 08:12,28/Oct/20 02:27,1.11.0,1.12.0,,,,1.11.3,1.12.0,,API / Python,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6452&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=6945d9e3-ebef-5993-0c44-838d8ad079c0]

{code}
2020-09-10T21:37:42.9988117Z install conda ... [SUCCESS]
2020-09-10T21:37:43.0018449Z install miniconda... [SUCCESS]
2020-09-10T21:37:43.0082244Z installing python environment...
2020-09-10T21:37:43.0100408Z installing python3.5...
2020-09-10T21:37:58.7214400Z install python3.5... [SUCCESS]
2020-09-10T21:37:58.7253792Z installing python3.6...
2020-09-10T21:38:06.5855143Z install python3.6... [SUCCESS]
2020-09-10T21:38:06.5903358Z installing python3.7...
2020-09-10T21:38:11.5444706Z 
2020-09-10T21:38:11.5484852Z ('Connection broken: OSError(""(104, \'ECONNRESET\')"")', OSError(""(104, 'ECONNRESET')""))
2020-09-10T21:38:11.5513130Z 
2020-09-10T21:38:11.8044086Z conda install 3.7 failed.            You can retry to exec the script.
{code}",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 02:27:47 UTC 2020,,,,,,,,,,"0|z0iieo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/20 01:54;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8118&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3;;;","23/Oct/20 01:54;dian.fu;cc [~hxbks2ks];;;","23/Oct/20 02:02;hxbks2ks;[~dian.fu] Thanks a lot for reporting. I will increase the number of retries to avoid network problems.;;;","26/Oct/20 01:46;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8250&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360;;;","28/Oct/20 02:27;dian.fu;Fixed in master via e196f7b0c015ea1153b51a5da012ce6ed3fc6145 and release-1.11 via 2cf4656dc7d18cc50b90277159b366ba40d86436;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The UDF split and split_index get wrong result ,FLINK-19194,13326860,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,faaronzheng,faaronzheng,10/Sep/20 12:17,24/Sep/20 07:38,13/Jul/23 08:12,24/Sep/20 07:38,1.10.0,1.10.1,,,,1.11.2,,,Table SQL / Runtime,,,,,0,,,,,,"If we run sql

{code:sql}
select split_index('ab-123""xyz\cd','-',1) ;
{code}
or

{code:sql}
select split('ab-123""xyz\cd','-')[2] ;
{code}

{noformat}
 in sql-client, we should get result 123""xyz\cd, not 123\""xyz\\cd 
{noformat}
",,faaronzheng,jark,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/20 06:36;faaronzheng;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13011533/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 08:23:00 UTC 2020,,,,,,,,,,"0|z0ihcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/20 03:08;jark;Hi [~faaronzheng], I'm not sure this is a bug in Table/SQL. Flink doesn't provide {{split}} or {{split_index}}. I just tested {{select UPPER('ab-123""xyz\cd')}} in SQL CLI, and the result is {{AB-123""XYZ\CD}} which is as expected. ;;;","15/Sep/20 03:08;jark;Could you provide a program which can reproduce your result? ;;;","15/Sep/20 06:39;faaronzheng;Hi [~jark],  please check it again. !screenshot-1.png!  [systemFunctions|https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/functions/systemFunctions.html]

PS. I use flink 1.10 with core module, and split and split_index can run both.;;;","15/Sep/20 08:23;jark;Hi [~faaronzheng], yes, SPLIT_INDEX is supported. I verified it is correct on {{flink-1.11}} 


{code}
123""xyz\cd
{code}

but is wrong on {{flink-1.10.0}}

{code}
123\""xyz\\cd
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-connector-hive module compile failed with ""cannot find symbol: variable TableEnvUtil""",FLINK-19183,13326785,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,dian.fu,dian.fu,10/Sep/20 05:06,10/Sep/20 06:01,13/Jul/23 08:12,10/Sep/20 06:01,1.12.0,,,,,1.12.0,,,Connectors / Hive,,,,,0,test-stability,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6416&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=9b1a0f88-517b-5893-fc93-76f4670982b4]

{code}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/vsts/work/1/s/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java:[584,33] cannot find symbol
  symbol:   variable TableEnvUtil
  location: class org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase
[ERROR] /home/vsts/work/1/s/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java:[589,33] cannot find symbol
  symbol:   variable TableEnvUtil
  location: class org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase
{code}",,dian.fu,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19070,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 05:59:12 UTC 2020,,,,,,,,,,"0|z0igw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/20 05:56;lzljs3620320;Fixed in master: e319820b1b14304f00e40f3dba2b9c6e4ecda6bf;;;","10/Sep/20 05:59;dian.fu;Thanks [~lzljs3620320] for the quick fix~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests in JoinITCase do not test BroadcastHashJoin,FLINK-19175,13326710,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,dwysakowicz,dwysakowicz,09/Sep/20 16:33,24/Nov/20 06:33,13/Jul/23 08:12,24/Nov/20 06:33,,,,,,1.12.0,,,Table SQL / Planner,Tests,,,,0,pull-request-available,,,,,"The tests in JoinITCase claim to test the {{BroadcastHashJoin}}, but they actually do not. None of the tables used in the tests have proper statistics therefore, none of the tables meet the threshold for the broadcast join. At the same time the {{ShuffleHashJoin}} is not disabled, therefore they silently fallback to {{ShuffleHashJoin}}.

In summary none (or at least not all of the tests) are executed for BroadcastHashJoin, but are executed twice for ShuffleHashJoin.",,danny0405,dwysakowicz,godfreyhe,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 06:33:36 UTC 2020,,,,,,,,,,"0|z0igfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/20 04:01;danny0405;Nice catch, [~dwysakowicz], can i take this issue ?;;;","10/Sep/20 04:04;jark;Assigned to you [~danny0405];;;","24/Nov/20 06:33;godfreyhe;master: a8d24b84be8d55dd4fcb1d50cf131e33040a7eae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
idleTimeMsPerSecond can report incorrect values (0 or values more than one second),FLINK-19174,13326692,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pnowojski,pnowojski,pnowojski,09/Sep/20 14:45,25/Aug/22 11:55,13/Jul/23 08:12,06/Jan/21 07:51,1.11.2,1.12.0,,,,1.13.0,,,Runtime / Metrics,,,,,0,,,,,,"If task is blocked for more than 60 seconds ({{org.apache.flink.metrics.MeterView#DEFAULT_TIME_SPAN_IN_SECONDS}}), {{idleTimeMsPerSecond}} can be reported as zero, despite task being completely idle. Once the task is unblocked and the {{idleTimeMsPerSecond}} metric is updated, it can for the next 60 seconds exceed 1000ms/second.

Average value over the longer periods of time will be correct and accurate.",,pnowojski,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 07:51:34 UTC 2021,,,,,,,,,,"0|z0igbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/21 07:51;pnowojski;PR:
https://github.com/apache/flink/pull/14526

Merged as 7aafc4c51b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter naming error,FLINK-19170,13326617,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sulei,sulei,sulei,09/Sep/20 07:28,14/Sep/20 11:40,13/Jul/23 08:12,14/Sep/20 11:24,1.11.1,,,,,1.11.3,1.12.0,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,,,,rmetzger,sulei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 14 11:27:14 UTC 2020,,,,,,,,,,"0|z0ifvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/20 11:24;rmetzger;Merged in https://github.com/apache/flink/commit/ac4e2accaa53fd86e1f6ff7400bee5d4964f9525

Thanks a lot!;;;","14/Sep/20 11:27;rmetzger;Merged to master in https://github.com/apache/flink/commit/7d41f1b728ba12c8bc4a2886d6d172a5e80b0e08;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingFileWriter should register Listener before the initialization of buckets,FLINK-19166,13326581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Sep/20 03:05,09/Sep/20 14:17,13/Jul/23 08:12,09/Sep/20 14:17,1.11.1,,,,,1.11.2,1.12.0,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"In [http://apache-flink.147419.n8.nabble.com/StreamingFileSink-hive-metadata-td6898.html]

The feedback of User indicates that some partitions have not been committed since the job failed.

This maybe due to FLINK-18110, in FLINK-18110, it has fixed Buckets, but forgot fixing {{StreamingFileWriter}} , it should register Listener before the initialization of buckets, otherwise, will loose listening too.",,gaoyunhaii,jkillers,lirui,lzljs3620320,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 14:17:00 UTC 2020,,,,,,,,,,"0|z0ifnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/20 14:17;zhuzh;Fixed via
master: 03fd81937a77d4952a49d53891880b0489485c0c
release-1.11: 03fd81937a77d4952a49d53891880b0489485c0c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release scripts break other dependency versions unintentionally,FLINK-19164,13326467,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ssoydan,ssoydan,ssoydan,08/Sep/20 13:34,23/Sep/21 17:25,13/Jul/23 08:12,09/Jun/21 10:21,,,,,,1.14.0,,,Deployment / Scripts,Release System,,,,0,auto-unassigned,pull-request-available,,,,"All the scripts below has a line to change the old version to new version in pom files.

[https://github.com/apache/flink/blob/master/tools/change-version.sh#L31]

[https://github.com/apache/flink/blob/master/tools/releasing/create_release_branch.sh#L60]

[https://github.com/apache/flink/blob/master/tools/releasing/update_branch_version.sh#L52]

 

It works like find & replace so it is prone to unintentional errors. Any dependency with a version equals to ""old version"" might be automatically changed to ""new version"". See below to see how to produce a similar case. 

 

+How to re-produce the bug:+
 * Clone/Fork Flink repo and for example checkout version v*1.11.1* 
 * Apply any changes you need
 * Run ""create_release_branch.sh"" script with OLD_VERSION=*1.11.1* NEW_VERSION={color:#de350b}*1.12.0*{color}
 ** In parent pom.xml, an auto find&replace of maven-dependency-analyzer version will be done automatically and *unintentionally* which will break the build.

 
                        <dependency>
                            <groupId>org.apache.maven.shared</groupId>
                            <artifactId>maven-dependency-analyzer</artifactId>
                            <version>*1.11.1*</version>
                        </dependency>
 
                        <dependency>
                            <groupId>org.apache.maven.shared</groupId>
                            <artifactId>maven-dependency-analyzer</artifactId>
                            <version>{color:#de350b}*1.12.0*{color}</version>
                        </dependency>",,aljoscha,david.artiga,rmetzger,ssoydan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 09 10:21:19 UTC 2021,,,,,,,,,,"0|z0iey8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/20 13:43;ssoydan;Adding 
[~mxm] & [~aljoscha]

for informing and maybe a possible solution (who had changes/commits before for the stated lines);;;","08/Sep/20 14:09;rmetzger;I believe this is a known problem. These scripts are only helper scripts. Their modifications are usually reviewed by many eyes as part of the release process.;;;","08/Sep/20 21:11;ssoydan;[~rmetzger], thanks for the comment. Although it is only a helper script and the results are reviewed by many eyes for the official release process, it still exists in the project and open to use for people that may use it within forked repos or other cases. The possible side effect is not clear and everyone should be able to somehow use it without worrying that it may break. Actually we experienced the issue in our project and spent hours trying to figure out where the problem is (there are around 185 poms that needs to be reviewed).

The below line seem to update versions of all modules in the project (within pom files) by a manual find & replace:
 {color:#de350b}find . -name 'pom.xml' -type f -exec perl -pi -e 's#<version>(.*)'$OLD_VERSION'(.*)</version>#<version>${1}'$NEW_VERSION'${2}</version>#' {} \;{color}

*A possible solution is to replace it with ""versions maven plugin"" and running the command below within the script (tried it locally and it seems to work properly):*

{color:#de350b}mvn versions:set -DnewVersion=$NEW_VERSION -DprocessAllModules{color}

or use it directly in the script only without adding the plugin to the project

{color:#de350b}mvn org.codehaus.mojo:versions-maven-plugin:2.8.1:set -DnewVersion=$NEW_VERSION -DprocessAllModules{color};;;","09/Sep/20 07:07;rmetzger;I see, you are raising a valid point, and I like your proposal for addressing this. Would you be up for opening a pull request to change the scripts?

I would use the second variant (without adding the plugin to the project).;;;","09/Sep/20 07:49;ssoydan;Yeah, sure. I can create a PR for that in a couple of days.

Can you (or someone else) assign the issue to me?

Thank you.;;;","09/Sep/20 07:55;rmetzger;Awesome, thanks a lot. 
I assigned you to the ticket.;;;","14/Apr/21 22:42;flink-jira-bot;This issue and all of its Sub-Tasks have not been updated for 180 days. So, it has been labeled ""stale-minor"". If you are still affected by this bug or are still interested in this issue, please give an update and remove the label. In 7 days the issue will be closed automatically.;;;","22/Apr/21 22:44;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","22/May/21 10:51;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","29/May/21 23:08;flink-jira-bot;This issue was marked ""stale-assigned"" 7 ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","09/Jun/21 10:21;chesnay;master: 36891da2f2d0662a04e122f768297c3709ebd982;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application mode deletes HA data in case of suspended ZooKeeper connection,FLINK-19154,13326280,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,568793005@qq.com,568793005@qq.com,07/Sep/20 09:36,02/Nov/20 08:44,13/Jul/23 08:12,26/Oct/20 13:40,1.11.1,1.12.0,,,,1.11.3,1.12.0,,Client / Job Submission,,,,,1,pull-request-available,,,,,"A user reported that Flink's application mode deletes HA data in case of a suspended ZooKeeper connection [1]. 

The problem seems to be that the {{ApplicationDispatcherBootstrap}} class produces an exception (that the request job can no longer be found because of a lost ZooKeeper connection) which will be interpreted as a job failure. Due to this interpretation, the cluster will be shut down with a terminal state of FAILED which will cause the HA data to be cleaned up. The exact problem occurs in the {{JobStatusPollingUtils.getJobResult}} which is called by {{ApplicationDispatcherBootstrap.getJobResult()}}.

The above described behaviour can be found in this log [2].

[1] http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Checkpoint-metadata-deleted-by-Flink-after-ZK-connection-issues-td37937.html
[2] https://pastebin.com/raw/uH9KDU2L

","Run a stand-alone cluster that runs a single job (if you are familiar with the way Ververica Platform runs Flink jobs, we use a very similar approach). It runs Flink 1.11.1 straight from the official docker image.",568793005@qq.com,casidiablo,dian.fu,kkl0u,klion26,maguowei,Paul Lin,rmetzger,stevenz3wu,trohrmann,tzulitai,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19909,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 08:56:25 UTC 2020,,,,,,,,,,"0|z0idso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/20 12:55;rmetzger;Let's first understand the problem properly on the user mailing list.;;;","28/Sep/20 13:57;trohrmann;[~aljoscha], [~kkl0u], there seems to be a problem with the application mode and the failure handling. I believe that some framework errors are treated as a proper Flink job failure which leads to the deletion of HA data even though one would like to keep this data. Could you take care of this problem?

I think there are two problems here: First of all not every exception bubbling up in the future returned by {{ApplicationDispatcherBootstrap.fixJobIdAndRunApplicationAsync()}} indicates a job failure. Some of them can also indicate a framework failure which should not lead to the clean up of HA data. The other problem is that the polling logic cannot properly handle a temporary connection loss to ZooKeeper which is a normal situation.;;;","28/Sep/20 15:06;kkl0u;I will have a look.;;;","29/Sep/20 17:47;casidiablo;Hello guys! Good timing. This happened again yesterday. And it happened around the time one of our zookeeper nodes restarted (typical kubernetes shuffling, so not a ZK issue). I would be super happy to provide more details.

One interesting but challenging characteristic of this bug is that this only affected one of the jobs out of more than 40 we run. The other jobs just restarted, but their state was preserved.

But for one of the jobs we were unlucky and the job manager wiped out the state out of ZK. Pretty much the same logs as stated in this ticket.;;;","30/Sep/20 13:57;kkl0u;Disclaimer: I am not super-familiar with ZK and the steps involved in HA failover and what I am saying may be wrong. 

From the discussion here I understand that there is a ZK transitive issue and Flink detects it and restarts the affected jobs. During the restarting process, Flink tells ZK to delete the HA data. 

In this last part, there seems to be a race condition, right? ZK is down (triggered the failure) and then it comes back up. In the meantime jobs are shutting down and tell ZK to delete their data. Can it be that for most of the jobs ZK is still down when they try to delete their HA data and the request fails while for the ""unlucky one"" this is not the case?

[~casidiablo] Is there anything in the logs of the jobs that successfully restarted (their logs during failure) that could justify such an explanation?;;;","30/Sep/20 14:51;trohrmann;I think the causing problem is 

{code}
2020-09-04 17:32:07,950 WARN  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application FAILED: 
java.util.concurrent.CompletionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (00000000000000000000000000000000)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$requestJobStatus$17(Dispatcher.java:529) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_262]
	at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_262]
	at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_262]
	at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:523) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$getJobResult$0(JobStatusPollingUtils.java:57) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.pollJobResultAsync(JobStatusPollingUtils.java:81) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$3(JobStatusPollingUtils.java:96) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_262]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_262]
	at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154) [flink-dist_2.11-1.11.1.jar:1.11.1]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [usercode.jar:?]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) [usercode.jar:?]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [usercode.jar:?]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [usercode.jar:?]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [usercode.jar:?]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [usercode.jar:?]
Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (00000000000000000000000000000000)
	at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGatewayFuture(Dispatcher.java:807) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:518) ~[flink-dist_2.11-1.11.1.jar:1.11.1]
	... 12 more
{code}

which is caused by the ZK being down. Since we treat exceptions coming from the {{ApplicationDispatcherBootstrap.fixJobIdAndRunApplicationAsync}} as a {{FAILED}} job state, Flink will clean the HA data up.;;;","30/Sep/20 14:55;kkl0u;Yes, I agree. This is what we have to disambiguate. Some exceptions should be treated as job failures and some not. But this seems like a pretty impossible task that even if it is done correctly, it is also hard to maintain, right? There does not seem to be a specific type of exception that we should filter out.;;;","30/Sep/20 15:41;trohrmann;At the moment there is no easy distinction between framework exceptions and user code exceptions, unfortunately.;;;","05/Oct/20 06:25;rmetzger;[~kkl0u] can I assign you to this ticket?;;;","05/Oct/20 08:03;kkl0u;[~rmetzger] I assigned it to me. ;;;","09/Oct/20 20:39;casidiablo;I failed to capture the logs from the pods where this didn't happen... but it happened again to almost ALL of our jobs. This is a really nasty bug with a huge impact on production systems.

I really hope it gets the traction and attention it needs. I don't know why no one else has reported it (we barely run 70 Flink jobs, I'm sure there are companies running way more).;;;","09/Oct/20 21:00;kkl0u;Hi Christian, I have a fix and I am planning to open a PR soon. I will send you here when I do so that you can also check it out.;;;","09/Oct/20 21:05;kkl0u;This is the branch https://github.com/kl0u/flink/tree/FLINK-19154 based on which I am planning to open a PR as soon as our tests give the green light. It would be really helpful if you could try it out and let us know if this solves the issue.;;;","26/Oct/20 13:40;kkl0u;master: f29a18e0f01c5bddffb8fa60c7b769b8c418f453
1.11: 3dd57624b2df7a856352a64a89af8606ae0c61a4;;;","27/Oct/20 18:34;casidiablo;When are you guys planning to release these changes?

I tried using them (i.e. building a Flink docker image with these changes) but hit a wall: these changes are not backwards compatible.

The changes to the `flink-clients` (moving the AbstractDispatcherBootstrap class) mean that not only I need to upgrade the Flink cluster but also re compile all my jobs against these new changes. Since these changes are not in Maven yet, I also need to publish the jars to my own repository, etc.

What's more, since this is not backwards compatible, I guess it makes no sense to make it part of the 1.11 branch?;;;","27/Oct/20 21:42;casidiablo;For the record, this does seem to fix the issue. We haven't been able to reproduce the problem after upgrading our test environment to this new version;;;","28/Oct/20 07:16;tzulitai;Hi [~casidiablo], there is an ongoing discussion for releasing Flink 1.11.3 that would include this fix.
Hopefully this should happen soon:
http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Releasing-Apache-Flink-1-11-3-td45989.html;;;","28/Oct/20 08:56;trohrmann;Thanks for trying this fix out [~casidiablo] and happy to hear that it solved your problem.

If you want to use Flink's snapshot artifacts, then you have to add

{code}
<repositories>
  <repository>
    <id>snapshot</id>
    <name>Apache Snapshot repository</name>
    <url>https://repository.apache.org/content/repositories/snapshots/</url>
    <snapshotPolicy>always</snapshotPolicy>
  </repository>
</repositories>
{code}

to your {{pom.xml}}.

If you do not bundle Flink dependencies with your user jar which you put into your image, then it should actually not be necessary to recompile the user jar (unless we introduced an incompatible change with Flink 1.12).
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink does not normalize container resource with correct configurations when Yarn FairScheduler is used ,FLINK-19151,13326262,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csbliss,xtsong,xtsong,07/Sep/20 08:08,21/Sep/20 10:00,13/Jul/23 08:12,10/Sep/20 11:12,1.11.2,,,,,1.11.3,1.12.0,,Deployment / YARN,,,,,0,pull-request-available,,,,,"h3. Problem

It's a Yarn protocol that the requested container resource will be normalized for allocation. That means, the allocated container may have different resource (larger than or equal to) compared to what is requested.

Currently, Flink matches the allocated containers to the original requests by reading the Yarn configurations and calculate how the requested resources should be normalized.

What has been overlooked is that, Yarn FairScheduler (and its subclass SLSFairScheduler) has overridden the normalization behavior. To be specific,
 * By default, Yarn normalize container resources to integer multiple of ""yarn.scheduler.minimum-allocation-[mb|vcores]""
 * FairScheduler normalize container resources to integer multiple of ""yarn.resource-types.[memory-mb|vcores].increment-allocation"" (or the deprecated keys ""yarn.scheduler.increment-allocation-[mb|vcores]""), while making sure the resource is no less than ""yarn.scheduler.minimum-allocation-[mb|vcores]""

h3. Proposal for short term solution

To fix this problem, a quick and easy way is to also read Yarn configuration and learn which scheduler is used, and perform normalization calculations accordingly. This should be good enough to cover behaviors of all the schedulers that Yarn currently provides. The limitation is that, Flink will not be able to deal with custom Yarn schedulers which override the normalization behaviors.
h3. Proposal for long term solution

For long term, it would be good to use Yarn ContainerRequest#allocationRequestId to match the allocated containers with the original requests, so that Flink no longer needs to understand how Yarn normalize container resources. 

Yarn ContainerRequest#allocationRequestId is introduced in Hadoop 2.9, while ATM Flink claims to be compatible with Hadoop 2.4+. Therefore, this solution would not work at the moment.

Another idea is to support various Hadoop versions with different container matching logics. We can abstract the container matching logics into a dedicating component, and provide different implementations for it. This will allow Flink to take advantages of the new versions (e.g., work well with custom schedulers), while stay compatible with the old versions without those advantages.

Given that we need the resource based matching anyway for the old Hadoop versions, and the cost for maintaining two sets of matching logics, I tend to think this approach as a back-up option to be worked on when we indeed see a need for it.",,csbliss,guoyangze,kezhuw,lingyaKK,Paul Lin,wangm92,wangyang0918,xtsong,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19141,,,,,,,,,,,FLINK-19324,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 11:12:50 UTC 2020,,,,,,,,,,"0|z0idoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/20 10:23;csbliss;[~xintongsong]

Can we add two fields to the class WorkerSpecContainerResourceAdapter: unitMemMB and unitVcore.
Calculate the unit value in the WorkerSpecContainerResourceAdapter constructor according to different yarn scheduler, such as FairScheduler or other yarn scheduler;;;","07/Sep/20 10:50;xtsong;+1 on adding unit memory/vcore in WorkerSpecContainerResourceAdapter and deciding the unit resources according to the Yarn scheduler.

I would suggest to decide the unit resources outside `WorkerSpecContainerResourceAdapter`. The adapter does not need to be aware of the Yarn scheduler differences. It should also make the adapter easy to test.;;;","07/Sep/20 10:55;csbliss;[~xintongsong] Yes, we can set the unit resources in the constructor of YarnResourceManager;;;","08/Sep/20 02:32;csbliss;[~xintongsong] Can i have this issue?;;;","08/Sep/20 02:38;xtsong;[~csbliss]

Sure. Thanks for the volunteering. I have assigned you.

Please make sure you read the community [contribution|https://flink.apache.org/contributing/contribute-code.html] and [code style|https://flink.apache.org/contributing/code-style-and-quality-preamble.html] guidelines in advance and go ahead.;;;","08/Sep/20 03:01;wangyang0918;I prefer the long term solution and +1 for the different implementations based on hadoop versions. Currently, many hadoop versions are going to EOL, including [2.0.x - 2.9.x], [3.0.x]. And the community strongly suggests to upgrade to hadoop-3.1. It should be stable enough.

 

[https://cwiki.apache.org/confluence/display/HADOOP/EOL+(End-of-life)+Release+Branches];;;","08/Sep/20 05:06;xtsong;Thanks for the input, [~fly_in_gis].

I think how do we support various hadoop versions w.r.t. the API and feature differences is a big topic, which probably deserve a separate discussion thread. 

In addition to the container allocation request id, currently in Flink there are also other usages of Yarn APIs/features that are not supported by all hadoop versions. E.g., getting previous attempt containers and scheduler resource types from register application response, requesting containers with external resources (a.t.m. GPU), submitting application with specific tags and node labels, e.t.c. I think it would be better to take all these issues into consideration, and try to come up with some general principles how we handle such version diversities.

Such discussion could take some time, at mean time it might be good to fix the problem with the proposed short-term solution, which should not require much efforts.

WDYT?;;;","08/Sep/20 06:54;wangyang0918;[~xintongsong] Thanks for the explanation. I agree with you that we could start with the short-term solution. Since it do not need too much efforts and could fix the issues. ;;;","10/Sep/20 11:12;xtsong;Fixed via
* master: 00bf41f33a10b59850f0ee4fe31c0271484d6d4c
* release-1.11: bfff6b15ec7dd3a4415f6a5a9d8535ea7960e474;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table crashed in Flink Table API & SQL  Docs ,FLINK-19148,13326247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ShawnHx,leonard,leonard,07/Sep/20 06:49,09/Sep/20 14:14,13/Jul/23 08:12,07/Sep/20 12:57,,,,,,1.11.2,1.12.0,,Documentation,,,,,0,pull-request-available,starter,,,,"Table crashed in Flink Table API & SQL  Docs 

[1] [https://ci.apache.org/projects/flink/flink-docs-master/dev/table/types.html#new-blink-planner|https://ci.apache.org/projects/flink/flink-docs-master/dev/table/types.html#new-blink-planner]",,jark,leonard,ShawnHx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 07 12:57:38 UTC 2020,,,,,,,,,,"0|z0idlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/20 07:28;ShawnHx;Hi, [~Leonard Xu].

I'm willing to fix it. Could you assign it to me ?

Thanks~;;;","07/Sep/20 07:43;jark;Assigned to you [~ShawnHx];;;","07/Sep/20 12:57;jark;Fixed in 
- master (1.12.0): 82f6f86959b434d32186b87147d268d6b58cea02
- 1.11.3: fab7076d5378a7f341bb1a5a95e1da6b3fecc870;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local recovery can be broken if slot hijacking happened during a full restart,FLINK-19142,13326074,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,azagrebin,azagrebin,04/Sep/20 12:11,15/Dec/21 01:44,13/Jul/23 08:12,09/Dec/21 02:46,1.12.0,,,,,1.14.3,1.15.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The ticket originates from [this PR discussion|https://github.com/apache/flink/pull/13181#discussion_r481087221].

The previous AllocationIDs are used by PreviousAllocationSlotSelectionStrategy to schedule subtasks into the slot where they were previously executed before a failover. If the previous slot (AllocationID) is not available, we do not want subtasks to take previous slots (AllocationIDs) of other subtasks.

The MergingSharedSlotProfileRetriever gets all previous AllocationIDs of the bulk from SlotSharingExecutionSlotAllocator but only from the current bulk. The previous AllocationIDs of other bulks stay unknown. Therefore, the current bulk can potentially hijack the previous slots from the preceding bulks. On the other hand the previous AllocationIDs of other tasks should be taken if the other tasks are not going to run at the same time, e.g. not enough resources after failover or other bulks are done.

Local recovery can be broken due to this. e.g. when multiple regions of a streaming job are restarted at the same time(due to global failover, or task failover with `full` failover strategy).",,aitozi,azagrebin,guoyangze,kezhuw,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18689,,,,,,,,,,,,,,FLINK-24793,,,,,,,,FLINK-16430,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 09 02:45:59 UTC 2021,,,,,,,,,,"0|z0iciw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/21 10:47;chesnay;[~zhuzh] Would you have time to investigate this?;;;","26/Feb/21 11:10;zhuzh;[~chesnay] Yes I will look into it. ;;;","05/Mar/21 10:13;zhuzh;In my understanding, the problem is that {{MergingSharedSlotProfileRetriever}} does not properly set the {{previousExecutionGraphAllocations}} of a {{SlotProfile}}. So that some restarted region may take the previous slots of other failed regions, and the state local recovery will be affected.

To fix the problem, we need {{SlotProfile.previousExecutionGraphAllocations}} to include previous allocationId of all the vertices which need to be restarted at that time.

The suggestion from [~azagrebin] to ""give to MergingSharedSlotProfileRetriever all previous AllocationIDs of bulks which are going to run at the same time."" is theoretically correct. However, it is not easy for the scheduler to identify which bulks will run at the same time. A simpler solution I can think of is to find all the vertices which are *not* DEPLOYING/RUNNING/FINISHED at that moment,  which indicates they may still want their previous slots,  and set their prior allocation IDs to {{MergingSharedSlotProfileRetriever}}.

WDYT? [~trohrmann][~chesnay];;;","05/Mar/21 15:23;trohrmann;How would the system behave if we lose two slots (a and b) which have been used for bulk_1 and bulk_2. Now with the decreased resources bulk_1 and bulk_2 cannot be run concurrently and we need to use slot b for bulk_1 to make it work. Would this also work with your proposal?;;;","08/Mar/21 05:43;zhuzh;[~trohrmann] do you mean a case like this? 
 - Previously bulk_1 uses 2 slot \{a1, a2\} and bulk_2 uses 2 slots \{b1,b2\} and there are only these 4 slots in JM slot pool. Later slot a1 and slot b1 get lost, bulk_1 and bulk_2 need to restart. And bulk_1 cannot use b2 and it will need to request one more new slot.

If so, I think it is not a problem for streaming jobs because the job is expected to acquire all required slots at the same time sooner or later.
For batch jobs, it is a problem that resource deadlocks can happen if newly slot requirements cannot be fulfilled. However, from what I know, local recovery, in which case PreviousAllocationSlotSelectionStrategy is used, is not expected for batch jobs. So maybe we can let the job use LocationPreferenceSlotSelectionStrategy if it is batch job even if local recover is enabled?;;;","10/Mar/21 08:59;trohrmann;You are right [~zhuzh], for batch jobs this is not required. Then I guess it is ok to do it as you proposed. Let us guard the assumption that we are running a streaming job when doing this, though. This helps to find the place if we should change for which type of jobs local recovery is supported for whatever reason in the future.;;;","10/Mar/21 11:58;zhuzh;Thanks for the confirmation! [~trohrmann]
Agreed to throw an exception if local recovery is enabled for batch jobs.
I will prepare a PR for it a bit later.;;;","16/Apr/21 10:52;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","19/Apr/21 07:52;trohrmann;Zhu Zhu has opened a PR for this issue which waits for review.;;;","25/May/21 09:21;trohrmann;There is a PR available for fixing this problem.;;;","04/Nov/21 12:12;zhuzh;Fixed in master(1.15):
4618b6a6e0584fc9054505035bb6a3b4e951c937
4d9de6c5b826ab482f47f33c8da957ac9b550c32;;;","05/Nov/21 09:24;zhuzh;I would keep this ticket open for now and keep watching it.
If there is no problem reported against the fix after some time, I will back port it to 1.14.;;;","08/Dec/21 10:31;trohrmann;[~zhuzh] shall we do the backport?;;;","08/Dec/21 11:35;zhuzh;I think it's time to do the backport. I'm now doing it. 
I have also updated it in the release 1.14.1 ML.;;;","09/Dec/21 02:45;zhuzh;1.14:
63cf221ca2d963aa1394fb0244a8702b9e8c3835
347becbe43209fb9c65bcc8ae3859a071469e587;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join with Table Function (UDTF) SQL example is incorrect,FLINK-19140,13326048,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,04/Sep/20 09:25,19/Feb/21 07:22,13/Jul/23 08:12,22/Sep/20 14:55,,,,,,1.11.3,1.12.0,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"Section ""*Join with Table Function (UDTF)*"" of [https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html] currently is:
{code:SQL}
SELECT users, tag
FROM Orders, LATERAL TABLE(unnest_udtf(tags)) t AS tag
-- and 
SELECT users, tag
FROM Orders LEFT JOIN LATERAL TABLE(unnest_udtf(tags)) t AS tag ON TRUE
{code}
 
however, it should be:
{code:SQL}
SELECT users, tag
FROM Orders, LATERAL TABLE(unnest_udtf(tags)) AS t(tag)
-- and 
SELECT users, tag
FROM Orders LEFT JOIN LATERAL TABLE(unnest_udtf(tags)) AS t(tag) ON TRUE
{code}",,libenchao,nicholasjiang,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 14:55:06 UTC 2020,,,,,,,,,,"0|z0icd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/20 09:43;twalthr;btw the `AS t(tag)` is optional starting 1.12 due to the new FLIP-65 functions;;;","06/Sep/20 02:49;libenchao;[~twalthr] Thanks for your input, we can fix this for released versions e.t. 1.10/1.11.

And for 1.12(master), we can document both ways w/o `AS t(tag)`, WDYT?;;;","07/Sep/20 07:09;twalthr;Sounds good to me.;;;","17/Sep/20 09:31;nicholasjiang;[~libenchao]Have you already updated the document?;;;","20/Sep/20 10:12;libenchao;[~nicholasjiang] I haven't raised the pr yet.;;;","22/Sep/20 14:55;twalthr;Fixed in 1.12.0: 9258fcb363db8be90add5557156644894928b1a4
Fixed in 1.11.3: 98160583d408cd06c792bb1981843a7f8f4b5a63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDF supports directly specifying input_types as DataTypes.ROW,FLINK-19138,13326012,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,04/Sep/20 02:57,04/Nov/20 01:26,13/Jul/23 08:12,04/Nov/20 01:26,1.11.0,,,,,1.11.3,1.12.0,,API / Python,,,,,0,pull-request-available,,,,,Python UDF supports input_types=DataTypes.ROW ,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 01:26:02 UTC 2020,,,,,,,,,,"0|z0ic54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/20 01:26;dian.fu;Fixed in 
- master via a5bb18a222c35c90390ee7db7ff3a215d0d7dc3a 
- release-1.11 via 4bf1f02c3d626878f41abe20a5c170e1bf0d56de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(Stream)ExecutionEnvironment.execute() should not throw ExecutionException,FLINK-19135,13325956,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,03/Sep/20 16:43,15/Sep/20 10:11,13/Jul/23 08:12,15/Sep/20 10:11,1.10.0,1.11.0,1.12.0,,,1.10.3,1.11.3,1.12.0,API / DataSet,API / DataStream,,,,0,pull-request-available,,,,,"In FLINK-14850 we changed the {{execute()}} method to be basically
{code}
final JobClient jobClient = executeAsync(...);
return jobClient.getJobExecutionResult(userClassloader).get();
{code}

Unfortunately, this means that {{execute()}} now throws an {{ExecutionException}} instead of a {{ProgramInvocationException}} or {{JobExecutionException}} as before. The {{ExecutionException}} is wrapping the other exceptions that we were throwing before.

We didn't notice this in tests because most tests use {{Test(Stream)Environment}} which overrides the {{execute()}} method and so doesn't go through the {{PipelineExecutor}} logic or the normal code path of delegating to {{executeAsync()}}.

We should fix this to go back to the old behaviour.",,aljoscha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19123,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 10:11:47 UTC 2020,,,,,,,,,,"0|z0ibso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/20 14:44;aljoscha;master: faca0e69d90162cc1b24b8f5865f1a8a604f355e;;;","15/Sep/20 09:57;aljoscha;release-1.10: 4749e4c23ab71dcfe7ec6fefd2fa93112f9cb66b;;;","15/Sep/20 10:11;aljoscha;release-1.11: aa15708339c85556a44aea74fefd6ee4a4681ef5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add BasicArrayTypeInfo and coder for PrimitiveArrayTypeInfo for Python DataStream API.,FLINK-19134,13325917,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,csq,csq,03/Sep/20 12:48,19/Feb/21 07:27,13/Jul/23 08:12,25/Sep/20 09:13,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,The proto to coder converter for Array type is mistakenly added a list of element coders.,,csq,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 09:13:42 UTC 2020,,,,,,,,,,"0|z0ibk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/20 09:13;dian.fu;master: 8525c5ccf202ba6e9bb0d05b492d762b0228138c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User provided kafka partitioners are not initialized correctly,FLINK-19133,13325915,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,03/Sep/20 12:32,18/Sep/20 06:48,13/Jul/23 08:12,06/Sep/20 16:19,1.11.1,,,,,1.11.2,1.12.0,,Connectors / Kafka,,,,,1,pull-request-available,,,,,"Reported in the ML: https://lists.apache.org/thread.html/r94275a7314d44154eb1ac16237906e0f097e8a9d8a5a937e8dcb5e85%40%3Cdev.flink.apache.org%3E

If a user provides a partitioner in combination with SerializationSchema it is not initialized correctly and has no access to the parallel instance index or number of parallel instances.",,danny0405,dwysakowicz,kezhuw,kkrugler,libenchao,Paul Lin,zhuqi,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19239,FLINK-19285,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 17 11:20:43 UTC 2020,,,,,,,,,,"0|z0ibjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/20 18:50;kkrugler;Yes, we ran into this exact issue recently with 1.11, where only partition 0 was receiving data. ""Fixed"" it by passing Optional.empty() for the partition, so it would use Kafka partitioning vs. the FlinkFixedPartitioner, but good to see we weren't imagining things.;;;","04/Sep/20 05:57;danny0405;Hi, [~dwysakowicz], may i take this issue ~;;;","04/Sep/20 06:05;dwysakowicz;I have the PR prepared already, was just waiting for a successfull build in my azure.;;;","06/Sep/20 16:19;dwysakowicz;Fixed in:
* master
** 25b54a5e261d8dbc8133d0fdc9cae2e653af1ea7
* 1.11.2
** ee05ef006041f766e083d823b340af79fd4da274;;;","17/Sep/20 09:52;ZhuShang;[~dwysakowicz],does flink sql has this problem?;;;","17/Sep/20 10:09;dwysakowicz;I am afraid yes. If you do specify a partitioner via {{sink.partitioner}} you might face this problem in versions 1.11.0 and 1.11.1.;;;","17/Sep/20 11:20;ZhuShang;[~dwysakowicz],yes you are right,i have just verified flink sql,if `sink.partitioner` is spefified ,the problem appeared.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestStreamEnvironment does not use shared MiniCluster for executeAsync(),FLINK-19123,13325706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,02/Sep/20 09:52,08/Oct/20 07:46,13/Jul/23 08:12,06/Oct/20 07:08,,,,,,1.12.0,,,API / DataStream,Tests,,,,0,pull-request-available,,,,,"TestStreamEnvironment does override {{execute()}} but not {{executeAsync()}} . Now, {{execute()}} goes against the {{MiniCluster}} session that was started by a {{MiniClusterWithClientResource}} or some other method that uses {{TestStreamEnvironment}}. However, {{executeAsync()}} will go through the normal {{StreamExecutionEnvironment}} logic and tries to find an executor, does not know that it is a testing environment.

Up until recently, you would have gotten an exception that tells you that no executor is configured, then we would have found out that we need to override {{executeAsync()}} in {{TestStreamEnvironment}}. However, we currently configure a local executor in the constructor: [https://github.com/apache/flink/blob/2160c3294ef87143ab9a4e8138cb618651499792/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/streaming/util/TestStreamEnvironment.java#L59]. With this, you basically get the “local environment” behaviour when you call {{executeAsync()}}, which starts a cluster for the job and shuts it down when the job finishes. This basically makes the {{TestStreamEnvironment}} cluster sharing useless.",,aljoscha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19264,,,,,,,,,,,,,,,,,,,,,FLINK-19516,,FLINK-19135,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 07:08:23 UTC 2020,,,,,,,,,,"0|z0ia94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/20 07:08;aljoscha;master: f9fc140ada79130d11739fb213a6a1c6f1148140;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid accessing HDFS frequently in HiveBulkWriterFactory,FLINK-19121,13325675,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Sep/20 05:56,19/Feb/21 07:28,13/Jul/23 08:12,18/Sep/20 06:29,1.11.1,1.12.0,,,,1.11.3,1.12.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,"In HadoopPathBasedBulkWriter, getSize will invoke `FileSystem.exists` and `FileSystem.getFileStatus`, but it is invoked per record.

There will be lots of visits to HDFS, may make HDFS pressure too high.",,liyu,lzljs3620320,Paul Lin,Peihui,VictorDinoZhang,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 07:28:14 UTC 2020,,,,,,,,,,"0|z0ia28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/20 07:41;lzljs3620320;master: 41c3a19b235ad1351e9376d2d70101bd8090a4a8

release-1.11: 7d86498e088aac28e9d7c77499e2118370f5cc96;;;","18/Sep/20 06:50;lzljs3620320;The fix did not go into release-1.11.2, fixed in release-1.11.3.;;;","01/Dec/20 07:28;ZhuShang;[~lzljs3620320],If we use flink BulkWriter,we still need to check whether should roll on event.

WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No access to metric group in ScalarFunction when optimizing,FLINK-19112,13325553,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,nkruber,nkruber,01/Sep/20 16:26,09/Sep/20 07:26,13/Jul/23 08:12,09/Sep/20 07:26,1.11.1,,,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"Under some circumstances, I cannot access {{context.getMetricGroup()}} in a {{ScalarFunction}} like this (full job attached):
{code:java}
  public static class MyUDF extends ScalarFunction {
    @Override
    public void open(FunctionContext context) throws Exception {
      super.open(context);
      context.getMetricGroup();
    }

    public Integer eval(Integer id) {
      return id;
    }
  }
{code}
which leads to this exception:
{code:java}
Exception in thread ""main"" java.lang.UnsupportedOperationException: getMetricGroup is not supported when optimizing
	at org.apache.flink.table.planner.codegen.ConstantFunctionContext.getMetricGroup(ExpressionReducer.scala:249)
	at com.ververica.MetricsGroupBug$MyUDF.open(MetricsGroupBug.java:57)
	at ExpressionReducer$2.open(Unknown Source)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:118)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:696)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:618)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:303)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:328)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:562)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:427)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:264)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:223)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:210)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:164)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:84)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1264)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:700)
	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:565)
	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:549)
	at com.ververica.MetricsGroupBug.main(MetricsGroupBug.java:50)
{code}
I also tried to work around this with a try-catch, assuming that this method is called once during optimisation and another time at runtime. However, it seems as if {{open()}} is actually only called once (during optimization) thus giving me no choice to access the metrics group.

It seems that removing the where condition before my UDF call also fixes it but it shouldn't be that way...",,aljoscha,dwysakowicz,jark,klion26,libenchao,nkruber,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/20 16:22;nkruber;MetricsGroupBug.java;https://issues.apache.org/jira/secure/attachment/13010854/MetricsGroupBug.java",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 07:26:30 UTC 2020,,,,,,,,,,"0|z0i9b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/20 16:50;twalthr;Can you check if the problem exists if you set `isDetermistic` to false?;;;","01/Sep/20 16:56;twalthr;I guess the optimizer is smart and calls `function(0)` already during pre-flight phase. So the function will not be called anymore during runtime. Let's improve the documentation again to make this process clearer. I will assign this issue to me.;;;","01/Sep/20 16:57;nkruber;[~twalthr] that seems to work;;;","01/Sep/20 17:01;nkruber;oh, yes, I basically just need to use a field that is non-constant (I actually don't care about this field for my purpose). I definitely need to call the function for every element though (and need to access the metric group). In that case, my function is non-deterministic!

I'm not sure I would have looked into the docs after seeing that the exception explicitly stated this is not supported. Maybe extend the exception message instead?;;;","02/Sep/20 05:57;jark;I think we can have a mock MetricGroup (maybe {{UnregisteredMetricsGroup}}) to avoid this exception?;;;","02/Sep/20 07:18;twalthr;I was thinking about giving a more helpful exception as well. But yes a mock MetricGroup sounds good to me as well. Together with better documentation about ExpressionReduction in the implementation guide;;;","02/Sep/20 07:49;dwysakowicz;Just throwing in another idea.

How about catching all exceptions in the expression reducer. If the UDF throws an exception consider it not reducible.;;;","02/Sep/20 10:27;nkruber;A mock MetricGroup has the disadvantage that the user doesn't immediately see that their optimised function doesn't actually produce any metrics anymore...but accompanied with a WARN in the logs, this should be fine as the function may have been optimised in some incarnations but not in others and both should just work.

Catching the exception and then not optimising sounds compelling at first, but I'm not sure it helps much in the general case: you'd at least have to clean up after the failed {{open()}} call, but then we don't actually know which state the UDF object is in. This may also hide other errors that were not expected / should be fixed (both in user code and framework code).;;;","09/Sep/20 07:26;twalthr;Fixed in 1.12.0: 8a61d10433e26bdd9b044e29445e0f1d14b6df9c

[FLINK-19112][table] Improve usability during constant expression reduction
    - Throw more meaningful exceptions.
    - Make metrics a no-op instead of throwing an exception.
    - Use job parameters instead of entire configuration.
    - Give meaningful exception for getExternalResourceInfos instead of NPE
    - Add more documentation at various locations
    - Skip expression reduction in case of an exception;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split Reader eats chained periodic watermarks,FLINK-19109,13325470,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,alpinegizmo,alpinegizmo,01/Sep/20 07:31,22/Jun/21 13:55,13/Jul/23 08:12,16/Sep/20 19:08,1.10.0,1.10.1,1.10.2,1.11.0,1.11.1,1.10.3,1.11.2,1.12.0,Runtime / Task,,,,,0,pull-request-available,,,,,"Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as in
{code:java}
SingleOutputStreamOperator<Event> results = env
  .readTextFile(...)
  .map(...)
  .assignTimestampsAndWatermarks(bounded)
  .keyBy(...)
  .process(...);{code}
leads to the Watermarks failing to be produced. Breaking the chain, via {{disableOperatorChaining()}} or a {{rebalance}}, works around the bug. Using punctuated watermarks also avoids the issue.

Looking at this in the debugger reveals that timer service is being prematurely quiesced.

In many respects this is FLINK-7666 brought back to life.

The problem is not present in 1.9.3.

There's a minimal reproducible example in [https://github.com/alpinegizmo/flink-question-001/tree/bug].",,AHeise,aljoscha,alpinegizmo,kezhuw,klion26,libenchao,nkruber,Paul Lin,pnowojski,roman,zhuzh,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 19:07:23 UTC 2020,,,,,,,,,,"0|z0i8sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/20 16:48;roman;I see that with chaining enabled TimestampsAndWatermarksOperator works as expected - until ContinuousFileReaderOperator starts reading first elements. After that, it schedules a timer which is executed with 1-2 second delay. 

This delay is caused by MailboxProcessor not picking up a mail of an already fired timer (timer services are OK).

This seems reasonable since the priority of operators MailboxExecutor is defined by its chain index.

(the chain is ContinuousFileReaderOperator -> Map -> TimestampsAndWatermarksOperator).

 

[~pnowojski] does it makes sense to you?

Do you have any idea how to fix this?

 

Update:

In 1.10, upon getting END_OF_INPUT (which is received early - after adding all splits to a queue), StreamTask closes all operators. 

ContinuousFileReaderOperator.close calls waitSplitReaderFinished() which doesn't allow Task thread to process any mails. Once it's done, mails are drained and the remaining timer callback is executed.

 

In 1.11, ContinuousFileReaderOperator.close should allow timers to execute and (thanks to FLINK-14231) services shouldn't be closed. But as ContinuousFileReaderOperator -enqueues a higher priority mail-, ignores mails if mailbox loop is stopped, it doesn't help.;;;","02/Sep/20 16:46;roman;I've published a [PR|https://github.com/apache/flink/pull/13305] to fix the issue in 1.12 and 1.11.

For 1.10, the fix would be problematic as it would likely require backporting [changes to ContinuousFileReaderOperator|https://github.com/apache/flink/pull/10435] and [OperatorChain / timers|https://github.com/apache/flink/pull/10151].  Given that there are several workarounds, I think the fix for 1.11 and 1.12 will suffice.;;;","03/Sep/20 14:20;alpinegizmo;I think we should try harder to find a fix for 1.10. 1.10 is still supported, and while there are workarounds this is a difficult problem to diagnose. Not all users will find these workarounds on their own.

I tracked this down after hearing from a user who wasted a day trying to figure out what was going on with an event time streaming app where none of the windows were closing until the end of the job. Getting from there to the actual problem requires a pretty solid understanding of the internals. ;;;","07/Sep/20 06:49;zhuzh;Hi [~roman_khachatryan], do you think we can get the fix merged today? 
I'm asking this because it is the deadline to cut 1.11.2 RC1 later today.;;;","07/Sep/20 07:42;roman;Hi Zhu Zhu,

Yes, it will be merged today (I asked [~zjwang] to do that).;;;","07/Sep/20 07:44;zhuzh;Thanks for the updates! [~roman_khachatryan];;;","07/Sep/20 08:44;zjwang;[~zhuzh][~roman_khachatryan], I will merge it in 1.11 after azure pass, I think it can be done later today.;;;","07/Sep/20 11:29;pnowojski;[~alpinegizmo] those two PRs that the fix depends on are quite big changes (+2000 lines of code), with some follow up bug fixes. Back porting them could mean destabilising the 1.10.x release branch, especially given that our release testing for minor changes is not that good. Furthermore, they are changing behaviour of the system, for example the [ContinuousFileReaderOperator changes|https://github.com/apache/flink/pull/10435] will be affecting performance (hopefully improving), while the other one FLINK-14228 is changing semantic of timers during shutdown.

Is there other way to fix the problem [~roman_khachatryan]?;;;","07/Sep/20 15:00;zjwang;Merged in master: 361ccb3ad734490aa4df9aa9d0430a31680581db
Merged in release-1.11: bae7d4b7d821cc9cca3d17e1f432be22cd7dbf76;;;","07/Sep/20 17:22;roman;Thanks [~zjwang] for merging the PR.

 

Regarding 1.10, I tried to use yield in this draft PR: [https://github.com/apache/flink/pull/13342]

I agree with [~pnowojski] that it can destabilize 1.10: yield() can lead to indefinite wait for a mail, particularly because timer service can be shut down prematurely in 1.10.;;;","10/Sep/20 09:07;aljoscha;I think we definitely need to find a fix for 1.10.x, as David said. This is quite critical and users (and me) can waste quite some time diagnosing what is happening.;;;","10/Sep/20 10:38;pnowojski;As I wrote, I think we can not backport the proper fix. [~roman_khachatryan]'s fix looks like can deadlock. Maybe it could be solved, but it's still hacky and risky. If you insist on fixing it in 1.10, I would suggest to disable chaining for the {{ContinoutFileReaderOperator}}.;;;","10/Sep/20 13:48;alpinegizmo;I like [~pnowojski]'s idea of disabling chaining for the {{ContinousFileReaderOperator}} in 1.10. This strikes me as a pragmatic solution, though it will impact performance for some users who aren't impacted by the bug. Would it make sense to try to optimize this a bit, and only disable chaining when the time characteristic is event time?;;;","10/Sep/20 14:51;arvid;Yes, it's possible to contain the fix to just event time characteristics. I'll prepare a PR soonish.;;;","11/Sep/20 15:49;arvid;Added a PR. However, I'm quite positive that we need to disallow all chaining of {{ProcessingTimeCallback}} after {{ContinousFileReaderOperator}}. For now I only disabled them in event time, but it might be also relevant for processing/ingestion time.;;;","14/Sep/20 18:17;arvid;Since an operator can also register processing time timers without implementing {{ProcessingTimeCallback}}, we need to disable any chaining of CFRO in 1.10.

Best, we could do is to add an {{enableChaining}}/{{forceChaining}} to {{DataStream}} to allow expert users to force the old behavior on their own risk. [~aljoscha], any opinion on this? (Might be something to offer in general);;;","16/Sep/20 19:07;arvid;Merged in release-1.10: 05ff71c813650f65604d960978f93ba82f09a48d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop expanding the identifiers with scope aliased by the system with 'EXPR$' prefix,FLINK-19108,13325468,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,danny0405,danny0405,01/Sep/20 07:18,04/Sep/20 05:53,13/Jul/23 08:12,04/Sep/20 05:53,1.11.2,1.12.0,,,,1.11.2,,,Table SQL / API,,,,,0,pull-request-available,,,,,"For query

{code:sql}
create view tmp_view as
select * from (
  select f0,
  row_number() over (partition by f0 order by f0 desc) as rowNum
  from source) -- the query would be aliased as ""EXPR$1""
  where rowNum = 1
{code}

When validation, the inner query would have alias assigned by the system with prefix ""EXPR$1"", when in the `Expander`, we replace the id in the inner query all with this prefix which is wrong because we do not add the alias to the inner query anymore.

To solve the problem, skip the expanding of id with ""EXPR$"" just like how {{SqlUtil#deriveAliasFromOrdinal}} added it.

This was introduced by FLINK-18750.",,danny0405,jark,leonard,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18750,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 04 05:53:13 UTC 2020,,,,,,,,,,"0|z0i8sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/20 05:53;lzljs3620320;master: fb29aa22e29d62ce0bb58befa969e6e5555b09af

release-1.11: f2cc139d99e2ac08182c7203f63f83168a821c44;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revise the description of watermark strategy in Flink Table document ,FLINK-19094,13325255,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,31/Aug/20 02:58,09/Sep/20 14:12,13/Jul/23 08:12,09/Sep/20 08:21,1.10.0,1.11.0,,,,1.11.2,1.12.0,,Documentation,,,,,0,pull-request-available,,,,,"The description of watermark strategy is wrong in doc[1]
 * Strictly ascending timestamps: {{WATERMARK FOR rowtime_column AS rowtime_column}}.

Emits a watermark of the maximum observed timestamp so far. -Rows that have a timestamp smaller to the max timestamp are not late.-
 * Ascending timestamps: {{WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '0.001' SECOND}}.
 Emits a watermark of the maximum observed timestamp so far minus 1. -Rows that have a timestamp equal and smaller to the max timestamp are not late.-

[1] [https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/create.html]",,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 08:21:58 UTC 2020,,,,,,,,,,"0|z0i7gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/20 08:21;jark;Fixed in
 - master (1.12.0): bc555779d1754d8bc964abf5215fe99ea38cb98d
 - 1.11.3: 56bce6ef4aa6fe7119ea5273e1a1fbb1dcde82ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Elasticsearch (v6.3.1) sink end-to-end test"" failed with ""SubtaskCheckpointCoordinatorImpl was closed without closing asyncCheckpointRunnable 1""",FLINK-19093,13325182,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,dian.fu,dian.fu,30/Aug/20 02:00,02/Sep/20 10:50,13/Jul/23 08:12,02/Sep/20 10:50,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5986&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6

{code}
2020-08-29T22:20:02.3500263Z 2020-08-29 22:20:00,851 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) - asynchronous part of checkpoint 1 could not be completed.
2020-08-29T22:20:02.3501112Z java.lang.IllegalStateException: SubtaskCheckpointCoordinatorImpl was closed without closing asyncCheckpointRunnable 1
2020-08-29T22:20:02.3502049Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:217) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3503280Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.registerAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:371) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3504647Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.lambda$registerConsumer$2(SubtaskCheckpointCoordinatorImpl.java:479) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3505882Z 	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:95) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3506614Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_265]
2020-08-29T22:20:02.3507203Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_265]
2020-08-29T22:20:02.3507685Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]
2020-08-29T22:20:02.3509577Z 2020-08-29 22:20:00,927 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: ca890bc4df19c66146370647d07bf510, jobId: 3522a3e4940d4b3cefc6dc1f22123f4b).
2020-08-29T22:20:02.3511425Z 2020-08-29 22:20:00,939 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 3522a3e4940d4b3cefc6dc1f22123f4b from job leader monitoring.
2020-08-29T22:20:02.3512499Z 2020-08-29 22:20:00,939 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 3522a3e4940d4b3cefc6dc1f22123f4b.
2020-08-29T22:20:02.3513174Z Checking for non-empty .out files...
2020-08-29T22:20:02.3513706Z No non-empty .out files.
2020-08-29T22:20:02.3513878Z 
2020-08-29T22:20:02.3514679Z [FAIL] 'Elasticsearch (v6.3.1) sink end-to-end test' failed after 0 minutes and 37 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
2020-08-29T22:20:02.3515138Z 
{code}",,dian.fu,guoyangze,zhuzh,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19012,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 10:50:11 UTC 2020,,,,,,,,,,"0|z0i70o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/20 02:03;dian.fu;cc [~zjwang] [~roman_khachatryan];;;","31/Aug/20 13:16;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6009&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","01/Sep/20 01:35;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6027&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729;;;","01/Sep/20 03:55;zhuzh;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6011&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=13722;;;","02/Sep/20 10:50;zjwang;Merged in master: 76f37551ad51f7eae9688495130a39b0f202c3bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression 2020-08-27 in globalWindow benchmark,FLINK-19086,13324992,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,28/Aug/20 08:00,07/Sep/20 11:58,13/Jul/23 08:12,07/Sep/20 11:58,1.12.0,,,,,1.12.0,,,Benchmarks,Runtime / Task,,,,0,pull-request-available,,,,,"[http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&env=2]

[http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=tumblingWindow&env=2&revs=200&equid=off&quarts=on&extr=on] 

The results started to decrease 2 days before decomissioning of an old jenkins node.

The other tests, however, were stable.

 

cc: [~pnowojski]",,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19052,,,,,,,,,,,,,,,"28/Aug/20 08:02;roman;Screenshot_2020-08-28_09-58-31.png;https://issues.apache.org/jira/secure/attachment/13010662/Screenshot_2020-08-28_09-58-31.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 07 11:58:09 UTC 2020,,,,,,,,,,"0|z0i5ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/20 15:55;roman;Locally couldn't confirm:
{code:java}
642d4fe48c (Aug 26)
Benchmark                         Mode  Cnt     Score     Error   Units
WindowBenchmarks.globalWindow    thrpt   30  4580.346 ± 215.247  ops/ms
WindowBenchmarks.sessionWindow   thrpt   30   377.308 ±  40.897  ops/ms
WindowBenchmarks.slidingWindow   thrpt   30   448.708 ±  20.665  ops/ms
WindowBenchmarks.tumblingWindow  thrpt   30  3199.008 ±  43.838  ops/ms

e690793c6 (Aug 27)
Benchmark                         Mode  Cnt     Score     Error   Units
WindowBenchmarks.globalWindow    thrpt   30  4814.900 ± 100.372  ops/ms
WindowBenchmarks.sessionWindow   thrpt   30   590.319 ±   7.214  ops/ms
WindowBenchmarks.slidingWindow   thrpt   30   509.285 ±   4.370  ops/ms
WindowBenchmarks.tumblingWindow  thrpt   30  3014.359 ±  60.543  ops/ms
{code}
 ;;;","28/Aug/20 16:31;roman;However, the ""old"" machine still gives better results for older commit than for a newer:
{code:java}
642d4fe48c (Aug 26)
""Benchmark"",""Mode"",""Threads"",""Samples"",""Score"",""Score Error (99.9%)"",""Unit""
""org.apache.flink.benchmark.WindowBenchmarks.globalWindow"",""thrpt"",1,30,7484.147160,418.570329,""ops/ms""
""org.apache.flink.benchmark.WindowBenchmarks.sessionWindow"",""thrpt"",1,30,921.140648,5.433683,""ops/ms""
""org.apache.flink.benchmark.WindowBenchmarks.slidingWindow"",""thrpt"",1,30,791.730208,4.176828,""ops/ms""
""org.apache.flink.benchmark.WindowBenchmarks.tumblingWindow"",""thrpt"",1,30,4539.169233,105.467920,""ops/ms""

278d2a043 (Aug 28)
""org.apache.flink.benchmark.WindowBenchmarks.globalWindow"",""thrpt"",1,30,7117.974175,202.626669,""ops/ms"",,,,
""org.apache.flink.benchmark.WindowBenchmarks.sessionWindow"",""thrpt"",1,30,905.537850,2.796048,""ops/ms"",,,,
""org.apache.flink.benchmark.WindowBenchmarks.slidingWindow"",""thrpt"",1,30,788.335585,5.004561,""ops/ms"",,,,
""org.apache.flink.benchmark.WindowBenchmarks.tumblingWindow"",""thrpt"",1,30,4303.568852,85.755644,""ops/ms"",,,,
{code};;;","31/Aug/20 13:03;roman;new host - confirms the issue:

 

642d4fe48c - ""older"" but faster commit - see [run 214|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/214/]
{code:java}
globalWindow: 6629.419125,229.649936,ops/ms
sessionWindow: 818.793062,9.702122,ops/ms
slidingWindow: 718.801205,5.159397,ops/ms
tumblingWindow: 3755.088518,44.014276,ops/ms
{code}
 

278d2a043 - ""newer"" but slower commit - see [run 215|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/215/]
{code:java}
 
globalWindow: 5762.352564,99.540663,ops/ms
sessionWindow: 813.267524,6.591522,ops/ms
slidingWindow: 699.681922,14.152090,ops/ms
tumblingWindow: 3521.620899,53.002998,ops/ms

{code};;;","31/Aug/20 13:36;pnowojski;Could you bisect the commits to find the culprit? Or analyse it to find a likely candidate?;;;","31/Aug/20 14:18;roman;Sure, that's what I'm doing right now.;;;","02/Sep/20 19:32;roman;Looks like the problem is caused by [162e3e (FLINK-19052 [serialization] add constructor cache in PojoSerializer)|https://github.com/apache/flink/commit/162e3ead63e5766cf2116af09922a88537889e53]  .

I published a [simple PR|https://github.com/apache/flink/pull/13310] to revert it.

 

[~rgrebennikov], [~aljoscha] can you take a look?

(Roman G. maybe you can verify regression/fix in your environment?)

 

cc: [~pnowojski];;;","03/Sep/20 08:28;pnowojski;I'm +1 for trying to revert it. Especially that the I couldn't find the performance improvement of FLINK-19052 on the hetzner machine:
http://codespeed.dak8s.net:8000/timeline/?ben=serializerPojo&env=2;;;","07/Sep/20 11:58;pnowojski;Merged commit reverting FLINK-19052 as 73e0e5c671 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseRowDataInputFormat is leaking resources,FLINK-19064,13324846,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,nicholasjiang,rmetzger,rmetzger,27/Aug/20 13:23,19/Feb/21 07:20,13/Jul/23 08:12,21/Sep/20 02:23,1.12.0,,,,,1.12.0,,,Connectors / HBase,,,,,3,pull-request-available,,,,,"{{HBaseRowDataInputFormat.configure()}} is calling {{connectToTable()}}, which creates a connection to HBase that is not closed again.

A user reported this problem on the user@ list: https://lists.apache.org/thread.html/ra04f6996eb50ee83aabd2ad0d50bec9afb6a924bfbb48ada3269c6d8%40%3Cuser.flink.apache.org%3E",,f.pompermaier,jark,kezhuw,leonard,nicholasjiang,rmetzger,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 02:23:27 UTC 2020,,,,,,,,,,"0|z0i4y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/20 13:56;nicholasjiang;[~rmetzger] HBaseRowDataInputFormat should create HBase connection in method configure() and close the connection in method close(). [~jark], what do you think about this?;;;","27/Aug/20 14:19;rmetzger;The problem is that the {{DataSourceNode.computeOperatorSpecificDefaultEstimates}} method does not call the close method.
I guess the {{HBaseRowDataInputFormat}} should override the open() method, instead of configure, then the Optimizer won't create a new connection, and the close() method is properly called to close the connection again.;;;","27/Aug/20 14:21;jark;[~nicholasjiang] you are right. The created connection should definitely be closed at the end. Are you interested to open a pull request?;;;","27/Aug/20 14:29;jark;What [~rmetzger] said makes sense to me. {{configure()}} is not a recommended method to create connection. According to the Javadoc, we should override the {{openInputFormat()}} to allocate resources and release them in {{closeInputFormat()}}.;;;","27/Aug/20 16:54;ZhuShang;[~jark] [~rmetzger] ,According to the Javadoc,the connection should be created in configure() definitely.

Now the problem is the connection should be closed in closeInputFormat().

i'd like to do this,if it has not be picked by [~nicholasjiang].;;;","28/Aug/20 02:38;jark;Hi [~ZhuShang], why the connection should be created in {{configure()}}?;;;","31/Aug/20 06:12;nicholasjiang;Back to this issue, method DataSourceNode#computeOperatorSpecificDefaultEstimates only calls format#configure, doesn't call RichInputFormat#closeInputFormat. In my opinion, method computeOperatorSpecificDefaultEstimates should close the input format, in another word, which configure the format should be responsible for closing the input format. [~jark], what do you consider about this leaking resources?;;;","31/Aug/20 07:19;jark;From the definition of {{configure()}}, it dosn't open any resources, so calling {{close()}} doesn't make sense to me. 
I think the current method interfaces of {{InputFormat}} is chaotic and I'm not familiar with {{InputFormat}} enough to fix it. Currently, the {{openInputFormat}} doesn't happen before {{createInputSplits}}. One approach will be moving the calling of {{format.openInputFormat()}} from {{InputFormatSourceFunction#run(ctx)}} to {{InputFormatSourceFunction#open(params)}}. However, as I said before, I'm not clear about what's the side effect. We may need to pull in someone familiar with {{InputFormat}}.

After all, I don't think this is a critical bug, as the problem only happens in the legacy {{DataSet}} API which is not maintained for some time and will be removed in the future. Besides, maybe it's time to migrate the HBase source to the new FLIP-27 source to have the clearly defined interfaces. ;;;","02/Sep/20 05:46;nicholasjiang;Yeah, I agree with [~jark]. From my perspective, set aside the issue scene, other scenes need to close the HBase connection in RichInputFormat#closeInputFormat.

And in this issue, the approach above mentioned make sense to me, but this change would affect other cases for other connectors.

In my opinion, there is no perfect solution to solve this leaking resources problem. [~rmetzger], do you have any great approach to handle this case?;;;","02/Sep/20 05:56;rmetzger;Why can't we open the connection in the {{open()}} method, and close it in the {{close()}} method? That should solve the problem for {{InputFormatSourceFunction}} and {{DataSourceNode#computeOperatorSpecificDefaultEstimates}}.;;;","02/Sep/20 07:22;nicholasjiang;[~rmetzger], DataSourceNode#computeOperatorSpecificDefaultEstimates just call configure(), doesn't call close(). And in current behavior, open() is called after createInputSplits() which need use the hbase connection. Therefore, this is why we can't open the connection in the open() method, and close it in the close() method.;;;","02/Sep/20 07:38;rmetzger;In my understanding of the {{InputFormat}} interface, the open() and close() method have a different lifecycle than the createInputSplits() method.

If in the HBase case createInputSplits() needs a connection, then this method needs to establish and close the connection in this method call.
Afaik the createInputSplits() method is called on the JobManager once during initialization.


The open() and close() methods are opened and closed for each split on the TaskManagers.

Thus we can not open the connection once and use it for all purposes. We need to establish a connection on the master, and then on the TaskManagers as well.

DataSourceNode#computeOperatorSpecificDefaultEstimates() is irrelevant for the Hbase format, since the getStatistics() call always returns null.;;;","21/Sep/20 02:23;jark;Fixed in master (1.12.0): a4d037b406185cd5864357b83ddbb823b611de9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog fails to get partition column stats if partition value contains special characters,FLINK-19061,13324796,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,27/Aug/20 09:13,07/Sep/20 06:50,13/Jul/23 08:12,07/Sep/20 06:20,1.11.0,,,,,1.11.2,1.12.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,lirui,lzljs3620320,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 07 06:50:06 UTC 2020,,,,,,,,,,"0|z0i4n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/20 06:24;lzljs3620320;master: f924a665b6f0bff815f87a221495b101b035315e;;;","07/Sep/20 05:53;zhuzh;[~lzljs3620320] I can see the fix is merged into master but not into 1.11 yet. Is there any concern for that?
I'm asking this to understand whether we need to include it in the upcoming bugfix release 1.11.2.;;;","07/Sep/20 06:20;lzljs3620320;Hi [~zhuzh], it is ready to be merged. It can be included in 1.11.2.

release-1.11: 7047eed55f66081048dbb72229141329126fdcda;;;","07/Sep/20 06:50;zhuzh;Thanks for the updates [~lzljs3620320];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset transformations document page error,FLINK-19058,13324765,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Fin-Chan,Fin-Chan,Fin-Chan,27/Aug/20 05:46,27/Aug/20 08:50,13/Jul/23 08:12,27/Aug/20 06:41,1.11.0,,,,,1.12.0,,,API / DataSet,Documentation,,,,0,pull-request-available,,,,,"In OuterJoin with Flat-Join Function 

Java：

public void join(Tuple2<String,String> movie, Rating rating

Collector<Tuple2<String, Integer>> out)

Missing "",""",,dwysakowicz,Fin-Chan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 27 06:41:00 UTC 2020,,,,,,,,,,"0|z0i4g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/20 06:41;dwysakowicz;Fixed in f9a5383f3e001ecf5fbe9499a75b0a82112a9379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doc of MAX_DECIMAL_PRECISION should be DECIMAL,FLINK-19050,13324462,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhuShang,ZhuShang,ZhuShang,25/Aug/20 12:22,13/Apr/21 20:41,13/Jul/23 08:12,28/Aug/20 12:29,1.11.1,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"{code:java}
// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
// https://www.postgresql.org/docs/12/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL
private static final int MAX_DECIMAL_PRECISION = 1000;
private static final int MIN_DECIMAL_PRECISION = 1;
{code}
[https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java#L43]

the doc of decimal precision constants should be DECIMAL not  TIMESTAMP",,libenchao,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 28 12:28:14 UTC 2020,,,,,,,,,,"0|z0i2lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/20 12:28;libenchao;fixed via b5a459c6d11fc7d655e7bcf26a8cea0594a693dd (1.12.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix validation of table functions in projections,FLINK-19049,13324445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,rmetzger,rmetzger,25/Aug/20 11:15,01/Sep/20 14:53,13/Jul/23 08:12,01/Sep/20 13:40,1.12.0,,,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"While working on another change, I realized that the {{FunctionITCase.testInvalidUseOfTableFunction()}} tests throws a NullPointerException during execution.

This error is not visible, because TableEnvironmentImpl.executeInternal() does not wait for the final job status.
It submits the job using the job client ({{JobClient jobClient = execEnv.executeAsync(pipeline);}}), and it doesn't wait for the job to complete before returning a result. 

This is the null pointer that is hidden:
{code}

Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'insert-into_default_catalog.default_database.SinkTable'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1823)
	at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:57)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:681)
	... 34 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:148)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$submitJob$2(PerJobMiniClusterFactory.java:92)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:195)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:182)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:422)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.runtime.collector.WrappingCollector.outputResult(WrappingCollector.java:43)
	at StreamExecCalc$245$TableFunctionResultConverterCollector$243.collect(Unknown Source)
	at org.apache.flink.table.functions.TableFunction.collect(TableFunction.java:201)
	at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase$RowTableFunction.eval(FunctionITCase.java:1024)
	at StreamExecCalc$245.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.pushToOperator(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:603)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:563)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
{code}",,dian.fu,godfreyhe,jark,libenchao,lzljs3620320,nicholasjiang,rmetzger,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 01 14:53:12 UTC 2020,,,,,,,,,,"0|z0i2hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/20 07:34;nicholasjiang;[~rmetzger] Do you mean that it should check the job result of pipeline job after executing JobClient jobClient = execEnv.executeAsync(pipeline)?;;;","26/Aug/20 07:42;rmetzger;Yes, probably it makes sense to call execEnv.execute(), then this call will block until the job has finished.
I'm not sure what the exact desired behavior in this code is. But I believe failures should be reported to the user.;;;","26/Aug/20 08:01;nicholasjiang;[~rmetzger] I thought that the user could get table result async through execEnv.executeAsync(), and check the job status by the user. And should the failures be exposed to the user?;;;","26/Aug/20 08:23;rmetzger;I don't know what the semantics of this part of the code are. I would suggest to ask [~ykt836] or [~twalthr].
In my opinion, if the job fails with an error, the {{executeInternal}} method should fail with an exception.;;;","26/Aug/20 08:29;nicholasjiang;[~rmetzger] The entire definition of executeSql is asynchronous execution. If you use execute, you can only return to TableResult (attach mode) after the job ends, and the getJobClient of TableResult is meaningless.;;;","26/Aug/20 08:35;lzljs3620320;CC: [~godfreyhe];;;","26/Aug/20 09:02;godfreyhe;[~rmetzger] The correct behavior of {{testInvalidUseOfTableFunction}} method is submitting job through {{executeSql}} , then waiting the job finished, last checking the execution result.  Just as [~nicholasjiang] said, {{executeInternal}} method just submits the job and then return a {{TableResult}} which contains a {{JobClient}} instance to associate the submitted job. So the {{executeInternal}} will only throw exceptions when submitting a job. The execution exceptions should be thrown when waiting job finished.;;;","31/Aug/20 09:34;twalthr;The test should use {{execInsertSqlAndWaitResult}}. The NPE exception is valid because the table function is used incorrectly but could be replaced with a nicer exception message. I will open a PR for this.;;;","01/Sep/20 13:40;twalthr;Update: The old exception was a runtime `NullPointerException` that is not covered by just running `execute()` without waiting for job completion. This was a mistake in the test. With this PR we can produce an exception in pre-flight phase. So calling `execInsertSqlAndWaitResult` is not necessary anymore as we fail earlier. Btw we will have a better API for waiting shortly: https://github.com/apache/flink/pull/12688;;;","01/Sep/20 13:40;twalthr;Fixed in 1.12.0: 5193b0f9c03421c177ff685bf6cbae0c6b5e1cb0;;;","01/Sep/20 14:53;rmetzger;Thanks for the fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceOperator is not closing SourceReader,FLINK-19040,13324298,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,24/Aug/20 15:49,25/Aug/20 07:57,13/Jul/23 08:12,25/Aug/20 07:57,1.11.1,1.12.0,,,,1.11.2,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,,{{SourceOperator}} is creating {{SourceReader}} but {{SourceReader}} is never closed.,,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 07:57:14 UTC 2020,,,,,,,,,,"0|z0i1kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/20 07:57;pnowojski;Merged to master as f666b81e75. Merged to release-1.11 as 89ad9ca93c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnParallelRemoteChannel failed because of test timeout,FLINK-19027,13324123,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,dian.fu,dian.fu,23/Aug/20 01:28,22/Jun/21 14:04,13/Jul/23 08:12,11/Dec/20 12:50,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5789&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=ec103906-d047-5b8a-680e-05fc000dfca9]

{code}
2020-08-22T21:13:05.5315459Z [ERROR] shouldPerformUnalignedCheckpointOnParallelRemoteChannel(org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 300.075 s  <<< ERROR!
2020-08-22T21:13:05.5316451Z org.junit.runners.model.TestTimedOutException: test timed out after 300 seconds
2020-08-22T21:13:05.5317432Z 	at sun.misc.Unsafe.park(Native Method)
2020-08-22T21:13:05.5317799Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-08-22T21:13:05.5318247Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-08-22T21:13:05.5318885Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-08-22T21:13:05.5327035Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-08-22T21:13:05.5328114Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-08-22T21:13:05.5328869Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1719)
2020-08-22T21:13:05.5329482Z 	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-08-22T21:13:05.5330138Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)
2020-08-22T21:13:05.5330771Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1681)
2020-08-22T21:13:05.5331351Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:158)
2020-08-22T21:13:05.5332015Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.shouldPerformUnalignedCheckpointOnParallelRemoteChannel(UnalignedCheckpointITCase.java:140)
{code}",,AHeise,dian.fu,mapohl,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20309,,,,,FLINK-17768,FLINK-17315,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 12:50:22 UTC 2020,,,,,,,,,,"0|z0i0i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/20 07:30;rmetzger;[~AHeise] can you take a look at this failure?;;;","07/Sep/20 10:35;arvid;Hi [~rmetzger] , we are currently doing a larger refactoring of unaligned checkpoint and this test. The PR is already in the second round. Given that there was just one reported instance, I'd like to push it back and hope that the PR fixes the underlying issue.;;;","07/Sep/20 10:45;rmetzger;Okay, I see. If there are more reports, we can consider temporarily ignoring the test. But for now I'm fine with waiting.;;;","29/Sep/20 06:39;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc;;;","02/Oct/20 00:36;dian.fu;Another instance on master:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7158&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=5c8e7682-d68f-54d1-16a2-a09310218a49

Found the following exceptions in the log, hope it could help to identify the issue:
{code}
21:38:30,457 [           Map (1/1)] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_0) switched from RUNNING to FAILED.
java.lang.Exception: Failing map @ 4 (0 attempt); last value 391
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.failMapper(UnalignedCheckpointITCase.java:616) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.checkFail(UnalignedCheckpointITCase.java:611) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.map(UnalignedCheckpointITCase.java:605) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.map(UnalignedCheckpointITCase.java:580) ~[test-classes/:?]
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:164) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:179) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:152) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:368) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:185) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
21:38:30,458 [           Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_0).
21:38:30,458 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Map (1/1) cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_0.
21:38:30,459 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_0) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@2535c61e.
java.lang.Exception: Failing map @ 4 (0 attempt); last value 391
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.failMapper(UnalignedCheckpointITCase.java:616) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.checkFail(UnalignedCheckpointITCase.java:611) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.map(UnalignedCheckpointITCase.java:605) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.map(UnalignedCheckpointITCase.java:580) ~[test-classes/:?]
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:164) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:179) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:152) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:368) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:185) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
21:38:30,460 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 0a448493b4782967b150582570326227_0.
21:38:30,460 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 3 tasks should be restarted to recover the failed task 0a448493b4782967b150582570326227_0. 
21:38:30,460 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (cb2b6ee8f94a6516932dfa923bfede3f) switched from state RUNNING to RESTARTING.
21:38:30,460 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0) switched from RUNNING to CANCELING.
21:38:30,461 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from RUNNING to CANCELING.
21:38:30,461 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0).
21:38:30,461 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0) switched from RUNNING to CANCELING.
21:38:30,461 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0).
21:38:30,462 [Channel state writer Source: source (1/1)] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Source: source (1/1) discarding 0 drained requests
21:38:30,462 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0).
21:38:30,462 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from RUNNING to CANCELING.
21:38:30,462 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0).
21:38:30,462 [SourceCoordinator-Source: source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Handling subtask 0 failure of source Source: source.
21:38:30,462 [Source: source (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0) switched from CANCELING to CANCELED.
21:38:30,462 [Source: source (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0).
21:38:30,463 [Channel state writer Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Sink: Unnamed (1/1) discarding 0 drained requests
21:38:30,463 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: source (1/1) cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0.
21:38:30,463 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Last received records [[198]] @ 0 subtask (0 attempt)
21:38:30,463 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_0) switched from CANCELING to CANCELED.
21:38:30,463 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from CANCELING to CANCELED.
21:38:30,463 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0).
21:38:30,463 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Sink: Unnamed (1/1) cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0.
21:38:30,481 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from CANCELING to CANCELED.
21:38:30,569 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (cb2b6ee8f94a6516932dfa923bfede3f) switched from state RESTARTING to RUNNING.
21:38:30,570 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job cb2b6ee8f94a6516932dfa923bfede3f from Checkpoint 4 @ 1601588310432 for cb2b6ee8f94a6516932dfa923bfede3f located at file:/tmp/junit4240320968876241473/junit1530824567618577449/cb2b6ee8f94a6516932dfa923bfede3f/chk-4.
21:38:30,571 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No master state to restore
21:38:30,571 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from CREATED to SCHEDULED.
21:38:30,571 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from CREATED to SCHEDULED.
21:38:30,571 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) switched from CREATED to SCHEDULED.
21:38:30,573 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from SCHEDULED to DEPLOYING.
21:38:30,573 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: source (1/1) (attempt #1) with attempt id cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1 to 64dbfbce-1d29-40cb-b8c7-075b84d49f9c @ localhost (dataPort=-1) with allocation id 6e56c476945fffb3db10c0debda0f1df
21:38:30,573 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from SCHEDULED to DEPLOYING.
21:38:30,573 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Map (1/1) (attempt #1) with attempt id cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1 to 64dbfbce-1d29-40cb-b8c7-075b84d49f9c @ localhost (dataPort=-1) with allocation id 6e56c476945fffb3db10c0debda0f1df
21:38:30,573 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 6e56c476945fffb3db10c0debda0f1df.
21:38:30,574 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) switched from SCHEDULED to DEPLOYING.
21:38:30,574 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Sink: Unnamed (1/1) (attempt #1) with attempt id cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1 to 64dbfbce-1d29-40cb-b8c7-075b84d49f9c @ localhost (dataPort=-1) with allocation id 6e56c476945fffb3db10c0debda0f1df
21:38:30,574 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1), deploy into slot with allocation id 6e56c476945fffb3db10c0debda0f1df.
21:38:30,574 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 6e56c476945fffb3db10c0debda0f1df.
21:38:30,574 [Source: source (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from CREATED to DEPLOYING.
21:38:30,575 [Source: source (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) [DEPLOYING].
21:38:30,575 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1), deploy into slot with allocation id 6e56c476945fffb3db10c0debda0f1df.
21:38:30,575 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 6e56c476945fffb3db10c0debda0f1df.
21:38:30,575 [Source: source (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) [DEPLOYING].
21:38:30,575 [           Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from CREATED to DEPLOYING.
21:38:30,576 [           Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) [DEPLOYING].
21:38:30,576 [           Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) [DEPLOYING].
21:38:30,576 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1), deploy into slot with allocation id 6e56c476945fffb3db10c0debda0f1df.
21:38:30,576 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) switched from CREATED to DEPLOYING.
21:38:30,576 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) [DEPLOYING].
21:38:30,577 [Source: source (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined state backend: File State Backend (checkpoints: 'file:/tmp/junit4240320968876241473/junit1530824567618577449', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
21:38:30,577 [Source: source (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: File State Backend (checkpoints: 'file:/tmp/junit4240320968876241473/junit1530824567618577449', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
21:38:30,577 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) [DEPLOYING].
21:38:30,577 [           Map (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined state backend: File State Backend (checkpoints: 'file:/tmp/junit4240320968876241473/junit1530824567618577449', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
21:38:30,577 [           Map (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: File State Backend (checkpoints: 'file:/tmp/junit4240320968876241473/junit1530824567618577449', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
21:38:30,577 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using job/cluster config to configure application-defined state backend: File State Backend (checkpoints: 'file:/tmp/junit4240320968876241473/junit1530824567618577449', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
21:38:30,577 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Using application-defined state backend: File State Backend (checkpoints: 'file:/tmp/junit4240320968876241473/junit1530824567618577449', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
21:38:30,577 [Source: source (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from DEPLOYING to RUNNING.
21:38:30,578 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: source (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from DEPLOYING to RUNNING.
21:38:30,590 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) switched from DEPLOYING to RUNNING.
21:38:30,590 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_ea632d67b7d595e5b851708ae9ad79d6_0_1) switched from DEPLOYING to RUNNING.
21:38:30,597 [           Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from DEPLOYING to RUNNING.
21:38:30,598 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from DEPLOYING to RUNNING.
21:38:30,602 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Initialized last snapshotted records [[186]] @ 0 subtask (1 attempt)
21:38:30,656 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5 (type=CHECKPOINT) @ 1601588310655 for job cb2b6ee8f94a6516932dfa923bfede3f.
21:38:30,657 [Source: source (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Snapshotted next input 782 @ 0 subtask (? attempt)
21:38:30,833 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Last snapshotted records [[390]] @ 0 subtask (1 attempt)
21:38:30,836 [jobmanager-future-thread-29] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5 for job cb2b6ee8f94a6516932dfa923bfede3f (10230 bytes in 181 ms).
21:38:30,837 [SourceCoordinator-Source: source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 5 as completed for source Source: source.
21:38:30,837 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CHECKPOINT) @ 1601588310837 for job cb2b6ee8f94a6516932dfa923bfede3f.
21:38:30,838 [Source: source (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Snapshotted next input 978 @ 0 subtask (? attempt)
21:38:30,839 [ Sink: Unnamed (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Last snapshotted records [[395]] @ 0 subtask (1 attempt)
21:38:30,839 [           Map (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Could not complete snapshot 6 for operator Map (1/1). Failure reason: Checkpoint was declined.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 6 for operator Map (1/1). Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:216) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:157) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:313) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:941) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:931) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:899) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:97) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.processBarrier(CheckpointBarrierUnaligner.java:107) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandler.processBarrier(AlternatingCheckpointBarrierHandler.java:71) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:129) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:121) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:87) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:301) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:183) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: org.apache.flink.util.SerializedThrowable: Failing snapshotState @ 4 (1 attempt); last value 780
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.failMapper(UnalignedCheckpointITCase.java:616) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.checkFail(UnalignedCheckpointITCase.java:611) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.snapshotState(UnalignedCheckpointITCase.java:630) ~[test-classes/:?]
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:120) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:101) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:187) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 25 more
21:38:30,839 [Channel state writer Map (1/1)] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Map (1/1) discarding 0 drained requests
21:38:30,840 [           Map (1/1)] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Map (1/1) discarding 1 drained requests
21:38:30,840 [           Map (1/1)] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from RUNNING to FAILED.
java.io.IOException: Could not perform checkpoint 6 for operator Map (1/1).
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:912) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:97) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.processBarrier(CheckpointBarrierUnaligner.java:107) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandler.processBarrier(AlternatingCheckpointBarrierHandler.java:71) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:129) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:121) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:87) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:301) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:183) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 6 for operator Map (1/1). Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:216) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:157) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:313) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:941) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:931) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:899) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 15 more
Caused by: org.apache.flink.util.SerializedThrowable: Failing snapshotState @ 4 (1 attempt); last value 780
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.failMapper(UnalignedCheckpointITCase.java:616) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.checkFail(UnalignedCheckpointITCase.java:611) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.snapshotState(UnalignedCheckpointITCase.java:630) ~[test-classes/:?]
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:120) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:101) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:187) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:157) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:313) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:941) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:931) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:899) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 15 more
21:38:30,840 [           Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1).
21:38:30,841 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Map (1/1) cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1.
21:38:30,872 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/1) (cb2b6ee8f94a6516932dfa923bfede3f_0a448493b4782967b150582570326227_0_1) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@6e989a5d.
java.io.IOException: Could not perform checkpoint 6 for operator Map (1/1).
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:912) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:97) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.processBarrier(CheckpointBarrierUnaligner.java:107) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandler.processBarrier(AlternatingCheckpointBarrierHandler.java:71) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:129) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:121) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:87) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:301) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:183) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242]
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 6 for operator Map (1/1). Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:216) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:157) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:313) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:941) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:931) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:899) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 15 more
Caused by: org.apache.flink.util.SerializedThrowable: Failing snapshotState @ 4 (1 attempt); last value 780
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.failMapper(UnalignedCheckpointITCase.java:616) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.checkFail(UnalignedCheckpointITCase.java:611) ~[test-classes/:?]
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase$FailingMapper.snapshotState(UnalignedCheckpointITCase.java:630) ~[test-classes/:?]
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:120) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:101) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:187) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:157) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:313) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:603) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:529) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:496) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:266) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:941) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:931) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:899) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 15 more
{code};;;","02/Oct/20 05:57;arvid;Thanks, [~dian.fu] , I'm digging in.;;;","02/Oct/20 07:17;aljoscha;Another instance: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=323&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a&l=3860;;;","02/Oct/20 11:23;arvid;I found the cause: the checkpoint is timing out for yet unknown reason after redeployment causing a 10 min delay. The test has a timeout of 5 min though.

 
{noformat}
13521 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 12 (type=CHECKPOINT) @ 1601636334880 for job 0b6b6a79f7a331282e512f87292c48ea.
13524 [Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Map (1/1) (0b6b6a79f7a331282e512f87292c48ea_0a448493b4782967b150582570326227_0_4) [DEPLOYING].
13526 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Sink: Unnamed (1/1) (0b6b6a79f7a331282e512f87292c48ea_ea632d67b7d595e5b851708ae9ad79d6_0_4), deploy into slot with allocation id c56bdbebf5db9193d896037c313d53ed.
13526 [Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - Sink: Unnamed (1/1) (0b6b6a79f7a331282e512f87292c48ea_ea632d67b7d595e5b851708ae9ad79d6_0_4) switched from CREATED to DEPLOYING.
13527 [Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Sink: Unnamed (1/1) (0b6b6a79f7a331282e512f87292c48ea_ea632d67b7d595e5b851708ae9ad79d6_0_4) [DEPLOYING].
13527 [Map (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using job/cluster config to configure application-defined state backend: File State Backend (checkpoints: 'file:/var/folders/dm/pnwfg9352vsft8vp3n743mmc0000gn/T/junit6732308533903563238/junit620424462495595133', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
13527 [Map (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using application-defined state backend: File State Backend (checkpoints: 'file:/var/folders/dm/pnwfg9352vsft8vp3n743mmc0000gn/T/junit6732308533903563238/junit620424462495595133', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
13528 [Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Sink: Unnamed (1/1) (0b6b6a79f7a331282e512f87292c48ea_ea632d67b7d595e5b851708ae9ad79d6_0_4) [DEPLOYING].
13528 [Map (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - Map (1/1) (0b6b6a79f7a331282e512f87292c48ea_0a448493b4782967b150582570326227_0_4) switched from DEPLOYING to RUNNING.
13529 [Source: source (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Snapshotted next input 1757 @ 0 subtask (? attempt)
13529 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Map (1/1) (0b6b6a79f7a331282e512f87292c48ea_0a448493b4782967b150582570326227_0_4) switched from DEPLOYING to RUNNING.
13530 [Sink: Unnamed (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using job/cluster config to configure application-defined state backend: File State Backend (checkpoints: 'file:/var/folders/dm/pnwfg9352vsft8vp3n743mmc0000gn/T/junit6732308533903563238/junit620424462495595133', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
13530 [Sink: Unnamed (1/1)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using application-defined state backend: File State Backend (checkpoints: 'file:/var/folders/dm/pnwfg9352vsft8vp3n743mmc0000gn/T/junit6732308533903563238/junit620424462495595133', savepoints: 'null', asynchronous: TRUE, fileStateThreshold: 20480)
13531 [Sink: Unnamed (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - Sink: Unnamed (1/1) (0b6b6a79f7a331282e512f87292c48ea_ea632d67b7d595e5b851708ae9ad79d6_0_4) switched from DEPLOYING to RUNNING.
13532 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Sink: Unnamed (1/1) (0b6b6a79f7a331282e512f87292c48ea_ea632d67b7d595e5b851708ae9ad79d6_0_4) switched from DEPLOYING to RUNNING.
13542 [Sink: Unnamed (1/1)] INFO  org.apache.flink.test.checkpointing.UnalignedCheckpointITCase [] - Initialized last snapshotted records [[130]] @ 0 subtask (4 attempt)
613532 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Checkpoint 12 of job 0b6b6a79f7a331282e512f87292c48ea expired before completing.{noformat}
I'm adding a quick fix to lower the checkpoint timeout.;;;","02/Oct/20 11:36;aljoscha;Another instance: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=326&view=logs&j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&t=944c7023-8984-5aa2-b5f8-54922bd90d3a&l=4046;;;","03/Oct/20 00:49;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7174&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185;;;","03/Oct/20 06:21;arvid;Merged the quick fix for the test as 713d02ef5cc5c668ecaef700257c893201080657.;;;","06/Oct/20 11:25;arvid;Merged proper fix as 61a997364b020b44bd26df76208e76106c6390b5 into master.

 ;;;","06/Oct/20 13:20;arvid;I don't see any failures on release-1.11, probably because it's using the tight memory management that master has for the test. Closing.;;;","24/Nov/20 01:40;dian.fu;Another instance on master: 
 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9981&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56]

{code}
2020-11-23T23:16:59.1558291Z [ERROR] execute[Parallel cogroup, p = 10](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase) Time elapsed: 300.067 s <<< ERROR! 2020-11-23T23:16:59.1559082Z org.junit.runners.model.TestTimedOutException: test timed out after 300 seconds 2020-11-23T23:16:59.1559528Z at sun.misc.Unsafe.park(Native Method) 2020-11-23T23:16:59.1561934Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) 2020-11-23T23:16:59.1562838Z at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707) 2020-11-23T23:16:59.1563864Z at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323) 2020-11-23T23:16:59.1564698Z at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742) 2020-11-23T23:16:59.1565389Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) 2020-11-23T23:16:59.1566148Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839) 2020-11-23T23:16:59.1567016Z at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:70) 2020-11-23T23:16:59.1567878Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1819) 2020-11-23T23:16:59.1568732Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1801) 2020-11-23T23:16:59.1569601Z at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:121) 2020-11-23T23:16:59.1570497Z at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:153)
{code};;;","25/Nov/20 06:54;arvid;I tracked down the issue to be related by duplicate splits, such that some sources never finish while waiting for checkpoints to complete but some sources already finished, so no new checkpoint would ever be created.;;;","25/Nov/20 07:17;arvid;Same underlying issue as FLINK-20309, so tracking progress there.;;;","11/Dec/20 08:15;mapohl;[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=137&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87] failed caused by timeouts again:
{code:java}
 	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:72)
	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:756)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_87#1921737344]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
	... 1 more
{code}
It included [af46ae2|https://github.com/apache/flink/commit/af46ae294dc0298b9f0f3890e0dac5681d54d287] which fixed FLINK-20309.

I'm going to reopen the issue.;;;","11/Dec/20 12:50;mapohl;Closing the ticket again as this seems to be related to what's covered by FLINK-20521.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AkkaRpcActor failed to start but no exception information,FLINK-19022,13323964,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tartarus,tartarus,tartarus,21/Aug/20 14:00,20/Oct/20 21:03,13/Jul/23 08:12,20/Oct/20 21:03,1.10.0,1.11.1,1.12.0,,,1.10.3,1.11.3,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"My job appeared that JM could not start normally, and the JM container was finally killed by RM.

In the end, I found through debug that AkkaRpcActor failed to start because the version of yarn in my job was incompatible with the version in the cluster.

[AkkaRpcActor exception handling|https://github.com/apache/flink/blob/478c9657fe1240acdc1eb08ad32ea93e08b0cd5e/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActor.java#L550]

I add log printing here，and then found the specific problem.
{code:java}
2020-08-21 21:31:16,985 ERROR org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState [flink-akka.actor.default-dispatcher-4]  - Could not start RpcEndpoint resourcemanager.
java.lang.NoSuchMethodError: org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster(Lcom/google/protobuf/RpcController;Lorg/apache/hadoop/yarn/proto/YarnServiceProtos$RegisterApplicationMasterRequestProto;)Lorg/apache/hadoop/yarn/proto/YarnServiceProtos$RegisterApplicationMasterResponseProto;
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:106)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy25.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:222)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:214)
	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl.registerApplicationMaster(AMRMClientAsyncImpl.java:138)
	at org.apache.flink.yarn.YarnResourceManager.createAndStartResourceManagerClient(YarnResourceManager.java:229)
	at org.apache.flink.yarn.YarnResourceManager.initialize(YarnResourceManager.java:262)
	at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:204)
	at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:192)
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:185)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:544)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:169)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107){code}
Should we add logs here to help find problems?

 
 [ |http://dict.youdao.com/search?q=http&keyfrom=chrome.extension]",,tartarus,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 21:03:59 UTC 2020,,,,,,,,,,"0|z0hzio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/20 14:04;tartarus;[~chesnay]  [~trohrmann]  How about adding log printing here to help quickly find the problem?

Please assign to me, thanks;;;","24/Aug/20 07:58;trohrmann;Thanks for reporting this issue [~tartarus]. I agree that this is a problem. Could you share the logs with us? I would like to learn why the RM is shutting down the container eventually.

Concerning the problem: One problem is that https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L212 only catches {{Exception}} instead of {{Throwable}}. The same actually applies to https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L248.

The other problem is as you've mentioned that the {{AkkaRpcActor}} does not log the failure cause in case of failed start or shut down attempt. I think it would be a good improvement to add the logs in these places.

Last but not least, I believe that the {{DispatcherResourceManagerComponent}} should also react if either of the {{Dispatcher}} or {{ResourceManager}} component failed during the start up. One way to do it, is to combine the termination futures {{Dispatcher.getTerminationFuture}} and {{ResourceManager.getTerminationFuture}} into the {{shutDownFuture}} of the {{DispatcherResourceManagerComponent}}.;;;","24/Aug/20 10:00;tartarus;[~trohrmann] thanks for your reply.

Sorry, my description is not very clear.

RM shutting down the container because the failed happen on {{AMRMClientAsyncImpl#registerApplicationMaster, so JM not registed to RM yet.}}

[https://github.com/apache/flink/blob/b4705edc841a8cf380d9a12d71551a4d38ec9e31/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java#L223] 

beacause 

[https://github.com/apache/flink/blob/b4705edc841a8cf380d9a12d71551a4d38ec9e31/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java#L280]

here only catch {{Exception}} but my case is a {{Error}} ,  {{NoSuchMethodError}} 

From the current code logic, only {{AkkaRpcActor.StoppedState#start}} catch the throwable, but not print log, so we miss the error message.;;;","24/Aug/20 10:18;tartarus;[~trohrmann] do your means is add {{TerminationFuture}} for {{ResourceManager}} and register to {{DispatcherResourceManagerComponent}} like this
{code:java}
private void registerShutDownFuture() {
   FutureUtils.forward(dispatcherRunner.getShutDownFuture(), shutDownFuture);
}
{code}
 ;;;","24/Aug/20 12:27;trohrmann;Well, the idea was that the {{DispatcherResourceManagerComponent}} must somehow react if one of the component shuts unexpectedly down. For example, one could monitor the termination futures of the {{ResourceManager}} and the {{Dispatcher}} and call a yet to be passed in {{FatalErrorHandler}} if they terminate while the {{DispatcherResourceManagerComponent}} is still {{isRunning}}.;;;","24/Aug/20 13:42;tartarus;[~trohrmann]  I agree with you.

If we do, then 3 questions need to be confirmed:

1) We may need to catch {{Throwable in }}{{ResourceManager}} and {{Dispatcher}};

2)We call FatalErrorHandler directly in ResourceManager and Dispatcher or  terminate through DispatcherResourceManagerComponent;

3)If terminate through DispatcherResourceManagerComponent, we need register both {{TerminationFuture}}  to DispatcherResourceManagerComponent;

Is there anything else that needs attention?;;;","25/Aug/20 06:40;trohrmann;Yes, for 3) we also need to pass a {{FatalErrorHandler}} to the {{DispatcherResourceManagerComponent}}.

And I would say that we add 4) logging to the {{AkkaRpcActor.StartedState.terminate}} and {{AkkaRpcActor.StoppedState.start}}.;;;","26/Aug/20 04:14;tartarus;[~trohrmann] I want to confirm the little details.

we pass a {{FatalErrorHandler}} to the {{DispatcherResourceManagerComponent}}, and then we need to remove {{FatalErrorHandler}} from {{ResourceManager}} and {{Dispatcher}}?

Just register the {{TerminationFuture}} of {{ResourceManager}} and {{Dispatcher}} to {{DispatcherResourceManagerComponent}}.;;;","26/Aug/20 07:09;trohrmann;No, we don't remove the {{FatalErrorHandler}} from the {{Dispatcher}} and the {{ResourceManager}}.

In the {{DispatcherResourceManagerComponent}} we should react to the termination future of the {{Dispatcher}} and {{ResourceManager}} in such a way that we call the fatal error handler if either of them completes while the {{DispatcherResourceManagerComponent}} is still running (not closing).;;;","26/Aug/20 12:15;tartarus;ok，I will try to complete this issue as soon as possible;;;","30/Aug/20 12:37;tartarus;[~trohrmann] hello, I have one questions need to be confirmed when I do this work:

1) {{ResourceManager.getTerminationFuture}} has been implemented in {{RpcEndpoint}};
{code:java}
/**
 * Return a future which is completed with true when the rpc endpoint has been terminated.
 * In case of a failure, this future is completed with the occurring exception.
 *
 * @return Future which is completed when the rpc endpoint has been terminated.
 */
public CompletableFuture<Void> getTerminationFuture() {
	return rpcServer.getTerminationFuture();
}
{code}
how about we add a {{terminationFuture}} to {{ResourceManager}} and register current {{ResourceManager.getTerminationFuture}} with {{terminationFuture}}?
and then if  {{ResourceManager}} started fail we call completeExceptionally to complete {{terminationFuture}}.
at last we register this {{terminationFuture}} with {{DispatcherResourceManagerComponent}}'s {{shutDownFuture}}.

;;;","31/Aug/20 06:59;trohrmann;[~tartarus] this is what the underlying {{AkkaRpcActor}} should already do for us. So there should be no need to add another {{terminationFuture}}.;;;","02/Sep/20 11:02;tartarus;[~trohrmann] sorry , There is another question to confirm.
if we catch Throwable in {{ResourceManager}} and {{Dispatcher}}, then they will call {{fatalErrorHandler.onFatalError(throwable);}}
this method will call {{ClusterEntrypoint.onFatalError()}}, then it is necessary to register {{terminationFuture}} with {{DispatcherResourceManagerComponent}}?;;;","02/Sep/20 14:53;trohrmann;I think we should do both things in order to be on the safe side.;;;","20/Oct/20 21:03;trohrmann;Fixed via

1.12.0:
806b9f332efe86926d26861e64b528e4c6ce1cd6
9e31510df66b508e28ab19ff8cf024c02ec423b1
11a4232a9218841c2b01bc3a53cadfd318c59813
b813c99afa0fb1130af0a48c4879c9c3f0d6dfb4
0ff7d7a6ddf28237797ff1c9ed6fc525a9245d90

1.11.3:
027b143aa7228099227e01b1d08167d20183c86b
4fc6afa95a395b5c75cd783f9dd2f533bab6f931
58942fe6a3abe5ba167c167ecad464764dce6d36
9a0281b70ad1246ee532b11b602f042ff432bd59
e501de03ba8311811a4a613a1f862e87e5becbc6

1.10.3:
26344b2aa173c6ef70da5dc18f16525d99ad8a98
e0b4de2bd1bfe9e5129750b4431da6dce26f516d
2c6bef2639d8cd188e44c915f9d0f4a4fc4f70a2
4ffc99ce07fb4be57ee4f5ff291c6b5780df729d
9fe6c782ab92d77b4d8f3ee44519db22806449c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"E2E test fails with ""Cannot register Closeable, this subtaskCheckpointCoordinator is already closed. Closing argument.""",FLINK-19012,13323902,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,rmetzger,rmetzger,21/Aug/20 07:41,03/Sep/20 13:26,13/Jul/23 08:12,03/Sep/20 13:26,1.12.0,,,,,1.12.0,,,Runtime / Checkpointing,Runtime / Task,Tests,,,0,pull-request-available,test-stability,,,,"Note: This error occurred in a custom branch with unreviewed changes. I don't believe my changes affect this error, but I would keep this in mind when investigating the error: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8307&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d
 
{code}
2020-08-20T20:55:30.2400645Z 2020-08-20 20:55:22,373 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
2020-08-20T20:55:30.2402392Z 2020-08-20 20:55:22,401 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-08-20T20:55:30.2404297Z 2020-08-20 20:55:22,413 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to RUNNING.
2020-08-20T20:55:30.2405805Z 2020-08-20 20:55:22,786 INFO  org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge [] - Pinging Elasticsearch cluster via hosts [http://127.0.0.1:9200] ...
2020-08-20T20:55:30.2407027Z 2020-08-20 20:55:22,848 INFO  org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge [] - Elasticsearch RestHighLevelClient is connected to [http://127.0.0.1:9200]
2020-08-20T20:55:30.2409277Z 2020-08-20 20:55:29,205 INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) discarding 0 drained requests
2020-08-20T20:55:30.2410690Z 2020-08-20 20:55:29,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FINISHED.
2020-08-20T20:55:30.2412187Z 2020-08-20 20:55:29,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0).
2020-08-20T20:55:30.2414203Z 2020-08-20 20:55:29,224 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) cbc357ccb763df2852fee8c4fc7d55f2_0_0.
2020-08-20T20:55:30.2415602Z 2020-08-20 20:55:29,219 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) - asynchronous part of checkpoint 1 could not be completed.
2020-08-20T20:55:30.2416411Z java.io.UncheckedIOException: java.io.IOException: Cannot register Closeable, this subtaskCheckpointCoordinator is already closed. Closing argument.
2020-08-20T20:55:30.2418956Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.lambda$registerConsumer$2(SubtaskCheckpointCoordinatorImpl.java:468) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2420100Z 	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:91) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2420927Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_265]
2020-08-20T20:55:30.2421455Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_265]
2020-08-20T20:55:30.2421879Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]
2020-08-20T20:55:30.2422348Z Caused by: java.io.IOException: Cannot register Closeable, this subtaskCheckpointCoordinator is already closed. Closing argument.
2020-08-20T20:55:30.2423416Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.registerAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:378) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2424635Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.lambda$registerConsumer$2(SubtaskCheckpointCoordinatorImpl.java:466) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2425174Z 	... 4 more
2020-08-20T20:55:30.2426945Z 2020-08-20 20:55:29,339 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: f5938e3baed0f9564aa17169f68947bd, jobId: 333b1654bd93574471bd44ad45847379).
2020-08-20T20:55:30.2428701Z 2020-08-20 20:55:29,354 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 333b1654bd93574471bd44ad45847379 from job leader monitoring.
{code}",,dian.fu,gaoyunhaii,mapohl,pnowojski,rmetzger,roman,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19093,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 03 13:26:35 UTC 2020,,,,,,,,,,"0|z0hz4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/20 02:43;dian.fu;Another instance on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5780&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b;;;","25/Aug/20 01:51;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5834&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529];;;","25/Aug/20 02:04;dian.fu;Upgrade to ""Critical"" as this issue seems not occur accidentally and it has occurred several times these days.;;;","25/Aug/20 10:58;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5845&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","27/Aug/20 18:26;roman;I think the problem is that AsyncCheckpointRunnable throws an exception when it sees that SubtaskCheckpointCoordinator is closed. 

Upon close, SubtaskCheckpointCoordinator closes its runnables but doesn't stop their threads. This also changes their statuses.

So AsyncCheckpointRunnable should check its status before throwing an exception.

 

The tests started to fail after increasing the log level in FLINK-18962.;;;","28/Aug/20 07:26;gaoyunhaii;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5947&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529];;;","28/Aug/20 08:52;zjwang;Merged in master : 378115ffd1642c782c7d0f203c5c46e07d18fc23;;;","03/Sep/20 13:02;mapohl;Hi guys,
the problem still seem to appear: [Azure Pipeline|https://dev.azure.com/mapohl/flink/_build/results?buildId=27&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d]

...even though, 378115ffd1642c782c7d0f203c5c46e07d18fc23 is part of my branch.

I'm gonna open that issue again.;;;","03/Sep/20 13:13;roman;Hi [~mapohl],

thanks for reporting this.

I think it's FLINK-19093 which was recently merged into master. Can you rebase and check again?;;;","03/Sep/20 13:26;mapohl;Thanks for pointing that our. You are right. FLINK-19093 looks more like it's the problem I faced. Its fix was not part of my branch, yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"wrong way to calculate the ""downtime"" metric",FLINK-19009,13323869,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bachelorsu,zncheng,zncheng,21/Aug/20 02:08,13/Apr/21 20:40,13/Jul/23 08:12,27/Aug/20 17:45,1.7.2,1.8.0,,,,1.12.0,,,Runtime / Coordination,Runtime / Metrics,,,30/Sep/20 00:00,0,pull-request-available,,,,,"Currently the way to calculate the Flink system metric ""downtime""  is not consistent with the description in the doc, now the downtime is actually the current timestamp minus the time timestamp when the job started.
   
But Flink doc (https://flink.apache.org/gettinghelp.html) obviously describes the time as the current timestamp minus the timestamp when the job failed.
 
I believe we should update the code this metric as the Flink doc shows. The easy way to solve this is using the current timestamp to minus the latest uptime timestamp.",,lemony,trohrmann,zncheng,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,FLINK-20829,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 27 17:45:44 UTC 2020,,,,,,,,,,"0|z0hyxk:",9223372036854775807,The down time metric is now calculated as described in the documentation.,,,,,,,,,,,,,,,,,,,"25/Aug/20 07:39;chesnay;hmm...it is indeed weird that we use {{System.currentTimeMillis() - runningTimestamp}}; I suppose we should use the timestamp for {{FAILING}}.;;;","25/Aug/20 08:32;lemony;I would like to fix this bug. [~chesnay] Would you mind assigning it to me? 

My solution is using the current system timestamp to minus the latest failing time of the ExecutionGraph except for the following situations:
1. The start time of 'RUNNING' is greater than or equal to the latest 'FAILING' time. That means 'RUNNING' is a later status or the job has never run, and therefore downtime should be 0.
2. Jobs are terminated, so downtime is -1.;;;","27/Aug/20 17:45;trohrmann;Fixed via 05cbf1eaebeba281bf13a4c3bc76f7d05a31ea66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document JDBC drivers as source of Metaspace leaks,FLINK-19005,13323727,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,gestevez,gestevez,20/Aug/20 12:24,17/Sep/20 10:28,13/Jul/23 08:12,17/Sep/20 09:08,1.11.1,,,,,1.12.0,,,Client / Job Submission,Runtime / Configuration,Runtime / Coordination,,,1,pull-request-available,,,,,"Hi !

Im running a 1.11.1 flink cluster, where I execute batch jobs made with DataSet API.

I submit these jobs every day to calculate daily data.

In every execution, cluster's used metaspace increase by 7MB and its never released.

This ends up with an OutOfMemoryError caused by Metaspace every 15 days and i need to restart the cluster to clean the metaspace

taskmanager.memory.jvm-metaspace.size is set to 512mb

Any idea of what could be causing this metaspace grow and why is it not released ?

 

================================================
=== Summary ======================================
================================================

Case 1, reported by [~gestevez]:
* Flink 1.11.1
* Java 11
* Maximum Metaspace size set to 512mb
* Custom Batch job, submitted daily
* Requires restart every 15 days after an OOM

 Case 2, reported by [~Echo Lee]:
* Flink 1.11.0
* Java 11
* G1GC
* WordCount Batch job, submitted every second / every 5 minutes
* eventually fails TaskExecutor with OOM

Case 3, reported by [~DaDaShen]
* Flink 1.11.0
* Java 11
* WordCount Batch job, submitted every 5 seconds
* growing Metaspace, eventually OOM
 ",,Echo Lee,eskabetxe,gestevez,godfreyhe,kezhuw,mapohl,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16408,,,,,,,,,,,"21/Aug/20 10:52;gestevez;heap_dump_after_10_executions.zip;https://issues.apache.org/jira/secure/attachment/13010207/heap_dump_after_10_executions.zip","21/Aug/20 12:44;gestevez;heap_dump_after_1_execution.zip;https://issues.apache.org/jira/secure/attachment/13010216/heap_dump_after_1_execution.zip","25/Aug/20 09:26;Echo Lee;heap_dump_echo_lee.tar.xz;https://issues.apache.org/jira/secure/attachment/13010453/heap_dump_echo_lee.tar.xz","01/Sep/20 13:17;gestevez;heapdump-after_metaspace_dropped.hprof;https://issues.apache.org/jira/secure/attachment/13010840/heapdump-after_metaspace_dropped.hprof","28/Aug/20 02:45;DaDaShen;modified-jdbc-inputformat.png;https://issues.apache.org/jira/secure/attachment/13010648/modified-jdbc-inputformat.png","28/Aug/20 02:35;DaDaShen;origin-jdbc-inputformat.png;https://issues.apache.org/jira/secure/attachment/13010647/origin-jdbc-inputformat.png",,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 17 10:28:02 UTC 2020,,,,,,,,,,"0|z0hy28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/20 13:53;mapohl;Hi Guillermo,

this looks like a Metaspace leak. The memory used for the Metaspace pool is quite high comparing it to other use cases (e.g. in this [mailing list thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/MaxMetaspace-default-may-be-to-low-td33049.html#a33072]).

One proposal is to create a heap dump to check whether there are classes that are not cleaned up. This could be either ab indication of Metaspace memory leak in the user code or Flink. Would it be possible to analyse the heap dump yourself or provide it for an assisted analysis? In the latter case, I would propose moving the discussion into [User Mailing List|https://flink.apache.org/community.html#mailing-lists].

Best,
Matthias
 ;;;","20/Aug/20 13:56;mapohl;I close this issue. Feel free to move the discussion into the User Mailing List if you need further assistance.;;;","21/Aug/20 08:12;DaDaShen;[~mapohl] Hi Matthias,
I  meet the same problem as [~gestevez] does. To figure out Metaspace leak is not caused by my code, I specifically submit the word count job. And for every times, the Metaspace grows up and never release. No matter how large the Metaspace I configured, the Metaspace OOM will always  occur. ;;;","21/Aug/20 08:20;chesnay;[~DaDaShen] Are you also using 1.11.0? Are you submitting the batch WordCount?;;;","21/Aug/20 08:31;DaDaShen;[~chesnay] Yes, I use flink 1.11.0 with it's batch word count on jdk11 environment.;;;","21/Aug/20 08:33;chesnay;Re-opening this issue while I try to reproduce the metaspace issue.;;;","21/Aug/20 09:10;Echo Lee;[~chesnay] I also encountered this problem, which I raised in the comment of issue [FLINK-16408|https://issues.apache.org/jira/browse/FLINK-16408];;;","21/Aug/20 09:11;chesnay;[~DaDaShen] Does this happen on a local cluster? After how many jobs does it happen for you?;;;","21/Aug/20 09:21;DaDaShen;[~chesnay] I didn't do this on a local cluster. I use a script to submit job for every 5 seconds on standalone cluster, so I don't know how many times execution will trigger the OOM.  it's a long time to occur by default Metaspace configuration. But you can observe the usage of metatspace, you'll find that the space never release and grows up continuously.;;;","21/Aug/20 09:35;gestevez;Its the same problem that I have;;;","21/Aug/20 11:00;gestevez;[~chesnay], [~mapohl], I have attached heap dump files [^heap_dump_after_1_execution.zip] and [^heap_dump_after_10_executions.zip], in case you can analyze it.;;;","21/Aug/20 12:20;mapohl;Hi [~gestevez], thanks for getting back to us. It looks like {{heap_dump_after_1_executions.zip}} contains the {{heap_dump_after_10_execution.zip}}. May you update the {{heap_dump_after_1_executions.zip}} attachment?;;;","21/Aug/20 12:49;gestevez;I have uploaded heap_dump_after_1_executions.zip fixed

Thanks;;;","21/Aug/20 13:07;mapohl;Thanks for the update.
A quick diff shows already that there is a growing number of classes generated through reflection:
{code:bash}
# analysis on the ""after 1 execution"" heap dump
FLINK-19005 grep -o ""class .*</a>"" after_1/out.html | sed -e 's~class ~~g' -e 's~</a>~~g' | cut -d'$' -f1 | sed 's/[0-9]*$//g'| sort | uniq -c | sort -rn | head
 287 jdk.internal.reflect.GeneratedSerializationConstructorAccessor
 150 jdk.internal.reflect.GeneratedMethodAccessor
 136 oracle.jdbc.driver.Redirector
  41 org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache
  37 akka.remote.WireFormats
  37 akka.remote.EndpointManager
  35 org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.cache.LocalCache
  35 akka.remote.RemoteSettings
  33 org.apache.flink.shaded.hadoop2.com.google.common.cache.LocalCache
  31 akka.remote.serialization.MiscMessageSerializer
{code}
{code:bash}
# analysis on the ""after 10 executions"" heap dump
FLINK-19005 grep -o ""class .*</a>"" after_10/out.html | sed -e 's~class ~~g' -e 's~</a>~~g' | cut -d'$' -f1 | sed 's/[0-9]*$//g'| sort | uniq -c | sort -rn | head
 575 jdk.internal.reflect.GeneratedSerializationConstructorAccessor
 223 jdk.internal.reflect.GeneratedMethodAccessor
 136 oracle.jdbc.driver.Redirector
  49 com.sun.proxy.
  41 org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache
  37 akka.remote.WireFormats
  37 akka.remote.EndpointManager
  36 jdk.internal.reflect.GeneratedConstructorAccessor
  35 org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.cache.LocalCache
  35 akka.remote.RemoteSettings
{code};;;","24/Aug/20 06:11;Echo Lee;[~chesnay] Is there any new progress on this issue? ;;;","24/Aug/20 11:58;chesnay;I've added a summary to the Jira description.

[~Echo Lee] [~DaDaShen] [~gestevez] Could you tell us exactly which Java 11 version you are using? (output of {{java -version}})

So far I was not really able to reproduce the issue. I'm submitting the Batch Wordcount example in a loop, such that the next one is submitted once the previous one finishes. I do see the Metaspace going up, but once it gets close to the Metaspace size maximum the GC kicks in. My current run has been going for about an hour and ran roughly 1500 jobs, and I can see several dips in the metaspace usage.

In one instance I did get a TaskExecutor crash of sorts, but increasing the Metaspace by as little as 2mb fixed this issue. I do not consider this a successful reproduction, as I've conducted the tests initially with a very low max size of 40mb, and it is somewhat expected that things may fail when it is configured to such a low value.;;;","24/Aug/20 12:37;chesnay;Small update, the TaskExecutor crashed while things ran in the background after around 2200 jobs. Trying to figure out what exactly happened, the logs don't contain an exception, but maybe it happened in some component hat logs it on DEBUG level.;;;","24/Aug/20 22:23;chesnay;My conclusion is that Flink is not leaking anything, and the errors are due to unfortunate timings or some JDK issue.

I was able to reproduce the issue when submitting jobs in directly after another / with 5 seconds in between, but after increasing the backoff to 1 minute the OOM no longer occurred. The GC states also showed that the Metaspace usage did not continuously increase; the GC created distinct dips that frequently managed to match or even undercut prior dips.

[Stephans comment|https://issues.apache.org/jira/browse/FLINK-16408?focusedCommentId=17180577&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17180577] appears to apply here, at the very least for all the mentioned cases where Wordcounts are frequently run.

As for the original issue by [~gestevez], this looks like a clear case of classloaders being leaked. There are (at least) a bunch of {{oracle.jdbc.driver.BlockSource.ThreadedCachingBlockSource.BlockReleaser}} threads hanging around preventing the garbage collection.
So technically, this is a thread leak inherent to this library or caused by improper usage.;;;","25/Aug/20 09:32;Echo Lee;[~chesnay] Wordcount.jar is not easy to reproduce this issue, but our own application is easy to reproduce. I attached the heap dump. Please help to analyze the reason. Thank you!

The execution environment is as follows:
 * Flink 1.11.0
 * openjdk 11.0.2 (2019-01-15)
 * G1GC
 * Max Metaspace Size 90m
 * Metaspace Size 80m
 * Batch job, submitted every 15 second
 * eventually fails TaskExecutor with OOM

 

The heap dump consists of two files, where first.bin Indicates the heap dump of a job, fourth.bin Indicates the heap dump that executes the job four times.

[^heap_dump_echo_lee.tar.xz];;;","25/Aug/20 09:43;chesnay;[~Echo Lee] similar problem, user classloader is being leaked. Are you using JMX in any way? I see various classes from within the user classloader being referenced from the outside by a {{StandardMBeanIntrospector}}.;;;","25/Aug/20 10:07;Echo Lee;Yes, I turned on JMX in order to monitor the changes of Metaspace through JavaVisual.;;;","25/Aug/20 10:09;chesnay;[~Echo Lee] The JMX thingie is probably fine since it uses a {{WeakHashMap}}, but there is the {{java.sql.DriverManager}} leaking classes. This is relatively well known issue for JDBC.;;;","25/Aug/20 10:53;DaDaShen;[~chesnay] I'm willing to know how you can draw a conclusion that the class leaking is caused by java.sql.DriverManager from the dump files. I'm still no thinking to locate the key problem. 
BTW, I tried several times to using wordcount job to reproduce metaspace OOM. But this time flink was running well and no metaspace OOM occurred, so It was my mistake.;;;","25/Aug/20 11:21;chesnay;[~DaDaShen]
 * load heapdump in ecliplse MAT
 * create histogram
 * group classes by classloader
 * among others we can see several ChildFirstClassLoader objects
 ** these are the user classloaders
 ** because they are still around, something is leaking it
 * select one of these entries, and merge the shortest paths to GC roots
 * there is now one entry for the system classloader
 * drilling down into it we find the {{java.sql.DriverManager}}
 ** the contained registeredDrivers array contains multiple drivers for druid, postgresql and calcite
 * select any of these drivers, use Java Basics -> Class Loader explorer
 * you are now shown a ChildFirstClassLoader

This means that the driver originates from the user classloader, but is referenced from the system classloader. If the reference in the latter is not removed (due to improper cleanup), then the user classloader cannot be garbage collected.;;;","28/Aug/20 02:47;DaDaShen;[~chesnay] 
Thanks for your detailed instruction. 
But I still think there's maybe something wrong in Flink. I find that the JdbcInputFormat & JdbcOutputFormat is key reason cause the Metaspace OOM, because the java.sql.DriverManager doesn't release the reference of the Driver. The DriverManager is loaded by java.internal.ClassLoader but the driver is loaded by ChildFisrtClassLoader, which means the ChildFirstClassLoader can't be garbage collected according analyzation of dump file.  
The following code is used by me to reproduce the issue and  I use org.postgresql.Driver as jdbc Driver.
{code:java}
public static void main(String[] args) throws Exception {
	EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
			.useBlinkPlanner() !origin-jdbc-inputformat.png! 
			.inBatchMode()
			.build();
	TableEnvironment tEnv = TableEnvironment.create(envSettings);

	tEnv.executeSql(
			""CREATE TABLE "" + INPUT_TABLE + ""("" +
					""id BIGINT,"" +
					""timestamp6_col TIMESTAMP(6),"" +
					""timestamp9_col TIMESTAMP(6),"" +
					""time_col TIME,"" +
					""real_col FLOAT,"" +
					""decimal_col DECIMAL(10, 4)"" +
					"") WITH ("" +
					""  'connector.type'='jdbc',"" +
					""  'connector.url'='"" + DB_URL + ""',"" +
					""  'connector.table'='"" + INPUT_TABLE + ""',"" +
					""  'connector.USERNAME'='"" + USERNAME + ""',"" +
					""  'connector.PASSWORD'='"" + PASSWORD + ""'"" +
					"")""
	);

	TableResult tableResult = tEnv.executeSql(""SELECT timestamp6_col, decimal_col FROM "" + INPUT_TABLE);
	tableResult.collect();
}
{code}
And below diagram shows the Metaspace usage constantly growing up, and finally TaskManager will be offline.
 !origin-jdbc-inputformat.png! 

----
Additional, I try to fix this issue by appending the following code to the function closeInputFormat() which can finally trigger garbage collect in Metaspace.

{code:java}
try{
	final Enumeration<Driver> drivers = DriverManager.getDrivers();
	while (drivers.hasMoreElements()) {
		DriverManager.deregisterDriver(drivers.nextElement());
	}
} catch (SQLException se) {
	LOG.info(""Inputformat couldn't be closed - "" + se.getMessage());
}
{code}
The following diagram shows the usage of Metaspace will be decreased.
 !modified-jdbc-inputformat.png! 
So, do you think it's a flink problem, and should we create a new issue to fix.;;;","28/Aug/20 07:20;chesnay;Well of course that works; you clean _all drivers,_ then things can get GC'd.

However, you also mess with any other Task from the same job running in that JVM that requires JDBC at the same time, because all of a sudden you are de-registering drivers, and there is unfortunately no guarantee that they will be re-registered when the class is reloaded (because JDBC is a mess).

Additionally, you are also de-registering all drivers that were loaded by the SystemClassLoader, which presumably should be left alone.;;;","28/Aug/20 07:54;DaDaShen;[~chesnay] 
Looks like it's hard to solve the problem.
Do we any plan or good idea to fix this? Or is there  any other place to discuss the problem?;;;","28/Aug/20 08:36;trohrmann;One thing you can do is to use Flink's per job mode deployment. That way you will create a new cluster for every job and terminate the cluster once the job is done. That way, you won't run into the problem of accumulating class loader leaks.

If the JDBC classes are strictly loaded by the user code class loader, then FLINK-17554 might help by allowing to register shut down hooks for every user code class loader.;;;","28/Aug/20 08:41;chesnay;I don't think anyone has a plan to solve it. I can offer a potential workaround though:

What you could try is putting the jdbc drivers into /lib instead of bundling them in the user-jar, and adding them to the list of [parent-first loaded classes|[https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#classloader-parent-first-patterns-additional].]That should ensure that the driver is only created once, outside the user-classloader.;;;","28/Aug/20 11:08;eskabetxe;[~trohrmann] that can be applied to a standalone cluster? ([~DaDaShen]  and  [~gestevez]  are using standalone cluster)

 It seems that Flink is not clearing the JDBC references, if we only use TableEnvironment or InputFormat we only provide the jdbc library, all iteration with the library is done by Flink, and the cleaning up must be done by Flink as we dont have access to it.

I will help [~gestevez]  to prove the [~chesnay]  option of add it to /lib instead of bundling them in the user-jar..;;;","31/Aug/20 06:47;trohrmann;The per-job mode does also work in standalone mode via using {{standalone-job.sh}}. But it won't stop the started {{TaskManagers}}. Hence it is easier to use it when running on Yarn or K8s.

At the moment I think Chesnay's suggestion to put the dependency into {{/lib}} is the best option to work around the problem. Once we have FLINK-17554, we might also solve this problem in a more convenient way.;;;","01/Sep/20 13:29;gestevez;Thanks [~chesnay], [~trohrmann], 
I tried your solution, changed oracle library dependency on my jobs pom.xml to provided, and now it consumes much less metaspace and creates less classes on every execution, and finally, used metaspace drops some hours after executions finished.

I have uploaded [^heapdump-after_metaspace_dropped.hprof] file, generated after metaspace had dropped, after 6 executions of my batch job.
Could you examine it to check if there is still any memory leak or it is completely solved?;;;","15/Sep/20 12:37;chesnay;[~gestevez]From what I can tell the dump looks OK to me. The ChildFirstClassLoaders are still around, but hold way less references, and will likely be gc'd at a later point. In one of the loaders there are some lingering threads, but given that these are not present in other loaders I'd assume that they will shutdown at a later point.

I will add a note to the classloading-debugging documentation about JDBC.;;;","15/Sep/20 12:50;gestevez;Thanks [~chesnay], the problem seems to be fixed.
Used metaspace goes down as it should, some time after job executions finish.;;;","17/Sep/20 09:08;chesnay;master: 1baabe9db87662db498a58d2685664f832f9bf15;;;","17/Sep/20 09:26;sewen;Is there a way to provide a ""classloader cleanup hook"" (from FLINK-17554) to clear the JDBC registry?;;;","17/Sep/20 09:44;chesnay;[~sewen] Maybe; we can clear the registry as [~DaDaShen] outlined (retrieve all drivers from the DriverManager, then unregister them), but I have [concerns|https://issues.apache.org/jira/browse/FLINK-19005?focusedCommentId=17186325&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17186325] about it.;;;","17/Sep/20 10:21;sewen;Right, cleaning everything would be too much. But would cleaning a specific driver class (the one registered in that specific connector) not be possible?;;;","17/Sep/20 10:26;chesnay;The {{DriverManager}} works on instances, not classes, and from a quick glance I did not see a way to figure out which specific {{Driver}} instance was registered by the connector.;;;","17/Sep/20 10:28;chesnay;hmm...maybe we could iterate over all drivers, and compare the classloader it was loaded by with the user code classloader.;;;",,,,,,,,,,,,,
Temporary generic table doesn't work with HiveCatalog,FLINK-18999,13323527,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,19/Aug/20 09:28,14/Oct/20 03:40,13/Jul/23 08:12,14/Oct/20 03:40,,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Suppose current catalog is a {{HiveCatalog}}. If user creates a temporary generic table, this table cannot be accessed in SQL queries. Will hit exception like:
{noformat}
Caused by: org.apache.hadoop.hive.metastore.api.NoSuchObjectException: DB.TBL table not found
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java:55064) ~[hive-exec-2.3.4.jar:2.3.4]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java:55032) ~[hive-exec-2.3.4.jar:2.3.4]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result.read(ThriftHiveMetastore.java:54963) ~[hive-exec-2.3.4.jar:2.3.4]
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[hive-exec-2.3.4.jar:2.3.4]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table_req(ThriftHiveMetastore.java:1563) ~[hive-exec-2.3.4.jar:2.3.4]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table_req(ThriftHiveMetastore.java:1550) ~[hive-exec-2.3.4.jar:2.3.4]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1344) ~[hive-exec-2.3.4.jar:2.3.4]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169) ~[hive-exec-2.3.4.jar:2.3.4]
        at com.sun.proxy.$Proxy28.getTable(Unknown Source) ~[?:?]
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.getTable(HiveMetastoreClientWrapper.java:112) ~[flink-connector-hive_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.connectors.hive.HiveTableSource.initAllPartitions(HiveTableSource.java:415) ~[flink-connector-hive_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:171) ~[flink-connector-hive_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.PhysicalLegacyTableSourceScan.getSourceTransformation(PhysicalLegacyTableSourceScan.scala:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:98) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlan(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:65) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:65) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1262) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{noformat}",,libenchao,lirui,lsy,lzljs3620320,oikomi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 14 03:40:35 UTC 2020,,,,,,,,,,"0|z0hwts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/20 09:50;lzljs3620320;Maybe reading a temporary table should not use {{TableFactory}} in catalog.;;;","20/Aug/20 03:17;lirui;[~oikomi] Let me know if you still want to work on this, otherwise I'll take it.;;;","20/Aug/20 03:28;oikomi;[~lirui] sorry for the misunderstand, we focus on the other problem, you can take it , thanks;;;","14/Oct/20 03:40;lzljs3620320;master:

24f88069f596673c6f58b136d85d1b00ddf840f2

e6d7f97e63a727566125489e527f1dabb225d91e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some Hive functions fail because they need to access SessionState,FLINK-18995,13323469,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,19/Aug/20 03:49,20/Aug/20 03:23,13/Jul/23 08:12,20/Aug/20 03:23,,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 20 03:23:45 UTC 2020,,,,,,,,,,"0|z0hwgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/20 03:23;lzljs3620320;master: 2fc0899dd3e818b0f60c5d5de2eba9d44b5375a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"There is one typo in ""Set up TaskManager Memory""",FLINK-18994,13323461,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xtsong,wooplevip,wooplevip,19/Aug/20 02:13,19/Aug/20 06:21,13/Jul/23 08:12,19/Aug/20 06:21,1.11.0,,,,,1.11.2,1.12.0,,chinese-translation,,,,,0,pull-request-available,,,,,"From document https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/ops/memory/mem_setup_tm.html, I found one typo as below:
""用于 Flink 应用的算计及用户代码的堆外内存"" should be ""用于 Flink 应用的算子及用户代码的堆外内存"".",,wooplevip,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 06:21:13 UTC 2020,,,,,,,,,,"0|z0hwf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/20 04:01;xtsong;Thanks for reporting this, [~wooplevip].;;;","19/Aug/20 06:21;xtsong;Fixed via
* master: e39ede95134aeccb441141d0db68e32be2da209b
* release-1.11: 84bfa2ced4dff42eeadee2057f6c6192b7eab825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invoke sanityCheckTotalFlinkMemory method incorrectly in JobManagerFlinkMemoryUtils.java,FLINK-18993,13323458,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wooplevip,wooplevip,wooplevip,19/Aug/20 01:56,21/Aug/20 16:51,13/Jul/23 08:12,21/Aug/20 16:51,1.11.0,,,,,1.11.2,1.12.0,,Runtime / Configuration,,,,,0,pull-request-available,,,,,"In deriveFromRequiredFineGrainedOptions method from JobManagerFlinkMemoryUtils.java, it will call sanityCheckTotalFlinkMemory method to check heap, off-heap and total memory as below.
{code:java}
sanityCheckTotalFlinkMemory(totalFlinkMemorySize, jvmHeapMemorySize, totalFlinkMemorySize);
{code}
As I understand it, the third argument should be {color:#de350b}*offHeapMemorySize.*{color}
{code:java}
sanityCheckTotalFlinkMemory(totalFlinkMemorySize, jvmHeapMemorySize, offHeapMemorySize);
{code}
 ",,trohrmann,wooplevip,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 21 16:51:04 UTC 2020,,,,,,,,,,"0|z0hweg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/20 16:51;trohrmann;Fixed via

master: df525b77d29ccd89649a64e5faad96c93f61ca08
1.11.2: 5e9bd610f3f1cb62e00c7baf1f02409c2d017c05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API renameColumns method annotation error,FLINK-18992,13323455,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Fin-Chan,tzxxh,tzxxh,19/Aug/20 01:08,31/Aug/20 09:16,13/Jul/23 08:12,31/Aug/20 09:16,1.11.1,,,,,1.11.2,1.12.0,,Table SQL / API,,,,,0,pull-request-available,,,,,"!image-2020-08-19-09-07-28-148.png!

 

when I use this method

!image-2020-08-19-09-08-30-227.png!",,Fin-Chan,twalthr,tzxxh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/20 01:07;tzxxh;image-2020-08-19-09-07-06-405.png;https://issues.apache.org/jira/secure/attachment/13010063/image-2020-08-19-09-07-06-405.png","19/Aug/20 01:07;tzxxh;image-2020-08-19-09-07-28-148.png;https://issues.apache.org/jira/secure/attachment/13010062/image-2020-08-19-09-07-28-148.png","19/Aug/20 01:08;tzxxh;image-2020-08-19-09-08-30-227.png;https://issues.apache.org/jira/secure/attachment/13010061/image-2020-08-19-09-08-30-227.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 31 09:16:54 UTC 2020,,,,,,,,,,"0|z0hwds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/20 06:27;Fin-Chan;hello，i create the pull request，please assign to me. thx;;;","31/Aug/20 01:12;tzxxh;I am sorry ,I do not konw how to assign to someboday.I do not have the assign button.
;;;","31/Aug/20 09:16;twalthr;Fixed in 1.12.0: 3d290c8980c1513c5fea6cdfd2c5cb10855fcb70
Fixed in 1.11.2: c701ffae759b5b3fd11388a99ef63b62c29ace76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Continuous query with LATERAL and LIMIT produces wrong result,FLINK-18988,13323375,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,fhueske,fhueske,18/Aug/20 15:00,24/Nov/20 14:16,13/Jul/23 08:12,24/Nov/20 14:06,1.11.1,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"I was trying out the example queries provided in this blog post: [https://materialize.io/lateral-joins-and-demand-driven-queries/] to check if Flink supports the same and found that the queries were translated and executed but produced the wrong result.

I used the SQL Client and Kafka (running at kafka:9092) to store the table data. I executed the following statements:
{code:java}
-- create cities table
CREATE TABLE cities (
  name STRING NOT NULL,
  state STRING NOT NULL,
  pop INT NOT NULL
) WITH (
  'connector' = 'kafka',
  'topic' = 'cities',
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.group.id' = 'mygroup', 
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'json'
);

-- fill cities table
INSERT INTO cities VALUES
  ('Los_Angeles', 'CA', 3979576),
  ('Phoenix', 'AZ', 1680992),
  ('Houston', 'TX', 2320268),
  ('San_Diego', 'CA', 1423851),
  ('San_Francisco', 'CA', 881549),
  ('New_York', 'NY', 8336817),
  ('Dallas', 'TX', 1343573),
  ('San_Antonio', 'TX', 1547253),
  ('San_Jose', 'CA', 1021795),
  ('Chicago', 'IL', 2695598),
  ('Austin', 'TX', 978908);

-- execute query
SELECT state, name 
FROM
  (SELECT DISTINCT state FROM cities) states,
  LATERAL (
    SELECT name, pop
    FROM cities
    WHERE state = states.state
    ORDER BY pop
    DESC LIMIT 3
  );

-- result
state                      name
   CA               Los_Angeles
   NY                  New_York
   IL                   Chicago

-- expected result
state | name
------+-------------
TX    | Dallas
AZ    | Phoenix
IL    | Chicago
TX    | Houston
CA    | San_Jose
NY    | New_York
CA    | San_Diego
CA    | Los_Angeles
TX    | San_Antonio

{code}
As you can see from the query result, Flink computes the top3 cities over all states, not for every state individually. Hence, I assume that this is a bug in the query optimizer or one of the rewriting rules.

There are two valid ways to solve this issue:
 * Fixing the rewriting rules / optimizer (obviously preferred)
 * Disabling this feature and throwing an exception",,alpinegizmo,danny0405,fhueske,godfreyhe,jark,knaufk,libenchao,lsy,twalthr,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20323,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 14:06:47 UTC 2020,,,,,,,,,,"0|z0hvw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/20 01:58;danny0405;Hi, [~fhueske], i would like to take this issue ~ I guess you use the Blink planner right ?;;;","19/Aug/20 02:38;jark;We can translate this pattern into StreamExecRank which is already there. I guess Materialize does in the similar way. ;;;","19/Aug/20 02:38;jark;Assigned to you [~danny0405];;;","19/Aug/20 07:30;fhueske;[~danny0405] Yes, I used the Blink planner for this. Thanks for picking up this issue!;;;","01/Oct/20 07:10;knaufk;[~danny0405] Thank you for working on this issue. Are you also planning to fix this for Flink 1.11 or only master?;;;","17/Nov/20 12:34;danny0405;Sorry for the delay reply, i will fix both master and 1.11.;;;","24/Nov/20 14:06;twalthr;Fixed in 1.12.0: a3320d10d8e4bd19131f9eb8da72a3d97fd6876d

[~knaufk] The implementation also introduce an additional planner phase. I would rather not add this to a minor release in 1.11 but only to 1.12. What do you think?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Avro Confluent Schema Registry nightly end-to-end test"" hangs",FLINK-18980,13323134,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,dian.fu,dian.fu,18/Aug/20 01:53,11/Sep/20 17:55,13/Jul/23 08:12,11/Sep/20 17:55,1.11.0,,,,,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5629&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994]

{code}
2020-08-17T22:10:48.3400852Z Job has been submitted with JobID d9a2a95d9204d149165af0e0a1a7c488
2020-08-17T22:10:49.1383170Z [2020-08-17 22:10:49,137] INFO 127.0.0.1 - - [17/Aug/2020:22:10:49 +0000] ""GET /schemas/ids/1 HTTP/1.1"" 200 389  6 (io.confluent.rest-utils.requests:77)
2020-08-17T22:10:51.8854897Z [2020-08-17 22:10:49,256] INFO Wait to catch up until the offset of the last message at 3 (io.confluent.kafka.schemaregistry.storage.KafkaStore:356)
2020-08-17T22:10:51.8856950Z [2020-08-17 22:10:49,775] INFO 127.0.0.1 - - [17/Aug/2020:22:10:49 +0000] ""POST /subjects/test-output-subject/versions HTTP/1.1"" 500 61  524 (io.confluent.rest-utils.requests:77)
2020-08-18T00:20:27.5068516Z ##[error]The operation was canceled.
2020-08-18T00:20:27.5085465Z ##[section]Finishing: Run e2e tests
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 14:43:03 UTC 2020,,,,,,,,,,"0|z0hueg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/20 14:43;rmetzger;Added debugging messages https://github.com/apache/flink/commit/92e2f3b12165cfc0d0c5bc96656e87029ee5e694;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unfulfillable slot requests of Blink planner batch jobs never timeout,FLINK-18972,13323016,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,zhuzh,zhuzh,17/Aug/20 09:20,18/Aug/20 03:15,13/Jul/23 08:12,18/Aug/20 03:15,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The unfulfillability check of batch slot requests are unconditionally disabled in {{SchedulerImpl#start() -> BulkSlotProviderImpl#start()}}.
This means slot allocation timeout will not be triggered if a Blink planner batch job cannot obtain any slot.",,Thesharing,trohrmann,wangm92,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 03:15:09 UTC 2020,,,,,,,,,,"0|z0hto8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/20 03:14;zhuzh;Remove 1.11 from fix versions because {{BulkSlotProviderImpl}} was only added in master(1.12-SNAPSHOT).;;;","18/Aug/20 03:15;zhuzh;Fixed via 2182b8d7541615fc5333e2f96310e269a49d404e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ExecutionContextTest.testCatalogs failed with ""ClassNotFoundException: org.apache.hadoop.fs.BlockStoragePolicySpi""",FLINK-18965,13322842,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,dian.fu,dian.fu,15/Aug/20 02:10,18/Aug/20 06:38,13/Jul/23 08:12,18/Aug/20 06:38,1.12.0,,,,,1.12.0,,,Connectors / Hive,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5541&view=logs&j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&t=46a16c18-c679-5905-432b-9be5d8e27bc6]

{code}
2020-08-14T21:10:09.3503802Z [ERROR] testCatalogs(org.apache.flink.table.client.gateway.local.ExecutionContextTest)  Time elapsed: 0.148 s  <<< ERROR!
2020-08-14T21:10:09.3505006Z org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.
2020-08-14T21:10:09.3505856Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:870)
2020-08-14T21:10:09.3506790Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createExecutionContext(ExecutionContextTest.java:324)
2020-08-14T21:10:09.3508011Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createCatalogExecutionContext(ExecutionContextTest.java:360)
2020-08-14T21:10:09.3509273Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:133)
2020-08-14T21:10:09.3510548Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-08-14T21:10:09.3511496Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-08-14T21:10:09.3512417Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-08-14T21:10:09.3513883Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-08-14T21:10:09.3514563Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-08-14T21:10:09.3515604Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-08-14T21:10:09.3516643Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-08-14T21:10:09.3517498Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-08-14T21:10:09.3518189Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-08-14T21:10:09.3519625Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-08-14T21:10:09.3520621Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-08-14T21:10:09.3521328Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-08-14T21:10:09.3521978Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-08-14T21:10:09.3522787Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-08-14T21:10:09.3523469Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-08-14T21:10:09.3524045Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-08-14T21:10:09.3524652Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-08-14T21:10:09.3525307Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-08-14T21:10:09.3526086Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-08-14T21:10:09.3526996Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-08-14T21:10:09.3527737Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-08-14T21:10:09.3528564Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-08-14T21:10:09.3529381Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-08-14T21:10:09.3530153Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-08-14T21:10:09.3530883Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-08-14T21:10:09.3531641Z Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create Hive Metastore client
2020-08-14T21:10:09.3532669Z 	at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:52)
2020-08-14T21:10:09.3533609Z 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:240)
2020-08-14T21:10:09.3534606Z 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:71)
2020-08-14T21:10:09.3535549Z 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:35)
2020-08-14T21:10:09.3536359Z 	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:224)
2020-08-14T21:10:09.3537715Z 	at org.apache.flink.table.client.gateway.local.DependencyTest$TestHiveCatalogFactory.createCatalog(DependencyTest.java:276)
2020-08-14T21:10:09.3538628Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.createCatalog(ExecutionContext.java:378)
2020-08-14T21:10:09.3539739Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$null$5(ExecutionContext.java:626)
2020-08-14T21:10:09.3540476Z 	at java.util.HashMap.forEach(HashMap.java:1289)
2020-08-14T21:10:09.3541368Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:625)
2020-08-14T21:10:09.3542770Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:264)
2020-08-14T21:10:09.3544224Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:624)
2020-08-14T21:10:09.3545180Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:523)
2020-08-14T21:10:09.3546096Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:183)
2020-08-14T21:10:09.3547025Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:136)
2020-08-14T21:10:09.3547910Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:859)
2020-08-14T21:10:09.3548486Z 	... 28 more
2020-08-14T21:10:09.3548911Z Caused by: java.lang.reflect.InvocationTargetException
2020-08-14T21:10:09.3549464Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-08-14T21:10:09.3550087Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-08-14T21:10:09.3550860Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-08-14T21:10:09.3551523Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-08-14T21:10:09.3552430Z 	at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:50)
2020-08-14T21:10:09.3553356Z 	... 43 more
2020-08-14T21:10:09.3553943Z Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
2020-08-14T21:10:09.3554764Z 	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1708)
2020-08-14T21:10:09.3555577Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
2020-08-14T21:10:09.3556451Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
2020-08-14T21:10:09.3557374Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:89)
2020-08-14T21:10:09.3557975Z 	... 48 more
2020-08-14T21:10:09.3558396Z Caused by: java.lang.reflect.InvocationTargetException
2020-08-14T21:10:09.3558941Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-08-14T21:10:09.3559664Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-08-14T21:10:09.3560509Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-08-14T21:10:09.3561281Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-08-14T21:10:09.3561995Z 	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706)
2020-08-14T21:10:09.3564984Z 	... 51 more
2020-08-14T21:10:09.3565646Z Caused by: MetaException(message:java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi)
2020-08-14T21:10:09.3566467Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:83)
2020-08-14T21:10:09.3567399Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
2020-08-14T21:10:09.3568474Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6891)
2020-08-14T21:10:09.3569795Z 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:164)
2020-08-14T21:10:09.3570366Z 	... 56 more
2020-08-14T21:10:09.3570953Z Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi
2020-08-14T21:10:09.3572567Z 	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:91)
2020-08-14T21:10:09.3573320Z 	at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:480)
2020-08-14T21:10:09.3574237Z 	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:279)
2020-08-14T21:10:09.3574929Z 	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)
2020-08-14T21:10:09.3575654Z 	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)
2020-08-14T21:10:09.3576395Z 	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
2020-08-14T21:10:09.3577207Z 	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
2020-08-14T21:10:09.3578029Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
2020-08-14T21:10:09.3578863Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
2020-08-14T21:10:09.3579708Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
2020-08-14T21:10:09.3580543Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
2020-08-14T21:10:09.3581365Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
2020-08-14T21:10:09.3582048Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-08-14T21:10:09.3582783Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-08-14T21:10:09.3583560Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-08-14T21:10:09.3584236Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-08-14T21:10:09.3584931Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
2020-08-14T21:10:09.3585749Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
2020-08-14T21:10:09.3586543Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
2020-08-14T21:10:09.3587215Z 	... 59 more
2020-08-14T21:10:09.3587710Z Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi
2020-08-14T21:10:09.3588331Z 	at java.lang.ClassLoader.defineClass1(Native Method)
2020-08-14T21:10:09.3588892Z 	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
2020-08-14T21:10:09.3589529Z 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
2020-08-14T21:10:09.3590215Z 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
2020-08-14T21:10:09.3590838Z 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
2020-08-14T21:10:09.3591457Z 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
2020-08-14T21:10:09.3592015Z 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
2020-08-14T21:10:09.3592677Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-08-14T21:10:09.3593215Z 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
2020-08-14T21:10:09.3593778Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
2020-08-14T21:10:09.3594353Z 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
2020-08-14T21:10:09.3594945Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
2020-08-14T21:10:09.3595460Z 	at java.lang.Class.forName0(Native Method)
2020-08-14T21:10:09.3595914Z 	at java.lang.Class.forName(Class.java:348)
2020-08-14T21:10:09.3597121Z 	at org.apache.hadoop.hive.shims.Hadoop23Shims.<init>(Hadoop23Shims.java:113)
2020-08-14T21:10:09.3597790Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-08-14T21:10:09.3598487Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-08-14T21:10:09.3599362Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-08-14T21:10:09.3600126Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-08-14T21:10:09.3600971Z 	at java.lang.Class.newInstance(Class.java:442)
2020-08-14T21:10:09.3601577Z 	at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:130)
2020-08-14T21:10:09.3602593Z 	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124)
2020-08-14T21:10:09.3603611Z 	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:88)
2020-08-14T21:10:09.3604096Z 	... 77 more
2020-08-14T21:10:09.3604616Z Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.BlockStoragePolicySpi
2020-08-14T21:10:09.3605271Z 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
2020-08-14T21:10:09.3605867Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
2020-08-14T21:10:09.3606454Z 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
2020-08-14T21:10:09.3607164Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
2020-08-14T21:10:09.3608030Z 	... 100 more
2020-08-14T21:10:09.3608257Z 
{code}",,dian.fu,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18659,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 06:38:09 UTC 2020,,,,,,,,,,"0|z0hslk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/20 09:54;dian.fu;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5555&view=logs&j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&t=46a16c18-c679-5905-432b-9be5d8e27bc6];;;","16/Aug/20 10:01;dian.fu;Upgrade to ""blocker"" as the nightly ""test_cron_scala212 libraries"" will always fail if this issue is not fixed.;;;","17/Aug/20 01:22;dian.fu;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5570&view=logs&j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&t=46a16c18-c679-5905-432b-9be5d8e27bc6];;;","17/Aug/20 07:05;lirui;The failure is because we have a new {{hadoop-hdfs}} (2.8.3) and an old {{hadoop-common}} (2.7.5) in classpath. Hive tries to load class {{BlockStoragePolicy}} from {{hadoop-hdfs}} which in turn tries to load {{BlockStoragePolicySpi}}. {{BlockStoragePolicySpi}} doesn't exist in the old {{hadoop-common}} and thus the error.

I checked the dependency tree and found {{hadoop-hdfs}} is introduced as a transitive dep of {{orc-core}}. And the version is managed by our dependency management ({{orc-core}} depends on {{hadoop-hdfs-2.2.0}}). That's why only nightly build fails because nightly build specifies {{-Dhadoop.version=2.8.3}}, while by default hadoop version is 2.4.1.

The failure is related to FLINK-18659 which upgraded the orc version of hive connector (it seems previous orc-core doesn't depend on {{hadoop-hdfs}}).;;;","17/Aug/20 07:08;lirui;I don't think sql-client tests need the hadoop-hdfs dependency so I'll try excluding it.;;;","17/Aug/20 07:14;dian.fu;Thanks [~lirui] for the analysis and fixing this issue.;;;","18/Aug/20 06:38;lzljs3620320;master: 0c9a13184883bcfa1822b821b8fc9fa48b131e5b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to archiveExecutionGraph because job is not finished when dispatcher close,FLINK-18959,13322706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiangang,Jiangang,Jiangang,14/Aug/20 08:40,04/Sep/20 07:40,13/Jul/23 08:12,04/Sep/20 07:40,1.10.0,1.11.1,1.12.0,,,1.10.3,1.11.2,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"When job is cancelled, we expect to see it in flink's history server. But I can not see my job after it is cancelled.

After digging into the problem, I find that the function archiveExecutionGraph is not executed. Below is the brief log:
{panel:title=log}
2020-08-14 15:10:06,406 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [flink-akka.actor.default-dispatcher- 15] - Job EtlAndWindow (6f784d4cc5bae88a332d254b21660372) switched from state RUNNING to CANCELLING.

2020-08-14 15:10:06,415 DEBUG org.apache.flink.runtime.dispatcher.MiniDispatcher [flink-akka.actor.default-dispatcher-3] - Shutting down per-job cluster because the job was canceled.

2020-08-14 15:10:06,629 INFO org.apache.flink.runtime.dispatcher.MiniDispatcher [flink-akka.actor.default-dispatcher-3] - Stopping dispatcher akka.tcp://flink@bjfk-c9865.yz02:38663/user/dispatcher.

2020-08-14 15:10:06,629 INFO org.apache.flink.runtime.dispatcher.MiniDispatcher [flink-akka.actor.default-dispatcher-3] - Stopping all currently running jobs of dispatcher akka.tcp://flink@bjfk-c9865.yz02:38663/user/dispatcher.

2020-08-14 15:10:06,631 INFO org.apache.flink.runtime.jobmaster.JobMaster [flink-akka.actor.default-dispatcher-29] - Stopping the JobMaster for job EtlAndWindow(6f784d4cc5bae88a332d254b21660372).

2020-08-14 15:10:06,632 DEBUG org.apache.flink.runtime.jobmaster.JobMaster [flink-akka.actor.default-dispatcher-29] - Disconnect TaskExecutor container_e144_1590060720089_2161_01_000006 because: Stopping JobMaster for job EtlAndWindow(6f784d4cc5bae88a332d254b21660372).

2020-08-14 15:10:06,646 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [flink-akka.actor.default-dispatcher-29] - Job EtlAndWindow (6f784d4cc5bae88a332d254b21660372) switched from state CANCELLING to CANCELED.

2020-08-14 15:10:06,664 DEBUG org.apache.flink.runtime.dispatcher.MiniDispatcher [flink-akka.actor.default-dispatcher-4] - There is a newer JobManagerRunner for the job 6f784d4cc5bae88a332d254b21660372.
{panel}
From the log, we can see that job is not finished when dispatcher closes. The process is as following:
 * Receive cancel command and send it to all tasks async.
 * In MiniDispatcher, begin to shutting down per-job cluster.
 * Stopping dispatcher and remove job.
 * Job is cancelled and callback is executed in method startJobManagerRunner.
 * Because job is removed before, so currentJobManagerRunner is null which not equals to the original jobManagerRunner. In this case, archivedExecutionGraph will not be uploaded.

In normal cases, I find that job is cancelled first and then dispatcher is stopped so that archivedExecutionGraph will succeed. But I think that the order is not constrained and it is hard to know which comes first. 

Above is what I suspected. If so, then we should fix it.

 ",,aljoscha,Jiangang,maguowei,rmetzger,trohrmann,wanglijie,ZhuShang,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15116,,,,,,,,,,,,,,,"14/Aug/20 11:13;Jiangang;flink-debug-log;https://issues.apache.org/jira/secure/attachment/13009856/flink-debug-log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 04 07:40:30 UTC 2020,,,,,,,,,,"0|z0hrrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/20 10:53;rmetzger;How are you submitting and cancelling your Flink job? Can you attach the full log on DEBUG level to this ticket?;;;","14/Aug/20 11:17;Jiangang;Thank you, [~rmetzger] .The job is per-job and is running on yarn. I am using command ""flink cancel"" to stop the job. The log is attached. ;;;","14/Aug/20 11:46;trohrmann;Thanks a lot for reporting this issue [~Jiangang]. I think your analysis is correct. At the moment, the Flink cluster does not wait for the job to complete {{Dispatcher.jobReachedGLoballyTerminalState()}} because it completes directly the {{shutDownFuture}} in {{MiniDispatcher.cancelJob}}. I think this is a regression which has been introduced with FLINK-15116.

[~aljoscha], [~tison] why was it necessary to directly shut down the cluster? Why is it not possible to wait for the normal shutdown cycle after the job has reached a globally terminal state?;;;","16/Aug/20 03:48;Jiangang;Thank you, [~trohrmann]. I have the same confusion with you. In previous implementation, job reaches globally terminal state first and then archiveExecutionGraph is done. Finally, the dispatcher is shut down.;;;","20/Aug/20 11:14;ZhuShang;i also found that archiveExecutionGraph can not be reached when the job is running;;;","21/Aug/20 08:15;Jiangang;Hi, [~trohrmann]. The problem has been raised for some time. We should try to fix it. I suggest not to shut down per-job cluster at once after cancelJob in MiniDispatcher. Instead, we should shut down per-job cluster after jobReachedGloballyTerminalState, no matter executionMode is DETACHED or NORMAL. In this way, archivedExecutionGraph will be uploaded successfully and the semantic will not change. cc [~aljoscha] [~tison];;;","21/Aug/20 12:47;trohrmann;I agree that we should fix this issue [~Jiangang]. The thing I am asking myself is why it has been done differently in the first place because the original behaviour has been like you described it in your solution proposal. That's why I've pulled in Aljoscha and Tison who worked on the change.

What you could try to do is to revert the {{MiniDispatcher}} changes and then see whether some tests fail [~Jiangang].;;;","22/Aug/20 07:37;ZhuShang;hi [~trohrmann], 

FLINK-18685 is related to this issue?when i want to get accumulators from minicluster when the job is running. 
{code:java}
 miniCluster
   .getExecutionGraph(jobID)
   .thenApply(AccessExecutionGraph::getAccumulatorsSerialized)
   .thenApply(accumulators -> {
      try {
         return AccumulatorHelper.deserializeAndUnwrapAccumulators(accumulators, classLoader);
      } catch (Exception e) {
         throw new CompletionException(""Cannot deserialize and unwrap accumulators properly."", e);
      }
   });
{code}
but it dosen't work.;;;","24/Aug/20 08:04;trohrmann;[~ZhuShang] I believe that this is unrelated since the problem of this ticket should only affect the per-job cluster deployments (deployments which use the {{MiniDispatcher}}).;;;","24/Aug/20 10:19;Jiangang;[~trohrmann], thanks for your advice. I just try to revert the change in MiniDispatcher caused by FLINK-15116. Can you assign this ticket to me? I will do my best to fix it.;;;","24/Aug/20 12:08;trohrmann;I've assigned it to you [~Jiangang].;;;","24/Aug/20 12:24;aljoscha;Yes, this is an unintended side effect of FLINK-15116. Before that change, shutting down a per-job cluster was handled by the client, which meant that the cluster could get ""stuck"" in case the client disconnected. With the change, we're eagerly shutting down the cluster when cancelling from ""within the cluster"". It seems we're shutting down too eagerly, though, and we should wait in the cluster until cancel/shutdown went through all the stages (including storing the archived execution graph).;;;","24/Aug/20 12:29;trohrmann;So it should be fine to simply remove {{MiniDispatcher.cancelJob}} [~aljoscha]?;;;","24/Aug/20 14:22;aljoscha;If that means the cluster still shuts down by itself upon cancellation, then yes.;;;","25/Aug/20 02:21;Jiangang;Thanks for the reply, [~trohrmann] and [~aljoscha]. After reverting the code in MiniDispatcher, all tests run ok. But I also notice that something else should be changed. In MiniDispatcher's method jobReachedGloballyTerminalState, the cluster shuts down only if executionMode is DETACHED. Upon cancellation, the dispatcher will not shut down if executionMode is NORMAL. So we should shut down cluster no matter executionMode is DETACHED or NORMAL.;;;","25/Aug/20 06:28;trohrmann;This is true [~Jiangang]. Was this the requirement you had [~aljoscha] that a per job cluster also shuts down in NORMAL mode when calling cancel w/o requesting the final job result?;;;","25/Aug/20 07:43;aljoscha;Yes, exactly. The cluster should shut down in both modes. Maybe we can even get rid of the split between NORMAL and DETACHED by now.;;;","25/Aug/20 09:06;trohrmann;How would this work [~aljoscha]? Are you referring to removing the {{NORMAL}} and {{DETACHED}} mode in general or only for the cancel call? If you are deploying a per job cluster in normal mode, then you might want to obtain the final job result (w/o cancellation). If the cluster does not wait until it is served, then there is no way to obtain it.;;;","28/Aug/20 10:20;aljoscha;I think for this issue we would just change the behaviour for cancellation. In the long run, I think we should not block on shutdown anymore because this can lead to stuck clusters if the client (or connection) is down.

The job/cluster lifecycle should be independent of the client once it is submitted and we should rely on the (long-running) cluster management system to allow retrieval of final job results: When running Flink in standalone/session mode, the Flink cluster itself is the long-running system that can return job results and that can be queried for past jobs. When using YARN, this should be the long-running system that allows querying jobs and job results, same for Kubernetes. We could also think about delegating to the history server for these purposes. WDYT?
;;;","31/Aug/20 08:37;trohrmann;In theory I agree that it would be great to get rid of this special case behaviour. If we always had a system where one could persist the final job result, this would work. However, I am not sure whether this is always the case.  And I am not sure whether it would improve Flink's usability if our users would always have to deploy/start a second system in order to use Flink's per-job mode.

On the other hand, I also agree that depending on the client behaviour for deciding when to shut down or not, is also not ideal. One could mitigate the problem of a deadlocked cluster by introducing a timeout for the final job result serving.;;;","01/Sep/20 08:56;aljoscha;Agreed on everything. But that's a new Jira issue, right? For this one, we should just change the CANCEL path to go through all the normal steps.;;;","01/Sep/20 09:27;trohrmann;Yes exactly. The scope of this ticket is to fix the current regression which means to that cancelling a job should still trigger its archiving.;;;","02/Sep/20 00:34;Jiangang;Hi, [~trohrmann] and [~aljoscha]. If we just revert the {{MiniDispatcher}} changes and do nothing else,  the dispatcher may not shutdown at last in the case that the client is shutdown unexpectedly before. The discussion above also proves it. Should we resolve the problem or just revert the {{MiniDispatcher}} changes in this ticket and create another ticket for the problem? Thank you.;;;","02/Sep/20 07:56;trohrmann;I think we can record whether {{MiniDispatcher.cancelJob}} has been called and then complete the {{shutDownFuture}} if either this has happened or the execution mode is {{DETACHED}}.;;;","04/Sep/20 07:40;trohrmann;Fixed via

master: d9bddfd3c49203034141fafa0b1d522e7ea0b6e7
1.11.2: 7e2294c2b5ca9a17c924ff948a0dcd87d90c8327
1.10.3: 40f81b7794bde837aee8acac8944929ec43b6ef6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lose column comment when create table,FLINK-18958,13322703,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,fsk119,fsk119,14/Aug/20 08:30,10/Mar/23 15:04,13/Jul/23 08:12,10/Mar/23 15:04,1.11.0,,,,,,,,Table SQL / API,,,,,0,auto-unassigned,pull-request-available,stale-assigned,,,"Currently, table column will not store column comment and user can't see column comment when use {{describe table}} sql.",,Ada.h,dian.fu,fsk119,hackergin,jark,kezhuw,leonard,libenchao,lirui,lsy,oikomi,peng713128,qingyue,,,,,,,,,,,,,,,,,,,,,,FLINK-22936,,,,,FLINK-21197,,,,,,,FLINK-29679,,,,FLINK-21197,FLINK-22941,,,,,,,,,,,,,,,,FLINK-18981,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 28 22:37:19 UTC 2021,,,,,,,,,,"0|z0hrqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/20 09:07;oikomi;I would love to solve this issue. In my production system, this feature is now work well. thanks!;;;","17/Aug/20 09:32;jark;Assigined to you [~oikomi];;;","16/Apr/21 10:53;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:52;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","31/May/21 23:25;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Jun/21 00:26;Ada.h;I hope to be assigned this issue if no one takes. This problem also affects my project. Thanks!;;;","09/Jun/21 02:18;jark;Hi [~Ada.h], as I commented in the pull request: https://github.com/apache/flink/pull/13177, we should support {{comment}} in {{Schema}} and {{ResolvedSchema}} first. ;;;","09/Jun/21 02:33;jark;I created FLINK-22936. ;;;","28/Jul/21 08:21;jark;[~hackergin] do you want to do this? Which is a follow-up issue of FLINK-22936.;;;","28/Jul/21 08:22;hackergin;[~jark]  Sure, please assign this ticket to me. ;;;","29/Jul/21 12:48;hackergin;[~jark]. I  create https://issues.apache.org/jira/browse/FLINK-23545  to replace tableSchema with new Schema first. ;;;","28/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTask.invoke should catch Throwable instead of Exception,FLINK-18956,13322696,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,14/Aug/20 08:08,15/Aug/20 01:57,13/Jul/23 08:12,15/Aug/20 01:57,1.11.0,,,,,1.11.2,1.12.0,,Runtime / Task,,,,,0,pull-request-available,,,,,"In StreamTask.invoke, we should catch Throwable. Otherwise, cleanUpInvoke() will not be called if Error is thrown:

{code}
	@Override
	public final void invoke() throws Exception {
		try {
			beforeInvoke();

			// final check to exit early before starting to run
			if (canceled) {
				throw new CancelTaskException();
			}

			// let the task do its work
			runMailboxLoop();

			// if this left the run() method cleanly despite the fact that this was canceled,
			// make sure the ""clean shutdown"" is not attempted
			if (canceled) {
				throw new CancelTaskException();
			}

			afterInvoke();
		}
		catch (Exception invokeException) {
			failing = !canceled;
			try {
				cleanUpInvoke();
			}
			// TODO: investigate why Throwable instead of Exception is used here.
			catch (Throwable cleanUpException) {
				Throwable throwable = ExceptionUtils.firstOrSuppressed(cleanUpException, invokeException);
				throw (throwable instanceof Exception ? (Exception) throwable : new Exception(throwable));
			}
			throw invokeException;
		}
		cleanUpInvoke();
	}
{code}",,dian.fu,libenchao,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 15 01:57:05 UTC 2020,,,,,,,,,,"0|z0hrp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/20 01:57;dian.fu;master: 14665dd468a638b6d6c5f5f241455c8d81825b76
release-1.11: 792855277be38fafed27a7aaa7c98628ea256850;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSink shouldn't try to create BulkWriter factory when using MR writer,FLINK-18942,13322670,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,14/Aug/20 06:24,17/Aug/20 08:05,13/Jul/23 08:12,17/Aug/20 08:05,1.11.1,,,,,1.11.2,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 08:05:41 UTC 2020,,,,,,,,,,"0|z0hrjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/20 08:05;lzljs3620320;release-1.11: a1d8f6a04623a04eec04f71c47a21b0281565dc8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"There are some typos in ""Set up JobManager Memory""",FLINK-18941,13322646,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kecheng,wooplevip,wooplevip,14/Aug/20 01:13,21/Aug/20 06:42,13/Jul/23 08:12,21/Aug/20 06:42,1.11.1,,,,,1.11.2,1.12.0,,chinese-translation,,,,,0,pull-request-available,,,,,"From document [https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/ops/memory/mem_setup_jobmanager.html] , I found two typos as below:
 # ""一下情况可能用到堆外内存"" should be ""以下情况可能用到堆外内存""
 # ""这种情况下，_对外内存_的默认大小将不会生效"" should be ""这种情况下，_堆外内存_的默认大小将不会生效""",,azagrebin,wooplevip,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 21 06:42:01 UTC 2020,,,,,,,,,,"0|z0hre0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/20 07:51;azagrebin;cc [~xintongsong];;;","14/Aug/20 08:05;xtsong;Thanks for reporting this, [~wooplevip]. I'll fix this.;;;","21/Aug/20 06:42;xtsong;Fixed via
* master: b51ca7352f5ce9746f0fc00a395ca229cd76f9ef
* release-1.11: 2001a7c99539120e729a0d4a057b3142c6df169a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompletedOperationCache accepts new operations after closeAsync has been called,FLINK-18935,13322479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,13/Aug/20 07:35,14/Aug/20 17:45,13/Jul/23 08:12,14/Aug/20 17:45,1.10.1,1.11.1,1.12.0,,,1.12.0,,,Runtime / REST,,,,,0,pull-request-available,,,,,The {{CompletedOperationCache}} accepts new operations after {{closeAsync}} has been called. This is problematic because the returned termination future of {{closeAsync}} won't include these new operations. I think it would be better to fail with an {{IllegalStateException}} when one tries to enqueue a new operation after the {{CompletedOperationCache}} has been shut down.,,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 17:45:21 UTC 2020,,,,,,,,,,"0|z0hqcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/20 17:45;trohrmann;Fixed via 61caf2ea139684f216df873ed3b864b6cb9d4a4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idle stream does not advance watermark in connected stream,FLINK-18934,13322476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,kien_truong,kien_truong,13/Aug/20 07:30,11/Oct/21 08:58,13/Jul/23 08:12,08/Oct/21 15:15,1.11.0,1.12.0,1.13.0,,,1.14.0,,,API / DataStream,Runtime / Task,,,,2,pull-request-available,stale-assigned,,,,"Per Flink documents, when a stream is idle, it will allow watermarks of downstream operator to advance. However, when I connect an active data stream with an idle data stream, the output watermark of the CoProcessOperator does not increase.

Here's a small test that reproduces the problem.

https://github.com/kien-truong/flink-idleness-testing",,aljoscha,dian.fu,dwysakowicz,elanv,godfreyhe,jingzhang,kezhuw,kien_truong,kkrugler,knaufk,leonard,libenchao,liupengcheng,Ming Li,nkruber,Paul Lin,pnowojski,shazeline,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22881,FLINK-22904,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 08 15:14:32 UTC 2021,,,,,,,,,,"0|z0hqc8:",9223372036854775807,"We added `processWatermarkStatusX` method to classes such as `AbstractStreamOperator`, `Input` etc. It allows to take the `WatermarkStatus` into account when combining watermarks in two/multi input operators.",,,,,,,,,,,,,,,,,,,"13/Aug/20 14:45;kkrugler;Hi [~kien_truong] - I would suggest posting this question to the Flink user mailing list, and after reviewing the responses, decide whether this is really a bug.;;;","24/Aug/20 12:42;aljoscha;Is there any update on this?;;;","24/Aug/20 14:04;kien_truong;Sorry, I've forgotten about this issue so I've only made a post to the mailing list just now.

Maybe we could wait a few days to see if there's any response. Thanks.;;;","28/Aug/20 21:48;kkrugler;On the list, [~dwysakowicz] said:
{quote}Hi Kien,
I am afraid this is a valid bug. I am not 100% sure but the way I
understand the code the idleness mechanism applies to input channels,
which means e.g. when multiple parallell instances shuffle its results
to downstream operators.
In case of a two input operator, combining the watermark of two
different upstream operators happens inside of the operator itself.
There we do not have the idleness status. We do not have a status that a
whole upstream operator became idle. That's definitely a bug/limitation.
I'm also cc'ing Aljoscha who could maybe confirm my analysis.
Best,
Dawid{quote}

And [~aljoscha] added:
{quote}Yes, I'm afraid this analysis is correct. The StreamOperator, AbstractStreamOperator to be specific, computes the combined watermarks from both inputs here: https://github.com/apache/flink/blob/f0ed29c06d331892a06ee9bddea4173d6300835d/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractStreamOperator.java#L573. The operator layer is not aware of idleness so it will never notice. The idleness only works on the level of inputs but is never forwarded to an operator itself.

To fix this we would have to also make operators aware of idleness such that they can take this into account when computing the combined output watermark.

Best,
Aljoscha{quote}
;;;","29/Aug/20 01:45;libenchao;+1 for fixing this issue, we also found this limitation when using 'temporal table function', and fixed it in our internal branch.;;;","31/Aug/20 12:31;pnowojski;Side note, if you need to fix {{AbstractStreamOperator}}, please remember about fixing {{AbstractStreamOperatorV2}} as well.;;;","03/Sep/20 03:38;zhuzh;[~knaufk] [~aljoscha] Given that this issue was proposed as a blocker for releasing 1.11.2, I have a couple of questions to understand the state of it. Appreciate if you can help with them.
 - Should it be a strict blocker of releasing 1.11.2?
 - Is there a concise on how we can fix it? 
 - Does next Monday as the deadline to cut RC1 work for the fix?;;;","07/Sep/20 06:24;zhuzh;I think this ticket still needs some time to figure out a proper solution, so moving it to 1.11.3.;;;","22/Sep/20 13:25;pnowojski;Note to anyone who will be fixing this issue. In the {{MultipleInputStreamTaskTest#testWatermarkAndStreamStatusForwarding}} (implemented as part of FLINK-18907 ) there is a workaround of this bug, that needs to be removed when this bug is fixed.;;;","11/Nov/20 03:49;dian.fu;What's the status of this issue? Is it still target to fixed in 1.12.0?;;;","11/Nov/20 12:48;aljoscha;I don't think we can fix it for Flink 1.12. This requires extending the operator interface to ""teach"" operators about idleness as well so that they can combine the two sides and forward an appropriate watermark/idleness.;;;","11/Nov/20 12:53;dian.fu;[~aljoscha] Thanks a lot. I have updated the fix version to 1.13.0.;;;","12/Apr/21 11:18;knaufk;As this is clearly not a Blocker (open for > 6 months), I've deprioritized it to Major for now (see also https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process).;;;","12/Apr/21 11:19;dwysakowicz;Actually it is a blocker that we've not handled well so far :(;;;","12/Apr/21 11:24;knaufk;[~dwysakowicz] Why is this a blocker? It is not an infrastructure failure and its not a bug that is blocking us from releasing. ;;;","12/Apr/21 11:29;dwysakowicz;We agreed offline it is an important bug that lingers for a long time and thus we should consider this a must for 1.13.1 and 1.14;;;","21/Apr/21 22:44;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","25/May/21 06:40;dwysakowicz;Fixed in 214ba6817762c595110db289a7a3550cba5024e6..ee9f9b227a7703c2688924070c4746a70bff3fd8;;;","08/Oct/21 15:14;dwysakowicz;It has not been backported to previous versions because it needed to change the {{Input}} interface which is marked as {{PublicEvolving}}. We did not want to introduce such a change in a minor release.;;;","08/Oct/21 15:14;dwysakowicz;Reopen to add release notes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIXED_PATH(dummy Hadoop Path) with WriterImpl may cause ORC writer OOM,FLINK-18915,13322445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,wei.wei,wei.wei,13/Aug/20 04:52,17/Oct/20 18:20,13/Jul/23 08:12,16/Oct/20 17:59,1.11.0,1.11.1,,,,1.11.3,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"# OrcBulkWriterFactory
{code:java}
@Override
public BulkWriter<T> create(FSDataOutputStream out) throws IOException {
   OrcFile.WriterOptions opts = getWriterOptions();
   opts.physicalWriter(new PhysicalWriterImpl(out, opts));

   return new OrcBulkWriter<>(vectorizer, new WriterImpl(null, FIXED_PATH, opts));
}{code}
 

# MemoryManagerImpl
{code:java}
// 
public void addWriter(Path path, long requestedAllocation,
                            Callback callback) throws IOException {
  checkOwner();
  WriterInfo oldVal = writerList.get(path);
  // this should always be null, but we handle the case where the memory
  // manager wasn't told that a writer wasn't still in use and the task
  // starts writing to the same path.
  if (oldVal == null) {
    oldVal = new WriterInfo(requestedAllocation, callback);
    writerList.put(path, oldVal);
    totalAllocation += requestedAllocation;
  } else {
    // handle a new writer that is writing to the same path
    totalAllocation += requestedAllocation - oldVal.allocation;
    oldVal.allocation = requestedAllocation;
    oldVal.callback = callback;
  }
  updateScale(true);
}
{code}
SinkTask may have multi BulkWriter create, FIXED_PATH will cause overlay the last writer callback；Last writer's WriterImpl#checkMemory will never called;

 

 ",,gaoyunhaii,kkl0u,lzljs3620320,wei.wei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 17:59:50 UTC 2020,,,,,,,,,,"0|z0hq5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/20 02:46;lzljs3620320;CC: [~gaoyunhaii];;;","01/Sep/20 06:01;gaoyunhaii;Very thanks for reporting this issue! I'll have a look, and I think [~zenfenan] should also could help on this issue. ;;;","12/Oct/20 07:23;gaoyunhaii;Very thansk [~wei.wei] and [~lzljs3620320] for reporting and analyzing this issue! I checked the implementation and the issue should do exists. Since a sink task should always create its own OrcBulkWriterFactory on startup and it should not create duplicate writer for the same target path, I think we should simply change the FIX_PATH to a random path ? ;;;","12/Oct/20 07:34;lzljs3620320;I think yes, a UUID path can help.;;;","12/Oct/20 07:34;lzljs3620320;[~gaoyunhaii] Do you want to contribute this?;;;","12/Oct/20 07:36;lzljs3620320;CC: [~kkl0u];;;","12/Oct/20 07:38;gaoyunhaii;OK, no problem, I would open a PR for it.;;;","16/Oct/20 17:59;kkl0u;Merged on master  with 3a9a93053903dfefc3fb651e47800afc00d13a9e
and on release-1.11 with 393b13cfd103896e0cb60c6c7230101bef9a956f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot serve results of asynchronous REST operations in per-job mode,FLINK-18902,13322262,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,trohrmann,trohrmann,12/Aug/20 13:00,13/Aug/20 08:18,13/Jul/23 08:12,13/Aug/20 08:18,1.10.2,1.11.2,1.12.0,,,1.10.2,1.11.2,1.12.0,Runtime / REST,,,,,0,pull-request-available,,,,,"Due to the changes introduced with FLINK-18663 a Flink per-job cluster can no longer properly shut down. The problem is that we no longer serve asynchronous results (e.g. resulting from a cancel-with-savepoint operation) while the cluster waits to shut down.

In order to solve this problem, Flink needs to serve REST request also while it waits for shutting itself down.",,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18663,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 08:18:08 UTC 2020,,,,,,,,,,"0|z0hp1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/20 08:18;trohrmann;Fixed via

master: ee0f55fbba7c9408e3f8647f913260adb4b2c7ab
1.11.2: 7e975a49717a7122ab2394397726e5c8fb1c047b
1.10.2: fd7b3719616435cd2aebf65929dbc193a53e68f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog should error out when listing partitions with an invalid spec,FLINK-18900,13322215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lirui,lirui,12/Aug/20 08:26,28/Aug/20 03:59,13/Jul/23 08:12,28/Aug/20 03:59,1.11.1,,,,,1.11.2,1.12.0,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Take the following case as an example:
{code}
create table tbl (x int) partitioned by (p int);
alter table tbl add partition (p=1);
{code}
If we list partitions with partition spec {{foo=1}}, HiveCatalog returns partition {{p=1}}, which is wrong.",,lirui,lzljs3620320,nicholasjiang,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 28 03:59:38 UTC 2020,,,,,,,,,,"0|z0hor4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/20 08:45;nicholasjiang;[~lzljs3620320] Could you please assign this to me? I have discussed with [~lirui].;;;","24/Aug/20 03:38;lzljs3620320;master: 478c9657fe1240acdc1eb08ad32ea93e08b0cd5e

release-1.11: ed09bd65b64ef64d4a0b6966a7227bbdf8231c29;;;","28/Aug/20 02:07;lzljs3620320;We should not modify method throws exception for {{Catalog.listPartitions}} in release-1.11.;;;","28/Aug/20 02:11;lirui;Perhaps we can revert the change in 1.11, given that 1.11 doesn't support SHOW PARTITIONS.;;;","28/Aug/20 03:59;lzljs3620320;Revert interface modification for 1.11: a5767906916548ca51bf2d4b9e75c833ea6522a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The property yarn.application-attempts default value is not none,FLINK-18899,13322203,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wooplevip,wooplevip,12/Aug/20 07:09,27/Aug/20 16:47,13/Jul/23 08:12,27/Aug/20 16:47,1.11.0,,,,,1.12.0,,,Deployment / YARN,Documentation,,,,0,pull-request-available,usability,,,,"The document [https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#yarn-application-attempts] shows the yarn.application-attempts default value is none, but I found codes in  org.apache.flink.yarn.YarnClusterDescriptor class like below:
{code:java}
if (HighAvailabilityMode.isHighAvailabilityModeActivated(configuration)) {
   // activate re-execution of failed applications
   appContext.setMaxAppAttempts(
         configuration.getInteger(
               YarnConfigOptions.APPLICATION_ATTEMPTS.key(),
               YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));

   activateHighAvailabilitySupport(appContext);
} else {
   // set number of application retries to 1 in the default case
   appContext.setMaxAppAttempts(
         configuration.getInteger(
               YarnConfigOptions.APPLICATION_ATTEMPTS.key(),
               1));
}
{code}
This means, if HA mode, the default value is from Yarn(default 2), otherwise the default value is 1.

 ",,rmetzger,trohrmann,wangyang0918,wooplevip,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 27 16:47:41 UTC 2020,,,,,,,,,,"0|z0hoog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/20 04:46;rmetzger;[~fly_in_gis] What do you think about this ticket? Does it make sense to extend the description of ""yarn.application-attempts"" and explain the overwritten values in HA mode?;;;","17/Aug/20 03:53;wangyang0918;Hi [~wooplevip], thanks for creating this ticket. IIUC, the reason why we leave the default value of {{yarn.application-attempts}} to none is to configure it based on the HA configuration. If the users do not specify a explicitly value of {{yarn.application-attempts}}, then
 * For non-HA mode, we should set the attempt to 1 to make the application fail-fast. Because when a new jobmanager is launched, it could not recover from the latest checkpoint.
 * For HA mode, we should respect the configuration of Yarn and relaunch the jobmanager more than once.

 

We could update the description to show the default behavior. Does it make sense to you?;;;","17/Aug/20 06:12;wooplevip;Got it. Thanks [~fly_in_gis]. ;;;","17/Aug/20 07:05;trohrmann;[~fly_in_gis] do you wanna update the description?;;;","17/Aug/20 08:49;wangyang0918;[~trohrmann] Yeah, i will add a simple PR to update the description to show the default behavior.;;;","27/Aug/20 16:47;trohrmann;Fixed via e690793c6e65b711ab6cbd699e0b781adcafd03b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python tests fails with ""AppendStreamTableSink requires that Table has only insert changes.""",FLINK-18893,13322092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,rmetzger,rmetzger,11/Aug/20 18:06,12/Aug/20 02:56,13/Jul/23 08:12,12/Aug/20 02:56,1.11.2,1.12.0,,,,1.12.0,,,API / Python,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5392&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=455fddbf-5921-5b71-25ac-92992ad80b28

{code}
2020-08-11T09:33:00.6405057Z 
2020-08-11T09:33:00.6407257Z =================================== FAILURES ===================================
2020-08-11T09:33:00.6407774Z _________ StreamPandasConversionTests.test_to_pandas_for_retract_table _________
2020-08-11T09:33:00.6408007Z 
2020-08-11T09:33:00.6409313Z a = ('xro10353', <py4j.java_gateway.GatewayClient object at 0x7f9572c53ac8>, 'z:org.apache.flink.table.runtime.arrow.ArrowUtils', 'collectAsPandasDataFrame')
2020-08-11T09:33:00.6409732Z kw = {}
2020-08-11T09:33:00.6410610Z s = 'org.apache.flink.table.api.TableException: AppendStreamTableSink requires that Table has only insert changes.'
2020-08-11T09:33:00.6413478Z stack_trace = 'org.apache.flink.table.plan.nodes.datastream.DataStreamSink.writeToAppendSink(DataStreamSink.scala:118)\n\t at org.ap....api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\t at java.lang.Thread.run(Thread.java:748)'
2020-08-11T09:33:00.6414468Z exception = 'org.apache.flink.table.api.TableException'
2020-08-11T09:33:00.6414663Z 
2020-08-11T09:33:00.6414858Z     def deco(*a, **kw):
2020-08-11T09:33:00.6415060Z         try:
2020-08-11T09:33:00.6415274Z >           return f(*a, **kw)
2020-08-11T09:33:00.6415419Z 
2020-08-11T09:33:00.6415615Z pyflink/util/exceptions.py:147: 
2020-08-11T09:33:00.6415918Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-08-11T09:33:00.6416154Z 
2020-08-11T09:33:00.6416459Z answer = 'xro10353'
2020-08-11T09:33:00.6416746Z gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f9572c53ac8>
2020-08-11T09:33:00.6419376Z target_id = 'z:org.apache.flink.table.runtime.arrow.ArrowUtils'
2020-08-11T09:33:00.6420325Z name = 'collectAsPandasDataFrame'
2020-08-11T09:33:00.6420574Z 
2020-08-11T09:33:00.6421009Z     def get_return_value(answer, gateway_client, target_id=None, name=None):
2020-08-11T09:33:00.6421735Z         """"""Converts an answer received from the Java gateway into a Python object.
2020-08-11T09:33:00.6422153Z     
2020-08-11T09:33:00.6422538Z         For example, string representation of integers are converted to Python
2020-08-11T09:33:00.6423113Z         integer, string representation of objects are converted to JavaObject
2020-08-11T09:33:00.6423424Z         instances, etc.
2020-08-11T09:33:00.6423594Z     
2020-08-11T09:33:00.6423844Z         :param answer: the string returned by the Java gateway
2020-08-11T09:33:00.6424203Z         :param gateway_client: the gateway client used to communicate with the Java
2020-08-11T09:33:00.6424604Z             Gateway. Only necessary if the answer is a reference (e.g., object,
2020-08-11T09:33:00.6424874Z             list, map)
2020-08-11T09:33:00.6425375Z         :param target_id: the name of the object from which the answer comes from
2020-08-11T09:33:00.6425735Z             (e.g., *object1* in `object1.hello()`). Optional.
2020-08-11T09:33:00.6426069Z         :param name: the name of the member from which the answer comes from
2020-08-11T09:33:00.6426422Z             (e.g., *hello* in `object1.hello()`). Optional.
2020-08-11T09:33:00.6426648Z         """"""
2020-08-11T09:33:00.6426868Z         if is_error(answer)[0]:
2020-08-11T09:33:00.6427112Z             if len(answer) > 1:
2020-08-11T09:33:00.6427358Z                 type = answer[1]
2020-08-11T09:33:00.6427665Z                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2020-08-11T09:33:00.6428032Z                 if answer[1] == REFERENCE_TYPE:
2020-08-11T09:33:00.6428313Z                     raise Py4JJavaError(
2020-08-11T09:33:00.6428691Z                         ""An error occurred while calling {0}{1}{2}.\n"".
2020-08-11T09:33:00.6429083Z >                       format(target_id, ""."", name), value)
2020-08-11T09:33:00.6429798Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
2020-08-11T09:33:00.6430529Z E                   : org.apache.flink.table.api.TableException: AppendStreamTableSink requires that Table has only insert changes.
2020-08-11T09:33:00.6431132Z E                   	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.writeToAppendSink(DataStreamSink.scala:118)
2020-08-11T09:33:00.6431793Z E                   	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.writeToSink(DataStreamSink.scala:74)
2020-08-11T09:33:00.6432378Z E                   	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.translateToPlan(DataStreamSink.scala:59)
2020-08-11T09:33:00.6433145Z E                   	at org.apache.flink.table.planner.StreamPlanner.org$apache$flink$table$planner$StreamPlanner$$translateToCRow(StreamPlanner.scala:274)
2020-08-11T09:33:00.6434258Z E                   	at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:120)
2020-08-11T09:33:00.6435151Z E                   	at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:117)
2020-08-11T09:33:00.6436010Z E                   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-08-11T09:33:00.6436565Z E                   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-08-11T09:33:00.6437051Z E                   	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-08-11T09:33:00.6437492Z E                   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-08-11T09:33:00.6437963Z E                   	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-08-11T09:33:00.6438568Z E                   	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-08-11T09:33:00.6439109Z E                   	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
2020-08-11T09:33:00.6439574Z E                   	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2020-08-11T09:33:00.6440142Z E                   	at org.apache.flink.table.planner.StreamPlanner.translate(StreamPlanner.scala:117)
2020-08-11T09:33:00.6440681Z E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1264)
2020-08-11T09:33:00.6441279Z E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:733)
2020-08-11T09:33:00.6441924Z E                   	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:570)
2020-08-11T09:33:00.6442447Z E                   	at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:643)
2020-08-11T09:33:00.6442934Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-08-11T09:33:00.6443477Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-08-11T09:33:00.6444005Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-08-11T09:33:00.6444476Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-08-11T09:33:00.6444953Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-08-11T09:33:00.6445545Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-08-11T09:33:00.6446079Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-08-11T09:33:00.6446631Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-08-11T09:33:00.6447221Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-08-11T09:33:00.6447764Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-08-11T09:33:00.6448224Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-08-11T09:33:00.6448437Z 
2020-08-11T09:33:00.6449124Z .tox/py35-cython/lib/python3.5/site-packages/py4j/protocol.py:328: Py4JJavaError
{code}",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 02:54:21 UTC 2020,,,,,,,,,,"0|z0hnzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/20 01:50;dian.fu;Thanks [~rmetzger], I will fix this issue ASAP.;;;","12/Aug/20 02:51;dian.fu;Just found that *Table.execute* doesn't support upsert table in 1.11 and so we should not cherry-pick FLINK-18848 to release-1.11.  So fixing this issue by:
- Revert the PR of FLINK-18848 from release-1.11
- As this issue also exists in master, submit a PR to fix it in master;;;","12/Aug/20 02:54;dian.fu;master: Fixed via 3932683871e7d1aaa74d318aa4cee90d9b2f9e61
release-1.11: Revert FLINK-18848 via 0ca2720b13510e3f76cda2942268614304c64916;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect configurations defined in flink-conf.yaml and environment variables when executing in local mode,FLINK-18880,13321908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dianfu,dian.fu,dian.fu,11/Aug/20 02:16,13/Sep/21 03:19,13/Jul/23 08:12,13/Sep/21 03:19,1.11.0,,,,,1.13.3,1.14.0,,API / Python,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Currently, the configurations defined in flink-conf.yaml and environment variables are not respected when PyFlink jobs are executed in local mode. It will cause a few problems, including but not limited to the following:
- Users could not configure the heap memory used by the gateway server. It may cause OOM issues in scenarios such as Table.to_pandas when the content of the Table is big. 
- There is no easy way for users to specify hadoop classpath",,dian.fu,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 13 03:19:23 UTC 2021,,,,,,,,,,"0|z0hmuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:10;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","22/May/21 10:52;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","13/Sep/21 03:19;dianfu;Fixed in:
- master via e3d716783a8421638095a3938e3a9d8f7911f4ba
- release-1.14 via ed022d21708626ded50b7d0e5b6a1aa79d8b59aa
- release-1.13 via 978d90715e1cddef816c2ddfb08ba3b27f38758c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobServer moves file with an open FileInputStream,FLINK-18876,13321790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,10/Aug/20 13:04,13/Apr/21 20:41,13/Jul/23 08:12,12/Aug/20 08:04,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"{{BlobServer#putInputStream}} moves a file while it has an open FileInputStream, causing tests like the BlobServerPutTest to fail on Windows.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11261,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 08:04:52 UTC 2020,,,,,,,,,,"0|z0hm5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/20 08:04;chesnay;master: bff753717754df3062e639c912a5b86d5cb08091;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-deterministic function could break retract mechanism,FLINK-18871,13321748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,,libenchao,libenchao,10/Aug/20 08:26,18/Sep/22 13:19,13/Jul/23 08:12,18/Sep/22 13:19,,,,,,1.16.0,,,Table SQL / Planner,Table SQL / Runtime,,,,0,auto-deprioritized-major,auto-deprioritized-minor,,,,"For example, we have a following SQL:
{code:sql}
create view view1 as
select 
  max(a) as m1,
  max(b) as m2 -- b is a timestmap
from T
group by c, d;

create view view2 as
select * from view1
where m2 > CURRENT_TIMESTAMP;

insert into MySink
select sum(m1) as m1 
from view2
group by c;
{code}
view1 will produce retract messages, and the same message in view2 maybe produce different results. and the second agg will produce wrong result.
 For example,
{noformat}
view1:
+ (1, 2020-8-10 16:13:00)
- (1, 2020-8-10 16:13:00)
+ (2, 2020-8-10 16:13:10)

view2:
+ (1, 2020-8-10 16:13:00)
- (1, 2020-8-10 16:13:00) // this record may be filtered out
+ (2, 2020-8-10 16:13:10)

MySink:
+ (1, 2020-8-10 16:13:00)
+ (2, 2020-8-10 16:13:10) // will produce wrong result.
{noformat}
In general, the non-deterministic function may break the retract mechanism. All operators downstream which will rely on the retraction mechanism will produce wrong results, or throw exception, such as Agg / some Sink which need retract message / TopN / Window.

(The example above is a simplified version of some production jobs in our scenario, just to explain the core problem)

CC [~ykt836] [~jark]",,felixzheng,FrankZou,godfreyhe,jark,leonard,libenchao,lincoln.86xy,Paul Lin,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28570,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 18 13:19:02 UTC 2022,,,,,,,,,,"0|z0hlw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/20 09:02;jark;That's true. This is a known issue. Users should pay attention when using non-deterministic function between aggregations (or other operators produce changelog). A solution might be to support Stateful Calc operator, that emit the retractions with previous value instead of generate new values from the non-deterministic functions. ;;;","22/Apr/21 11:10;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:07;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","03/Nov/21 10:45;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Nov/21 10:39;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Sep/22 13:19;lincoln.86xy;The non-deterministic function exists in changelog pipeline can be validated if it affect correctness since 1.16.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch SQL end-to-end test unstable due to terminated process,FLINK-18869,13321734,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,rmetzger,rmetzger,10/Aug/20 06:38,11/Nov/20 01:13,13/Jul/23 08:12,11/Nov/20 01:13,1.12.0,,,,,1.12.0,,,Table SQL / Runtime,Tests,,,,0,test-stability,,,,,"{code}
==============================================================================
Running 'Batch SQL end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-34920900539
Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
Starting cluster.
Starting standalonesession daemon on host fv-az670.
Starting taskexecutor daemon on host fv-az670.
Waiting for Dispatcher REST endpoint to come up...
Waiting for Dispatcher REST endpoint to come up...
Waiting for Dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
Job has been submitted with JobID 7a941de4e728b6da942dbff7badc955e
pass BatchSQL
Stopping taskexecutor daemon (pid: 21149) on host fv-az670.
Stopping standalonesession daemon (pid: 20848) on host fv-az670.
Skipping taskexecutor daemon (pid: 20970), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 21329), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 21742), because it is not running anymore on fv-az670.
Stopping taskexecutor daemon (pid: 22119) on host fv-az670.
Skipping taskexecutor daemon (pid: 25564), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 9154), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 9552), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 9851), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 15190), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 18068), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 18476), because it is not running anymore on fv-az670.
Skipping taskexecutor daemon (pid: 18783), because it is not running anymore on fv-az670.
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_batch_sql.sh: line 72: 22119 Terminated              [[ ${zookeeper_process_count} -gt 0 ]]
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
No non-empty .out files.

[FAIL] 'Batch SQL end-to-end test' failed after 0 minutes and 16 seconds! Test exited with exit code 1
{code}",,dian.fu,leonard,libenchao,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 01:13:51 UTC 2020,,,,,,,,,,"0|z0hlsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/20 16:34;rmetzger;No log file provided. Closing.;;;","28/Oct/20 01:42;dian.fu;Seems another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8432&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873;;;","28/Oct/20 06:13;rmetzger;Reopening issue.

I could not find anything suspicious in the logs archive. JM and TM seem to have no errors.
I guess there is something wrong with the test scripts somewhere.;;;","04/Nov/20 09:19;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8955&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9;;;","10/Nov/20 13:00;dian.fu;cc [~lzljs3620320] [~Leonard Xu];;;","10/Nov/20 19:57;rmetzger;This might have been caused by the CI environment, and might have been fixed through FLINK-19436 already.;;;","11/Nov/20 01:01;leonard;[~dian.fu] I agree [~rmetzger]'s opinion that this should have been fixed through  FLINK-19436 , I tend to close this one, we can reopen it once this test fail again.;;;","11/Nov/20 01:13;dian.fu;[~rmetzger] [~Leonard Xu] Thanks a lot! Let's close it. We can reopen it if it happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generic table stored in Hive catalog is incompatible between 1.10 and 1.11,FLINK-18867,13321724,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,10/Aug/20 04:22,13/Aug/20 04:15,13/Jul/23 08:12,13/Aug/20 04:15,1.11.1,,,,,1.11.2,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Generic table stored in 1.10 cannot be accessed in 1.11, because we changed how table schema is stored.",,gyfora,jark,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15901,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 04:15:02 UTC 2020,,,,,,,,,,"0|z0hlqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/20 08:29;lirui;The prefix of the key associated with table schema was changed in FLINK-15901. We should try the old key if we can't get table schema with the new key.;;;","13/Aug/20 04:15;lzljs3620320;master: 47bf0aa39079dd1167f6306854490b38406903fb

release-1.11: 38cb9201ff6319b4fa1f8aa4721219fb35c26bf3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix LISTAGG throws BinaryRawValueData cannot be cast to StringData exception in runtime,FLINK-18862,13321690,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,YUJIANBO,YUJIANBO,09/Aug/20 15:36,11/Aug/20 08:33,13/Jul/23 08:12,11/Aug/20 08:33,1.11.1,,,,,1.11.2,1.12.0,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"1. Env：flinksql、 version 1.11.1，perjob mode
2. Error：org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData

3、Job：

(1) create a kafka table
{code:java}
    CREATE TABLE kafka(
        x String,
        y String
    )with(
       'connector' = 'kafka',
        ......
    )
{code}


(2)create a view:
{code:java}
   CREATE VIEW view1 AS
   SELECT 
       x, 
       y, 
       CAST(COUNT(1) AS VARCHAR) AS ct
   FROM kafka
   GROUP BY 
       x, y
{code}


(3) aggregate on the view:
{code:java}
    select 
         x, 
         LISTAGG(CONCAT_WS('=', y, ct), ',') AS lists
    FROM view1
    GROUP BY x
{code}

And then the exception is thrown：org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData
The problem is that, there is no RawValueData in the query. The result type of count(1) should be bigint, not RawValueData. 
   
(4) If there is no aggregation, the job can run succefully.
{code:java}
    select 
	    x, 
	    CONCAT_WS('=', y, ct)
    from view1
{code}

The detailed exception:
{code:java}
java.lang.ClassCastException: org.apache.flink.table.data.binary.BinaryRawValueData cannot be cast to org.apache.flink.table.data.StringData
	at org.apache.flink.table.data.GenericRowData.getString(GenericRowData.java:169) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.table.data.JoinedRowData.getString(JoinedRowData.java:139) ~[flink-table-blink_2.11-1.11.1.jar:?]
	at org.apache.flink.table.data.RowData.get(RowData.java:273) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:156) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:123) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:50) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:715) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:205) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:161) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:178) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:153) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:345) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:558) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) ~[ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [ad_features_auto-1.0-SNAPSHOT.jar:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [ad_features_auto-1.0-SNAPSHOT.jar:?]
{code}
",,godfreyhe,jark,kezhuw,leonard,libenchao,lsy,twalthr,YUJIANBO,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/20 16:20;YUJIANBO;sql 中agg算子的 count(1) 的结果强转成String类型再经过一次agg的操作就不能成功.txt;https://issues.apache.org/jira/secure/attachment/13009346/sql+%E4%B8%ADagg%E7%AE%97%E5%AD%90%E7%9A%84+count%281%29+%E7%9A%84%E7%BB%93%E6%9E%9C%E5%BC%BA%E8%BD%AC%E6%88%90String%E7%B1%BB%E5%9E%8B%E5%86%8D%E7%BB%8F%E8%BF%87%E4%B8%80%E6%AC%A1agg%E7%9A%84%E6%93%8D%E4%BD%9C%E5%B0%B1%E4%B8%8D%E8%83%BD%E6%88%90%E5%8A%9F.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 03:23:05 UTC 2020,,,,,,,,,,"0|z0hlj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/20 01:51;leonard;Hi, [~YUJIANBO] 

Thanks for creating this issue. But could you using English to describe this issue? It's recommended using English in community.

 ;;;","10/Aug/20 05:36;jark;Thanks for reporting this [~YUJIANBO], I have reproduced this problem in local. I will take this issue. ;;;","10/Aug/20 07:13;jark;I have looked into this issue. The root cause is that the {{ListAggWithRetractAggFunction#getTypeInference}} is not considered when it is a built-in agg function. However, if we want to call {{getTypeInference}}, we have to pass the {{DataTypeFactory}} for all the related methods in {{AggregateUtils}}, e.g. {{transformToStreamAggregateInfoList, transformToBatchAggregateInfoList, deriveAggregateInfoList}}, which is a big refactoring. Do you have any thoughts [~twalthr]?;;;","10/Aug/20 07:59;twalthr;[~jark] I'm about to open PR for FLINK-18809. Only one test is failing in my builds. After that all built-in agg functions use the new type system. It might also solve this issue.;;;","10/Aug/20 08:12;jark;Thanks a lot [~twalthr]. I just figure out a quick fix for this: override the {{ListAggWithRetractAggFunction#getResultType}} and return {{InternalTypeInfo.of(DataTypes.STRING().getLogicalType())}}. This uses the legacy APIs, but should work. ;;;","10/Aug/20 08:49;jark;Considering we still need a fix for 1.11 series, I will open a pull request with the above approach. We can switch to the type inference approach in FLINK-18809 but keep the new added tests. [~twalthr];;;","10/Aug/20 10:07;twalthr;Yes, this could be a fix for the 1.11 branch.;;;","11/Aug/20 03:23;jark;Fixed in 
 - master (1.12.0): bfbdca9f574b4201ba7a400607c5169134ecf0a2
 - 1.11.2: a72a8cd430a00c51ba55d7d9eed3e39dcbac3164;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ExecutionGraphNotEnoughResourceTest.testRestartWithSlotSharingAndNotEnoughResources failed with ""Condition was not met in given timeout.""",FLINK-18859,13321665,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,dian.fu,dian.fu,09/Aug/20 03:14,10/Aug/20 15:03,13/Jul/23 08:12,10/Aug/20 15:03,1.10.1,1.11.0,,,,1.10.2,1.11.2,1.12.0,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5300&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=66b5c59a-0094-561d-0e44-b149dfdd586d]

{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.673 s <<< FAILURE! - in org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest
[ERROR] testRestartWithSlotSharingAndNotEnoughResources(org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest)  Time elapsed: 3.158 s  <<< ERROR!
java.util.concurrent.TimeoutException: Condition was not met in given timeout.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:129)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:119)
	at org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest.testRestartWithSlotSharingAndNotEnoughResources(ExecutionGraphNotEnoughResourceTest.java:130)
{code}",,dian.fu,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 10 15:03:19 UTC 2020,,,,,,,,,,"0|z0hldk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/20 03:58;zhuzh;I tried running the case 1000 times and failed to reproduce in my dev environment.
And the transfer.sh unfortunately failed and I cannot see the detailed execution log.
I guess this instability was caused by a temporarily slowness of the CI environment, which resulted in the job to not reach the expected state(FAILED) within 2000 ms.;;;","10/Aug/20 07:33;trohrmann;[~zhuzh] shall we increase the timeout to 10s?;;;","10/Aug/20 08:31;zhuzh;Sure. Will increase it as a mitigation and see whether it can still happen.;;;","10/Aug/20 15:03;zhuzh;Fixed via
master: 77cdd1358a91adade7ceb7e26f1afcd76c730567
release-1.11: 69865ac0729402d01cee90d7378032ef501fd814
release-1.10: fafee5ef94f7e3ed92788b6b8b748bb729c939f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinator ignores checkpointing.min-pause,FLINK-18856,13321620,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,08/Aug/20 07:19,13/Aug/20 10:32,13/Jul/23 08:12,13/Aug/20 10:32,1.11.1,,,,,1.11.2,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,See discussion: http://mail-archives.apache.org/mod_mbox/flink-dev/202008.mbox/%3cCA+5xAo2enuzFyq+E-MMr2LuUueU_ZFjJoABjtQxow6tkGdRSOw@mail.gmail.com%3e,,klion26,pnowojski,roman,stevenz3wu,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18675,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 10:32:37 UTC 2020,,,,,,,,,,"0|z0hl3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/20 06:27;klion26;Seems this issue is similar with FLINK-18675;;;","09/Aug/20 21:35;roman;My bad, I didn't search for existing issues before creating this one. Marking it as a duplicate.

I think we can keep it open though, as there is a PR already.;;;","13/Aug/20 10:32;pnowojski;Merged to master as 801102f20d..91298b803c. Merged to release 1.11 as 19468a25a9..68d6bf573b

Thanks [~thw@apache.org] for reporting the issue and helping us track it down. Thanks [~roman_khachatryan] for tracking the issue down and fixing it :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamScan should keep the same parallelism as the input,FLINK-18852,13321516,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liupengcheng,liupengcheng,liupengcheng,07/Aug/20 13:23,02/Sep/20 04:30,13/Jul/23 08:12,02/Sep/20 04:30,1.11.1,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Currently, the parallelism for StreamTableSourceScan/DataStreamScan is not inherited from  the upstream input, but retrieved from the config. I think this is unexpected.

I find this issue through UT, here is an example:

{code:java}

// env parallelism is set to 4
val env = StreamExecutionEnvironment.getExecutionEnvironment
    val tEnv = StreamTableEnvironment.create(env)
    StreamITCase.testResults = new mutable.MutableList[String]
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(4)

// DataSource parallelism is set to 1
val table1 = env.fromCollection(left)
      .setParallelism(1)
      .assignTimestampsAndWatermarks(new TimestampAndWatermarkWithOffset[(Long, String)](0))
      .toTable(tEnv, 'a, 'b)
    val table2 = env.fromCollection(right)
      .setParallelism(1)
      .assignTimestampsAndWatermarks(new TimestampAndWatermarkWithOffset[(Long, String)](0))
      .toTable(tEnv, 'a, 'b)
{code}

But when you start the execution, and visualize the execution plan, you can find that the ""from""(the StreamScan) operator's parallelism is 4.

  !image-2020-08-07-21-22-57-843.png! 
",,fsk119,godfreyhe,jark,kezhuw,klion26,libenchao,liupengcheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/20 13:23;liupengcheng;image-2020-08-07-21-22-57-843.png;https://issues.apache.org/jira/secure/attachment/13009289/image-2020-08-07-21-22-57-843.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 04:30:55 UTC 2020,,,,,,,,,,"0|z0hkgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/20 04:27;jark;I think this only happens in legacy planner (maybe this line [1]). Did you try the blink planner? 

[1]: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/StreamScan.scala#L91;;;","12/Aug/20 03:13;liupengcheng;hi [~jark], yes, I only tested the legacy planner, and the bug is at the line you pointed out. I take a look at the blink code, and tested it again, it's ok for blink planner. 

So can I create a PR for the issue in the legacy planner?;;;","12/Aug/20 03:16;jark;Sure. Assigned to you [~liupengcheng];;;","02/Sep/20 04:30;jark;Fixed in master (1.12.0): fccf7ee03b3653fc91ef49d023bc2e316f2cda0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table.to_pandas should handle retraction rows properly,FLINK-18848,13321471,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,07/Aug/20 08:42,12/Aug/20 02:59,13/Jul/23 08:12,11/Aug/20 08:31,1.12.0,,,,,1.12.0,,,API / Python,,,,,0,pull-request-available,,,,,"Currently, the retraction rows are not handled properly and should be removed.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 02:59:00 UTC 2020,,,,,,,,,,"0|z0hk6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/20 08:31;dian.fu;master: 2333d48a0638299f6dcb1b7d866b0e9e37939f4c
release-1.11: 33aa742e5661d05e33cb2d937328cc0aa70b54ad;;;","12/Aug/20 02:59;dian.fu;As Table.execute doesn't support upsert table in 1.11 and so we should not cherry-pick this change to release-1.11 branch. Revert the changes in 1.11 via 0ca2720b13510e3f76cda2942268614304c64916. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"e2e test failed to download ""localhost:9999/flink.tgz"" in ""Wordcount on Docker test""",FLINK-18842,13321422,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,dian.fu,dian.fu,07/Aug/20 02:01,19/Feb/21 07:28,13/Jul/23 08:12,28/Sep/20 09:35,1.11.0,,,,,1.12.0,,,flink-docker,Test Infrastructure,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5260&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9

{code}
2020-08-06T20:51:31.2499580Z [91m+ wget -nv -O flink.tgz localhost:9999/flink.tgz
2020-08-06T20:51:31.2501498Z [0m127.0.0.1 - - [06/Aug/2020 20:51:31] ""GET /flink.tgz HTTP/1.1"" 200 -
2020-08-06T20:51:31.2502214Z [91mfailed: Connection refused.
2020-08-06T20:51:31.6885068Z [0m[91m2020-08-06 20:51:31 URL:http://localhost:9999/flink.tgz [322693675/322693675] -> ""flink.tgz"" [1]
2020-08-06T20:51:31.6888547Z [0m[91m+ [ false = true ]
2020-08-06T20:51:31.6889384Z [0m[91m+ tar -xf flink.tgz --strip-components=1
2020-08-06T20:51:34.8125585Z [0m[91m+ rm flink.tgz
2020-08-06T20:51:34.8699287Z [0m[91m+ chown -R flink:flink .
2020-08-07T00:20:42.7919165Z [0m
2020-08-07T00:20:43.0365895Z ##[error]The operation was canceled.
{code}",,dian.fu,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 28 09:35:58 UTC 2020,,,,,,,,,,"0|z0hjvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/20 18:03;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5392&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9;;;","12/Aug/20 13:49;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5446&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9;;;","07/Sep/20 08:06;rmetzger;[~mapohl] can you look into this test instability and see how we can stabilize it?;;;","08/Sep/20 07:36;mapohl;All three builds reached the upper limit of 260mins (4 hours 20 minutes). That caused the cancellation of the build. Hence, it's not related to the test itself. We have to discuss increasing the limit or improving the run-time of our e2e pipeline.;;;","08/Sep/20 08:41;rmetzger;But the problem is that we are stuck in this test for 3 hours:
{code}
2020-08-06T20:51:34.8699287Z [0m[91m+ chown -R flink:flink .
2020-08-07T00:20:42.7919165Z [0m
2020-08-07T00:20:43.0365895Z ##[error]The operation was canceled.
2020-08-07T00:20:43.0380991Z ##[section]Finishing: Run e2e tests
{code}

I guess this is caused by the download error.

I would agree with you if the previous tests took much more time.;;;","10/Sep/20 07:40;mapohl;I discussed the issue with [~rmetzger]: It is hard to debug since it's happening within the Docker build. It does not have to be related to the {{connection refused}} error that appears since it's also appearing in succeeding builds.
I created a new branch which runs this test multiple times trying to reproduce the problem without success: 
* [Build #38|https://dev.azure.com/mapohl/flink/_build/results?buildId=38&view=logs&j=aba23331-5614-58a0-ecb7-9c9e540c50fe&t=b30be84f-4d76-59f8-beed-6fd3501240e4] - 50 runs without any error
* [Build #39|https://dev.azure.com/mapohl/flink/_build/results?buildId=39&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d] - 100 runs without any error
* [Build #40|https://dev.azure.com/mapohl/flink/_build/results?buildId=40&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d] - 100 runs without any error (the failure was caused by another test that tried to download Kafka (cmp. [FLINK-19158|https://issues.apache.org/jira/browse/FLINK-19158])

I propose to wait for [~chesnay] to have a look at the issue.;;;","10/Sep/20 08:12;rmetzger;Thanks for looking into this. It really seems to be a rare issue.;;;","14/Sep/20 08:52;chesnay;Maybe we could harden the start of the fileserver; we currently just _try_ starting it but don't do any verification that it actually started successfully.

Alternatively, run {{common_docker.sh}} with a timeout + retries.;;;","16/Sep/20 17:04;mapohl;{quote}
Maybe we could harden the start of the fileserver; we currently just try starting it but don't do any verification that it actually started successfully.
{quote}
I tried to reproduce it by simulating a not-started file server. ...without luck. But when going through the logs of the failed builds again, I realized that we can exclude a not-running file server as a cause: Each of the failed builds includes {{127.0.0.1 - - [12/Aug/2020 09:24:06] ""GET /flink.tgz HTTP/1.1"" 200 -}}. Hence, the file server was running.

Next thing to try: timeout + retries.;;;","28/Sep/20 09:35;chesnay;10 minute timeout added (implicitly causing a retry)
master: 51fe48737398af49ddadb41f9459dfe272c22c93;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDTF doesn't work well when the return type isn't generator,FLINK-18836,13321230,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,dian.fu,dian.fu,06/Aug/20 05:30,20/Oct/20 06:36,13/Jul/23 08:12,20/Oct/20 02:27,1.11.0,,,,,1.11.3,1.12.0,,API / Python,,,,,0,pull-request-available,,,,,"For the following Python UDTF which return type is not a generator:
{code}
# test specify the input_types
@udtf(input_types=[DataTypes.BIGINT()],
           result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
def split(x):
    return Row(10, 10, 10)
{code}

When used in a job, the operator containing the UDTF will not emit data to the downstream operator and there is also no exception thrown. The job just finished without any result.

We should properly handle this case: either support this use case or throw a proper exception if we don't want to support this case.",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 06:36:47 UTC 2020,,,,,,,,,,"0|z0hip4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/20 09:46;dian.fu;[~hxbks2ks] could you help to take a look at this issue?;;;","10/Sep/20 09:49;hxbks2ks;[~dian.fu] Thanks a lot for reporting this. I will take a look.;;;","20/Oct/20 02:27;dian.fu;Merged to master via 2208f7731fa1486f9079056ab7dbc78552312729;;;","20/Oct/20 02:28;dian.fu;[~hxbks2ks] Could you submit a PR for release-1.11? It has conflicts with release-1.11 and I think we should also backport this fix to 1.11.;;;","20/Oct/20 03:16;hxbks2ks;[~dian.fu] Thanks a lot for review. I have submitted the PR [https://github.com/apache/flink/pull/13695] for release-1.11.;;;","20/Oct/20 06:36;dian.fu;Merged to release-1.11 via 515e1f3cb0995feb9218fcb3abafcf77257cab78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Could not locate the included file 'generated/rocks_db_native_metric_configuration.html',FLINK-18834,13321215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,06/Aug/20 03:11,06/Aug/20 07:41,13/Jul/23 08:12,06/Aug/20 03:56,1.12.0,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"When I execute the script ./build_docs.sh, it throws the following exception:

{code:java}
Liquid Exception: Could not locate the included file 'generated/rocks_db_native_metric_configuration.html' in any of [""/Users/duanchen/sourcecode/flink/docs/_includes""]. Ensure it exists in one of those directories and, if it is a symlink, does not point outside your site source. in ops/config.md
{code}

It seems `rocks_db_native_metric_configuration` has been renamed to `rocksdb_native_metric_configuration.html`, but it has not been synchronized in ops/config.md
",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18805,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 03:55:08 UTC 2020,,,,,,,,,,"0|z0him0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/20 03:55;dian.fu;master: c59d4b24bf9d20763e16f56b7ef4a4f42bcb0518;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedBlockingSubpartition does not work with StreamTask,FLINK-18832,13321107,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjwang,trohrmann,trohrmann,05/Aug/20 13:56,08/Sep/20 08:10,13/Jul/23 08:12,07/Sep/20 15:02,1.10.1,1.11.1,1.12.0,,,1.10.3,1.11.2,1.12.0,Runtime / Network,Runtime / Task,,,,0,pull-request-available,,,,,"The {{BoundedBlockingSubpartition}} does not work with a {{StreamTask}} because the {{StreamTask}} instantiates an {{OutputFlusher}} which concurrently accesses the {{BoundedBlockingSubpartition}}. This concurrency can lead to a double closing of the underlying {{BufferConsumer}} which manifests in this stack trace:

{code}
[9:15 PM] Caused by: org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0, increment: 1
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.retain0(ReferenceCountUpdater.java:123)
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.retain(ReferenceCountUpdater.java:110)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.retain(AbstractReferenceCountedByteBuf.java:80)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.retainBuffer(NetworkBuffer.java:174)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.retainBuffer(NetworkBuffer.java:47)
	at org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.retainBuffer(ReadOnlySlicedNetworkBuffer.java:127)
	at org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.retainBuffer(ReadOnlySlicedNetworkBuffer.java:41)
	at org.apache.flink.runtime.io.network.buffer.BufferConsumer.build(BufferConsumer.java:108)
	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.writeAndCloseBufferConsumer(BoundedBlockingSubpartition.java:156)
	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.flushCurrentBuffer(BoundedBlockingSubpartition.java:144)
	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.flush(BoundedBlockingSubpartition.java:135)
	at org.apache.flink.runtime.io.network.partition.ResultPartition.flushAll(ResultPartition.java:245)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.flushAll(RecordWriter.java:183)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.flush(RecordWriterOutput.java:156)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.flushOutputs(OperatorChain.java:344)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:602)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:544)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
	at java.lang.Thread.run(Thread.java:748)
{code}",,dwysakowicz,kevin.cyj,kezhuw,klion26,pnowojski,roman,trohrmann,wangm92,wind_ljy,zhuzh,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15750,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 07 15:02:34 UTC 2020,,,,,,,,,,"0|z0hhz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/20 13:57;trohrmann;cc [~pnowojski] and [~zjwang];;;","06/Aug/20 04:14;zjwang;Thanks for reporting this bug [~trohrmann]!

By design I guess the BoundedBlockingSubpartition assumes no concurrent issue for `#flushCurrentBuffer` method. But actually the task thread and flusher thread can touch this method concurrently. I can think of some options for resolving it:

* Disable flusher thread for batch jobs, because it has no benefits for latency concern as the downstream will only request partition after upstream finishes based on current schedule way. Even it would bring harm for upstream writer to spill partial buffer after flush triggered.
* From long term goal, the flusher thread should be delegated by mailbox model, so we can avoid concurrent issue even if the flusher timeout valid for batch jobs.
* Breaks the previous assumption to allow concurrent access of  `BoundedBlockingSubpartition#flushCurrentBuffer`.

If we can realize the second option soon, then we can bypass this bug. I remembered [~pnowojski]already submitted the PR for it before, but have not merged yet. If this way can not be realized in short time, then i prefer the first option to work around. WDYT?;;;","06/Aug/20 07:18;pnowojski;Thanks [~zjwang] for reminding about this older PR of mine (FLINK-15750). There were some issues that I didn't have time to resolve (it was my side project). I can dig out this PR tomorrow, try to rebase it on the latest master and check what issues are remaining to be addressed.

Either way, option number one also sounds fine to me - at least until we have some mixed streaming/batch workloads inside one job - so let's give it a try if I won't make a progress with FLINK-15750;;;","10/Aug/20 16:11;pnowojski;[~trohrmann] [~zjwang] why has this bug appeared, in what scenario? What are the use cases? Do we need to support flusher for the batch jobs?;;;","11/Aug/20 11:56;trohrmann;[~dwysakowicz] reported this issue when running a {{DataStream}} job with lazy scheduling and blocking data exchanges. Not sure whether this configuration can actually occur atm.;;;","11/Aug/20 12:06;dwysakowicz;As far as I can tell currently that can not occur. The Blink batch planner, which is the only user of the lazy scheduling with blocking data exchanges disables the output flashers by setting: {{execEnv.setBufferTimeout(-1);}}. I am working on exposing the ""{{BATCH}}"" behaviour in the DataStream as well but I did not set this parameter, as I was not aware of the consequences. I think it is not a pressing issue if we say it is an illegal combination.
;;;","12/Aug/20 03:30;zjwang;Thanks for the explanation [~dwysakowicz]. 

If so, I think we can bypass this issue if in future we want to unify the DataStream API to simulate batch job with explicitly setBufferTimeout(-1). ATM, the blink planner and BatchTask from DataSet API already explicitly setBufferTimeout(-1) as well.

If nobody has other concerns, I will close this ticket now. And if we want to support buffer timeout for batch job in future, we can focus on it then.;;;","12/Aug/20 07:40;pnowojski;+1

[~zjwang] should we maybe add a check state somewhere in the code, to fail early with a nice error message ""buffer timeout can not be used with blocking subpartition""? Ideally somewhere in the {{BoundedSubpartitionXXX}} constructor, but I'm not sure if we have there an easy access to the task's configuration.;;;","13/Aug/20 07:39;zjwang;[~pnowojski] Agree with you proposal. It would be nice to give some friendly message for the current limitation. I would submit the PR for it.;;;","07/Sep/20 15:02;zjwang;Merged in master: 13e0b355d1d9ba513671de1638d3c35edb6b96a3
Merged in release-1.11: 11edb3dd47999bcad94c93a204cd20fdd5360b41
Merged in release-1.10: 85c181745855db4396f28c53eb6b478040c902cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty client retry mechanism may cause PartitionRequestClientFactory#createPartitionRequestClient to wait infinitely,FLINK-18821,13321023,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,TsReaper,TsReaper,05/Aug/20 06:58,13/Aug/20 09:10,13/Jul/23 08:12,12/Aug/20 08:30,1.10.1,1.11.1,1.12.0,,,1.10.2,1.11.2,1.12.0,Runtime / Network,,,,,0,pull-request-available,,,,,"When running TPCDS 10T benchmark on Flink I found some of the task slots stuck. After some investigation there seems to be a bug in {{PartitionRequestClientFactory}}.

When a task tries to require a partition of data from its upstream task but fails, {{PartitionRequestClientFactory#connect}} will throw {{RemoteTransportException}} and {{PartitionRequestClientFactory#connectWithRetries}} will throw {{CompletionException}}. However this exception is not caught by {{PartitionRequestClientFactory#connect}} and it will eventually fail the task.

But {{PartitionRequestClientFactory}} lives in a task manager not in a task. In {{PartitionRequestClientFactory}} a {{ConcurrentHashMap}} named {{clients}} is maintained for reusing {{NettyPartitionRequestClient}}. When the above exception happens, {{clients}} is not cleaned up; When the next call to {{PartitionRequestClientFactory#connect}} with the same connection id comes, it will use the invalid {{CompletableFuture}} in {{clients}} and this future will never complete, causing the task to stuck forever.

Exception stack:
{code}
2020-08-05 03:37:07,539 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory [] - Failed 1 times to connect to <host-name>/<ip>:<port>
org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '<host-name>/<ip>:<port>' has failed. This might indicate that the remote task manager has been lost.
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:120) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:99) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:76) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:146) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:329) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:301) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:95) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.requestPartitions(StreamTask.java:514) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.readRecoveredChannelState(StreamTask.java:484) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:475) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:834) [?:1.8.0_102]
Caused by: java.lang.NullPointerException
	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.<init>(NettyPartitionRequestClient.java:73) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:114) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 16 more
{code}
",,godfreyhe,kevin.cyj,kezhuw,klion26,roman,TsReaper,ym,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 08:30:20 UTC 2020,,,,,,,,,,"0|z0hhgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/20 07:02;TsReaper;cc. [~ZhenqiuHuang], [~pnowojski], [~roman_khachatryan], [~zjwang].;;;","05/Aug/20 07:28;kevin.cyj;I also notice this bug, the issue occurs when tcp channel multiplexing exists and the connection setup fails.;;;","05/Aug/20 09:15;roman;I'm able to reproduce this issue with an older version too if the error occurs in nettyClient.connect (before adding ConnectingChannel listener).

Added 1.10 and 1.11 to affected versions.;;;","05/Aug/20 09:34;roman;[~TsReaper], [~kevin.cyj],

can you confirm that this issue is also present in 1.10?

 

I reproduced it using the following simple unit test and old PartitionRequestClientFactory:
{code:java}
@Test(expected = IOException.class)
public void testFailureReportedToSubsequentRequests() throws Exception {
  PartitionRequestClientFactory factory = new PartitionRequestClientFactory(new FailingNettyClient(), 2);
  try {
    factory.createPartitionRequestClient(new ConnectionID(new InetSocketAddress(InetAddress.getLocalHost(), 8080), 0));
  } catch (Exception e) {
    // expected
  }
  factory.createPartitionRequestClient(new ConnectionID(new InetSocketAddress(InetAddress.getLocalHost(), 8080), 0));
}{code}
https://github.com/apache/flink/blob/e3ad391b48b7ffc4a5ebd3717e0f99808522cb28/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientFactoryTest.java#L78;;;","05/Aug/20 12:23;kevin.cyj;[~roman_khachatryan] I think you are right, the issue is also present in 1.10.;;;","05/Aug/20 13:54;roman;Thanks for the confirmation [~kevin.cyj] and for reporting [~TsReaper].

I created a [PR|https://github.com/apache/flink/pull/13067], your feedback is welcomed.;;;","12/Aug/20 08:30;zjwang;Merged in release-1.10: e581d7f8abeac8951735afeb8d3b90977bf10c0a
Merged in release-1.11: ded539e85444f9c8d34f88da31784775ed281aba
Merged in master: 33bdc978a059c50ec55b8f24d40039d13a6b78e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceOperator should send MAX_WATERMARK to downstream operator when closed,FLINK-18820,13320999,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,05/Aug/20 03:06,29/Oct/20 14:22,13/Jul/23 08:12,29/Oct/20 14:22,,,,,,1.12.0,,,API / DataStream,,,,,0,pull-request-available,,,,,"SourceOperator should send MAX_WATERMARK to the downstream operator when closed. Otherwise, the watermark of the downstream operator may not advance. ",,dwysakowicz,jark,kezhuw,libenchao,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 14:22:41 UTC 2020,,,,,,,,,,"0|z0hhb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/20 14:22;dwysakowicz;Fixed in fada6fb6ac9fd7f6510f1f2d77b6baa06563e222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HadoopRenameCommitterHDFSTest.testCommitOneFile[Override: false] failed with ""java.io.IOException: The stream is closed""",FLINK-18818,13320991,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,dian.fu,dian.fu,05/Aug/20 01:51,19/Feb/21 07:32,13/Jul/23 08:12,01/Oct/20 09:29,1.12.0,,,,,1.11.3,1.12.0,,Connectors / FileSystem,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5177&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91

{code}
2020-08-04T20:56:51.1835382Z [ERROR] testCommitOneFile[Override: false](org.apache.flink.formats.hadoop.bulk.committer.HadoopRenameCommitterHDFSTest)  Time elapsed: 0.046 s  <<< ERROR!
2020-08-04T20:56:51.1835950Z java.io.IOException: The stream is closed
2020-08-04T20:56:51.1836413Z 	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:118)
2020-08-04T20:56:51.1836867Z 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
2020-08-04T20:56:51.1837313Z 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
2020-08-04T20:56:51.1837712Z 	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
2020-08-04T20:56:51.1838116Z 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
2020-08-04T20:56:51.1838527Z 	at org.apache.hadoop.hdfs.DataStreamer.closeStream(DataStreamer.java:987)
2020-08-04T20:56:51.1838974Z 	at org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:839)
2020-08-04T20:56:51.1839404Z 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:834)
2020-08-04T20:56:51.1839775Z 	Suppressed: java.io.IOException: The stream is closed
2020-08-04T20:56:51.1840184Z 		at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:118)
2020-08-04T20:56:51.1840641Z 		at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
2020-08-04T20:56:51.1841087Z 		at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
2020-08-04T20:56:51.1841509Z 		at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
2020-08-04T20:56:51.1841910Z 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
2020-08-04T20:56:51.1842207Z 		... 3 more
{code}",,aljoscha,dian.fu,gaoyunhaii,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13228,,,,,,,,,,,"28/Sep/20 06:40;gaoyunhaii;image-2020-09-28-14-40-28-859.png;https://issues.apache.org/jira/secure/attachment/13012216/image-2020-09-28-14-40-28-859.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 09:29:21 UTC 2020,,,,,,,,,,"0|z0hh9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/20 10:52;gaoyunhaii;I'll have a look at this issue.;;;","28/Sep/20 01:07;dian.fu;HadoopRenameCommitterHDFSTest.testCommitMultipleFilesMixed has also failed with the same error: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7008&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91;;;","28/Sep/20 06:43;gaoyunhaii;The detailed exception of the error is attached in the image.

From the detail log & exception, it seems we have met with the same issue we have met before: [FLINK-13228|https://issues.apache.org/jira/browse/FLINK-13228]. From the discussion of the FLINK-13228 it seems that the exception happens after the file is successfully written, and we should ignore the exception. 

I would open a PR for this fix. ;;;","01/Oct/20 09:29;aljoscha;master: 6cfdd9b8a993e8abd2a39d0123dca061adb6dbb4
release-1.11: 495defe2df4633acaafb47e86a8787540519b4a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractCloseableRegistryTest.testClose unstable,FLINK-18815,13320920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kezhuw,rmetzger,rmetzger,04/Aug/20 18:19,19/Feb/21 07:32,13/Jul/23 08:12,02/Oct/20 16:20,1.10.1,1.11.1,1.12.0,,,1.11.3,1.12.0,,FileSystems,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5164&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0
{code}
[ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.509 s <<< FAILURE! - in org.apache.flink.core.fs.SafetyNetCloseableRegistryTest
[ERROR] testClose(org.apache.flink.core.fs.SafetyNetCloseableRegistryTest)  Time elapsed: 1.15 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<-1>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.core.fs.AbstractCloseableRegistryTest.testClose(AbstractCloseableRegistryTest.java:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}
",,dian.fu,kevin.cyj,kezhuw,klion26,rmetzger,trohrmann,yunta,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18321,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 02 16:20:16 UTC 2020,,,,,,,,,,"0|z0hgu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/20 17:22;kezhuw;I think this is caused by {{SafetyNetCloseableRegistry.PhantomDelegatingCloseableRef#close}}. It unconditionally closes underlying {{innerCloseable}} without checking whether it was closed or unregistered before. Suppose that:
 # At some point, {{innerCloseable}}'s wrapping closeable was finalized and become phantom reference, its phantom reference was pushed to {{SafetyNetCloseableRegistry#REAPER_THREAD#referenceQueue}}.
 # The closeable registry containing {{innerCloseable}} was closed thus {{innerCloseable}} got closed and {{AbstractCloseableRegistry#closeableToRef}} got cleared.

 # {{SafetyNetCloseableRegistry#REAPER_THREAD}} executes {{SafetyNetCloseableRegistry.PhantomDelegatingCloseableRef#close}} which close {{innerCloseable}} unconditionally.

If we change {{Thread.sleep(2)}} from {{AbstractCloseableRegistryTest.ProducerThread}} to {{Thread.sleep(0)}}, then {{AbstractCloseableRegistryTest#testClose}} fails quite often.

Though {{Closeable#close}} declares itself be idempotent, but it does not specify any thread-safe guarantee, so I think this should be treated as bug and thus need to fix.

I think this could be fixed by closing {{SafetyNetCloseableRegistry.PhantomDelegatingCloseableRef#innerCloseable}} iff pair of {{innerCloseable}} and {{PhantomDelegatingCloseableRef}} was successfully from {{AbstractCloseableRegistry#closeableToRef}}.

I think this should be sufficient to fix concurrent removing/closing of {{Closeable}} in {{SafetyNetCloseableRegistry}} due to following observation:
 # There are only four removing paths: {{SafetyNetCloseableRegistry#close}}, {{ClosingFSDataInputStream#close}}, {{SafetyNetCloseableRegistry#unregisterCloseable}} and {{PhantomDelegatingCloseableRef#close}}.
 # I think the first three paths by design should have no concurrent contention.
 # {{PhantomDelegatingCloseableRef#close}} has no concurrent contention with {{SafetyNetCloseableRegistry#unregisterCloseable}} and {{ClosingFSDataInputStream#close}} since it is phantom reference of later removing/closing stream.

[~rmetzger] [~dianfu] [~aljoscha] [~srichter] Any opinions on this ? If we tend to fix this, could I take over it ?;;;","11/Aug/20 08:29;trohrmann;Hi [~kezhuw], thanks a lot for this detailed analysis. I think it is correct. As you suggested the proper fix would be to let {{PhantomDelegatingCloseableRef}} call {{innerCloseable.close()}} only iff {{innerCloseable}} could be successfully removed from the closeable registry via {{closeableRegistry.removeCloseableInternal(innerCloseable)}}.

Please go ahead and provide a fix for it. I've assigned this ticket to you.;;;","13/Aug/20 09:01;trohrmann;Fixed via

master: 1ff126617fa230604c8e2457d053286472208fde
1.11.2: 3549001b13bc14f170cad4b1427fd04ae9a93bcf
1.10.2: 69c5553206bf722175e919f0d35a9b16373729bf
;;;","03/Sep/20 02:31;dian.fu;This issue reproduced on master:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6116&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034];;;","03/Sep/20 11:49;kevin.cyj;Another instance: [https://dev.azure.com/kevin-flink/flink/_build/results?buildId=77&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374];;;","03/Sep/20 16:27;kezhuw;Seems that recently two cases are leaking while previous cases are duplicated closing. I think these two cases are caused by {{SafetyNetCloseableRegistry.close}} which interrupt reaper thread. Suppose that:
 1. A closeable became phantom reachable and queued in {{CloseableReaperThread.referenceQueue}} but did not get a chance to close.
 2. {{SafetyNetCloseableRegistry.close}} calls {{CloseableReaperThread.interrupt}} which set {{CloseableReaperThread.running}} to false and interrupt that java thread.
 3. {{CloseableReaperThread}} terminates due to false {{CloseableReaperThread.running}} or {{InterruptedException}}.
 4. That enqueued closeable leaks.

I think there are two different approaches to fix this issue:
 * Use at most one {{CloseableReaperThread}}, and don't close it. This may cause leaking if Flink is embedded as guest in other host application.
 * Count registered phantom references, and close reaper thread only if all registered phantom references are popped and {{CloseableReaperThread}} is dropped by caller.

Since Flink is not an end stop application, I think the counting approach maybe more appropriate ?

As a analogy, {{java.lang.ref.Cleaner}} has no close-like method, it [tracks all registered referents|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/java.base/share/classes/jdk/internal/ref/PhantomCleanable.java#L65], its underlying thread will terminate after [itself|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/java.base/share/classes/jdk/internal/ref/CleanerImpl.java#L101] and [all registered references|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/java.base/share/classes/jdk/internal/ref/CleanerImpl.java#L133] are cleaned.

[~kevin.cyj] [~dian.fu] [~trohrmann] Any thoughts ?;;;","04/Sep/20 01:48;dian.fu;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6180&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=b33fdd4f-3de5-542e-2624-5d53167bb672];;;","04/Sep/20 11:54;trohrmann;Thanks for the analysis [~kezhuw]. I am not totally convinced that what you describe is the problem we are seeing here. The problem with your explanation is that {{SafetyNetCloseableRegistry.close()}} should explicitly close all registered closeables which are still contained in {{AbstractCloseableRegistry.closeableToRef}}. Hence, it should not matter whether the reaper thread still have some phantom references enqueued in its {{referenceQueue}}. Once {{close()}} is called, no further {{Closeables}} should be able to register at the registry.

The only way I could see this problem happening is after we have removed the closeable from {{closeableRegistry}} in {{PhantomDelegatingCloseableRef.close()}} we interrupt the {{innerCloseable.close()}} call (https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/core/fs/SafetyNetCloseableRegistry.java#L183). However, in the test implementation we are using {{TestStream}} as {{Closeable}} and the {{close()}} implementation is not blocking.;;;","04/Sep/20 18:01;kezhuw;[~trohrmann] Your are right, I made mistake. After revise, I found {{AbstractCloseableRegistryTest.testClose}} itself is faulty for {{SafetyNetCloseableRegistry}}. The failed assertion could happens before {{innerCloseable.close}}. This could be easily verified by place endless loop before {{innerCloseable.close}}.

I think we can fix this with following changes:
 1. Join {{CloseableReaperThread}} before assertion for {{SafetyNetCloseableRegistryTest}}.  This should fix recently failed cases.
 2. Replace {{CloseableReaperThread.interrupt}} with {{CloseableReaperThread.stop}}, eg. set {{CloseableReaperThread.running}} to false without interrupting {{CloseableReaperThread}}. Replace {{CloseableReaperThread.referenceQueue.remove()}} with {{CloseableReaperThread.referenceQueue.remove(long timeout)}}. This prevent blocking {{Closeable.close}} from interruption, though I think it is not a must since {{Closeable.close}} are not allowed to throw {{InterruptedException}} in spec. It is a friendly improvement for registered {{Closeable}}.;;;","02/Oct/20 16:20;trohrmann;Fixed via 

1.12.0: 7a50e3e4ab77a896ac9f5d0d6308d1ca560d7eac
1.11.3: 5e4e96465c5a5898c222037139eb41a263e0e254;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigDocsGenerator does not handle multiple upper characters properly,FLINK-18805,13320639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,03/Aug/20 10:30,13/Apr/21 20:41,13/Jul/23 08:12,05/Aug/20 10:27,,,,,,1.12.0,,,Runtime / Configuration,,,,,0,pull-request-available,,,,,"When presented with a name containing multiple upper characters in sequence, like ""JMXOptions"", the generator does not properly convert this sequence, resulting in ""j_mx_options"".

I'd like to make a small change such that the conversions work like this:
JMXOptions -> jmx_options
RocksDBOptions -> rocksdb_options",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18834,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 10:27:58 UTC 2020,,,,,,,,,,"0|z0hf40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/20 10:27;chesnay;master: e5b648b08a7d9b25f496a3a09a304cec17a0edfc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisProducer.backpressureLatch should be volatile,FLINK-18796,13320514,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,roman,roman,roman,01/Aug/20 21:50,07/Aug/20 06:40,13/Jul/23 08:12,07/Aug/20 02:43,1.11.1,,,,,1.12.0,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,,"(confirm first)

 

cc: [~rmetzger]",,klion26,rmetzger,roman,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 06:40:12 UTC 2020,,,,,,,,,,"0|z0hec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/20 15:41;rmetzger;[~tzulitai] or [~thw] I was mentioned in this ticket, but I haven't looked at the FlinkKinesisProducer for four years :) ;;;","03/Aug/20 20:37;chesnay;Why would this be a problem _now_? We've used this pattern for years.;;;","03/Aug/20 21:26;roman;I'm not sure whether it's actually a problem, maybe there is some implicit memory fence after assignment.

But if it *is* a problem I'm also not sure how does it manifest itself, as it does not necessarily fail the sink; and sink is not exactly-once.

 

Which pattern do you mean?

 ;;;","05/Aug/20 18:31;chesnay;A callback accessing internal state initialized within open() without synchronization. The CassandraSinkBase does the same.;;;","05/Aug/20 21:02;roman;CassandraSinkBase is a bit different in that it starts other threads after assigning this state, so there is a HB relationship.

I'm also not sure that there is any noticeable performance difference, as there are already volatile accesses there (and eventually IO).;;;","05/Aug/20 21:18;chesnay;Isn't the same true for the FlinkKinesisProducer? The KinesisProducer internally uses a ThreadPoolExecutor to complete the callbacks, which creates threads lazily when required, so until we start processing data (i.e., call invoke), no thread has been created yet.;;;","07/Aug/20 02:43;zjwang;Merged in master: 33bdc978a059c50ec55b8f24d40039d13a6b78e7;;;","07/Aug/20 06:40;roman;During the offline discussion, we agreed that making field volatile makes sense; caching it would be overkill, and there should be no visible performance impact. 

 

Besides Kinesis and Cassandra, there is only one such sink: FlinkKafkaProducerBase.flushOnCheckpoint.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regresssion on 2020.07.29 in some state backend benchmarks,FLINK-18786,13320386,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,31/Jul/20 11:08,31/Aug/20 16:59,13/Jul/23 08:12,31/Aug/20 16:59,1.12.0,,,,,,,,Benchmarks,Runtime / State Backends,,,,0,,,,,,"E.g.

[http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=stateBackends.FS&env=2&revs=200&equid=off&quarts=on&extr=on]

[http://codespeed.dak8s.net:8000/timeline/?ben=stateBackends.ROCKS&env=2]

[http://codespeed.dak8s.net:8000/timeline/?ben=stateBackends.ROCKS&env=2#/?exe=1,3&ben=stateBackends.ROCKS_INC&env=2&revs=200&equid=off&quarts=on&extr=on]

[http://codespeed.dak8s.net:8000/timeline/?ben=slidingWindow&env=2] 

 

See attachment for some hints",,dian.fu,klion26,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/20 11:10;roman;statebackend-fs-maybe-regression.png;https://issues.apache.org/jira/secure/attachment/13008828/statebackend-fs-maybe-regression.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 09:46:50 UTC 2020,,,,,,,,,,"0|z0hdjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/20 15:23;roman;The same symptoms as in FLINK-18684: regression in some tests observed over time, but can't be reproduced retroactively:
{code:java}
(older commit) 4b25ba27922e1e7c4a17067fc94b69c5eac4ad2d
""MemoryStateBackendBenchmark.stateBackends"",""thrpt"",1,30,4562.465426,100.369594,""ops/ms"",,,MEMORY
""MemoryStateBackendBenchmark.stateBackends"",""thrpt"",1,30,4182.523419,79.437423,""ops/ms"",,,FS
""MemoryStateBackendBenchmark.stateBackends"",""thrpt"",1,30,4584.939673,152.263596,""ops/ms"",,,FS_ASYNC{code}
{code:java}
(newer commit) 27ecf02fb2a8e61b68e93190080f540e53aca171
""MemoryStateBackendBenchmark.stateBackends"",""thrpt"",1,30,4570.875302,141.128996,""ops/ms"",,,MEMORY
""MemoryStateBackendBenchmark.stateBackends"",""thrpt"",1,30,4221.538372,93.620711,""ops/ms"",,,FS
""MemoryStateBackendBenchmark.stateBackends"",""thrpt"",1,30,4575.985146,115.443546,""ops/ms"",,,FS_ASYNC{code}
Running locally:
{code:java}
(older commit) 4b25ba2
Benchmark (stateBackend) Mode Cnt Score Error Units
MemoryStateBackendBenchmark.stateBackends MEMORY thrpt 30 3072.503 ± 58.403 ops/ms
MemoryStateBackendBenchmark.stateBackends FS thrpt 30 2910.727 ± 32.738 ops/ms
MemoryStateBackendBenchmark.stateBackends FS_ASYNC thrpt 30 3145.262 ± 41.423 ops/ms{code}
{code:java}
(newer commit) 3ab1a1c66772320a0901ea085cfeaa6bf161bf9a
Benchmark (stateBackend) Mode Cnt Score Error Units
MemoryStateBackendBenchmark.stateBackends MEMORY thrpt 30 3117.415 ± 36.979 ops/ms
MemoryStateBackendBenchmark.stateBackends FS thrpt 30 2917.314 ± 40.304 ops/ms
MemoryStateBackendBenchmark.stateBackends FS_ASYNC thrpt 30 3062.132 ± 102.320 ops/ms{code}
 

 ;;;","05/Aug/20 09:46;roman;Two more regressions (3 days later - from Aug 3):

[http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=arrayKeyBy&env=2&revs=200&equid=off&quarts=on&extr=on]

[http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=mapRebalanceMapSink&env=2&revs=200&equid=off&quarts=on&extr=on] 

 
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""compile_cron_scala212"" failed to compile",FLINK-18776,13320304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,dian.fu,dian.fu,31/Jul/20 02:06,31/Jul/20 06:57,13/Jul/23 08:12,31/Jul/20 06:57,1.12.0,,,,,1.12.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5060&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065

{code}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-avro-confluent-registry ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7
Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21
Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21
Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0
Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21
Found Banned Dependency: org.apache.flink:flink-table-api-java-bridge_2.11:jar:1.12-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-table-runtime-blink_2.11:jar:1.12-SNAPSHOT
Found Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21
Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0
Found Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.12-SNAPSHOT
Found Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1
Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6
Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.12-SNAPSHOT
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 06:57:10 UTC 2020,,,,,,,,,,"0|z0hd1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/20 02:07;dian.fu;e2e_cron_scala212 also failed:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5060&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=ffa46b9d-687d-5a4a-3e77-010e0da138d3;;;","31/Jul/20 02:16;dian.fu;I think it should be caused by commit: https://github.com/apache/flink/commit/1c09c23810cf844001dd70d3b78a7a60b49611c7

It seems that we hardcoded the Scala version as 2.11 in this commit.

cc [~danny0405] ;;;","31/Jul/20 06:57;dian.fu;Fixed via 34f63be897bf328cbef79051a934328b0b383ab7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Kerberized YARN per-job on Docker test"" failed with ""Client cannot authenticate via:[TOKEN, KERBEROS]""",FLINK-18771,13320172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,dian.fu,dian.fu,30/Jul/20 11:46,31/Jul/20 02:10,13/Jul/23 08:12,30/Jul/20 12:13,1.12.0,,,,,1.12.0,,,Deployment / YARN,Tests,,,,0,,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5047&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-07-30T11:18:39.9217453Z java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: ""worker1.docker-hadoop-cluster-network/172.19.0.5""; destination host is: ""master.docker-hadoop-cluster-network"":9000; 
{code}",,dian.fu,guoyangze,kkl0u,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 02:10:03 UTC 2020,,,,,,,,,,"0|z0hc8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/20 12:12;kkl0u;This was a mistake on my side when merging. I will merge it directly without a PR because the problem was simply that i merged an earlier version of the contributor's PR.;;;","30/Jul/20 12:13;kkl0u;The commit is a0227e20430ee9eaff59464023de2385378f71ea for the fix and it is already on the master.;;;","30/Jul/20 12:13;kkl0u;Merged with a0227e20430ee9eaff59464023de2385378f71ea;;;","30/Jul/20 12:15;dian.fu;Thanks [~kkl0u] for the quick fix!;;;","30/Jul/20 14:26;kkl0u;[~dian.fu] Please let me know if my fix does not solve the problem.;;;","31/Jul/20 02:10;dian.fu;Thanks [~kkl0u], the problem should have been addressed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniBatch doesn't work with FLIP-95 source,FLINK-18769,13320159,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,nkruber,nkruber,30/Jul/20 10:22,27/Aug/20 07:57,13/Jul/23 08:12,05/Aug/20 04:33,1.11.1,,,,,1.11.2,1.12.0,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"The following Table API streaming job is stuck when enabling mini batching

{code}
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    EnvironmentSettings settings =
        EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);

    // disable mini-batching completely to get a result
    Configuration tableConf = tableEnv.getConfig()
        .getConfiguration();
    tableConf.setString(""table.exec.mini-batch.enabled"", ""true"");
    tableConf.setString(""table.exec.mini-batch.allow-latency"", ""5 s"");
    tableConf.setString(""table.exec.mini-batch.size"", ""5000"");
    tableConf.setString(""table.optimizer.agg-phase-strategy"", ""TWO_PHASE"");

    tableEnv.executeSql(
        ""CREATE TABLE input_table (""
            + ""location STRING, ""
            + ""population INT""
            + "") WITH (""
            + ""'connector' = 'kafka', ""
            + ""'topic' = 'kafka_batching_input', ""
            + ""'properties.bootstrap.servers' = 'localhost:9092', ""
            + ""'format' = 'csv', ""
            + ""'scan.startup.mode' = 'earliest-offset'""
            + "")"");

    tableEnv.executeSql(
        ""CREATE TABLE result_table WITH ('connector' = 'print') LIKE input_table (EXCLUDING OPTIONS)"");

    tableEnv
        .from(""input_table"")
        .groupBy($(""location""))
        .select($(""location"").cast(DataTypes.CHAR(2)).as(""location""), $(""population"").sum().as(""population""))
        .executeInsert(""result_table"");
{code}

I am using a pre-populated Kafka topic called {{kafka_batching_input}} with these elements:
{code}
""Berlin"",1
""Berlin"",2
{code}",,godfreyhe,jark,leslieyuan,libenchao,nkruber,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 04:33:05 UTC 2020,,,,,,,,,,"0|z0hc5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/20 04:33;jark;Fixed in 
- master (1.12.0): 94b23885ca34927e37334fce51b930933cfd79dd
- 1.11.2: 770a43580de3125aeac6bf81769e438753697fee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlValidatorException thrown when select from a view which contains a UDTF call,FLINK-18750,13319913,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,zhongwei,zhongwei,29/Jul/20 07:13,20/Oct/20 12:29,13/Jul/23 08:12,25/Aug/20 12:24,1.11.1,,,,,1.11.2,1.12.0,,Table SQL / API,,,,,0,pull-request-available,,,,,"When executing such code:

 
{code:java}
package com.example;

import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.functions.TableFunction;

public class TestUTDF {

   public static class UDTF extends TableFunction<String> {

      public void eval(String input) {
         collect(input);
      }
   }

   public static void main(String[] args) {

      StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
      StreamTableEnvironment tEnv = StreamTableEnvironment.create(
         env, EnvironmentSettings.newInstance().useBlinkPlanner().build());
      tEnv.createTemporarySystemFunction(""udtf"", new UDTF());
      tEnv.createTemporaryView(""source"", tEnv.fromValues(""a"", ""b"", ""c"").as(""f0""));
      String udtfCall = ""SELECT S.f0, T.f1 FROM source as S, LATERAL TABLE(udtf(f0)) as T(f1)"";
      System.out.println(tEnv.explainSql(udtfCall));
      String createViewCall = ""CREATE VIEW tmp_view AS"" + udtfCall;
      tEnv.executeSql(createViewCall);
      System.out.println(tEnv.from(""tmp_view"").explain());
   }
}
{code}
Such a SqlValidatorException would be thrown:

 

 
{code:java}
== Abstract Syntax Tree ==== Abstract Syntax Tree ==LogicalProject(f0=[$0], f1=[$1])+- LogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{0}])   :- LogicalProject(f0=[AS($0, _UTF-16LE'f0')])   :  +- LogicalValues(tuples=[[{ _UTF-16LE'a' }, { _UTF-16LE'b' }, { _UTF-16LE'c' }]])   +- LogicalTableFunctionScan(invocation=[udtf($cor0.f0)], rowType=[RecordType(VARCHAR(2147483647) EXPR$0)])
== Optimized Logical Plan ==Correlate(invocation=[udtf($cor0.f0)], correlate=[table(udtf($cor0.f0))], select=[f0,EXPR$0], rowType=[RecordType(CHAR(1) f0, VARCHAR(2147483647) EXPR$0)], joinType=[INNER])+- Calc(select=[f0])   +- Values(type=[RecordType(CHAR(1) f0)], tuples=[[{ _UTF-16LE'a' }, { _UTF-16LE'b' }, { _UTF-16LE'c' }]])
== Physical Execution Plan ==Stage 1 : Data Source content : Source: Values(tuples=[[{ _UTF-16LE'a' }, { _UTF-16LE'b' }, { _UTF-16LE'c' }]])
 Stage 2 : Operator content : Calc(select=[f0]) ship_strategy : FORWARD
 Stage 3 : Operator content : Correlate(invocation=[udtf($cor0.f0)], correlate=[table(udtf($cor0.f0))], select=[f0,EXPR$0], rowType=[RecordType(CHAR(1) f0, VARCHAR(2147483647) EXPR$0)], joinType=[INNER]) ship_strategy : FORWARDException in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 4, column 14 to line 4, column 17: Column 'f0' not found in any table at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$ToRelContextImpl.expandView(FlinkPlannerImpl.scala:204) at org.apache.calcite.plan.ViewExpanders$1.expandView(ViewExpanders.java:52) at org.apache.flink.table.planner.catalog.SqlCatalogViewTable.convertToRel(SqlCatalogViewTable.java:58) at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.expand(ExpandingPreparingTable.java:59) at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.toRel(ExpandingPreparingTable.java:55) at org.apache.calcite.rel.core.RelFactories$TableScanFactoryImpl.createScan(RelFactories.java:533) at org.apache.calcite.tools.RelBuilder.scan(RelBuilder.java:1044) at org.apache.calcite.tools.RelBuilder.scan(RelBuilder.java:1068) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:349) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:152) at org.apache.flink.table.operations.CatalogQueryOperation.accept(CatalogQueryOperation.java:69) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:149) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:131) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:92) at org.apache.flink.table.operations.CatalogQueryOperation.accept(CatalogQueryOperation.java:69) at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.scala:165) at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:82) at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:80) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:80) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:43) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:654) at org.apache.flink.table.api.internal.TableImpl.explain(TableImpl.java:575) at com.example.TestUTDF.main(TestUTDF.java:39)Caused by: org.apache.calcite.runtime.CalciteContextException: From line 4, column 14 to line 4, column 17: Column 'f0' not found in any table at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089) at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:259) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5882) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1785) at org.apache.calcite.sql.SqlOperator.constructArgTypeList(SqlOperator.java:609) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:236) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303) at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ... 31 moreCaused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'f0' not found in any table at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550) ... 72 more{code}
 
 After some debugging I think it is because the keyword ""lateral"" was discarded after executing the create view operation.",,danny0405,dwysakowicz,godfreyhe,jark,JinxinTang,kaibo.zhou,libenchao,lincoln.86xy,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19735,,,,,FLINK-19108,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 12:24:30 UTC 2020,,,,,,,,,,"0|z0hanc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/20 08:19;JinxinTang;Seems interesting, nice reproduce code, I will check this too.;;;","31/Jul/20 12:41;danny0405;The root cause is that, the `LATERAL` keyword of the View expanded query was lost, CALCITE-4077 (release 1.24.0) actually fixed this problem, while we can fix this first in Flink.;;;","25/Aug/20 12:24;dwysakowicz;Fixed in:
* master:
** 69fa07ec5c11c59499fc79a03323228e373ca0a9
* 1.11.2:
** 78bd8c8c57be3dee2dff765cea6175a3c8dfc59c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct dependencies in Kubernetes pom,FLINK-18749,13319893,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,29/Jul/20 05:50,05/Aug/20 08:58,13/Jul/23 08:12,05/Aug/20 08:58,1.11.1,1.12.0,,,,1.11.2,1.12.0,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"Inspired when developing this PR[1], i find some unused dependency(e.g. {{com.github.mifmif:generex:1.0.2}}) in flink-kubernetes/pom.xml. It will be good if could remove it.

 

[1]. [https://github.com/apache/flink/pull/12995/commits/8519f65321ba24c5164196a67a05d98fb268f490]",,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 08:58:24 UTC 2020,,,,,,,,,,"0|z0haiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/20 12:04;trohrmann;Thanks for reporting this issue [~fly_in_gis]. Is this also a problem for the {{1.12.0}}/master branch? I've assigned the ticket to you to fix it.;;;","04/Aug/20 11:11;wangyang0918;[~trohrmann] Yes, it is also a issue for master branch. I have opened a PR.;;;","05/Aug/20 02:15;wangyang0918;Hi [~trohrmann], I notice that you have changed the {{Fix Version}} to 1.12.0, 1.11.2. And I am not sure whether we should add 1.12.0 since 1.12.0 technically never had the bug if we fix it before releasing.

 

https://issues.apache.org/jira/browse/FLINK-18256?focusedCommentId=17133285&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17133285;;;","05/Aug/20 08:01;trohrmann;This should be fine. It basically says that we have fixed it in the master before {{1.12.0}} has been released. That way we also keep track of the things we want to fix for the {{1.12.0}} release.;;;","05/Aug/20 08:58;trohrmann;Fixed via

master: 8ece02fec6206c01bc73003ec681641bc2d0e973
1.11.2: 18c6f8580ac606a1c51411e7d264bd9ddef076a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint would be queued unexpected if pendingCheckpoints less than maxConcurrentCheckpoints,FLINK-18748,13319892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wayland,klion26,klion26,29/Jul/20 05:32,04/Aug/20 08:01,13/Jul/23 08:12,04/Aug/20 08:00,1.11.0,1.11.1,,,,1.11.2,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"Inspired by a [user-zh email|http://apache-flink.147419.n8.nabble.com/flink-1-11-rest-api-saveppoint-td5497.html]

After FLINK-17342, when triggering a checkpoint/savepoint, we'll check whether the request can be triggered in {{CheckpointRequestDecider#chooseRequestToExecute}}, the logic is as follow:
{code:java}
Preconditions.checkState(Thread.holdsLock(lock));
// 1. 
if (isTriggering || queuedRequests.isEmpty()) {
   return Optional.empty();
}

// 2 too many ongoing checkpoitn/savepoint
if (pendingCheckpointsSizeSupplier.get() >= maxConcurrentCheckpointAttempts) {
   return Optional.of(queuedRequests.first())
      .filter(CheckpointTriggerRequest::isForce)
      .map(unused -> queuedRequests.pollFirst());
}

// 3 check the timestamp of last complete checkpoint
long nextTriggerDelayMillis = nextTriggerDelayMillis(lastCompletionMs);
if (nextTriggerDelayMillis > 0) {
   return onTooEarly(nextTriggerDelayMillis);
}

return Optional.of(queuedRequests.pollFirst());
{code}
But if currently {{pendingCheckpointsSizeSupplier.get()}} < {{maxConcurrentCheckpointAttempts}}, and the request is a savepoint, the savepoint will still wait some time in step 3. 

I think we should trigger the savepoint immediately if {{pendingCheckpointSizeSupplier.get()}} < {{maxConcurrentCheckpointAttempts}}.",,klion26,pnowojski,roman,wayland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 08:01:24 UTC 2020,,,,,,,,,,"0|z0haio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/20 05:33;klion26;[~pnowojski] [~roman_khachatryan] What do you think about this problem, If this is valid, I can help to fix it.;;;","29/Jul/20 07:08;roman;Thanks [~klion26],

From the code above I see in the mentioned case (2):

.filter(CheckpointTriggerRequest::isForce)

which means that forced request *will* be triggered.

As savepoints *are* forced (unless running in unaligned checkpoints) this means that savepoint request *will* be triggered, even if pendingCheckpoints >= maxConcurrentCheckpoins.;;;","30/Jul/20 02:42;klion26;Hi [~roman_khachatryan] thanks for your reply. If {{pendingCheckpointsSizeSupplier.get() >= maxConcurrentCheckpointAttempts}} , then we would not do step 2, but if {{pendingCheckpointSizeSupplier.get() < maxConcurrentCheckpointAttempts}} we'll skip case 2, just check the min pause time in case 3. but in this case, I think the savepoint should be triggered.

If we have {{maxConcurrentCheckpointAttempts}} == 1, and now there is no ongoing checkpoint/savepoint. the user triggers a savepoint, then we will skip case 2 in the description. in case 3 we just check the min pause time, so we'll wait some time.

 ;;;","30/Jul/20 07:41;roman;Thanks for the clarification [~klion26],

in onTooEarly there is a check too, that if it's a forced request, it will be executed immediately:
{code:java}
private Optional<CheckpointTriggerRequest> onTooEarly(long nextTriggerDelayMillis) {
    CheckpointTriggerRequest first = queuedRequests.first();
    if (first.isForce()) {
        return Optional.of(queuedRequests.pollFirst());
{code};;;","30/Jul/20 12:42;roman;I see your point [~klion26]: in UC mode, savepoint can wait unnecessarily.

I think this can be easily fixed by calling onTooEarly only for periodic requests.;;;","30/Jul/20 13:21;klion26;[~wayland] Do you want to fix this problem?;;;","30/Jul/20 13:28;wayland;[~klion26] Yes, I'd like to fix this.   It's my pleasure.;;;","31/Jul/20 03:18;klion26;[~roman_khachatryan] could you please help to assign this ticket to [~wayland], thanks.
[~wayland] Please make sure you have read the [contribute documentation|https://flink.apache.org/contributing/how-to-contribute.html] before implementation. and you can ping us for review after filed a PR. thanks.;;;","31/Jul/20 05:40;wayland;[~klion26] Ok, I have read the doc, thanks.

I have some thoughts on this issue:

If pendingCheckpointSizeSupplier.get() < maxConcurrentCheckpointAttempts, we'll skip step 2, and if nextTriggerDelayMillis > 0,  onTooEarly will be called.

 

 
{code:java}
private Optional<CheckpointTriggerRequest> onTooEarly(long nextTriggerDelayMillis) {
   CheckpointTriggerRequest first = queuedRequests.first();

   // first.isForce() will be false if running in unaligned checkpoints mode
   if (first.isForce()) {
      return Optional.of(queuedRequests.pollFirst());
   } else if (first.isPeriodic) {
   // first.isPeriodic will be false too if the savepoint is trigger by user

      queuedRequests.pollFirst().completeExceptionally(new CheckpointException(MINIMUM_TIME_BETWEEN_CHECKPOINTS));
      rescheduleTrigger.accept(nextTriggerDelayMillis);
      return Optional.empty();
   } else {
   
   // we will get Optional.empty()
      return Optional.empty();
   }
}{code}
 

In this case, the onTooEarly function will return Optional.empty() if a user triggers a savepoint to early while the job is running in unaligned checkpoints mode.

I think this is the reason why some savepoint gets stuck at IN_PROGRESS state.

 

Maybe I'll change onTooEarly to this:

 
{code:java}
private Optional<CheckpointTriggerRequest> onTooEarly(long nextTriggerDelayMillis) {
   CheckpointTriggerRequest first = queuedRequests.first();
   if (first.isPeriodic) {
      queuedRequests.pollFirst().completeExceptionally(new CheckpointException(MINIMUM_TIME_BETWEEN_CHECKPOINTS));
      rescheduleTrigger.accept(nextTriggerDelayMillis);
      return Optional.empty();
   } 
   return Optional.of(queuedRequests.pollFirst());
}

{code}
By the way, I think nextTriggerDelayMillis will not cost any time, so there is no necessary to change any code in chooseRequestToExecute.

 

 ;;;","31/Jul/20 09:17;roman;Thanks for your analysis and proposal [~wayland],

I think nextTriggerDelayMillis might actually be quite heavy, as it involves a system call.

Why not skip this call at all if the request is periodic (and not forced) in chooseRequestToExecute?;;;","31/Jul/20 11:35;wayland;[~roman_khachatryan] thanks for your advice, I will skip nextTriggerDelayMillis.  

Maybe what you want to say is:  skip this call if the request is forced or is not periodic.

I will return Optional.of(queuedRequests.pollFirst()) immediately if the queuedRequests.first() is forced or is not periodic.

 ;;;","31/Jul/20 16:26;roman;Exactly, thanks for the clarification [~wayland].;;;","02/Aug/20 17:25;wayland;Hi, [~klion26] [~roman_khachatryan]: I have create a pull_request, would you like to review this: [https://github.com/apache/flink/pull/13045]?

thanks.;;;","04/Aug/20 07:52;pnowojski;merged to master as e422d42e09^ and e422d42e09;;;","04/Aug/20 08:00;pnowojski;merged to release-1.11 as 3835007356^ 3835007356;;;","04/Aug/20 08:01;pnowojski;Thanks for the contribution [~wayland] and for the reviews [~klion26] [~roman_khachatryan];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowStaggerTest.testWindowStagger failed,FLINK-18746,13319868,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,dian.fu,dian.fu,29/Jul/20 01:48,29/Jul/20 10:27,13/Jul/23 08:12,29/Jul/20 10:27,1.12.0,,,,,1.12.0,,,API / DataStream,,,,,0,,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4975&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392

{code}
2020-07-28T21:16:30.1350624Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.145 s <<< FAILURE! - in org.apache.flink.streaming.runtime.operators.windowing.WindowStaggerTest
2020-07-28T21:16:30.1352065Z [ERROR] testWindowStagger(org.apache.flink.streaming.runtime.operators.windowing.WindowStaggerTest)  Time elapsed: 0.012 s  <<< FAILURE!
2020-07-28T21:16:30.1352701Z java.lang.AssertionError
2020-07-28T21:16:30.1353104Z 	at org.junit.Assert.fail(Assert.java:86)
2020-07-28T21:16:30.1353810Z 	at org.junit.Assert.assertTrue(Assert.java:41)
2020-07-28T21:16:30.1354289Z 	at org.junit.Assert.assertTrue(Assert.java:52)
2020-07-28T21:16:30.1354914Z 	at org.apache.flink.streaming.runtime.operators.windowing.WindowStaggerTest.testWindowStagger(WindowStaggerTest.java:38)
2020-07-28T21:16:30.1355520Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-07-28T21:16:30.1356060Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-07-28T21:16:30.1356663Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-07-28T21:16:30.1357220Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-07-28T21:16:30.1357775Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-07-28T21:16:30.1358383Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-07-28T21:16:30.1358986Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-07-28T21:16:30.1359623Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-07-28T21:16:30.1360187Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-07-28T21:16:30.1360740Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-07-28T21:16:30.1361364Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-07-28T21:16:30.1361916Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-07-28T21:16:30.1362432Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-07-28T21:16:30.1362976Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-07-28T21:16:30.1363516Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-07-28T21:16:30.1364041Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-07-28T21:16:30.1364568Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-07-28T21:16:30.1365139Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-07-28T21:16:30.1365764Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-07-28T21:16:30.1366413Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-07-28T21:16:30.1367036Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-07-28T21:16:30.1367671Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-07-28T21:16:30.1368337Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-07-28T21:16:30.1368956Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-07-28T21:16:30.1369530Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,aljoscha,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 10:27:33 UTC 2020,,,,,,,,,,"0|z0hadc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/20 10:27;aljoscha;The problem was that WindowStagger.RANDOM is inclusive on the start and exclusive on the end of the interval while the test was assuming that the start is also exclusive.

master: 6efa8393be1dd7b648ad3c612c6df22068b601d2

cc [~TengHu];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The monotonicity of UNIX_TIMESTAMP function is not correct,FLINK-18731,13319541,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,27/Jul/20 12:27,18/Nov/20 08:33,13/Jul/23 08:12,18/Nov/20 08:33,,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Currently, the monotonicity of {{UNIX_TIMESTAMP}} function is always {{INCREASING}}, actually, when it has empty function arguments ({{UNIX_TIMESTAMP()}}, is equivalent to {{NOW()}}), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))",,godfreyhe,jark,kezhuw,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 08:33:58 UTC 2020,,,,,,,,,,"0|z0h8co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/20 12:27;godfreyhe;I would like to fix this;;;","18/Nov/20 08:33;jark;Fixed in master (1.12.0):
 - 8c301ba850d2ddc11a59802e4b916a9dcd8b42d9
 - 60906fa9c3217c4f36af2a9295ed9c19d5073606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceProfileInfo is not serializable,FLINK-18710,13319215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,24/Jul/20 18:39,14/Sep/20 14:46,13/Jul/23 08:12,26/Jul/20 14:12,1.11.0,,,,,1.11.2,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,{{ResourceProfileInfo}} should be {{Serializable}} because it is sent as an RPC payload.,,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 26 14:12:10 UTC 2020,,,,,,,,,,"0|z0h6c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/20 14:12;trohrmann;Fixed via

master: ec8a08eb7aefb837b8c9befddad825ba2459500a
1.11.2: b86ddff71c71452102f3ee908db6c0ddc23240c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct,FLINK-18708,13319183,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,24/Jul/20 14:25,24/Jul/20 15:41,13/Jul/23 08:12,24/Jul/20 15:41,1.11.1,1.12.0,,,,1.11.2,1.12.0,,Connectors / Kafka,Documentation,,,,0,pull-request-available,,,,,The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 15:41:10 UTC 2020,,,,,,,,,,"0|z0h654:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/20 15:41;dian.fu;master: ce24bf79a4e67133edc350d35015f1f1fb79963c
release-1.11: 699ffb4bcf604354d2c8b7429099bf759ae95053;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debezium-JSON throws NPE when tombstone message is received,FLINK-18705,13319159,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,24/Jul/20 11:30,31/Jul/20 11:49,13/Jul/23 08:12,31/Jul/20 11:49,1.11.0,1.11.1,,,,1.11.2,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,,,,,,"By default, Debezium will send two messages to Kafka for DELETE operation, one for delete message, the other for tombstone message (message value is null). However, debezium-json will throw NPE when processing such tombstone message. We should just skip such messages. 


As a workaround, we can diable tombstone on Debezium Connect {{tombstones.on.delete: false}}.",,jark,leonard,morsapaes,swhelan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/20 11:28;jark;Screen Shot 2020-07-24 at 09.35.56.png;https://issues.apache.org/jira/secure/attachment/13008350/Screen+Shot+2020-07-24+at+09.35.56.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 11:49:27 UTC 2020,,,,,,,,,,"0|z0h5zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/20 11:49;jark;Fixed in
 - master (1.12.0): c22c9c1f1090bf8e0bae7c3609c678cb7532f9df
 - 1.11.2: 4927c7dc23258b88c7331d64de162ff70fbd30ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink elasticsearch connector leaks threads and classloaders thereof,FLINK-18702,13319132,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qinjunjerry,qinjunjerry,qinjunjerry,24/Jul/20 09:33,07/Aug/20 18:51,13/Jul/23 08:12,07/Aug/20 18:51,1.10.0,1.10.1,,,,1.10.2,,,Connectors / ElasticSearch,,,,,0,,,,,,"Flink elasticsearch connector leaking threads and classloaders thereof.  This results in OOM Metaspace when ES sink fails and restarted many times. 

This issue is visible in Flink 1.10 but not in 1.11 because Flink 1.11 does not create new class loaders in case of recoveries (FLINK-16408)

 

Reproduction:
 * Start a job with ES sink in a Flink 1.10 cluster, without starting the ES cluster.

 

 ",,guoyangze,jark,klion26,nkruber,Paul Lin,qinjunjerry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 13:40:30 UTC 2020,,,,,,,,,,"0|z0h5ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/20 02:02;guoyangze;I think it is indeed the same issue as FLINK-11205. Not sure does it make sense to backport Flink-16408. As a workaround, you could increase the taskmanager.memory.jvm-metaspace.size[1].

[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#taskmanager-memory-jvm-metaspace-size;;;","27/Jul/20 08:17;qinjunjerry;Thanks Yangze for the update.

Increasing Metaspace will only delay the OOM.

I've checked the heap dump, there were as many ChildFirstClassLoaders in the heap as the number of restarts. Those ChildFirstClassLoader could not be cleaned up because of some threads (e.g., I/O dispatcher n, pool-n-thread-m) were still running.   It is likely to have the same cause as [Flink-13689|https://issues.apache.org/jira/browse/FLINK-13689]. I will give another try and verify. ;;;","27/Jul/20 13:23;qinjunjerry;Verified: [this PR|https://github.com/apache/flink/pull/10936] from  Flink-13689 indeed fixes the issue. ;;;","05/Aug/20 09:31;qinjunjerry;Created PR: https://github.com/apache/flink/pull/13068;;;","05/Aug/20 09:51;guoyangze;Hi, [~qinjunjerry]. I think it would be good to backport that patch to 1.10. Just not sure what is the correct procedure to do it. The PR LGTM.;;;","07/Aug/20 13:40;qinjunjerry;This has been merged into master. See: [https://github.com/apache/flink/pull/13068]

Closing now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debezium-json format throws Exception when PG table's IDENTITY config is not FULL,FLINK-18700,13319117,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,leonard,leonard,24/Jul/20 08:15,31/Jul/20 11:48,13/Jul/23 08:12,31/Jul/20 11:48,1.11.0,1.11.1,,,,1.11.2,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"If the cdc data comes from a PG table and the PG table's IDENTITY config is not FULL，

the UPDATE changelog's update message will be null lead a NPE error.

We can add document to remind user set proper  IDENTITY for PG table in debezium doc

and improve the Exception message.

 

It's reported in [http://apache-flink.147419.n8.nabble.com/flink-1-11-cdc-td5298.html]",,jark,leonard,morsapaes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 11:48:22 UTC 2020,,,,,,,,,,"0|z0h5qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/20 11:48;jark;Fixed in
 - master (1.12.0): 26c2289838fd983a6dc576b81307aeb2b48c3391
 - 1.11.2: d7c71ebc62c93ddb6ad17a15117c763a0eb80b37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding flink-table-api-java-bridge_2.11 to a Flink job kills the IDE logging,FLINK-18697,13319033,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,rmetzger,rmetzger,23/Jul/20 19:05,24/Jul/20 11:34,13/Jul/23 08:12,24/Jul/20 11:34,1.11.0,,,,,1.11.2,1.12.0,,Table SQL / API,,,,,0,pull-request-available,,,,,"Steps to reproduce:
- Set up a Flink project using a Maven archetype
- Add ""flink-table-api-java-bridge_2.11"" as a dependency
- Running Flink won't produce any log output

Probable cause:
""flink-table-api-java-bridge_2.11"" has a dependency to ""org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.11.0"", which contains a ""log4j2-test.properties"" file.

When I disable Log4j2 debugging (with ""-Dlog4j2.debug""), I see the following line:
{code}
DEBUG StatusLogger Reconfiguration complete for context[name=3d4eac69] at URI jar:file:/Users/robert/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.11.0/flink-streaming-java_2.11-1.11.0-tests.jar!/log4j2-test.properties (org.apache.logging.log4j.core.LoggerContext@568bf312) with optional ClassLoader: null
{code}

",,jark,libenchao,rmetzger,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 11:34:42 UTC 2020,,,,,,,,,,"0|z0h57s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 21:44;chesnay;So this is just missing the test scope?;;;","24/Jul/20 03:04;jark;I think so.;;;","24/Jul/20 11:34;jark;- master: a3b38c3e5cb9f9a7c408ef387ff788bf38d0e721
- release-1.11: ce270cd53630a176cc41098a1fee91bed0a274e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProjectionCodeGenerator#generateProjectionExpression should remove for loop optimization,FLINK-18688,13318934,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,TsReaper,TsReaper,TsReaper,23/Jul/20 12:42,10/Aug/20 08:09,13/Jul/23 08:12,10/Aug/20 08:09,1.11.0,,,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"If too many fields of the same type are projected, {{ProjectionCodeGenerator#generateProjectionExpression}} currently performs a ""for loop optimization"" which, instead of generating code separately for each field, they'll be squashed into a for loop.

However, if the indices of the fields with the same type are not continuous, this optimization will not write fields in index ascending order. This is not acceptable because {{BinaryWriter}} expects the users to write fields in index ascending order (that is to say, we *have to* first write field 0, then field 1, then...), otherwise the variable length area of the two binary rows with same data might be different. Although we can use {{getXX}} methods of {{BinaryRow}} to get the fields correctly, states for streaming jobs compare state keys with binary bits, not with the contents of the keys. So we need to make sure the binary bits of the binary rows be the same if two rows contain the same data.

What's worse, as the current implementation of {{ProjectionCodeGenerator#generateProjectionExpression}} uses a scala {{HashMap}}, the key order of the map might be different on different workers; Even if the projection does not meet the condition to be optimized, it will still be affected by this bug.

What I suggest is to simply remove this optimization. Because if we still want this optimization, we have to make sure that the fields of the same type have continuous order, which is a very strict and rare condition.",,dian.fu,jark,libenchao,lincoln.86xy,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18687,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 10 08:09:10 UTC 2020,,,,,,,,,,"0|z0h4ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 13:38;jark;It seems that FLINK-18687 is a duplicate issue?;;;","10/Aug/20 08:09;lzljs3620320;master: 6d72cc0e35f41c4e1e28b57406e2cadd60c8fccf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobClient.getAccumulators() blocks until streaming job has finished in local environment,FLINK-18685,13318912,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,rmetzger,rmetzger,23/Jul/20 11:43,28/May/21 08:14,13/Jul/23 08:12,27/Jan/21 15:11,1.11.0,,,,,1.13.0,,,API / DataStream,,,,,0,pull-request-available,starter,,,,"*Steps to reproduce:*
{code:java}
JobClient client = env.executeAsync(""Test"");

CompletableFuture<JobStatus> status = client.getJobStatus();
LOG.info(""status = "" + status.get());

CompletableFuture<Map<String, Object>> accumulators = client.getAccumulators(StreamingJob.class.getClassLoader());
LOG.info(""accus = "" + accumulators.get(5, TimeUnit.SECONDS));
{code}

*Actual behavior*
The accumulators future will never complete for a streaming job when calling this just in your main() method from the IDE.

*Expected behavior*
Receive the accumulators of the running streaming job.
The JavaDocs of the method state the following: ""Accumulators can be requested while it is running or after it has finished."". 
While it is technically true that I can request accumulators, I was expecting as a user that I can access the accumulators of a running job.
Also, I can request accumulators if I submit the job to a cluster.",,aljoscha,echauchot,rmetzger,ZhuShang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 16:38:55 UTC 2021,,,,,,,,,,"0|z0h4gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 11:44;rmetzger;I see two solutions
a) change the Javadocs
b) Change the {{PerJobMiniClusterJobClient}} to allow accessing the accumulators before the job has finished.;;;","23/Jul/20 13:16;aljoscha;For me the only option would be to fix the behaviour of the minicluster client. The API is clearly meant for asynchronous use.;;;","23/Jul/20 14:53;rmetzger;Okay, thanks for confirming the issue. I won't have time to work on this in the near future.;;;","14/Aug/20 12:16;ZhuShang;hi,[~rmetzger] 
i'm interested in this issue,can you assign to me? thanks;;;","14/Aug/20 12:18;rmetzger;Thanks. I assigned you.;;;","17/Aug/20 09:54;ZhuShang;hi [~rmetzger], you didn't assign this to me and have a code review when it is convenient for you.Thanks;;;","17/Aug/20 18:50;rmetzger;Sorry, I fixed the assignment.;;;","25/Aug/20 07:37;rmetzger;Just for the record: Some discussion on this ticket has happened here as well: https://github.com/apache/flink/pull/13162;;;","30/Nov/20 08:51;echauchot;Hi [~rmetzger] I'd like to take this ticket. I saw that after your change requests in the PR , the author closed the PR. I'd like to get this ticket fixed. Can you assign the ticket to me ?;;;","02/Dec/20 07:28;rmetzger;Thanks a lot! I assigned you.;;;","02/Dec/20 10:41;echauchot;Thanks [~rmetzger] for the assignment. I'm always glad to help !;;;","27/Jan/21 15:11;rmetzger;Resolved in https://github.com/apache/flink/commit/beae8e7a20dfa915ce02fd3e5186d4b5e71fbbef.

Thanks a lot for the fix Etienne!;;;","27/Jan/21 16:38;echauchot;[~rmetzger] my pleasure ! thanks for reviewing/merging ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector orc reader cannot read Hive 2.0.0 table,FLINK-18682,13318876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,23/Jul/20 09:07,12/Aug/20 07:31,13/Jul/23 08:12,12/Aug/20 07:31,1.11.1,,,,,1.11.2,,,Connectors / Hive,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,,"Trying to read a Hive 2.0.0 ORC table fails with:
{noformat}
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector
	at org.apache.flink.orc.OrcSplitReaderUtil.lambda$genPartColumnarRowReader$0(OrcSplitReaderUtil.java:83)
	at org.apache.flink.orc.OrcColumnarRowSplitReader.<init>(OrcColumnarRowSplitReader.java:65)
	at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:91)
	at org.apache.flink.connectors.hive.read.HiveVectorizedOrcSplitReader.<init>(HiveVectorizedOrcSplitReader.java:78)
	at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:137)
	at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:66)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:85)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 10 more
{noformat}",,godfreyhe,hk__lrzy,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 07:31:48 UTC 2020,,,,,,,,,,"0|z0h48w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 09:32;lzljs3620320;What do you mean about trying to read a Hive 2.0.0 ORC table? With Hive 2.0.0 dependencies?;;;","10/Aug/20 06:34;hk__lrzy;could you show your dependency version about hive-exec? `TimestampColumnVector` class added in version 2.0.1。;;;","10/Aug/20 08:19;lirui;[~hk__lrzy] The issue happens with Hive 2.0.0;;;","12/Aug/20 07:31;lzljs3620320;master: 808ec56bf9487f09178429c9045e51b48d1dc9c5

release-1.11: 06d5ce548d1b98901ae50848d152fd38692fafef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderRetrievalService does not invalidate leader in case of SUSPENDED connection,FLINK-18677,13318775,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,trohrmann,trohrmann,22/Jul/20 21:16,28/Aug/21 11:16,13/Jul/23 08:12,05/Aug/20 09:38,1.10.1,1.11.1,1.12.0,,,1.10.2,1.11.2,1.12.0,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The {{ZooKeeperLeaderRetrievalService}} does not invalidate the leader if the ZooKeeper connection gets SUSPENDED. This means that a {{TaskManager}} won't cancel its running tasks even though it might miss a leader change. I think we should at least make it configurable whether in such a situation the leader listener should be informed about the lost leadership. Otherwise, we might run into the situation where an old and a newly recovered instance of a {{Task}} can run at the same time.",,felixzheng,kezhuw,klion26,mapohl,Ming Li,Paul Lin,tison,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18451,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 14:22:11 UTC 2021,,,,,,,,,,"0|z0h3mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/20 15:20;mapohl;We were able to reproduce the behavior by submitting a long-running task (i.e. windowJoin example). By killing the StandaloneSession daemon and shortly afterwards stopping the ZooKeeper, we got into a situation where the TaskManager was not informed by the JobManager about the ZooKeeper connection being suspended.

The task kept processing data even though the suspended ZooKeeper connection was recognized (and logged out) by the TaskManager.;;;","31/Jul/20 07:07;trohrmann;Good work [~mapohl]. According to FLINK-10052 the {{ZooKeeperLeaderElectionService}} will revoke the leadership of the leader if the connection is {{SUSPENDED}}. Hence, as a first step I believe it would make sense to make this behaviour on the {{ZooKeeperLeaderRetrievalService}} side symmetric.;;;","05/Aug/20 09:38;trohrmann;Fixed via

master: cf8b882af788d467f43643d22d82a6a221860e24
1.11.2: 1c66bfa74dc88ea1b0733c529715dc434003e867
1.10.2: d954278bb1dc1672fedcd2de745bfe8f5bad6e34;;;","19/Apr/21 14:18;tison;It won't miss leader change because new leader will have a different leader session id.;;;","19/Apr/21 14:22;tison;But I agree that we can do that if the zk is lost because NodeCache won't trigger at this case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Scala code examples for UDF type inference annotations,FLINK-18672,13318705,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,fhueske,fhueske,22/Jul/20 12:48,23/Jul/20 08:04,13/Jul/23 08:12,23/Jul/20 08:04,,,,,,1.11.2,1.12.0,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"The Scala code examples for the [UDF type inference annotations|https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#type-inference] are not correct.

For example: the following {{FunctionHint}} annotation 

{code:scala}
@FunctionHint(
  input = Array(@DataTypeHint(""INT""), @DataTypeHint(""INT"")),
  output = @DataTypeHint(""INT"")
)
{code}


needs to be changed to

{code:scala}
@FunctionHint(
  input = Array(new DataTypeHint(""INT""), new DataTypeHint(""INT"")),
  output = new DataTypeHint(""INT"")
)
{code}",,fhueske,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 08:04:23 UTC 2020,,,,,,,,,,"0|z0h374:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/20 08:04;twalthr;Fixed in 1.12.0: eab5e72d14f8c010a53ef596f8117dc32677e7a5
Fixed in 1.11.2: 2aa66b7c8f77567ff0ea2ecd6137f9dab700b785;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BytesHashMap#growAndRehash should release newly allocated segments before throwing the exception,FLINK-18668,13318621,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,22/Jul/20 06:42,29/Jul/20 17:38,13/Jul/23 08:12,29/Jul/20 09:44,1.11.0,,,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"In {{BytesHashMap#growAndRehash}} we have the following code.

{code:java}
List<MemorySegment> newBucketSegments = new ArrayList<>(required);

try {
	int numAllocatedSegments = required - memoryPool.freePages();
	if (numAllocatedSegments > 0) {
		throw new MemoryAllocationException();
	}
	int needNumFromFreeSegments = required - newBucketSegments.size();
	for (int end = needNumFromFreeSegments; end > 0; end--) {
		newBucketSegments.add(memoryPool.nextSegment());
	}

	setBucketVariables(newBucketSegments);
} catch (MemoryAllocationException e) {
	LOG.warn(""BytesHashMap can't allocate {} pages, and now used {} pages"",
			required, reservedNumBuffers);
	throw new EOFException();
}
{code}

Newly allocated memory segments are temporarily stored in {{newBucketSegments}} before giving to the hash table. But if a {{MemoryAllocationException}} happens, these segments are not returned to the memory pool, causing the following exception stack trace.

{code}
java.lang.RuntimeException: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
        at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.growAndRehash(BytesHashMap.java:393) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.append(BytesHashMap.java:313) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at HashAggregateWithKeys$360.processElement(Unknown Source) ~[?:?]
        at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:161) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:178) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:153) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:345) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:560) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:834) [?:1.8.0_102]
        Suppressed: java.lang.RuntimeException: Should return all used memory before clean, page used: 2814
                at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.close(LazyMemorySegmentPool.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.free(BytesHashMap.java:486) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.free(BytesHashMap.java:475) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at HashAggregateWithKeys$360.close(Unknown Source) ~[?:?]
                at org.apache.flink.table.runtime.operators.TableStreamOperator.dispose(TableStreamOperator.java:44) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:707) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:687) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:626) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:542) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at java.lang.Thread.run(Thread.java:834) [?:1.8.0_102]
Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
        at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:229) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        ... 15 more
Caused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 16777216 bytes, only 0 bytes are remaining
        at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:159) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:85) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:227) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        ... 15 more
{code}

We should first return these segments to the memory pool before throwing the exception.",,libenchao,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 09:44:54 UTC 2020,,,,,,,,,,"0|z0h2og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/20 09:44;lzljs3620320;master: 2658510ff08056d0123835976d3f8f13f226e3ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem connector should use TableSchema exclude computed columns,FLINK-18665,13318508,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,jark,jark,21/Jul/20 15:06,23/Jul/20 05:35,13/Jul/23 08:12,23/Jul/20 02:15,1.11.1,,,,,1.11.2,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"This is reported in http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/How-to-use-a-nested-column-for-CREATE-TABLE-PARTITIONED-BY-td36796.html


{code}
create table navi (
  a STRING,
  location ROW<lastUpdateTime BIGINT, transId STRING>
) with (
  'connector' = 'filesystem',
  'path' = 'east-out',
  'format' = 'json'
)
CREATE TABLE output (
  `partition` AS location.transId
) PARTITIONED BY (`partition`)
WITH (
  'connector' = 'filesystem',
  'path' = 'east-out',
  'format' = 'json'
) LIKE navi (EXCLUDING ALL)
tEnv.sqlQuery(""SELECT type, location FROM navi"").executeInsert(""output"")
{code}

It throws the following exception 


{code}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: The field count of logical schema of the table does not match with the field count of physical schema
. The logical schema: [STRING,ROW<`lastUpdateTime` BIGINT, `transId` STRING>]
The physical schema: [STRING,ROW<`lastUpdateTime` BIGINT, `transId` STRING>,STRING].
{code}


The reason is that {{FileSystemTableFactory#createTableSource}} should use schema excluded computed column, not the original catalog table schema.

[1]: https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java#L78",,godfreyhe,jark,klion26,leonard,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 05:35:41 UTC 2020,,,,,,,,,,"0|z0h1zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/20 15:13;leonard;[~jark] 
 I'd like to work for this, could you assign this to me?;;;","22/Jul/20 05:39;klion26;Which versions will this issue affect?;;;","22/Jul/20 07:14;leonard;[~klion26] 
This issue affects 1.11.0 and 1.11.1, will fix in 1.11.2 and 1.11.0;;;","23/Jul/20 02:15;lzljs3620320;master: 675ace47d71ff482a38de66f05e98c53deaef47a

release-1.11: 4e8e542ba2999fc65bae913cfa8141b4d9de9c1b;;;","23/Jul/20 05:35;klion26;[~Leonard Xu] thanks for the update :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestServerEndpoint may prevent server shutdown,FLINK-18663,13318476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tartarus,tartarus,tartarus,21/Jul/20 11:11,12/Aug/20 13:00,13/Jul/23 08:12,03/Aug/20 20:22,1.10.0,1.10.1,1.11.0,,,1.10.2,1.11.2,1.12.0,Runtime / REST,,,,,0,pull-request-available,,,,,"AbstractHandler throw NPE cause by FlinkHttpObjectAggregator is null

when rest throw exception, it will do this code
{code:java}
private CompletableFuture<Void> handleException(Throwable throwable, ChannelHandlerContext ctx, HttpRequest httpRequest) {
	FlinkHttpObjectAggregator flinkHttpObjectAggregator = ctx.pipeline().get(FlinkHttpObjectAggregator.class);
	int maxLength = flinkHttpObjectAggregator.maxContentLength() - OTHER_RESP_PAYLOAD_OVERHEAD;
	if (throwable instanceof RestHandlerException) {
		RestHandlerException rhe = (RestHandlerException) throwable;
		String stackTrace = ExceptionUtils.stringifyException(rhe);
		String truncatedStackTrace = Ascii.truncate(stackTrace, maxLength, ""..."");
		if (log.isDebugEnabled()) {
			log.error(""Exception occurred in REST handler."", rhe);
		} else {
			log.error(""Exception occurred in REST handler: {}"", rhe.getMessage());
		}
		return HandlerUtils.sendErrorResponse(
			ctx,
			httpRequest,
			new ErrorResponseBody(truncatedStackTrace),
			rhe.getHttpResponseStatus(),
			responseHeaders);
	} else {
		log.error(""Unhandled exception."", throwable);
		String stackTrace = String.format(""<Exception on server side:%n%s%nEnd of exception on server side>"",
			ExceptionUtils.stringifyException(throwable));
		String truncatedStackTrace = Ascii.truncate(stackTrace, maxLength, ""..."");
		return HandlerUtils.sendErrorResponse(
			ctx,
			httpRequest,
			new ErrorResponseBody(Arrays.asList(""Internal server error."", truncatedStackTrace)),
			HttpResponseStatus.INTERNAL_SERVER_ERROR,
			responseHeaders);
	}
}
{code}
but flinkHttpObjectAggregator some case is null,so this will throw NPE,but this method called by  AbstractHandler#respondAsLeader
{code:java}
requestProcessingFuture
	.whenComplete((Void ignored, Throwable throwable) -> {
		if (throwable != null) {
			handleException(ExceptionUtils.stripCompletionException(throwable), ctx, httpRequest)
				.whenComplete((Void ignored2, Throwable throwable2) -> finalizeRequestProcessing(finalUploadedFiles));
		} else {
			finalizeRequestProcessing(finalUploadedFiles);
		}
	});
{code}
 the result is InFlightRequestTracker Cannot be cleared.

so the CompletableFuture does‘t complete that handler's closeAsync returned

!C49A7310-F932-451B-A203-6D17F3140C0D.png!

!e18e00dd6664485c2ff55284fe969474.png!

 ",,guoyangze,Jiangang,klion26,tartarus,trohrmann,yungthuis@hotmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18902,,,,,,,,,,,,,"21/Jul/20 13:15;tartarus;110.png;https://issues.apache.org/jira/secure/attachment/13008092/110.png","21/Jul/20 13:16;tartarus;111.png;https://issues.apache.org/jira/secure/attachment/13008093/111.png","21/Jul/20 11:15;tartarus;C49A7310-F932-451B-A203-6D17F3140C0D.png;https://issues.apache.org/jira/secure/attachment/13008077/C49A7310-F932-451B-A203-6D17F3140C0D.png","21/Jul/20 11:15;tartarus;e18e00dd6664485c2ff55284fe969474.png;https://issues.apache.org/jira/secure/attachment/13008078/e18e00dd6664485c2ff55284fe969474.png","28/Jul/20 03:12;tartarus;jobmanager.log.noyarn.tar.gz;https://issues.apache.org/jira/secure/attachment/13008529/jobmanager.log.noyarn.tar.gz",,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 20:22:54 UTC 2020,,,,,,,,,,"0|z0h1sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/20 11:35;chesnay;It would be good to understand why the {{FlinkHttpObjectAggregator}} was null. My understanding is that this should only happen if netty is being shut down, but that on the other hand should only happen after all handlers have shut down.

Have you made any modifications to Flink? Can you tell us more about the exact circumstances under which this occurred?;;;","21/Jul/20 13:16;tartarus;[~chesnay] thanks for your reply.

 

This problem does not occur frequently, but when the JM load is too high, such as GC，call /jobs/overview maybe happen exception
{code:java}
2020-07-21 12:39:21,458 WARN  org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler[flink-scheduler-1]  - ##### requestProcessingFuture url = /jobs/overview, throwable
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-919763123]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that
 the recipient actor didn't send a reply.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:878)
        at akka.dispatch.OnComplete.internal(Future.scala:263)
        at akka.dispatch.OnComplete.internal(Future.scala:261)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
        at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)
        at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
        at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
        at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
        at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
        at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
        at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-919763123]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't sen
d a reply.
        at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
        at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
        at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
        ... 9 more
{code}
there seem to be few changes to netty, but FLINK-16626.

We found that the problem was indeed when canceling the flink job.

there are some debug info,  Handling exceptions under normal circumstances !110.png!

When abnormal

!111.png!

 

 ;;;","22/Jul/20 06:20;tartarus;[~chesnay] Here, {{FlinkHttpObjectAggregator}} is only used to obtain maxContentLength, so why not pass {{RestHandlerConfiguration}} or {{RestServerEndpointConfiguration}} as construction parameters to AbstractHandler;;;","22/Jul/20 07:23;tartarus;[~chesnay] There is a separate monitoring service that requests /jobs/overview every 5 seconds;;;","23/Jul/20 02:16;tartarus;The request received by akka seems to be single-threaded. Since flink has 20000 tasks, it is suspected that the timeout exception has a certain relationship with this request;;;","24/Jul/20 07:30;trohrmann;Thanks for reporting this issue [~tartarus]. I agree with Chesnay that it would great to better understand why this can happen. Apparently, you can reproduce the problem. Would it be possible for you to add a logging statement in the first line of {{AbstractHandler.handleException}} which logs the handled exception. Moreover, it would be great if you could share the debug logs of such a run in order to better understand what is happening. To better understand what's happening in the shut down, it would be helpful if you could add {{log.info(""Shutting RestServerEndpoint down internally"")}} as the first line of {{RestServerEndpoint.shutDownInternal()}}. Thanks a lot for you help!;;;","24/Jul/20 07:31;trohrmann;I might actually also be helpful if you could set the logger to the {{TRACE}} level.;;;","24/Jul/20 07:38;trohrmann;One suspicion I have is the following: When calling {{AbstractHandler.respondAsLeader}} we don't check whether the handler has been terminated or not. Hence the following might happen:

1. We receive a REST request and we call into {{AbstractHandler.channelRead0}} (inherited from {{LeaderRetrievalHandler}})
2. The {{RestServerEndpoint}} is being shut down which closes all handlers
3. Since no requests are registered in {{AbstractHandler.inFlightRequestTracker}}, we immediately close the handlers
4. After having obtained the leader gateway, we call into {{AbstractHandler.respondAsLeader}} which registers the request in the {{inFlightRequestTracker}} but does not check whether the handler has been shut down.

If this should indeed be the problem, then I would suggest that we check under the {{lock}} whether {{terminationFuture}} has been set and also add the request to the {{inFlightRequestTracker}} under the {{lock}}.;;;","26/Jul/20 02:56;tartarus;[~trohrmann] thanks for your reply.

I think your suspicion is right. I will add the log and reproduce it,  and then upload the log.;;;","26/Jul/20 07:48;tartarus;[~trohrmann] hello, you said the {{terminationFuture}} is {{AbstractHandler.terminationFuture}} ?

{{AbstractHandler.terminationFuture}} wait for {{AbstractHandler.inFlightRequestTracker}}.;;;","27/Jul/20 07:38;trohrmann;Yes I mean the {{AbstractHandler.terminationFuture}}.;;;","28/Jul/20 03:34;tartarus;[~trohrmann]  the JobManager's log is too large，so I remove some yarn and container’s log.

{{AbstractHandler.terminationFuture}} has not complete, because AbstractHandler.inFlightRequestTracker Cannot be cleared.

[^jobmanager.log.noyarn.tar.gz]

I add the log you tell me , you can filter '#####' ,  the {{log.info(""Shutting RestServerEndpoint down internally"")}} dosen't happen ,because {{org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler}} not close yet.

and {{org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler}} not close because {{AbstractHandler.inFlightRequestTracker}} not cleared, because the exception on job from SCHEDULED to DEPLOYING
{code:java}
2020-07-27 21:57:26,685 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler[flink-scheduler-1]  - ##### handle exception for url /jobs/overview
2020-07-27 21:57:26,685 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler[flink-scheduler-1]  - ##### handle exception for url /jobs/overview
akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-88418157]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply. 
  at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635) 
  at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635) 
  at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648) 
  at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) 
  at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601) 
  at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109) 
  at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
  at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) 
  at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
  at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) 
  at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) 
  at java.lang.Thread.run(Thread.java:745)
2020-07-27 21:57:26,686 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph      [flink-akka.actor.default-dispatcher-48]  - 1-1.1_Sink: Unnamed (533/1500) (1b0945713f48026b5c677a2d1559f78f) switched from SCHEDULED to DEPLOYING.
2020-07-27 21:57:26,686 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler[flink-scheduler-1]  - ##### handleException happened exception
java.lang.NullPointerException 
  at org.apache.flink.runtime.rest.handler.AbstractHandler.handleException(AbstractHandler.java:204) 
  at org.apache.flink.runtime.rest.handler.AbstractHandler.lambda$respondAsLeader$1(AbstractHandler.java:182) 
  at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) 
  at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) 
  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) 
  at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) 
  at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:872) 
  at akka.dispatch.OnComplete.internal(Future.scala:263) 
  at akka.dispatch.OnComplete.internal(Future.scala:261) 
  at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
  at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) 
  at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74) 
  at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) 
  at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) 
  at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644) 
  at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) 
  at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601) 
  at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109) 
  at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599) 
  at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
  at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) 
  at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) 
  at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) at java.lang.Thread.run(Thread.java:745)
{code}
I am not very clear why {{FlinkHttpObjectAggregator}} was null，but,  {{FlinkHttpObjectAggregator here}} is only used to obtain maxContentLength, so why not pass {{RestHandlerConfiguration}} or {{RestServerEndpointConfiguration}} as construction parameters to AbstractHandler, then we can obtain some other config too, I feel more flexible like this.

But this is very redundant, after all, most of the parameters are not used.

 ;;;","28/Jul/20 08:57;trohrmann;Thanks for providing the logs [~tartarus]. There is some part missing which shows under which address the {{Dispatcher}} is being registered. Would it be possible to also post this part. The reason I'm asking is because I want to understand whether there was a failover happening and whether {{Actor[akka://flink/user/dispatcher#-88418157}} is the running {{Dispatcher}}. It is a bit odd that all the requests time out with an {{AskTimeoutException}} if it is the running {{Dispatcher}}.

It is true that we can get rid of the symptoms of the problem by passing in the {{maxContentLength}} but I would also like to understand the underlying problem. Have you tried what I proposed to add above? Concretely checking under {{lock}} whether {{terminationFuture}} has been set at the beginning of {{respondAsLeader}}? If it has been set, then this means that the handler is shutting down and we should ignore this request. Moreover, the {{inFlightRequestTracker.registerRequest();}} should also happen under {{lock}}.;;;","28/Jul/20 09:28;chesnay;The job seems to consist of several thousand tasks, and we can see that the deployment alone takes upwards of 30 seconds. This should explain the TimeoutException leading to the NPE.
The TimeoutExceptions in the beginning of the log can probably be attributed to the initialization of the job.

;;;","28/Jul/20 13:49;tartarus;[~trohrmann] I may not understand you too much before, but I'm sure {{AbstractHandler.terminationFuture}} has not complete, because I has dump a jvm.

I think [~chesnay] is right , this job has 15000 tasks, and GC frequently, so the TimeoutException is possible.

I am not very clear why {{FlinkHttpObjectAggregator}} was null.

[~trohrmann]  [~chesnay]  Do you have any suggestions on this issue?

 ;;;","28/Jul/20 14:33;chesnay;So far the only explanation that I could find for a pipeline returning null is that the channel was already closed.

Our current assumption was that this happened because the RestServerEndpoint is shutting down. From the logs you gave us it appears that the shutdown is initiated 2 minutes after the NPE; this doesn't seem to match our assumption.

I'm wondering what would happen if the netty connection were to be closed by the client. We know that the request processing is delayed by 10 seconds; if the client aborts the connection in between maybe netty starts cleaning up the pipeline.
;;;","28/Jul/20 15:00;trohrmann;[~tartarus] what I meant is to check whether {{AbstractHandler.terminationFuture}} is not {{null}}. If this is the case, then the handler is being shut down.

I agree with Chesnay that we might have to look into other explanations for the described problem.;;;","28/Jul/20 15:01;tartarus;[~chesnay]  There is a separate monitoring service that requests /jobs/overview every 5 seconds and timeout is 5 seconds too.

Then will close the client. ;;;","28/Jul/20 22:05;chesnay;Well there we go then, I was able to reproduce the issue. It is indeed due to the client closing the connection.;;;","28/Jul/20 23:48;yungthuis@hotmail.com;发自我的华为手机

-------- 原始邮件 --------
发件人： ""Chesnay Schepler (Jira)"" <jira@apache.org>
日期： 2020年7月29日周三 清晨6:06
收件人： issues@flink.apache.org
主 题： [jira] [Commented] (FLINK-18663) Fix Flink On YARN AM not exit

    [ https://issues.apache.org/jira/browse/FLINK-18663?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=17166717#comment-17166717 ]

Chesnay Schepler commented on FLINK-18663:
------------------------------------------

Well there we go then, I was able to reproduce the issue. It is indeed due to the client closing the connection.




--
This message was sent by Atlassian Jira
(v8.3.4#803005)
;;;","29/Jul/20 02:19;tartarus;[~chesnay] [~trohrmann] Then we discuss how to fix this problem?

what do you think about pass  {{maxContentLength}} as construction parameters to AbstractHandler?

Then modify all Handlers. Do you have any good suggestions? thanks;;;","29/Jul/20 08:42;chesnay;It is rather annoying to pass the {{maxContentLength}} to all handlers (yet another case why I'd prefer composition for our handlers at this point); I'm tempted to just catch the {{NullPointerException}} and log an error ""Connection was unexpectedly closed by the client.""; we wouldn't be able to send a response anyway.

We should also slightly change how the request is finalized, to ensure this happens in all cases, regardless of whether {{handleExceptions}} throws an exception:.
So I'd suggest to change it to this:
{code}
requestProcessingFuture
	.handle((ignored, throwable) -> {
		if (throwable != null) {
			return handleException(ExceptionUtils.stripCompletionException(throwable), ctx, httpRequest);
		}
		return CompletableFuture.<Void>completedFuture(null);
	})
	.thenCompose(Function.identity())
	.whenComplete((Void ignored, Throwable throwable) -> finalizeRequestProcessing(finalUploadedFiles));
{code}

Furthermore, even if our original theory did not hold, I do think that it is still possible for it to occur, so we should also fix that as [~trohrmann] suggested.;;;","29/Jul/20 09:16;trohrmann;Can we check the call sites where we try to access the handlers of the pipeline? A null check and a more meaningful exception than {{NullPointerException}} could be helpful for users/devs to understand the problem.

I agree with Chesnay that we should always run {{finalizeRequestProcessing}} independent of what happened in between. We should also log in the last {{whenComplete}} call the {{throwable}} if it is not null.;;;","29/Jul/20 09:44;tartarus;[~chesnay]  I think we should add a log to indicate that an exception occurred in handleException.
 The positioning problem took a lot of time this time because NPE did not print it out.

[~trohrmann] I agree with you.

how about this：
{code:java}
requestProcessingFuture
	.handle((Void ignored, Throwable throwable) -> {
		if (throwable != null) {
			return handleException(ExceptionUtils.stripCompletionException(throwable), ctx, httpRequest);
		}
		return CompletableFuture.<Void>completedFuture(null);
	}).thenCompose(Function.identity())
	.whenComplete((Void ignored, Throwable throwable) -> {
		if (throwable != null) {
			log.warn(""ignored the exception when execute the handleException method"", throwable);
		}
		finalizeRequestProcessing(finalUploadedFiles);
	});
{code};;;","29/Jul/20 10:05;chesnay;[~tartarus] The log message should be {{""An exception occurred while handling another exception.""}}.
And yes, the NPE should be catched within {{handleException}}.;;;","29/Jul/20 11:08;tartarus;[~chesnay] ok，I has submit a PR [https://github.com/apache/flink/pull/13020];;;","29/Jul/20 11:48;tartarus;[~chesnay] [~trohrmann]  about the checking for the termination future under the lock to ensure we don't start processing requests during a shutdown，how about this:
{code:java}
HttpRequest httpRequest = routedRequest.getRequest();
if (log.isTraceEnabled()) {
	log.trace(""Received request "" + httpRequest.uri() + '.');
}

synchronized (this) {
	if (terminationFuture != null) {
		String errorMsg = ""The handler instance for "" + untypedResponseMessageHeaders.getTargetRestEndpointURL()
			+ "" had already been closed"";
		log.warn(errorMsg);
		HandlerUtils.sendErrorResponse(
			ctx,
			httpRequest,
			new ErrorResponseBody(errorMsg),
			HttpResponseStatus.BAD_REQUEST,
			responseHeaders);
		return;
	}
}
{code}
 at the start of {{AbstractHandler#respondAsLeader}} ?;;;","03/Aug/20 20:22;chesnay;master: 
ee4b27f96e8156c6a1c4b825484d238055cecf5f
7619db019181347a16fd37f0e4e04c420f75eee9
1.11:
4c2f7c44fc7085edfd3f910135fbf7f54d5eadf1
862ac0f6921a7cf0a1f7fabfd3cff11292817fb0 
1.10:
bda138361e55437c84ea000fe38a6bce69cf7520
52a77e695ea9dea79f67f6b6a7281ceb1079f65e ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException when writing Hive orc tables,FLINK-18659,13318437,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lzljs3620320,lzljs3620320,21/Jul/20 08:58,17/Aug/20 07:09,13/Jul/23 08:12,14/Aug/20 01:55,1.11.1,,,,,1.11.2,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"Writing Hive orc tables with Hive 1.1 version, will be:
{code:java}
Caused by: java.io.FileNotFoundException: File does not exist: hdfs://xxx/warehouse2/tmp_table/.part-6b51dbc2-e169-43a8-93b2-eb8d2be45054-0-0.inprogress.d77fa76c-4760-4cb6-bb5b-97d70afff000	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1218)	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1210)	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1210)	at org.apache.flink.connectors.hive.write.HiveBulkWriterFactory$1.getSize(HiveBulkWriterFactory.java:54)	at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriter.getSize(HadoopPathBasedPartFileWriter.java:84)	at org.apache.flink.table.filesystem.FileSystemTableSink$TableRollingPolicy.shouldRollOnEvent(FileSystemTableSink.java:451)	at org.apache.flink.table.filesystem.FileSystemTableSink$TableRollingPolicy.shouldRollOnEvent(FileSystemTableSink.java:421)	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.write(Bucket.java:193)	at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.onElement(Buckets.java:282)	at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSinkHelper.onElement(StreamingFileSinkHelper.java:104)	at org.apache.flink.table.filesystem.stream.StreamingFileWriter.processElement(StreamingFileWriter.java:118)	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
{code}
This maybe due to lazy init in Orc writer. Until first record comes, orc writer not create this file.",,godfreyhe,klion26,lirui,lzljs3620320,Paul Lin,wei.wei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18965,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 01:55:03 UTC 2020,,,,,,,,,,"0|z0h1js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/20 13:40;lirui;I managed to reproduce the issue and did some debugging. In Hive 2.3.4, the in-progress file is created when Hive creates the Orc writer. In Hive 1.1.0, it seems the file is not created even after the first record is written. When a {{Bucket}} receives the first record, it creates the writer and writes this record with the writer. And on the 2nd record, the {{Bucket}} checks the underlying file size to see if rolling is needed and this is when we hit the exception because the file is not yet created.;;;","22/Jul/20 02:01;lzljs3620320;Also need fix flink native orc writer for Hive 1.x (orc-nohive).;;;","14/Aug/20 01:55;lzljs3620320;master: 6a7b464c708c64f359b731ae5cee97ebb6c62d07

release-1.11: b728f22e9f95a0ea8952aa234150718d796e72ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start Delay metric is always zero for unaligned checkpoints and source tasks,FLINK-18656,13318420,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,21/Jul/20 07:55,17/Sep/21 07:09,13/Jul/23 08:12,29/Jul/20 12:22,1.11.1,,,,,1.11.2,1.12.0,,Runtime / Metrics,Runtime / Network,,,,0,pull-request-available,,,,,"As visible in the attached screenshot, start delay is 0, despite very high end to end time and very quick async/sync times. This is impossible, as ""start delay"" should be equal to ""end_to_end_time - sync_time - async_time"".

The bug is probably in {{CheckpointBarrierUnaligner}} class, which seems to never invoke {{CheckpointBarrierHandler#markCheckpointStart}}.

*It's important to make sure this metric is also set for source tasks* (time to acquire checkpoint lock).",,klion26,pnowojski,stevenz3wu,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15603,,,,,,FLINK-18662,FLINK-22833,,,,"21/Jul/20 07:55;pnowojski;Screenshot 2020-07-21 at 09.53.21.png;https://issues.apache.org/jira/secure/attachment/13008060/Screenshot+2020-07-21+at+09.53.21.png","21/Jul/20 07:58;pnowojski;Screenshot 2020-07-21 at 09.58.29.png;https://issues.apache.org/jira/secure/attachment/13008061/Screenshot+2020-07-21+at+09.58.29.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 12:22:51 UTC 2020,,,,,,,,,,"0|z0h1g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/20 06:45;pnowojski;Merged to master as 5dccc996b3 and to release-1.11 as 59c590587f;;;","29/Jul/20 12:22;pnowojski;Fix for SourceStreamTask merged to release-1.11 as b122f8ff92^..b122f8ff92, to master as 35fed94415^..35fed94415;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set failOnUnableToExtractRepoInfo to false for git-commit-id-plugin in module flink-runtime,FLINK-18655,13318387,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,21/Jul/20 04:09,24/Jul/20 16:28,13/Jul/23 08:12,24/Jul/20 16:28,1.11.1,,,,,1.11.2,1.12.0,,Build System,,,,,0,pull-request-available,,,,,"Exception may be thrown when building source distribution without the .git folder(for the flink-runtime module):
{code:java}
[ERROR] Failed to execute goal pl.project13.maven:git-commit-id-plugin:4.0.0:revision (get-the-git-infos) on project flink-runtime_2.11: Could not complete Mojo execution... Error: Could not get HEAD Ref, are you sure you have set the dotGitDirectory property of this plugin to a valid path? -> [Help 1]
{code}
 

To solve the problem, we need to add the {{<failOnUnableToExtractRepoInfo>false</failOnUnableToExtractRepoInfo>}} configuration in addition to {{<failOnNoGitDirectory>false</failOnNoGitDirectory>}} in the pom of flink-runtime.

 

The reason is the plugin:git-commit-id-plugin would search up all the maven parent project hierarchy until a .git directory is found. For example, if we config dotGitDirectory as /a/b/c/.git and if /a/b/c/.git is invalid, the plugin would search /a/b/.git and /a/.git. However, once the plugin found a /a/.git folder, it would fail on extracting repo info which leads to the failure above. The search logic of the plugin can be found [here|https://github.com/git-commit-id/git-commit-id-maven-plugin/blob/v4.0.0/maven/src/main/java/pl/project13/maven/git/GitDirLocator.java#L74].


You can reproduce the exception with:
- download the 1.11.0 source distribution.
- put a .git folder under the path (or parent path) of ${project.basedir}/../.git. For example, my dotGitDirectory is: `/Users/hequn.chq/Downloads/flink-1.11.0/flink-runtime/../.git` and there is a .git folder under `/Users/hequn.chq/.git`.",,hequn8128,klion26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 16:27:53 UTC 2020,,,,,,,,,,"0|z0h18o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/20 05:30;hequn8128;CC [~chesnay];;;","24/Jul/20 16:27;hequn8128;Resolved 
in 1.12.0 via 09f1674cb3e6cb536a67f060cb882aae7e7a63bb
in 1.11.2 via 0c8f8d7ca645863024474477dea5995e990fd37a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The description of dispatcher in Flink Architecture document is not accurate,FLINK-18650,13318366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wooplevip,wooplevip,wooplevip,21/Jul/20 00:24,22/Jul/20 10:54,13/Jul/23 08:12,22/Jul/20 10:54,1.11.0,,,,,1.11.2,1.12.0,,Documentation,,,,,0,pull-request-available,,,,,"The description of dispatcher is like below:

The _Dispatcher_ provides a REST interface to submit Flink applications for execution and _{color:#de350b}*starts a new JobManager*{color}_ for each submitted job. It also runs the Flink WebUI to provide information about job executions.

 

As I understand it, is it ""starts a new *{color:#de350b}JobMaster{color}*"" rather than JobManager?  

 ",,aljoscha,wooplevip,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 10:54:26 UTC 2020,,,,,,,,,,"0|z0h140:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/20 08:00;aljoscha;Yes, this is wrong. Do you want to open a PR to fix it?;;;","21/Jul/20 08:48;wooplevip;OK, I have created a PR for it. Please help to review, thanks.;;;","22/Jul/20 10:54;aljoscha;release-1.11: 2a62f91d41552e481ce0ac5e7e238103ed6fae30
master: 23f65064db0d3aca6b725b14e0bd2cb4b7f6cc64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed memory released check can block RPC thread,FLINK-18646,13318259,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,azagrebin,azagrebin,20/Jul/20 13:20,26/Aug/20 15:32,13/Jul/23 08:12,13/Aug/20 08:19,1.10.1,1.11.0,1.12.0,,,1.10.2,1.11.2,1.12.0,Runtime / Task,,,,,0,pull-request-available,,,,,"UnsafeMemoryBudget#verifyEmpty, called on slot freeing, needs time to wait on GC of all allocated/released managed memory. If there are a lot of segments to GC then it can take time to finish the check. If slot freeing happens in RPC thread, the GC waiting can block it and TM risks to miss its heartbeat.",,azagrebin,godfreyhe,kevin.cyj,kezhuw,klion26,lzljs3620320,stevenz3wu,trohrmann,TsReaper,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15758,,FLINK-19055,,,,,,,,,,,,,"21/Jul/20 08:03;TsReaper;log1.png;https://issues.apache.org/jira/secure/attachment/13008062/log1.png","21/Jul/20 08:03;TsReaper;log2.png;https://issues.apache.org/jira/secure/attachment/13008063/log2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 08:19:45 UTC 2020,,,,,,,,,,"0|z0h0g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/20 13:21;azagrebin;The easiest solution could be to always wait for GC in another thread.

cc [~trohrmann] [~TsReaper];;;","21/Jul/20 07:08;TsReaper;I discover this problem when running TPCDS 10T benchmark. The {{System.gc()}} in the infinite loop of {{UnsafeMemoryBudget#reserveMemory}} is called about 400 times before the task slot is released.

I think this issue is caused by the usage of {{JavaGcCleanerWrapper.tryRunPendingCleaners()}}. In Java8, {{JavaGcCleanerWrapper.tryRunPendingCleaners()}} actually calls {{java.lang.ref.Reference.tryHandlePending(false)}} and it only runs 1 cleaner each call. If there are lots of pending cleaners, the infinite loop containing {{System.gc()}} might run multiple times before exiting (the cleaner of other slots might run before the cleaner of the current slot). So it seems that we need to change
{code:java}
if (!JavaGcCleanerWrapper.tryRunPendingCleaners()) {
    Thread.sleep(sleepTime);
    sleepTime <<= 1;
    sleeps++;
}
{code}
to
{code:java}
while (JavaGcCleanerWrapper.tryRunPendingCleaners()) {}

Thread.sleep(sleepTime);
sleepTime <<= 1;
sleeps++;
{code}

Also, it will be better if the cleaner only cleans the segments in its own thread.;;;","21/Jul/20 07:40;azagrebin;I do not think that {{System.gc()}} is actually doing something all the time, it is just a hint to JVM that it is allowed to spent more time for GC. Anyways we can indeed remove it to see whether it helps.
  
 If {{tryRunPendingCleaners == true}} there will be no {{sleeps}} and the outer loop is basically {{while (tryRunPendingCleaners) plus {{System.gc()}}, which is almost noop hint according to logs.}}. Also generally, it is not optimal for each reserve operation to wait for all cleaners by {{while (tryRunPendingCleaners)}} without trying to reserve because the reserve operation might want to reserve just a bit and needs to wait for a couple of cleaners.

The cleaners are already supposed to clean in some GC background threads. By calling {{tryRunPendingCleaners}} , we just try to speed it up giving it another thread and more CPU if the available memory is exhausted. Therefore, it makes sense to allocate a thread for the check because otherwise it waits for the full GC of the slot managed memory in RPC thread. I would try this in {{TaskSlot}}:
{code:java}
CompletableFuture<Void> closeAsync(Throwable cause) {
	.....
	ExecutorService executor = Executors.newSingleThreadExecutor(); // <------------
	final CompletableFuture<Void> cleanupFuture = FutureUtils
	  .waitForAll(tasks.values().stream().map(TaskSlotPayload::getTerminationFuture).collect(Collectors.toList()))
		.thenRunAsync(this::verifyMemoryFreed, executor) // <------------
		.thenRunAsync(() -> {
			this.memoryManager.shutdown();
			executor.shutdown(); // <------------
		});
        ....
}
{code};;;","21/Jul/20 08:06;TsReaper;*_the reserve operation might want to reserve just a bit and needs to wait for a couple of cleaners_*

I agree with this.

*_System.gc(), which is almost noop hint_*

For this I have some doubts. {{System.gc()}} is indeed just a hint, but we cannot make sure when the GC will happen after this method is called. It is possible that GC happens immediately after the call. I think we should only call {{System.gc()}} when {{tryRunPendingCleaners}} returns false, otherwise there are still pending cleaners and there is no need to hint for another GC.

I've added some logs before calling {{System.gc()}} in the infinite loop, and there are obvious gaps which are caused by full GCs.

!log1.png!

!log2.png!;;;","21/Jul/20 08:13;azagrebin;GC gaps are not always so not clear how much it slows things down. I agree the best is to try w/o always calling GC.
 I agree It makes sense to continue calling GC only if tryRunPendingCleaners is false and memory still cannot be allocated.
 The original idea was to retry GC only after a lot of waiting {{RETRIGGER_GC_AFTER_SLEEPS.}}
 It was a further optimisation comparing to the Java code for direct memory allocation.;;;","21/Jul/20 08:33;lzljs3620320;[https://stackoverflow.com/questions/2414105/why-is-it-bad-practice-to-call-system-gc]

Calling System.gc() multiple times seems to be costly. Although it's a hint, it can lead to a meaningless rush of the JVM.

The codes look not so complicate, maybe [~TsReaper] you can try again benchmark after trying w/o always calling GC.;;;","22/Jul/20 10:40;azagrebin;I agree we should not trigger GC if the cleaner queue processing is active. If we cannot still allocate memory and there are no ready cleaners detected by GC, we have to re-trigger GC or fail after sleeping and timeout.

We can increase timeout for slot closing verification (off-loaded to another thread) because we have to wait for GC of all segments to make sure there is no leak. Normal reservation should probably have smaller timeout to fail faster in case of leaks, especially if we do not use it in a way similar to verification: allocate all, release and then re-allocate all again.

All-in-all, the timeouts have to be tuned. If we still find ourselves often hitting the global limit, the global managed memory limit (not per slot) could also compensate for GC and/or we should not allocate all available memory and keep some buffer to compensate for GC which is usually the case for direct memory.;;;","22/Jul/20 21:30;trohrmann;[~TsReaper] do you know how the high number of GC calls happens? Looking at the code, we must have increased the {{sleeps}} counter to 9 and then after calling {{System.gc()}} once {{JavaGcCleanerWrapper.tryRunPendingCleaners()}} must return {{true}} for several calls which will keep the counter at 9 and calling {{System.gc()}} for every pending cleaner. This looks not like an intended behavior to me.

[~azagrebin] how long do we have to wait until we can expect that all segments have been detected by GC so that we can run the respective cleaners? What do you mean with keeping some buffer to compensate for GC? What do you mean with hitting the global limit?;;;","23/Jul/20 07:32;azagrebin;{quote}do you know how the high number of GC calls happens?
{quote}
From the offline discussion: 400 actual GCs, the number of System.gc() calls is indeed high for each while iteration
{quote}Looking at the code, we must have increased the {{sleeps}} counter to 9 and then after calling {{System.gc()}} once {{JavaGcCleanerWrapper.tryRunPendingCleaners()}} must return {{true}} for several calls which will keep the counter at 9 and calling {{System.gc()}} for every pending cleaner. This looks not like an intended behavior to me.
{quote}
From the offline discussion: this is true, it was overlooked. That is why it makes sense to call GC only if 'tryRunPendingCleaners' is false after sleep and memory still cannot be allocated. I have already a fix for it in a branch.
{quote}how long do we have to wait until we can expect that all segments have been detected by GC so that we can run the respective cleaners?
{quote}
JVM waits for 9 sleeps (0,5) for direct memory but it is generally used for large global limit and allocations which are small relative to the limit which requires to GC only so much as asked.
{quote}What do you mean with keeping some buffer to compensate for GC?
{quote}
If 120 Mb is the actual memory that can be used then make the interface limit of 100 Mb. Keep 20 Mb to allocate more actual memory than 100Mb if some memory is not GC'ed yet.
{quote}What do you mean with hitting the global limit?
{quote}
When we allocated all available actual memory, some of which is released but not GC'ed yet, and we try to allocate more assuming that we released some and it should be available.;;;","28/Jul/20 11:20;azagrebin;merged into master by 3d056c8fea72ca40b663d12570913679be87c0a9
 merged into 1.11 by bcc97082639280ab14f465463fb07b27167c37e3

 

[~TsReaper] I am closing the issue as the verification should not block the RPC thread any more. Reopen it if you notice any problems with it. If there are still problems with the normal memory allocation timeout (given there is no real leak), we can discuss it in another issue.;;;","12/Aug/20 12:17;trohrmann;Do we need to backport this ticket to Flink 1.10.2 [~azagrebin] because the issue causing this problem is also part of this release?;;;","13/Aug/20 08:19;trohrmann;Fixed via

1.10.2: 3e17833eec5f5062260eaef2cc06421dabbd7f56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Failure to finalize checkpoint"" error in MasterTriggerRestoreHook",FLINK-18641,13318181,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,Brian Zhou,Brian Zhou,20/Jul/20 07:10,09/Sep/20 06:34,13/Jul/23 08:12,09/Sep/20 01:33,1.11.0,,,,,1.11.2,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"https://github.com/pravega/flink-connectors is a Pravega connector for Flink. The ReaderCheckpointHook[1] class uses the Flink `MasterTriggerRestoreHook` interface to trigger the Pravega checkpoint during Flink checkpoints to make sure the data recovery. The checkpoint recovery tests are running fine in Flink 1.10, but it has below issues in Flink 1.11 causing the tests time out. Suspect it is related to the checkpoint coordinator thread model changes in Flink 1.11

Error stacktrace:

{code}
2020-07-09 15:39:39,999 30945 [jobmanager-future-thread-5] WARN  o.a.f.runtime.jobmaster.JobMaster - Error while processing checkpoint acknowledgement message
org.apache.flink.runtime.checkpoint.CheckpointException: Could not finalize the pending checkpoint 3. Failure reason: Failure to finalize checkpoint.
         at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1033)
         at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:948)
         at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$acknowledgeCheckpoint$4(SchedulerBase.java:802)
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
         at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.SerializedThrowable: Pending checkpoint has not been fully acknowledged yet
         at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
         at org.apache.flink.runtime.checkpoint.PendingCheckpoint.finalizeCheckpoint(PendingCheckpoint.java:298)
         at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1021)
         ... 9 common frames omitted
{code}

More detail in this mailing thread: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Pravega-connector-cannot-recover-from-the-checkpoint-due-to-quot-Failure-to-finalize-checkpoint-quot-td36652.html
Also in https://github.com/pravega/flink-connectors/issues/387",,becket_qin,Brian Zhou,klion26,pnowojski,SleePy,ym,yunta,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 01:33:40 UTC 2020,,,,,,,,,,"0|z0gzyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/20 13:52;pnowojski;[~SleePy] do you have some good guess what's the problem here?;;;","22/Jul/20 00:28;becket_qin;Looked into this a little bit. The problems is following:
 # The Pravega connector uses the {{MasterTriggerRestoreHook}} and {{ExternallyInducedSource}} for checkpointing. That means the tasks may start checkpointing after the {{MasterTriggerRestoreHook.triggerCheckpoint()}} is invoked.
 # In Flink 1.10, the checkpoint thread blocks on the future returned by the master hook. This becomes asynchronous in Flink 1.11. It only guarantees that the JM triggers the task checkpoint after the master state is checkpointed. However, this does not cover the case where the task checkpoints are triggered by the master hooks rather than by JM itself.
 # Due to the change in (2), what happens in this case is that the task checkpionts are triggered by the master hook, and acknowledged before the master hook future is completed. From JM's perspective, once all the task checkpoints are acked, it will try to finalize the checkpoint, but found that the master checkpoint has not completed yet, thus an exception is thrown.

This is a bug in JM when working with {{MasterTriggerRestoreHook}} and {{ExternallyInducedSource.}} I'll submit a fix for this.;;;","22/Jul/20 02:11;Brian Zhou;Thanks Jiangjie for looking into this. I noticed 1.11.1 release just happened, so can we get a quick 1.11.2 release to fix this? This one is now a blocker to Pravega Flink 1.11 connector, so I hope this could be fixed ASAP.;;;","22/Jul/20 07:20;SleePy;Thanks [~becket_qin] for analyzing this issue. The asynchronous checkpoint threading model breaks the assumption of {{ExternallyInducedSource}} could trigger a checkpoint before {{MasterTriggerRestoreHook}} finishes the trigger future. We can find a way to guarantee that. However it's not friendly to the scenario that doing some initialization or preparation in {{MasterTriggerRestoreHook}}. Because checkpoint might be triggered before initialization of preparation finishes. Hope nobody uses it like that :(

Currently the semantics of {{ExternallyInducedSource}} is highly bound with the implementation of Flink checkpoint which should be avoided IMO. I think we should redesign the {{ExternallyInducedSource}} as a long-term goal.

To [~becket_qin], do you already have any idea for fixing it? If not, I could help to fix it.

BTW, this change of {{CheckpointCoordinator}} is introduced in 1.10. Is it possible that the failure of testing case is exposed by the change of {{OperatorCoordinator}}? Because we add another asynchronous step between master hook triggering and task triggering. I'm not sure if there must be some {{OperatorCoordinator}} added or not in the scenario of Pravega connector testing. If not, there is a work-around way that try to finish future returned by {{MasterTriggerRestoreHook.triggerCheckpoint}} before trigger task checkpoint (I assume there is only one master hook in the case). ;;;","22/Jul/20 10:08;pnowojski;So the problem is that checkpoint is finalised after JM receives all of the ACKS, without waiting for async master hook to complete? And the solution would be to include waiting for async hook to complete, before completing/finalising the checkpoint?  
{quote}
In Flink 1.10, the checkpoint thread blocks on the future returned by the master hook. This becomes asynchronous in Flink 1.11.
{quote}
[~becket_qin], as [~SleePy] mentioned that's not correct. Those changes were done already in Flink 1.10.;;;","23/Jul/20 05:53;becket_qin;[~pnowojski] In Flink 1.10, the checkpoint thread blocks on the future returned by the master hook, therefore that thread will not handle the acks from the tasks until the master hook has completed.

[https://github.com/apache/flink/blob/bfe6c2eddedaf3b8067c973ba82a06b36d5095f9/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L627]

Now in Flink 1.11, the checkpoint thread no longer blocks on that future anymore, it uses handleAsync instead.

[https://github.com/apache/flink/blob/791e276c8346a49130cb096bafa128d7f1231236/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L708]

[https://github.com/apache/flink/blob/791e276c8346a49130cb096bafa128d7f1231236/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L544]

So that is what I meant by
{quote}In Flink 1.10, the checkpoint thread blocks on the future returned by the master hook. This becomes asynchronous in Flink 1.11.
{quote}
 ;;;","23/Jul/20 06:51;becket_qin;[~pnowojski] [~SleePy] There are actually more problems than the issues reported by this ticket. In checkpoint there are two orders:
 # the checkpoint taking order;
 # the checkpoint acknowledge handling order.

This ticket reports the problem of unexpected checkpoint acknowledge handling order. While the acknowledge handling order is not super important, the checkpoint taking order is critical because that may cause correctness problem.

By design, the checkpoint should always actually take the snapshot of the master hooks and OperatorCoordinator first before taking the checkpoint on the tasks.

Prior to FLIP-27, this ordering is always guaranteed because the checkpoint on the tasks are triggered in one of the following ways:
 # The CheckpointCoordinator takes snapshot of the master hooks, wait for it to finish, and then trigger the snapshot on tasks. (Strict snapshot order is guaranteed by the checkpoint coordinator.)
 # The CheckpointCoordinator takes snapshot of the master hooks. The master hook triggers the checkpoint on the tasks using ExternallyInducedSource. The CheckpointCoordinator waits until the master hooks finishes snapshot. (In this case, the master hooks have to guarantee the consistency between the master state and the task states. The tasks may ack the checkpoint before the master hooks completes, but the acks won't be handled until the master hooks acks are handled).

After FLIP-27, with the introduction of OperatorCoordinator, we need to make sure that the tasks checkpoint cannot be triggered before the OperatorCoordinator checkpoint. Given that the task checkpoint can be triggered by both the CheckpointCoordinator itself or the master hooks. We should first checkpoint the OperatorCoordinator, then the master hooks, lastly triggering the task checkpoint.

The current code does not do this correctly. That means the master hooks can trigger the snapshot of the tasks before the OperatorCoordinators are checkpointed.

Regarding the fix, I am thinking of the following:
 # Change the checkpoint order to checkpoint OperatorCoordinators first, i.e before checkpointing master hooks.
 # Keep the master hooks async, but invoke them after checkpointing the OperatorCoordinators. This is fine because according to the java doc of {{MasterTriggerRestoreHook#triggerCheckpoint()}} can choose to be synchronous if it wants to. Otherwise the CheckpointCoordinator will consider it as asynchronous friendly.
 # Change the logic to only call {{CheckpointCoordinator#completePendingCheckpoint()}} after all the checkpoints are acked, instead of only looking the task checkpoints. This is because the acknowledge handling order may vary given the current async checkpoint pattern.

I agree that In long run, the operator coordinator can actually supersede the master hooks. So we can probably mark the master hooks as deprecated.;;;","23/Jul/20 08:31;SleePy;Ah, sorry [~pnowojski], I forgot this change is not submitted to release-1.10 at that time, it's only submitted to master.

[~becket_qin], thanks for correcting it. There are two different things of your response, master hook and {{OperatorCoordinator}}. Regarding to this ticket, it's not caused by the {{OperatorCoordinator}} part, right? I think we could file another ticket for it and discuss it there.

{quote}By design, the checkpoint should always actually take the snapshot of the master hooks and OperatorCoordinator first before taking the checkpoint on the tasks.{quote}
Technically speaking, there is no clear semantics that master hook should be taken before task snapshotting. For the {{ExternallyInducedSource}}, the task snapshotting might be taken before master hook finishes the future returned. And if there are multiple master hooks, some hooks might be invoked after task snapshotting. It's concurrent somewhat. I don't think we should/could guarantee the ordering here.

Anyway we have to fix the issue of {{ExternallyInducedSource}} caused by the ordering.
Regarding to the fixing plan. I'm not sure how heavy the fixing of {{OperatorCoordinator}} might be. If it's not a simple fixing, it might be better to separate these things into different patches. As a quick fixing of this ticket, we could take the master hook synchronously with the coordinator-wide lock retaining just like before.

{quote}I agree that In long run, the operator coordinator can actually supersede the master hooks. So we can probably mark the master hooks as deprecated.{quote}
Totally agree!;;;","23/Jul/20 08:59;becket_qin;[~SleePy] Right, the master hooks with ExternalInducedSource are not really that robust in my opinion.

I think the fix is just a few lines of code. However, apparently we do not have a test covering the use case of {{ExternallyInducedSource}}. So as usual, I think the major work here will be writing the tests.;;;","23/Jul/20 11:04;pnowojski;[~becket_qin] yes you are right that the problem was introduced in FLINK-13905 in 1.11.0.

{quote}
By design, the checkpoint should always actually take the snapshot of the master hooks and OperatorCoordinator first before taking the checkpoint on the tasks.
{quote}
Regarding the ordering guarantees. I don't understand where does this strict ordering comes from? Java docs of the {{MasterTriggerRestoreHook}} doesn't seem to support this statement:
{code} 
	 * <p>If the action should be executed asynchronously and only needs to complete before the
	 * checkpoint is considered completed, then the method may use the given executor to execute the
	 * actual action and would signal its completion by completing the future. 
{code}
So synchronously waiting for the hook's future to complete would solve the problem, but what I proposed above:
{quote}
And the solution would be to include waiting for async hook to complete, before completing/finalising the checkpoint?
{quote}
Should also be correct, right?

As a sidenote, I'm +1 for the easiest solution to solve this bug without causing regressions compared to 1.10/1.9 and without undermining {{CheckpointCoordinator}} threading model refactor (which we still need to complete). As you both mentioned, {{MasterTriggerRestoreHook}} is on it's way out to be replaced by FLIP-27.;;;","24/Jul/20 06:18;becket_qin;[~pnowojski] Synchronously waiting for the hook's future is not sufficient after introduction of OperatorCoordinator. This is because when the master hooks are triggered on the {{ExternallyInducedSource}}, the tasks may start to checkpoint the operators before the OperatorCoordinator is checkpointed. This breaks the checkpoint contract between the OperatorCoordinator and the Operator, which is that the OperatorCoordinator is always checkpointed before the subtasks/operators are checkpointed.

Otherwise, yes, we only need to wait for the master hooks to finish before completing the checkpoint.;;;","28/Jul/20 01:32;SleePy;[~becket_qin], [~pnowojski], the ticket has not been assigned yet. Is there anyone working on this?;;;","29/Jul/20 06:50;becket_qin;[~SleePy] Sorry for the late reply. I have a fix ready and is working on the tests. Will submit a patch this week.;;;","04/Aug/20 03:46;becket_qin;[~pnowojski] [~SleePy] Somehow the PR info was not updated in the ticket. Would you help take a look? 

[https://github.com/apache/flink/pull/13044];;;","04/Sep/20 07:59;zhuzh;Hi [~becket_qin], what's the status of the pull request?;;;","07/Sep/20 00:44;becket_qin;[~zhuzh] Just some minor items left, should be checked in soon.

 ;;;","07/Sep/20 01:02;zhuzh;Thanks for the updates! [~becket_qin];;;","09/Sep/20 01:33;becket_qin;Master: d850871518..99cd44f203

release-1.11: 3dc019eaff9a30..7286c662af636;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error messages from BashJavaUtils are eaten,FLINK-18639,13318161,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,20/Jul/20 05:50,07/Aug/20 09:30,13/Jul/23 08:12,20/Jul/20 09:44,1.11.0,1.12.0,,,,1.11.2,1.12.0,,Deployment / Scripts,,,,,0,pull-request-available,,,,,"Shell scripts execute BashJavaUtils for generating memory related JVM parameters and dynamic configurations. When there's a problem in configuration that the memory sizes cannot be properly calculated, the script will not launch the Flink daemon and exit with non-zero code. In such cases, error messages from BashJavaUtils describing the reason of failure are missing, making it hard for users to understand what's wrong and how to fix the problem.",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18806,,,,FLINK-18806,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 09:44:24 UTC 2020,,,,,,,,,,"0|z0gzug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/20 09:44;xtsong;Fixed via:
 * master: 7adbbd3d1f085789dfd31eab4b170838ef665255
 * release-1.11: 85e66e856d11151da61ef5677941c99809c01f4c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowData's row kind do not assigned from input row data when sink code generate and physical type info is pojo type,FLINK-18632,13317430,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzy3261944,lzy3261944,lzy3261944,17/Jul/20 14:52,24/Jul/20 08:22,13/Jul/23 08:12,24/Jul/20 08:01,1.10.0,1.11.0,,,,1.11.2,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"I use tuple type and pojo type to test retract stream, the test data is same and when i use 

toRetractStream(Table table, Class<T> clazz) api, the retract msg become to insert msg from sink conversion. I found the SinkCodeGenerator object did not give a row kind to afterIndexModify variable, so the delete msg become insert msg when it comes into processElement function generator by SinkCodeGenerator.

At last i add line of code like the pic in attachment and it works, so is it a bug? 

 

 

 ",mac os 10.15.3,jark,libenchao,lzy3261944,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/20 14:48;lzy3261944;bug.jpg;https://issues.apache.org/jira/secure/attachment/13007872/bug.jpg","17/Jul/20 15:17;lzy3261944;test_code.png;https://issues.apache.org/jira/secure/attachment/13007878/test_code.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 08:01:45 UTC 2020,,,,,,,,,,"0|z0gvc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/20 15:22;lzy3261944;When i use pojo type, result are :

(true,Result\{user='Bob', cnt=1})
(true,Result\{user='Bob', cnt=1})
(true,Result\{user='Bob', cnt=1})

and tuple type's result are 

(true,(1,Bob))
(false,(1,Bob))
(true,(1,Bob))

the picture test_code.png in attachment is my test case.

 

 ;;;","21/Jul/20 12:35;jark;I think you are right. Are you interested to contribute a fix for it?;;;","22/Jul/20 03:05;lzy3261944;Hi,Jark Wu, i am glad to fix it, thank you!;;;","24/Jul/20 08:01;jark;- master: fc3623550f7729795847a37803292c6d3ee4d324
- release-1.11: f8b98cbc4e10e69ff0ea08388224f4994de474d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectedStreams#keyBy can not derive key TypeInformation for lambda KeySelectors,FLINK-18629,13317409,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,17/Jul/20 12:52,24/Jul/20 08:51,13/Jul/23 08:12,24/Jul/20 08:51,1.10.0,1.11.0,1.12.0,,,1.12.0,,,API / DataStream,,,,,0,pull-request-available,,,,,"Following test fails:
{code}
	@Test
	public void testKeyedConnectedStreamsType() {
		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

		DataStreamSource<Integer> stream1 = env.fromElements(1, 2);
		DataStreamSource<Integer> stream2 = env.fromElements(1, 2);

		ConnectedStreams<Integer, Integer> connectedStreams = stream1.connect(stream2)
			.keyBy(v -> v, v -> v);

		KeyedStream<?, ?> firstKeyedInput = (KeyedStream<?, ?>) connectedStreams.getFirstInput();
		KeyedStream<?, ?> secondKeyedInput = (KeyedStream<?, ?>) connectedStreams.getSecondInput();
		assertThat(firstKeyedInput.getKeyType(), equalTo(Types.INT));
		assertThat(secondKeyedInput.getKeyType(), equalTo(Types.INT));
	}
{code}

The problem is that the wildcard type is evaluated as {{Object}} for lambdas, which in turn produces {{GenericTypeInfo<Object>}} for any KeySelector provided as lambda.

I suggest changing the method signature to:
{code}
	public <K1, K2> ConnectedStreams<IN1, IN2> keyBy(
			KeySelector<IN1, K1> keySelector1,
			KeySelector<IN2, K2> keySelector2)
{code}

This would be a code compatible change. Might break the compatibility of state backend (would change derived key type info). 

Still there would be a workaround to use the second method for old programs:
{code}
	public <KEY> ConnectedStreams<IN1, IN2> keyBy(
			KeySelector<IN1, KEY> keySelector1,
			KeySelector<IN2, KEY> keySelector2,
			TypeInformation<KEY> keyType)
{code}",,aljoscha,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 08:51:52 UTC 2020,,,,,,,,,,"0|z0gv7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/20 12:53;dwysakowicz;cc [~aljoscha] WDYT?;;;","20/Jul/20 16:23;aljoscha;Yes, I believe this was a mistake. But why not
{code:java}
public <KEY> ConnectedStreams<IN1, IN2> keyBy(
			KeySelector<IN1, KEY> keySelector1,
			KeySelector<IN2, KEY> keySelector2) {code}
The key must be the same for the two streams after all.;;;","21/Jul/20 07:48;dwysakowicz;The reason is extreme conservatism. The option with a common KEY type theoretically disallows potential use case where the keys are of a different type in the same hierarchy.

E.g.
{code}
class A {};
class B extends A {};

ConnectedStreams<A, B> connectedStreams = stream1.connect(stream2)
	.keyBy(v -> v, v -> v);
{code}

Important fact here is that they would have to produce the same TypeInformation which is not easy to achieve (nevertheless possible through TypeInfoFactories).
But I am more than happy to use the version with a single key type.;;;","21/Jul/20 09:00;aljoscha;I'm happy with either approach. Currently, if you did use different keys it will just break later, when it tries to assemble the operator, right?;;;","24/Jul/20 08:51;dwysakowicz;Implemented in:
* master: 094cd530c052c2f0776883521e71c53748e7eb2c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression 2020.07.13 (most benchmarks),FLINK-18614,13317109,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,16/Jul/20 07:58,21/Jul/20 17:35,13/Jul/23 08:12,21/Jul/20 17:35,,,,,,,,,Benchmarks,,,,,0,,,,,,"[http://codespeed.dak8s.net:8000/timeline/]

 

Last good commit 0fbea46ac0271dd84fa8acd7f99f449a9a0d458c Jul 12 21:22:22 2020

FLINK-18552[tests] Update migration tests of StatefulJobWBroadcastStateMigrationITCase to cover migration till release-1.11

 

First bad commit a028ba216a35a70c4f2d337bdede195bf7701192 Jul 8 13:48:35 2020

FLINK-18528[table] Update UNNEST to new type system

 

Commits are adjacent.

Commit a028ba is not included into release-1.11 branch or tags.

 

 ",,jark,klion26,libenchao,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 17:35:03 UTC 2020,,,,,,,,,,"0|z0gtd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/20 09:03;roman;I [submitted|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/194/] an older version (before the regression - f730e16) to benchmark again.

Compared to the [previous|http://http//codespeed.dak8s.net:8080/job/flink-master-benchmarks/6187/] benchmark of this f730e16 commit, it shows significant regression in all benchmarks:
|Benchmark|f730e16 Jul 06|f730e16 Jul 15|delta|params|
|TwoInputBenchmark.twoInputMapSink|19554.543834|12433.798288|-36.41%| |
|SerializationFrameworkMiniBenchmarks.serializerPojo|697.942095|504.415277|-27.73%| |
|KeyByBenchmarks.tupleKeyBy|6326.143904|4638.543555|-26.68%| |
|BlockingPartitionBenchmark.uncompressedMmapPartition|9291.513297|6898.695382|-25.75%| |
|SerializationFrameworkMiniBenchmarks.serializerRow|862.468117|671.346967|-22.16%| |
|KeyByBenchmarks.arrayKeyBy|3549.952163|2797.727893|-21.19%| |
|SerializationFrameworkMiniBenchmarks.serializerTuple|993.588036|792.453082|-20.24%| |
|StreamNetworkThroughputBenchmarkExecutor.networkThroughput|75836.784163|61426.172823|-19.00%|100,100ms|
|RemoteChannelThroughputBenchmark.remoteRebalance|18300.374713|14980.789678|-18.14%| |
|TwoInputBenchmark.twoInputOneIdleMapSink|11798.691534|9793.882213|-16.99%| |
|SerializationFrameworkMiniBenchmarks.serializerAvro|568.687328|481.019554|-15.42%| |
|StreamNetworkThroughputBenchmarkExecutor.networkThroughput|14136.11755|12088.743814|-14.48%|1000,100ms,SSL|
|StreamNetworkThroughputBenchmarkExecutor.networkThroughput|45936.696655|39393.861324|-14.24%|1000,1ms|
|StreamNetworkThroughputBenchmarkExecutor.networkThroughput|17105.577957|14726.522754|-13.91%|100,100ms,SSL|
|StreamNetworkThroughputBenchmarkExecutor.networkThroughput|51594.0401|44693.704593|-13.37%|1000,100ms|
|MemoryStateBackendBenchmark.stateBackends|4682.529423|4073.686595|-13.00%|MEMORY|
|SerializationFrameworkMiniBenchmarks.serializerKryo|244.001776|212.473621|-12.92%| |
|MemoryStateBackendBenchmark.stateBackends|4335.240886|3787.143054|-12.64%|FS|
|BlockingPartitionBenchmark.uncompressedFilePartition|8988.964525|7930.936505|-11.77%| |
|InputBenchmark.mapRebalanceMapSink|10833.173473|9584.143495|-11.53%| |
|StreamNetworkThroughputBenchmarkExecutor.networkThroughput|37876.283084|33631.728543|-11.21%|1000,100ms,OpenSSL|
|WindowBenchmarks.globalWindow|7398.471169|6570.142998|-11.20%| |
|BlockingPartitionBenchmark.compressedFilePartition|8269.729104|7400.71175|-10.51%| |
|MemoryStateBackendBenchmark.stateBackends|4726.982779|4278.369844|-9.49%|FS_ASYNC|
|SerializationFrameworkMiniBenchmarks.serializerHeavyString|242.494046|222.855766|-8.10%| |
|StreamNetworkBroadcastThroughputBenchmarkExecutor.networkBroadcastThroughput|1085.52807|998.249968|-8.04%| |
|InputBenchmark.mapSink|16594.379276|15328.197699|-7.63%| |
|WindowBenchmarks.sessionWindow|891.5685|829.102496|-7.01%| |
|RocksStateBackendBenchmark.stateBackends|325.958457|305.387748|-6.31%|ROCKS|
|AsyncWaitOperatorBenchmark.asyncWait|1654.341203|1550.847558|-6.26%|UNORDERED|
|WindowBenchmarks.tumblingWindow|4703.042299|4566.554823|-2.90%| |
|DataSkewStreamNetworkThroughputBenchmarkExecutor.networkSkewedThroughput|32376.829723|31532.9253|-2.61%| |
|WindowBenchmarks.slidingWindow|749.309761|736.68323|-1.69%| |
|AsyncWaitOperatorBenchmark.asyncWait|1634.387558|1608.773111|-1.57%|ORDERED|
|RocksStateBackendBenchmark.stateBackends|319.227313|315.683627|-1.11%|ROCKS_INC|
|StreamNetworkLatencyBenchmarkExecutor.networkLatency1to1|10.213493|10.213394|0.00%| |
|ContinuousFileReaderOperatorBenchmark.readFileSplit|42153.469978|42963.95343|1.92%| |

 

So I suspect there is a problem with a benchmarking stand.

 

I also can't confirm regression reliably when benchmarking locally.;;;","16/Jul/20 15:00;roman;Starting from Jul 13 (1st problematic build), there are temperature problems on the worker machine:
{code:java}
Jul 13 13:39:09 Ubuntu-1604-xenial-64-minimal kernel: [17853272.937125] CPU3: Core temperature above threshold, cpu clock throttled (total events = 1)
Jul 13 13:39:09 Ubuntu-1604-xenial-64-minimal kernel: [17853272.937127] CPU1: Package temperature above threshold, cpu clock throttled (total events = 1)
Jul 13 13:39:09 Ubuntu-1604-xenial-64-minimal kernel: [17853272.937285] CPU3: Package temperature above threshold, cpu clock throttled (total events = 1)
Jul 13 13:39:09 Ubuntu-1604-xenial-64-minimal kernel: [17853272.938124] CPU3: Core temperature/speed normal
{code}
 

While running a single benchmark, I see the temperature is actually too high:

 
{code:java}
Physical id 0: +100.0°C (high = +80.0°C, crit = +100.0°C)
Core 0: +100.0°C (high = +80.0°C, crit = +100.0°C)
Core 1: +98.0°C (high = +80.0°C, crit = +100.0°C)
Core 2: +100.0°C (high = +80.0°C, crit = +100.0°C)
Core 3: +99.0°C (high = +80.0°C, crit = +100.0°C)
{code}
 

and the frequencies decreased from the initial 3.8 GHz to 3.3 GHz and lower.

 

Besides that, I see that the current governor is powersave, which is probably not what we want.

 ;;;","16/Jul/20 15:12;roman;Brought the node offline.;;;","21/Jul/20 16:57;roman;Fan was replaced, performance numbers returned back to normal.

I also changed the governor to performance (monitoring).;;;","21/Jul/20 17:19;roman;Reopening, benchmarks are failing (javac not found);;;","21/Jul/20 17:35;roman;Fixed by adding JAVA_HOME and jdk to Jenkins config.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WordCount example failure when setting relative output path,FLINK-18612,13317097,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,zjwang,zjwang,16/Jul/20 06:39,20/Jul/20 12:31,13/Jul/23 08:12,20/Jul/20 12:31,1.11.0,1.11.1,,,,1.11.2,1.12.0,,fs,,,,,0,pull-request-available,,,,,"The failure log can be found here [log|https://pipelines.actions.githubusercontent.com/revSbsLpzrFApLL6BmCvScWt72tRe3wYUv7fCdCtThtI5bydk7/_apis/pipelines/1/runs/27244/signedlogcontent/21?urlExpires=2020-07-16T06%3A35%3A49.4559813Z&urlSigningMethod=HMACV1&urlSignature=%2FfAsJgIlIf%2BDitViRJYh0DAGJZjJwhsCGS219ZyniAA%3D] and it looks like
{code:java}
java.io.IOException: Mkdirs failed to create

at org.apache.flink.core.fs.local.LocalFileSystem.create(LocalFileSystem.java:270)
at org.apache.flink.core.fs.local.LocalFileSystemTest.testCreatingFileInPwdWithRelativePath(LocalFileSystemTest.java:382)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
at org.junit.rules.RunRules.evaluate(RunRules.java:20)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}
 

When execute the following command, we can reproduce this problem locally.
 * bin/start-cluster.sh
 * bin/flink run -p 1 examples/streaming/WordCount.jar --input input --output result

It is caused by the [commit|https://github.com/apache/flink/commit/a2deff2967b7de423b10f7f01a41c06565c37e62#diff-2010e422f5e43a971cd7134a9e0b9a5f].",,dian.fu,gaoyunhaii,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 12:31:27 UTC 2020,,,,,,,,,,"0|z0gtag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/20 05:45;gaoyunhaii;The issue is caused by when we create a file under the current user directory, we need to create the parent directory if not exists (in this case it is the current user directory), then
 # path.getParent() returns the current directory, namely ""."" in this case.
 # Flink Path object stores the path in a URI. However, according to [RFC-3986|https://tools.ietf.org/html/rfc3986] Section, 5.2.4, ""."" and "".."" will be removed when the path is stored, thus Path.toUri().getPath() will return """".
 # When trying to create the directory, the Path is transformed into new File("""") via _LocalFileSystem#pathToFile._ 
 # Then the _ _condition to judge if the directory creation is successful in _LocalFileSystem#mkdirsInternal_, it will return false since _new File("""").isDirectory()_ returns false.

Therefore, when transforming the Path to File in the LocalFileSystem, if the path is """", we should return new File(""."") instead, whose _isDirectory()_ judgement returns true and also represents the current directory.

 ;;;","20/Jul/20 12:31;gaoyunhaii;Fix via

master: 858de0640aef525e7eeb2bbd80f3f489a77a0ac4

1.11: 5646f6f5ab90d119e8fdd5490b77326552914211;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CustomizedConvertRule#convertCast drops nullability,FLINK-18608,13316908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,f.pompermaier,f.pompermaier,15/Jul/20 12:07,16/Jul/20 08:14,13/Jul/23 08:12,16/Jul/20 08:14,1.11.0,,,,,1.11.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The following program shows that nullability is not respected during the tranlastion:


{code:java}
 final TableEnvironment tableEnv = DatalinksExecutionEnvironment.getBatchTableEnv();
    final Table inputTable = tableEnv.fromValues(//
        DataTypes.ROW(//
            DataTypes.FIELD(""col1"", DataTypes.STRING()), //
            DataTypes.FIELD(""col2"", DataTypes.STRING())//
        ), //
        Row.of(1L, ""Hello""), //
        Row.of(2L, ""Hello""), //
        Row.of(3L, """"), //
        Row.of(4L, ""Ciao""));
    tableEnv.createTemporaryView(""ParquetDataset"", inputTable);
    tableEnv.executeSql(//
        ""CREATE TABLE `out` (\n"" + //
            ""col1 STRING,\n"" + //
            ""col2 STRING\n"" + //
            "") WITH (\n"" + //
            "" 'connector' = 'filesystem',\n"" + //
            "" 'format' = 'parquet',\n"" + //
            "" 'update-mode' = 'append',\n"" + //
            "" 'path' = 'file://"" + TEST_FOLDER + ""',\n"" + //
            "" 'sink.shuffle-by-partition.enable' = 'true'\n"" + //
            "")"");

    tableEnv.executeSql(""INSERT INTO `out` SELECT * FROM ParquetDataset"");
{code}


---------------------------------

Exception in thread ""main"" java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" col1, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" col2) NOT NULL
converted type:
RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL col1, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL col2) NOT NULL
rel:
LogicalProject(col1=[$0], col2=[$1])
  LogicalUnion(all=[true])
    LogicalProject(col1=[_UTF-16LE'1':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""], col2=[_UTF-16LE'Hello':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""])
      LogicalValues(tuples=[[{ 0 }]])
    LogicalProject(col1=[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""], col2=[_UTF-16LE'Hello':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""])
      LogicalValues(tuples=[[{ 0 }]])
    LogicalProject(col1=[_UTF-16LE'3':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""], col2=[_UTF-16LE'':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""])
      LogicalValues(tuples=[[{ 0 }]])
    LogicalProject(col1=[_UTF-16LE'4':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""], col2=[_UTF-16LE'Ciao':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""])
      LogicalValues(tuples=[[{ 0 }]])

at org.apache.calcite.sql2rel.SqlToRelConverter.checkConvertedType(SqlToRelConverter.java:465)
at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:580)
at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)
at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)
at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)
at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)
at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)
at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)
at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)
at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)",,dwysakowicz,f.pompermaier,jark,leonard,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 16 08:14:39 UTC 2020,,,,,,,,,,"0|z0gs4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/20 08:14;dwysakowicz;Fixed in:
1.11.2: fbf2a522bfec2bbef657bafad490ad579c5e197d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kerberized YARN per-job on Docker test failed to download JDK 8u251,FLINK-18600,13316803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dian.fu,dian.fu,15/Jul/20 02:31,21/Jul/20 09:56,13/Jul/23 08:12,21/Jul/20 09:56,1.11.0,1.12.0,,,,1.11.2,1.12.0,,Deployment / YARN,Tests,,,,0,pull-request-available,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{code}
+ mkdir -p /usr/java/default
+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie
+ tar --strip-components=1 -xz -C /usr/java/default/

gzip: stdin: not in gzip format
tar: Child returned status 1
{code}",,aljoscha,dian.fu,dwysakowicz,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 09:55:58 UTC 2020,,,,,,,,,,"0|z0grhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/20 03:00;jark;Looks like we started to use this JDK link since FLINK-18485. Maybe this link is not stable. 

cc [~dwysakowicz];;;","15/Jul/20 03:15;dian.fu;I have checked that I can download the version *8u251* through the oracle website after accepting the license. However, it's not available if download through CLI using the same command. I'm wondering if it's because the cookie has changed or some other reason?;;;","15/Jul/20 03:17;dian.fu;Not sure the reason why we choose oracle JDK and if we could switch to Open JDK for the tests?;;;","15/Jul/20 07:04;dwysakowicz;Oracle must've made their policies stricter. The old link from pre FLINK-18485 also does not work. I will disable the test for now until we figure out a solution.

I think it's worth checking if we can switch to Open JDK. [~aljoscha]  Do you know of any obstacles for using openJDK here?;;;","15/Jul/20 10:09;aljoscha;No, I think using OpenJDK should actually be preferable.;;;","15/Jul/20 11:59;dwysakowicz;Disabled temporarily affected tests
* master:
** 9036bc0fd8b24d0a270f964a8116f5db781e4b3c
* 1.11:
** 065eb7242e2fb437962b89f276fd82513db9ffcd
;;;","21/Jul/20 09:55;dwysakowicz;Fixed in:
* master
** 8a4ec1a1ba3398b43cda640807b2f2bf98a7f992, 1908b2ce6ffb8efc7d339136787494b4fe70846f
* 1.11
** f56c2199774f3d60efabfa4257d8bff4b293cc8b, d3cc4aab80e97c1e5299cbd3788bf0abf4633d59;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock during job shutdown,FLINK-18595,13316632,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zjwang,dian.fu,dian.fu,14/Jul/20 07:38,30/Jul/20 07:36,13/Jul/23 08:12,30/Jul/20 03:32,1.10.0,1.10.1,1.11.0,1.11.1,,1.10.2,1.11.2,1.12.0,Runtime / Network,,,,,0,pull-request-available,,,,,"https://travis-ci.org/github/apache/flink/jobs/707843779

{code}
Found one Java-level deadlock:
=============================
""Canceler for Flat Map -> Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641)."":
  waiting to lock monitor 0x00007f51f655e228 (object 0x00000000812b9180, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue),
  which is held by ""Flat Map -> Sink: Unnamed (9/12)""
""Flat Map -> Sink: Unnamed (9/12)"":
  waiting to lock monitor 0x000055fb00bb4b88 (object 0x00000000812b9210, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue),
  which is held by ""Canceler for Flat Map -> Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).""

Java stack information for the threads listed above:
===================================================
""Canceler for Flat Map -> Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641)."":
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360)
	- waiting to lock <0x00000000812b9180> (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315)
	at org.apache.flink.runtime.io.network.b4511: No such processuffer.LocalBufferPool.recycle(LocalBufferPool.java:305)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.releaseAll(RemoteInputChannel.java:665)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:254)
	- locked <0x00000000812b9210> (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:431)
	- locked <0x0000000080ba2488> (a java.lang.Object)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:85)
	at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:901)
	at org.apache.flink.runtime.taskmanager.Task$$Lambda$434/985222953.run(Unknown Source)
	at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1370)
	at java.lang.Thread.run(Thread.java:748)
""Flat Map -> Sink: Unnamed (9/12)"":
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360)
	- waiting to lock <0x00000000812b9210> (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:305)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.addExclusiveBuffer(RemoteInputChannel.java:629)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.recycle(RemoteInputChannel.java:314)
	- locked <0x00000000812b9180> (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110)
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100)
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171)
	at org.apache.flink.streaming.runtime.io.CachedBufferStorage.close(CachedBufferStorage.java:113)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.cleanup(CheckpointedInputGate.java:216)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.close(StreamTaskNetworkInput.java:208)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.close(StreamOneInputProcessor.java:82)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanup(StreamTask.java:298)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:555)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:480)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
	at org.apache.flink.runtime.taskmanager.T
ask.run(Task.java:533)
	at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.
{code}",,dian.fu,felixzheng,klion26,ym,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 03:32:09 UTC 2020,,,,,,,,,,"0|z0gqfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/20 06:52;zjwang;Thanks for reporting this [~dian.fu]. It is caused by concurrently recycling exclusive and floating buffers by both task thread and canceller thread, and it is only valid in release-1.10 branch. I will consider how to fix it properly.;;;","30/Jul/20 03:32;zjwang;Merged in master: c137102eb05e438bd97ec5e397ecb3d17342e6c1
Merged in release-1.11: aa688c2e7474a90ba2e077abb8a7468c0d2784ef
Merged in release-1.10: e2686808b22563871513582fb82700c63e1c1fd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The link is broken in kafka doc,FLINK-18594,13316616,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,dian.fu,dian.fu,14/Jul/20 06:02,16/Jul/20 02:17,13/Jul/23 08:12,15/Jul/20 04:05,1.12.0,,,,,1.12.0,,,Documentation,,,,,0,pull-request-available,,,,,"https://ci.apache.org/builders/flink-docs-master/builds/1897/steps/Build%20docs/logs/stdio

{code}
  Liquid Exception: Could not find document 'dev/stream/state/checkpointing.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/kafka.zh.md
Could not find document 'dev/stream/state/checkpointing.md' in tag 'link'.
{code}",,dian.fu,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 16 02:17:28 UTC 2020,,,,,,,,,,"0|z0gqc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/20 06:38;leonard;Hi, [~dian.fu] 
I'd like to fix this, could you help assign this to me ?;;;","15/Jul/20 04:05;lzljs3620320;master: bfdb3c8339347ad59ef5debb71fb171ffa87d4c8;;;","16/Jul/20 02:17;dian.fu;follow-up fix: 3a7906e9729fa7bf84020502c339c1f76c34f7be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the format issue for metrics web page,FLINK-18591,13316599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjwang,zjwang,zjwang,14/Jul/20 03:36,15/Jul/20 08:18,13/Jul/23 08:12,14/Jul/20 10:42,1.11.0,,,,,1.11.1,1.12.0,,Documentation,Runtime / Metrics,,,,0,pull-request-available,,,,,The formatting issue is shown by link https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/metrics.html#checkpointing,,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 10:42:41 UTC 2020,,,,,,,,,,"0|z0gq88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/20 10:42;zjwang;Merged in release-1.11: d6473f32aa159c6242e4293a60be179135f0e1f2
Merged in master: ba006cef0e8cd7991a662e67f998e72b1fd81386;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive ddl create table should support 'if not exists',FLINK-18588,13316469,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wtog,wtog,wtog,13/Jul/20 14:11,17/Jul/20 10:45,13/Jul/23 08:12,17/Jul/20 10:45,1.11.0,,,,,1.11.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"FlinkHiveSqlParser cant parse sql 'create table if not exists tbl (x int)' due to 'if not exists'
{code:java}
// code placeholder
java.lang.RuntimeException: Error while parsing SQL: create table if not exists tbl (x int)java.lang.RuntimeException: Error while parsing SQL: create table if not exists tbl (x int)
 at org.apache.calcite.sql.parser.SqlParserTest$TesterImpl.parseStmtAndHandleEx(SqlParserTest.java:8861) at org.apache.calcite.sql.parser.SqlParserTest$TesterImpl.check(SqlParserTest.java:8845) at org.apache.calcite.sql.parser.SqlParserTest$Sql.ok(SqlParserTest.java:9192) at org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.testCreateTable(FlinkHiveSqlParserImplTest.java:177) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:40) at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Iterator.forEachRemaining(Iterator.java:116) at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80) at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:71) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:229) at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:197) at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:211) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:191) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128) at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered ""not"" at line 1, column 17.Was expecting one of:    <EOF>     ""ROW"" ...    ""COMMENT"" ...    ""LOCATION"" ...    ""PARTITIONED"" ...    ""STORED"" ...    ""TBLPROPERTIES"" ...    ""("" ...    ""."" ...
{code}
 ",,godfreyhe,jark,kezhuw,leonard,libenchao,lirui,lzljs3620320,txhsj,wtog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 10:45:02 UTC 2020,,,,,,,,,,"0|z0gpfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/20 15:24;wtog;[~lzljs3620320] please help to review this pr;;;","14/Jul/20 14:02;wtog;[~jark] please help to review this pr;;;","14/Jul/20 14:39;jark;[~wtog] I CC'ed someone familiar with this part to review in the PR. ;;;","17/Jul/20 06:39;lzljs3620320;Do other DDLs have same problem? (Like create database, create function...);;;","17/Jul/20 06:45;lirui;[~lzljs3620320] I quickly checked DATABASE and VIEW and both of them support IF NOT EXISTS. FUNCTION doesn't support this, which is the same as Hive syntax.;;;","17/Jul/20 10:45;lzljs3620320;master: 9c4a984565f28996d19beb78f87871746e0d6906

release-1.11: d2911e8ca8ae33b5aec9adf44141ce67b32f29e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic index can not work in new DynamicTableSink,FLINK-18585,13316414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,13/Jul/20 09:57,15/Jul/20 08:16,13/Jul/23 08:12,14/Jul/20 02:45,1.11.0,,,,,1.11.1,1.12.0,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,because the IndexGenerator.open() was not inited properly.,,dian.fu,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 02:45:40 UTC 2020,,,,,,,,,,"0|z0gp34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/20 02:45;jark;Fixed in 
- master (1.12.0): 273830730c9524278576255c56c17c195f7c6bf9
- 1.11.1: 3cd2a6deb2c08e58f633669c18435d6647e84cc0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink,FLINK-18583,13316406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,guoyangze,guoyangze,guoyangze,13/Jul/20 09:21,30/Nov/21 20:37,13/Jul/23 08:12,14/Jul/20 11:32,1.11.0,,,,,1.11.1,1.12.0,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,"The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink.

It is caused by [this line|https://github.com/apache/flink/blob/0fbea46ac0271dd84fa8acd7f99f449a9a0d458c/flink-connectors/flink-connector-elasticsearch6/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/Elasticsearch6DynamicSink.java#L285]",,dian.fu,guoyangze,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 11:32:27 UTC 2020,,,,,,,,,,"0|z0gp1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/20 09:22;guoyangze;[~jark] Could you assign this to me?;;;","13/Jul/20 10:07;jark;Assigned to you [~guoyangze].;;;","14/Jul/20 02:18;dian.fu;Hi [~jark] [~guoyangze], do you think this issue is a blocker of 1.11.1?;;;","14/Jul/20 02:24;leonard;Hi， [~dian.fu] 
This issue will lead loss data when write data to Es6， I think it should be a blocker of 1.11.1;;;","14/Jul/20 02:29;jark;I think this doesn't need to be a blocker, because it only affects es6. But AFAIK, this is promising to catch up with 1.11.1 if we will wait for FLINK-18573. ;;;","14/Jul/20 02:30;leonard;[~guoyangze] 
 Are you still working on this ？ and I want to have a quick fix for 1.11.1 if you do not mind。;;;","14/Jul/20 02:38;dian.fu;Thanks. I will change the priority to ""Critical"". Nevertheless, I think there are still some time to catch up with 1.11.1. :);;;","14/Jul/20 11:32;jark;Fixed in
- master (1.12.0): 492c7b0db94798290747f6aea5f0262b8dc012ce
- 1.11.1: 50f2fb9fbcfc162f5933a9f06519742de020001e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot find GC cleaner with java version previous jdk8u72(-b01),FLINK-18581,13316403,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,xtsong,xtsong,13/Jul/20 09:12,29/Jul/20 07:45,13/Jul/23 08:12,28/Jul/20 11:27,1.11.0,,,,,1.11.2,1.12.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"{{JavaGcCleanerWrapper}} is looking for the package-private method {{Reference.tryHandlePending}} using reflection. However, the method is first introduced in the version jdk8u72(-b01). Therefore, if an older version JDK is used, the method cannot be found and Flink will fail.

See also this [ML thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Error-GC-Cleaner-Provider-Flink-1-11-0-td36565.html].",,azagrebin,dian.fu,godfreyhe,guoyangze,jark,klion26,libenchao,rmetzger,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 11:27:14 UTC 2020,,,,,,,,,,"0|z0gp0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/20 09:13;xtsong;cc [~azagrebin];;;","20/Jul/20 15:13;azagrebin;Reference.tryHandlePending was introduced somewhen before 8u202 (I have 8u172 where it works).

I checked 8u40, it looks like JVM did not have the optimisation of triggering the GC phantom cleaner queue in direct memory reservation. They just tried one GC call which is quite suboptimal for us. Somewhere after 8u40, they factored out java.lang.ref.Reference#ReferenceHandler#run into static Reference#tryHandlePending to enable the optimisation.

java.lang.ref.Reference#ReferenceHandler#run is quite similar to Reference#tryHandlePending in 8u40. We could create an instance of ReferenceHandler, which is stateless, and call its run method, which uses the same static vars of Reference class.

Not sure that it is really worth the effort. So alternatively, I can binary-find the JVM version where they introduced the Reference#tryHandlePending and we can update the 1.11 release notes that we support Java 8 starting from that build.
cc [~trohrmann];;;","21/Jul/20 14:35;trohrmann;I found the first occurrence of {{Reference.tryHandlePending}} to be introduced in OpenJDK with the version jdk8u72-b01 http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/5ad1e9e8e841/src/share/classes/java/lang/ref/Reference.java. This version should be already a couple of years old.

[~azagrebin] I think calling {{Reference.ReferenceHandler.run}} would not work because it is an infinite loop.

I think this is price we are paying for using internal JVM methods which can change across minor versions and which is very unfortunate. Let's keep this as a warning for the future and let's try to avoid it.

I see two ways forward:

1) Only support Java8 version later or equal than jdk8u72-b01. This would entail that we throw an appropriate exception with a clear message saying that one needs to update the java version. The main problem I see with this approach is that other JVM implementation (other than OpenJDK) might not have these private methods. Have we ever tried running Flink with a different JVM implementation [~azagrebin]? Moreover, there is no guarantee that this method won't disappear in some future version.
2) Getting rid of the clean up mechanism which relies on internal JVM methods and implement our own mechanism. This solution which involve a much higher effort, though.

Maybe there is also some middle ground by trying to call {{tryHandlerPending}} and if this method does not exist to fall back to triggering the normal GC or some other mean to trigger the clean up.;;;","21/Jul/20 15:00;azagrebin;True, I overlooked the loop in ReferenceHandler.run.
{quote}1) Only support Java8 version later or equal than jdk8u72-b01. This would entail that we throw an appropriate exception with a clear message saying that one needs to update the java version. The main problem I see with this approach is that other JVM implementation (other than OpenJDK) might not have these private methods. Have we ever tried running Flink with a different JVM implementation [~azagrebin]? Moreover, there is no guarantee that this method won't disappear in some future version.
{quote}
I have Oracle JVM locally which I tried. Oracle jdk8u72 has the required method.
{quote}I think this is price we are paying for using internal JVM methods which can change across minor versions and which is very unfortunate. Let's keep this as a warning for the future and let's try to avoid it.
{quote}
I totally agree
{quote}2) Getting rid of the clean up mechanism which relies on internal JVM methods and implement our own mechanism. This solution which involve a much higher effort, though.
{quote}
This is hard to achieve our goal. We need to run release once the memory link is ready for GC which can be detected only by GC. Or we have to change the memory API/semantics somehow.
{quote}Maybe there is also some middle ground by trying to call {{tryHandlerPending}} and if this method does not exist to fall back to triggering the normal GC or some other mean to trigger the clean up.
{quote}
This is an option. We can wait a bit before calling GC to call it not so often and give some time to the previous GC call. This is less performant and we can output warning about the JVM version in this case. I can try it.;;;","28/Jul/20 11:27;azagrebin;merged into master by 2f03841d5414f9d4a4b810810317c0250065264e

merged into 1.11 by fe95187edfe742b64a1f7147e57856c931ef05c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DownloadPipelineArtifact fails occasionally,FLINK-18574,13316370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,dian.fu,dian.fu,13/Jul/20 02:20,29/Nov/21 12:21,13/Jul/23 08:12,29/Nov/21 12:21,1.13.3,1.14.0,1.15.0,,,,,,Build System / Azure Pipelines,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4414&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=c1dcc74d-b153-580b-95a4-73d5cd91b503

{code}
2020-07-11T20:38:15.9784577Z ##[section]Starting: DownloadPipelineArtifact
2020-07-11T20:38:15.9799212Z ==============================================================================
2020-07-11T20:38:15.9799811Z Task         : Download Pipeline Artifacts
2020-07-11T20:38:15.9800305Z Description  : Download build and pipeline artifacts
2020-07-11T20:38:15.9800753Z Version      : 2.3.1
2020-07-11T20:38:15.9801328Z Author       : Microsoft Corporation
2020-07-11T20:38:15.9801942Z Help         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/download-pipeline-artifact
2020-07-11T20:38:15.9802614Z ==============================================================================
2020-07-11T20:38:16.5226960Z Download from the specified build: #4414
2020-07-11T20:38:16.5246147Z Download artifact to: /home/agent07/myagent/_work/2/flink_artifact
2020-07-11T20:38:20.6397951Z Information, ApplicationInsightsTelemetrySender will correlate events with X-TFS-Session 38415228-535b-4652-8afc-41e3fcfa5c67
2020-07-11T20:38:20.6546003Z Information, DedupManifestArtifactClient will correlate http requests with X-TFS-Session 38415228-535b-4652-8afc-41e3fcfa5c67
2020-07-11T20:38:20.6563787Z Information, Minimatch patterns: [**]
2020-07-11T20:38:20.7577072Z Information, ArtifactHttpRetryMessageHandler.SendAsync: https://vsblobprodsu6weu.vsblob.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/_apis/dedup/nodes/4C7343B40F7AB12049F6929A6A4ECD738EFADAC85D010AF5877ACD0682CC6C5802 attempt 1/6 failed with StatusCode RedirectMethod, IsRetryableResponse False
2020-07-11T20:38:35.8527079Z Information, Filtered 130283 files from the Minimatch filters supplied.
2020-07-11T20:38:35.8865820Z Information, Downloaded 0.0 MB out of 1,170.3 MB (0%).
2020-07-11T20:38:40.8869716Z Information, Downloaded 4.6 MB out of 1,170.3 MB (0%).
2020-07-11T20:38:45.9379430Z Information, Downloaded 24.5 MB out of 1,170.3 MB (2%).
2020-07-11T20:38:50.9539945Z Information, Downloaded 41.8 MB out of 1,170.3 MB (4%).
2020-07-11T20:38:53.5482129Z Warning, [https://nsevsblobprodsu6weus42.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/454487F48A36900D2D7FE8928376FDB49F9F3BF90B3A5474B7EF8D9ABE8BDC7201?sv=2019-02-02&sr=b&sig=HGudFZh7Cd3TOBUMGO7rIq0DJxPQORzHkSVs3iYeDPc%3D&spr=https&se=2020-07-12T21%3A16%3A18Z&sp=r&rscl=x-e2eid-e61f9696-89054b42-93868ec3-c0739ba7-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:38:53.5484031Z  ---> System.Net.Http.CurlException: Couldn't connect to server
2020-07-11T20:38:53.5484420Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:38:53.5484929Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:38:53.5485733Z    --- End of inner exception stack trace ---
2020-07-11T20:38:53.5486394Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:38:53.5487612Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:38:53.5488656Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:38:53.5489700Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:38:53.5490174Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:38:53.5539270Z Warning, [https://nsevsblobprodsu6weus42.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/D6F53A2F3158430B1858F7F365D27B2ED5AFB447DADC95364976A53004BFFE2D01?sv=2019-02-02&sr=b&sig=q%2FnWMXOfzmPSDzt3xRm0tOkBDyW4V%2FlOnqDdCrg5W%2Fo%3D&spr=https&se=2020-07-12T20%3A50%3A24Z&sp=r&rscl=x-e2eid-e61f9696-89054b42-93868ec3-c0739ba7-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:38:53.5541156Z  ---> System.Net.Http.CurlException: SSL connect error
2020-07-11T20:38:53.5541505Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:38:53.5541943Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:38:53.5542682Z    --- End of inner exception stack trace ---
2020-07-11T20:38:53.5543271Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:38:53.5543957Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:38:53.5544806Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:38:53.5545448Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:38:53.5545898Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:38:53.6941267Z Warning, [https://5bsvsblobprodsu6weus71.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/7ADC0448A700DF14D6667CF4B868758DDEE62614797E9466EDF3D30DCE0F085F01?sv=2019-02-02&sr=b&sig=2jTPaSCJ46jG8XUmzv2hp0kf0eqaKj58hDooopGC5t0%3D&spr=https&se=2020-07-12T21%3A28%3A46Z&sp=r&rscl=x-e2eid-e61f9696-89054b42-93868ec3-c0739ba7-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:38:53.6942690Z  ---> System.Net.Http.CurlException: Couldn't connect to server
2020-07-11T20:38:53.6943216Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:38:53.6943649Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:38:53.6944184Z    --- End of inner exception stack trace ---
2020-07-11T20:38:53.6944800Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:38:53.6945819Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:38:53.6946586Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:38:53.6947444Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:38:53.6948020Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:38:54.0968867Z Warning, [https://pyvvsblobprodsu6weus63.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/D79B69D371FF651C939100C2D2CECA7910B4CDD55F9B765A1C315FFC93F8A75F01?sv=2019-02-02&sr=b&sig=6QrQdFremmEN430UNSvXDnyu2fu8srqS28FKGFDVTf4%3D&spr=https&se=2020-07-12T20%3A50%3A30Z&sp=r&rscl=x-e2eid-e61f9696-89054b42-93868ec3-c0739ba7-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:38:54.0970220Z  ---> System.Net.Http.CurlException: SSL connect error
2020-07-11T20:38:54.0970582Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:38:54.0971017Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:38:54.0971761Z    --- End of inner exception stack trace ---
2020-07-11T20:38:54.0972204Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:38:54.0972866Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:38:54.0973621Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:38:54.0974286Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:38:54.0974722Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:38:55.1626846Z Warning, [https://a8zvsblobprodsu6weus60.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/A3BA979530E92B3F056D191A0C82AFA691024B770C50AEA865F7FDE64C988CAC01?sv=2019-02-02&sr=b&sig=PLTkSJjDn7NQwDt3v9QWWxHjt6oFgfjbyzOYLwf7N6I%3D&spr=https&se=2020-07-12T21%3A38%3A22Z&sp=r&rscl=x-e2eid-b05b9726-9f394923-9f27bd6b-6182bfeb-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:38:55.1628843Z  ---> System.Net.Http.CurlException: SSL connect error
2020-07-11T20:38:55.1629389Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:38:55.1630048Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:38:55.1630972Z    --- End of inner exception stack trace ---
2020-07-11T20:38:55.1631630Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:38:55.1632702Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:38:55.1633725Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:38:55.1634448Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:38:55.1634903Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:38:55.9549746Z Information, Downloaded 76.3 MB out of 1,170.3 MB (7%).
2020-07-11T20:38:56.0827220Z Information, ArtifactHttpRetryMessageHandler.SendAsync: https://vsblobprodsu6weu.vsblob.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/_apis/dedup/urls attempt 1/6 failed with HttpRequestException: 'An error occurred while sending the request.', HttpStatusCode 
2020-07-11T20:38:56.0828823Z Information, ArtifactHttpRetryMessageHandler.SendAsync: https://vsblobprodsu6weu.vsblob.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/_apis/dedup/urls attempt 1/6 throwing HttpRequestException with AttemptCount 1, LastBackoff 10000ms
2020-07-11T20:39:00.9565227Z Information, Downloaded 102.5 MB out of 1,170.3 MB (9%).
2020-07-11T20:39:05.9566745Z Information, Downloaded 124.5 MB out of 1,170.3 MB (11%).
2020-07-11T20:39:08.3302410Z Warning, [https://6iwvsblobprodsu6weus86.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/BA1E29AA45DD8352ED0EFB79D195851A2514C0980BB40BCFD5BECBC5D0AFAAA901?sv=2019-02-02&sr=b&sig=oVISB8tNxuDTtHCO9au7iiWP3rMNtDPF%2FdVJa4JKZQE%3D&spr=https&se=2020-07-12T20%3A43%3A35Z&sp=r&rscl=x-e2eid-003ac1f0-b503435e-a86e8677-1218c06a-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:39:08.3304289Z  ---> System.Net.Http.CurlException: Failure when receiving data from the peer
2020-07-11T20:39:08.3304674Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:39:08.3305112Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:39:08.3305723Z    --- End of inner exception stack trace ---
2020-07-11T20:39:08.3306197Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:39:08.3306885Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:39:08.3307721Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:39:08.3308524Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:39:08.3308979Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:39:10.9567722Z Information, Downloaded 150.5 MB out of 1,170.3 MB (13%).
2020-07-11T20:39:15.9564344Z Information, Downloaded 167.9 MB out of 1,170.3 MB (14%).
2020-07-11T20:39:20.9568200Z Information, Downloaded 199.3 MB out of 1,170.3 MB (17%).
2020-07-11T20:39:22.3954599Z Warning, [https://w0cvsblobprodsu6weus7.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/789AA70FD3FBB149B23226CA15AE908929E957F321A3EBD3B1D82152015635D401?sv=2019-02-02&sr=b&sig=QwGIR1EgLMtLneq%2FJJ8n1xEIYkPiT%2FHtCrMfMZ0Zhqk%3D&spr=https&se=2020-07-12T21%3A28%3A15Z&sp=r&rscl=x-e2eid-32d7c142-d26641a3-be9e4491-516161ba-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:39:22.3956131Z  ---> System.Net.Http.CurlException: SSL connect error
2020-07-11T20:39:22.3956521Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:39:22.3956955Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:39:22.3957599Z    --- End of inner exception stack trace ---
2020-07-11T20:39:22.3958054Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:39:22.3958730Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:39:22.3959402Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:39:22.3960028Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:39:22.3960483Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:39:25.9568949Z Information, Downloaded 220.5 MB out of 1,170.3 MB (19%).
2020-07-11T20:39:26.0066603Z Information, ArtifactHttpRetryMessageHandler.SendAsync: https://vsblobprodsu6weu.vsblob.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/_apis/dedup/urls attempt 1/6 failed with TimeoutException: 'The HTTP request timed out after 00:00:50.'
2020-07-11T20:39:30.9578658Z Information, Downloaded 242.1 MB out of 1,170.3 MB (21%).
2020-07-11T20:39:35.9573693Z Information, Downloaded 273.2 MB out of 1,170.3 MB (23%).
2020-07-11T20:39:40.5597310Z Warning, [https://9ajvsblobprodsu6weus27.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/F12A8CBF081950C559873255AD113B0046BA59F3761CE7B3DA5E68026EEDBC2C01?sv=2019-02-02&sr=b&sig=ovjmvifD5XGXmuhjIVDrIROLrWdQWoXBmC8pZnnyboE%3D&spr=https&se=2020-07-12T20%3A56%3A32Z&sp=r&rscl=x-e2eid-a28dc714-d0334466-9c1c5d4a-f9ec4500-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Net.Http.HttpRequestException: An error occurred while sending the request.
2020-07-11T20:39:40.5599430Z  ---> System.Net.Http.CurlException: SSL connect error
2020-07-11T20:39:40.5599903Z    at System.Net.Http.CurlHandler.ThrowIfCURLEError(CURLcode error)
2020-07-11T20:39:40.5600338Z    at System.Net.Http.CurlHandler.MultiAgent.FinishRequest(StrongToWeakReference`1 easyWrapper, CURLcode messageResult)
2020-07-11T20:39:40.5600862Z    --- End of inner exception stack trace ---
2020-07-11T20:39:40.5601302Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:39:40.5602123Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:39:40.5602776Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:39:40.5603520Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:39:40.5603974Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:39:40.9576811Z Information, Downloaded 320.8 MB out of 1,170.3 MB (27%).
2020-07-11T20:39:44.7197907Z Warning, [https://2dbvsblobprodsu6weus13.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/97F9555111596F9C6A6029933166A8DEB65C82E3195CC53BD5B3530AF5F116AF01?sv=2019-02-02&sr=b&sig=nns%2FOPkeGlEC46D6gIgITFDFCLoZ6uvPIdev4%2FAJLPM%3D&spr=https&se=2020-07-12T21%3A35%3A35Z&sp=r&rscl=x-e2eid-f2823fc8-73754c21-9ef62354-f689c017-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:39:44.7199369Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:39:44.7200174Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:39:44.7200840Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:39:44.7201565Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:39:44.7202115Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:39:44.7202579Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:39:44.7203182Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:39:44.7203805Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:39:44.7204523Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:39:44.7204992Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:39:45.9575686Z Information, Downloaded 355.7 MB out of 1,170.3 MB (30%).
2020-07-11T20:39:50.9585767Z Information, Downloaded 358.4 MB out of 1,170.3 MB (31%).
2020-07-11T20:39:55.9590990Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:00.9597331Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:04.3181414Z Warning, [https://w0cvsblobprodsu6weus7.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/BC63116592C665D94FD27638B7CA0D00ECE1DE90685CD89C787A1AB98A77106D01?sv=2019-02-02&sr=b&sig=G91jwH0SfRSayAg5IEHlXRvd98ianrE9pyWTS2lwwlY%3D&spr=https&se=2020-07-12T20%3A44%3A07Z&sp=r&rscl=x-e2eid-bddfd3c4-6eb94764-b16e2524-6962b1cc-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:04.3183320Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:04.3184354Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:04.3185739Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:04.3186834Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:04.3187459Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:04.3187989Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:04.3188450Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:04.3188986Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:04.3189643Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:04.3190164Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:05.9594557Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:08.4277387Z Warning, [https://l78vsblobprodsu6weus64.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/C2A5B0E98CF867FE8DF16114EAC2703418ECD2760BCB475EC5F575EE6CC2275401?sv=2019-02-02&sr=b&sig=aU2IBruTvNM7MzVMH05pEx7vi9oLTW0QhT14qpe9IgE%3D&spr=https&se=2020-07-12T20%3A45%3A35Z&sp=r&rscl=x-e2eid-07eab3ca-cf804c35-8e1fd9be-1c874d72-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:08.4279294Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:08.4280109Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:08.4280908Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:08.4281649Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:08.4282155Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:08.4282669Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:08.4283213Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:08.4283974Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:08.4284894Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:08.4285397Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:10.9600115Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:15.9602943Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:20.9610944Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:21.0371725Z Warning, [https://alcvsblobprodsu6weus69.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/98F6B79B778F7B0A15415BD750C3A8A097D650511CB4EC8115188E115C47053F01?sv=2019-02-02&sr=b&sig=XZnRYHs2v3oyZewGKgqwH3SRbS3YUR1Z36dRBzUk8ug%3D&spr=https&se=2020-07-12T21%3A35%3A50Z&sp=r&rscl=x-e2eid-f78db5fe-69214bdd-a6d4a151-e929b091-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:21.0373470Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:21.0374415Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:21.0375136Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:21.0375869Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:21.0376310Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:21.0376827Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:21.0377395Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:21.0377989Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:21.0378668Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:21.0379101Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:22.3402969Z Warning, [https://5bsvsblobprodsu6weus71.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/06952CDEE202386A95F891F8A9BF755484E3CBD0D740C5085765AE0F35E0F2C001?sv=2019-02-02&sr=b&sig=bXNU757IMp9WpwE4NOM6zUOGiUmnbgez8a9RTUlza2Q%3D&spr=https&se=2020-07-12T21%3A01%3A32Z&sp=r&rscl=x-e2eid-8ec94767-49684cff-b9ff7495-be44bb40-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:22.3405015Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:22.3405696Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:22.3406418Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:22.3407285Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:22.3407733Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:22.3408294Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:22.3409040Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:22.3409557Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:22.3410269Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:22.3410700Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.4348528Z Warning, [https://i40vsblobprodsu6weus59.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/4EC9108ACC9238BE77169B8E565024467FC4595AD84A780C24338FE0662EE98001?sv=2019-02-02&sr=b&sig=85sojCVugmmya1kEs1XR2ioykLQUJgXaWAHA3ZzxUAw%3D&spr=https&se=2020-07-12T21%3A18%3A27Z&sp=r&rscl=x-e2eid-8ec94767-49684cff-b9ff7495-be44bb40-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:23.4350224Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:23.4350945Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:23.4351761Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:23.4352476Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:23.4353257Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.4353777Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:23.4354411Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.4355231Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:23.4356253Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:23.4356950Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.6507973Z Warning, [https://y78vsblobprodsu6weus75.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/93D347ED023C5269421DC780508F0F9411216CA883AF172916E11D40036D8E4F01?sv=2019-02-02&sr=b&sig=nfxhkOMSZpvyZlXrhh3SB2dVGgzTtsWsi2kv0lxnlmU%3D&spr=https&se=2020-07-12T21%3A34%3A38Z&sp=r&rscl=x-e2eid-673830c3-73ef4a7a-a99b9cb5-ec6cba7b-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:23.6509498Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:23.6510221Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:23.6511062Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:23.6511787Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:23.6512285Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.6512923Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:23.6513708Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.6514279Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:23.6514949Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:23.6515553Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.6583466Z Warning, [https://y78vsblobprodsu6weus75.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/DAD8B7A5CEF04D7E02A1122148A5C095D9B21BE9CA290A172453A42AE55565C001?sv=2019-02-02&sr=b&sig=0gWdG16PX0aW9mvNMpV7uuM%2FJFiU2F6sofsom8pei9o%3D&spr=https&se=2020-07-12T20%3A51%3A15Z&sp=r&rscl=x-e2eid-673830c3-73ef4a7a-a99b9cb5-ec6cba7b-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:23.6585036Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:23.6585704Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:23.6586453Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:23.6587730Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:23.6588250Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.6588724Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:23.6589268Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:23.6589785Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:23.6590481Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:23.6590964Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:24.1738268Z Warning, [https://r23vsblobprodsu6weus80.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/5147661AD68CED1FF3C4B6D86963853EA20317C8BD10323C666A38ACEAA9174201?sv=2019-02-02&sr=b&sig=Z0YfXz1NlJuPoKY15vwe2qNreW1WJLF5iA6VtIwUMZA%3D&spr=https&se=2020-07-12T21%3A19%3A03Z&sp=r&rscl=x-e2eid-673830c3-73ef4a7a-a99b9cb5-ec6cba7b-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:24.1739950Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:24.1740624Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:24.1741342Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:24.1742211Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:24.1742849Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:24.1743747Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:24.1744272Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:24.1744792Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:24.1745514Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:24.1746008Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:24.6192078Z Warning, [https://w25vsblobprodsu6weus3.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/D2071375ADC5F654B3321F312E6981DE40F3828214F8A2CB2CFAB19DFF465FF901?sv=2019-02-02&sr=b&sig=Mhzd8aJNktbNjGG4gfvY60jbQmMl0UbXcf1IKpNEECI%3D&spr=https&se=2020-07-12T20%3A49%3A12Z&sp=r&rscl=x-e2eid-11b3603c-dde9410e-bfad15c8-c0147fca-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:24.6194001Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:24.6194846Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:24.6195577Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:24.6196493Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:24.6196991Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:24.6197643Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:24.6198154Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:24.6198671Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:24.6199386Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:24.6199906Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:25.2619815Z Warning, [https://ikavsblobprodsu6weus35.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/D63EB41C24C408E7A805F0FB7047B52994E9F6882B83D46F66D12B26CF0946C501?sv=2019-02-02&sr=b&sig=ETt1YL7kf6uh3wqPu8CowisvSFY0biDdncY3eZnoFGo%3D&spr=https&se=2020-07-12T20%3A50%3A12Z&sp=r&rscl=x-e2eid-11b3603c-dde9410e-bfad15c8-c0147fca-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:25.2621307Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:25.2621979Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:25.2622794Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:25.2623670Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:25.2624173Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:25.2624847Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:25.2625374Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:25.2625888Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:25.2626573Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:25.2627061Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:25.9616303Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:28.8177974Z Warning, [https://buqvsblobprodsu6weus61.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/1295738C8A5FE839CCB1BFF47BC1B63E654A44162EAA55748240C6976D992E1C01?sv=2019-02-02&sr=b&sig=OfAr88COEqAbTtGt8FETtVhoka%2FES4NjsJFOTqxl78k%3D&spr=https&se=2020-07-12T21%3A04%3A21Z&sp=r&rscl=x-e2eid-b18360bb-ef8845b5-b099bb5f-56ef3eb2-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:28.8179894Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:28.8180736Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:28.8181529Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:28.8182558Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:28.8183011Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:28.8183780Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:28.8184537Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:28.8185317Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:28.8186473Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:28.8187294Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:30.9620980Z Information, Downloaded 358.8 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:33.3596160Z Warning, [https://o78vsblobprodsu6weus38.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/B92C3D709E2AE70E21A4F3ECCC66B2EE7414EA63B6744EDBE36559392054533B01?sv=2019-02-02&sr=b&sig=r6ePHKSTvef2bt4RQtIh7EYIiOaSQJCL99VAFIa%2FE08%3D&spr=https&se=2020-07-12T20%3A43%3A28Z&sp=r&rscl=x-e2eid-763fa331-b52c489b-bc0f2924-d333eb68-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:33.3598012Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:33.3598890Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:33.3599716Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:33.3600828Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:33.3601276Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:33.3601788Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:33.3602336Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:33.3603214Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:33.3604096Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:33.3604531Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:35.5936301Z Warning, [https://y78vsblobprodsu6weus75.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/43561FB64FD9D03A3409ABF3A7ABA25243EDC085A3E8CAE3B52BE11115F6346301?sv=2019-02-02&sr=b&sig=ameBiLmoLYw3vXNVnuK3z8kE3U4VEL9E8GCC7jsxLk0%3D&spr=https&se=2020-07-12T21%3A15%3A46Z&sp=r&rscl=x-e2eid-6d0d53e9-76874ad1-a9c4c80f-183091be-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:35.5937933Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:35.5938714Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:35.5939406Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:35.5940161Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:35.5940661Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:35.5941121Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:35.5941634Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:35.5942189Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:35.5942807Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:35.5943396Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:35.9622023Z Information, Downloaded 358.9 MB out of 1,170.3 MB (31%).
2020-07-11T20:40:36.1872424Z Warning, [https://ikavsblobprodsu6weus35.blob.core.windows.net/db2d3c0ac8fecf45be84076d87302181a9/5C07819B3E555146ED88DAE2ACD7783FBC8DCC9A376090AF8E9C229E1A328FCA01?sv=2019-02-02&sr=b&sig=BoKcvmZacrcGtyxlLQhqdwM9WEqyzyzNql4gbTYuvLc%3D&spr=https&se=2020-07-12T21%3A21%3A33Z&sp=r&rscl=x-e2eid-93f6321f-a69f402b-b0edd15d-a8c50989-session-38415228-535b4652-8afc41e3-fcfa5c67] Try 1/5, retryable exception caught. Retrying in 00:00:01. System.Threading.Tasks.TaskCanceledException: A task was canceled.
2020-07-11T20:40:36.1873998Z    at System.Net.Http.HttpClient.FinishSendAsyncBuffered(Task`1 sendTask, HttpRequestMessage request, CancellationTokenSource cts, Boolean disposeCts)
2020-07-11T20:40:36.1874675Z    at Microsoft.VisualStudio.Services.Common.TaskCancellationExtensions.EnforceCancellation[TResult](Task`1 task, CancellationToken cancellationToken, Func`1 makeMessage, String file, String member, Int32 line)
2020-07-11T20:40:36.1875668Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass57_0.<<GetRedirectResponseAsync>b__0>d.MoveNext()
2020-07-11T20:40:36.1876431Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:36.1876870Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:36.1877683Z    at Microsoft.VisualStudio.Services.Content.Common.ExceptionExtensions.ReThrow(Exception ex)
2020-07-11T20:40:36.1878222Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:36.1878735Z    at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreHttpClient.<>c__DisplayClass60_0.<<HandleRedirectAsync>b__0>d.MoveNext()
2020-07-11T20:40:36.1879610Z --- End of stack trace from previous location where exception was thrown ---
2020-07-11T20:40:36.1880051Z    at Microsoft.VisualStudio.Services.Content.Common.AsyncHttpRetryHelper`1.InvokeAsync(CancellationToken cancellationToken)
2020-07-11T20:40:38.5475309Z Information, ApplicationInsightsTelemetrySender correlated 2 events with X-TFS-Session 38415228-535b-4652-8afc-41e3fcfa5c67
2020-07-11T20:40:38.5520043Z ##[error]One or more errors occurred. (An error occurred while sending the request.)
2020-07-11T20:40:38.6311482Z ##[section]Finishing: DownloadPipelineArtifact
{code}",,dian.fu,dmvk,dwysakowicz,godfreyhe,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24796,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 29 12:21:00 UTC 2021,,,,,,,,,,"0|z0gotc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/21 15:47;dwysakowicz;I think this is a transient Azure problem. Closing for now.;;;","02/Jul/21 11:37;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19818&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=b7e5f999-572c-5083-7c6d-1a6930a17691;;;","09/Jul/21 13:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20225&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=b7e5f999-572c-5083-7c6d-1a6930a17691;;;","14/Jul/21 14:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20411&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=1cb92a80-45e8-57ff-9dd7-836132159f9d;;;","14/Jul/21 14:26;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20420&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=3a829166-b262-55a9-0500-d92804b0742d;;;","14/Jul/21 20:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20444&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=b97d99b0-29a0-5024-77e6-a62d129b5139;;;","14/Jul/21 20:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20445&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=e25d5e7e-2a9c-5589-4940-0b638d75a414;;;","22/Jul/21 17:38;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20828&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=b97d99b0-29a0-5024-77e6-a62d129b5139&l=1;;;","04/Aug/21 14:42;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21463&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=3a829166-b262-55a9-0500-d92804b0742d;;;","04/Aug/21 16:40;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21497&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0da23115-68bb-5dcd-192c-bd4c8adebde1;;;","18/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Aug/21 22:37;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","26/Oct/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Nov/21 10:40;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","05/Nov/21 11:00;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25986&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=62b92fd2-ef2a-5df9-c311-eb1e99401a69&l=130;;;","05/Nov/21 11:02;trohrmann;I have seen this problem popping up again and again over the past couple of days. I think we should try to fix this problem.;;;","05/Nov/21 11:19;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25995&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=3a829166-b262-55a9-0500-d92804b0742d&l=1073;;;","05/Nov/21 11:24;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25993&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=1cb92a80-45e8-57ff-9dd7-836132159f9d&l=25;;;","05/Nov/21 11:25;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25994&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=3a829166-b262-55a9-0500-d92804b0742d&l=391;;;","08/Nov/21 10:14;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26094&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=47de829b-6429-59b7-fd33-9a716530e3ed;;;","08/Nov/21 10:14;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26094&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=abf7a45d-0466-550d-0111-92ade54601f4;;;","08/Nov/21 10:14;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26094&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=90059fd6-f083-57c4-2f82-9fa8ab820523;;;","11/Nov/21 08:31;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26328&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=6dcbd4e6-0677-5ccf-3275-2b2560da56b4&l=52;;;","12/Nov/21 10:50;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26387&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=e1d7072f-14f7-5759-e321-0625c7a064a7;;;","29/Nov/21 12:21;trohrmann;Issue might be fixed by FLINK-24796 that reduces the size of the cached artifacts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
InfluxDB reporter cannot be loaded as plugin,FLINK-18573,13316366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,OneCloud,OneCloud,13/Jul/20 01:31,14/Jul/20 18:06,13/Jul/23 08:12,14/Jul/20 18:05,1.11.0,,,,,1.11.1,1.12.0,,Runtime / Metrics,,,,,0,pull-request-available,,,,,"The service entry is in the wrong directory (service vs services)

It causes the error:

 
2020-07-13 09:08:46.146 [main] WARN org.apache.flink.runtime.metrics.ReporterSetup - The reporter factory (org.apache.flink.metrics.influxdb.InfluxdbReporterFactory) could not be found for reporter influxdb. Available factories: [org.apache.flink.metrics.slf4j.Slf4jReporterFactory, org.apache.flink.metrics.datadog.DatadogHttpReporterFactory, org.apache.flink.metrics.graphite.GraphiteReporterFactory, org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterFactory, org.apache.flink.metrics.statsd.StatsDReporterFactory, org.apache.flink.metrics.prometheus.PrometheusReporterFactory, org.apache.flink.metrics.jmx.JMXReporterFactory].
2020-07-13 09:08:46.149 [main] INFO org.apache.flink.runtime.metrics.MetricRegistryImpl - Periodically reporting metrics in intervals of 60 SECONDS for reporter slf4j of type org.apache.flink.metrics.slf4j.Slf4jReporter.",,dian.fu,jark,liyu,OneCloud,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16966,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 18:05:14 UTC 2020,,,,,,,,,,"0|z0gosg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/20 04:10;yunta;[~OneCloud] Have you ever checked whether the flink-metrics-influxdb jars located under the plugins folder? You could download the pre-built jar from https://mvnrepository.com/artifact/org.apache.flink/flink-metrics-influxdb and you will see those influxdb jar containing the service file.;;;","13/Jul/20 05:50;OneCloud;[~yunta]  I checked the jar in the flink-1.11.0-bin-scala_2.11.tgz, and it should be ""services"", but it is named ""service"";;;","13/Jul/20 08:33;yunta;[~OneCloud] Yes, you're right, this is indeed a bug and introduced by FLINK-16966.

If you do not mind, I'll prepare a fix PR ASAP to solve this problem as community is discussing about release Flink-1.11.1 and  will create a RC today.;;;","13/Jul/20 08:41;chesnay;[~yunta] Go ahead, please also double-check the other reporters.;;;","13/Jul/20 09:21;OneCloud;[~yunta] Great! I rename and package to a new jar and upload to server now, it's ok now;;;","14/Jul/20 18:05;chesnay;master:
49d3cabb8ae53900138fc1a2f5178dbec46ce378
c3e0d450c82ea8246a7f0abab3a1f86a0f53de7d
1.11:
5c73b0c4a1b50d1b2b3a6a3a3637677a143e598f
60cf377b01595f257992d8d30d1b236d0f6f0103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when i use sql-client write to hive but throw a exception ""This is a bug. Please consider filing an issue""",FLINK-18571,13316276,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,JasonLee,JasonLee,11/Jul/20 09:49,12/Jul/20 07:31,13/Jul/23 08:12,12/Jul/20 07:31,1.11.0,,,,,1.12.0,,,Table SQL / Client,,,,,0,,,,,,"when i use the sql-client write to hive  there throw a exception  

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue. at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)Caused by: java.lang.RuntimeException: Error running SQL job. at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeUpdateInternal$14(LocalExecutor.java:598) at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:592) at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:515) at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:596) at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:315) at java.util.Optional.ifPresent(Optional.java:159) at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:212) at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeUpdateInternal$14(LocalExecutor.java:595) ... 10 moreCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:366) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:292) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.handler.RestHandlerException: Failed to deserialize JobGraph. at org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.lambda$loadJobGraph$2(JobSubmitHandler.java:127) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: cannot assign instance of java.util.Collections$UnmodifiableList to field org.apache.flink.runtime.jobgraph.JobVertex.operatorIDs of type java.util.ArrayList in instance of org.apache.flink.runtime.jobgraph.JobVertex at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133) at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) at java.util.ArrayList.readObject(ArrayList.java:791) at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) at java.util.HashMap.readObject(HashMap.java:1404) at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) at org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.lambda$loadJobGraph$2(JobSubmitHandler.java:125) ... 8 more] at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 4 more

and the sql  statements  are as follows

 

CREATE TABLE kafka_table (
 name VARCHAR COMMENT '姓名',
 age int COMMENT '年龄',
 ts BIGINT COMMENT '时间戳',
 t as TO_TIMESTAMP(FROM_UNIXTIME(ts/1000,'yyyy-MM-dd HH:mm:ss')),
 proctime as PROCTIME(),
 WATERMARK FOR t AS t - INTERVAL '5' SECOND
)
WITH (
 'connector.type' = 'kafka', -- 使用 kafka connector
 'connector.version' = 'universal', -- kafka 版本
 'connector.topic' = 'jason_flink', -- kafka topic
 'connector.startup-mode' = 'latest-offset', -- 从起始 offset 开始读取
 'connector.properties.bootstrap.servers' = 'master:9092,storm1:9092,storm2:9092', -- broker连接信息
 'connector.properties.group.id' = 'jason_flink_test',
 'update-mode' = 'append',
 'format.type' = 'json', -- 数据源格式为 json
 'format.derive-schema' = 'true' -- 从 DDL schema 确定 json 解析规则
);

CREATE TABLE fs_table (
 name STRING,
 age int,
 dt STRING
) PARTITIONED BY (dt) WITH (
 'connector'='filesystem',
 'path'='/home/jason/bigdata/',
 'format'='parquet',
 'sink.partition-commit.delay'='1s',
 'sink.partition-commit.policy.kind'='success-file'
);

INSERT INTO fs_table SELECT name, age, DATE_FORMAT(t, 'yyyy-MM-dd') FROM kafka_table;

i can run the same statement at local( in IDE) the cant run it in sql-client

 

 ",,JasonLee,kezhuw,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 12 07:31:25 UTC 2020,,,,,,,,,,"0|z0go8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/20 10:23;kezhuw;[~JasonLee] I guess you are using 1.11 client targeting 1.10 or other older version server ?

 [FLINK-16638|https://issues.apache.org/jira/browse/FLINK-16638] ([0114338da5ce52677d1dfa1ab4350b1567dc3522|https://github.com/apache/flink/commit/0114338da5ce52677d1dfa1ab4350b1567dc3522#diff-212390f3089d4401dd0260c33bbf4b94R158]) changed {{JobVertex#operatorIDs}} from {{ArrayList}} to {{List}} with concrete type derived from {{Collections.unmodifiableList}} or {{Collections.singletonList}}.

Sadly that {{JobVertex#serialVersionUID}} remains unchanged despite its incompatibility, otherwise you should got unmatched {{serialVersionUID}} error.;;;","12/Jul/20 07:31;JasonLee;hi kezhu  thanks i think you are right  today  i restart my Flink cluster now i can run the sql 

i will close this issues;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientHBaseITCase.testHBase fails on azure,FLINK-18570,13316193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,dwysakowicz,dwysakowicz,10/Jul/20 17:12,17/Nov/20 02:27,13/Jul/23 08:12,17/Nov/20 02:27,1.12.0,,,,,1.12.0,,,Connectors / HBase,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4403&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-07-10T17:06:32.1514539Z [INFO] Running org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-07-10T17:08:09.9141283Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 97.757 s <<< FAILURE! - in org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-07-10T17:08:09.9144691Z [ERROR] testHBase(org.apache.flink.tests.util.hbase.SQLClientHBaseITCase)  Time elapsed: 97.757 s  <<< ERROR!
2020-07-10T17:08:09.9145637Z java.io.IOException: 
2020-07-10T17:08:09.9146515Z Process execution failed due error. Error output:Jul 10, 2020 5:07:32 PM org.jline.utils.Log logr
2020-07-10T17:08:09.9147152Z WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
2020-07-10T17:08:09.9147776Z Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
2020-07-10T17:08:09.9148432Z 	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)
2020-07-10T17:08:09.9148828Z Caused by: java.lang.RuntimeException: Error running SQL job.
2020-07-10T17:08:09.9149329Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeUpdateInternal$14(LocalExecutor.java:598)
2020-07-10T17:08:09.9149932Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255)
2020-07-10T17:08:09.9150501Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:592)
2020-07-10T17:08:09.9151088Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:515)
2020-07-10T17:08:09.9151577Z 	at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:599)
2020-07-10T17:08:09.9152044Z 	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:315)
2020-07-10T17:08:09.9152456Z 	at java.util.Optional.ifPresent(Optional.java:159)
2020-07-10T17:08:09.9152874Z 	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:212)
2020-07-10T17:08:09.9153312Z 	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142)
2020-07-10T17:08:09.9153729Z 	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114)
2020-07-10T17:08:09.9154151Z 	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)
2020-07-10T17:08:09.9154685Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-07-10T17:08:09.9155272Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-07-10T17:08:09.9156047Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-07-10T17:08:09.9156474Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeUpdateInternal$14(LocalExecutor.java:595)
2020-07-10T17:08:09.9156802Z 	... 10 more
2020-07-10T17:08:09.9157069Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
2020-07-10T17:08:09.9157508Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:366)
2020-07-10T17:08:09.9157942Z 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2020-07-10T17:08:09.9158349Z 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2020-07-10T17:08:09.9158762Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-07-10T17:08:09.9159168Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-07-10T17:08:09.9159614Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:292)
2020-07-10T17:08:09.9160033Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-07-10T17:08:09.9160450Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-07-10T17:08:09.9160858Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-07-10T17:08:09.9161230Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-07-10T17:08:09.9161626Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-07-10T17:08:09.9162018Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-07-10T17:08:09.9162582Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-07-10T17:08:09.9163045Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-10T17:08:09.9163352Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-10T17:08:09.9163739Z Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
2020-07-10T17:08:09.9164231Z org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
2020-07-10T17:08:09.9164576Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:344)
2020-07-10T17:08:09.9164933Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-07-10T17:08:09.9165260Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-07-10T17:08:09.9165608Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-07-10T17:08:09.9165917Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-07-10T17:08:09.9166266Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-07-10T17:08:09.9166604Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-07-10T17:08:09.9166922Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-07-10T17:08:09.9167245Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-07-10T17:08:09.9167554Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-07-10T17:08:09.9167905Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not instantiate JobManager.
2020-07-10T17:08:09.9168269Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:398)
2020-07-10T17:08:09.9168640Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2020-07-10T17:08:09.9168885Z 	... 6 more
2020-07-10T17:08:09.9169154Z Caused by: org.apache.flink.runtime.JobException: Creating the input splits caused an error: Expecting at least one region.
2020-07-10T17:08:09.9169565Z 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:272)
2020-07-10T17:08:09.9169928Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:822)
2020-07-10T17:08:09.9170326Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:236)
2020-07-10T17:08:09.9170715Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:286)
2020-07-10T17:08:09.9171092Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:251)
2020-07-10T17:08:09.9171462Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:233)
2020-07-10T17:08:09.9171795Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:121)
2020-07-10T17:08:09.9172185Z 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:108)
2020-07-10T17:08:09.9172560Z 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:319)
2020-07-10T17:08:09.9172876Z 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:307)
2020-07-10T17:08:09.9173286Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:100)
2020-07-10T17:08:09.9173774Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:42)
2020-07-10T17:08:09.9174216Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:140)
2020-07-10T17:08:09.9174642Z 	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
2020-07-10T17:08:09.9175102Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:388)
2020-07-10T17:08:09.9175392Z 	... 7 more
2020-07-10T17:08:09.9175579Z Caused by: java.io.IOException: Expecting at least one region.
2020-07-10T17:08:09.9175932Z 	at org.apache.flink.connector.hbase.source.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:219)
2020-07-10T17:08:09.9176371Z 	at org.apache.flink.connector.hbase.source.HBaseRowDataInputFormat.createInputSplits(HBaseRowDataInputFormat.java:42)
2020-07-10T17:08:09.9176792Z 	at org.apache.flink.connector.hbase.source.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:46)
2020-07-10T17:08:09.9177202Z 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:258)
2020-07-10T17:08:09.9177445Z 	... 21 more
2020-07-10T17:08:09.9177545Z 
2020-07-10T17:08:09.9177675Z End of exception on server side>]
2020-07-10T17:08:09.9177931Z 	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
2020-07-10T17:08:09.9178261Z 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
2020-07-10T17:08:09.9178603Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
2020-07-10T17:08:09.9178936Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2020-07-10T17:08:09.9179182Z 	... 4 more
2020-07-10T17:08:09.9179266Z 
2020-07-10T17:08:09.9179537Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:127)
2020-07-10T17:08:09.9179979Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
2020-07-10T17:08:09.9180371Z 	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:221)
2020-07-10T17:08:09.9180815Z 	at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$StandaloneClusterController.submitSQLJob(LocalStandaloneFlinkResource.java:196)
2020-07-10T17:08:09.9181268Z 	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.executeSqlStatements(SQLClientHBaseITCase.java:185)
2020-07-10T17:08:09.9181665Z 	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.testHBase(SQLClientHBaseITCase.java:131)
2020-07-10T17:08:09.9181978Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-07-10T17:08:09.9182525Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-07-10T17:08:09.9182908Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-07-10T17:08:09.9183218Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-07-10T17:08:09.9183701Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-07-10T17:08:09.9184109Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-07-10T17:08:09.9184497Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-07-10T17:08:09.9184902Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-07-10T17:08:09.9185279Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-07-10T17:08:09.9185662Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-07-10T17:08:09.9186040Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-07-10T17:08:09.9186396Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-07-10T17:08:09.9186742Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-07-10T17:08:09.9187041Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-07-10T17:08:09.9187359Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-07-10T17:08:09.9187723Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-07-10T17:08:09.9188157Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-07-10T17:08:09.9188555Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-07-10T17:08:09.9188874Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-07-10T17:08:09.9189218Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-07-10T17:08:09.9189545Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-07-10T17:08:09.9189885Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-07-10T17:08:09.9190246Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-07-10T17:08:09.9190571Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-07-10T17:08:09.9190881Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-07-10T17:08:09.9191171Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-07-10T17:08:09.9191458Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-07-10T17:08:09.9191755Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-07-10T17:08:09.9192088Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-07-10T17:08:09.9192431Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-07-10T17:08:09.9192761Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-07-10T17:08:09.9193099Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-07-10T17:08:09.9193414Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-07-10T17:08:09.9193753Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-07-10T17:08:09.9194618Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-07-10T17:08:09.9195175Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-07-10T17:08:09.9195698Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-07-10T17:08:09.9200039Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-07-10T17:08:09.9200599Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-07-10T17:08:09.9201183Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-07-10T17:08:09.9201731Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-07-10T17:08:09.9202227Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-07-10T17:08:09.9202681Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dian.fu,dwysakowicz,godfreyhe,jark,kezhuw,leonard,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19863,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 02:27:47 UTC 2020,,,,,,,,,,"0|z0gnps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/20 08:20;rmetzger;I'm closing this for now. Please re-open if it surfaces again.;;;","24/Sep/20 11:12;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6885&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

I'm reopening this issue.;;;","05/Oct/20 06:36;rmetzger;[~Leonard Xu] Can you take a look at this test instability?;;;","06/Oct/20 08:47;leonard;[~rmetzger] 

I'll take a look at this ticket, could you help assign this one to me ?;;;","06/Oct/20 10:02;rmetzger;Thanks a lot, I assigned you.;;;","09/Oct/20 06:53;rmetzger;Another instance:
{code}
2020-10-08T23:56:27.3439641Z [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 240.208 s <<< FAILURE! - in org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-10-08T23:56:27.3440958Z [ERROR] testHBase[0: hbase-version:1.4.3](org.apache.flink.tests.util.hbase.SQLClientHBaseITCase)  Time elapsed: 103.786 s  <<< ERROR!
2020-10-08T23:56:27.3442115Z java.io.IOException: 
2020-10-08T23:56:27.3442638Z Process execution failed due error. Error output:Oct 08, 2020 11:53:38 PM org.jline.utils.Log logr
2020-10-08T23:56:27.3443239Z WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)
2020-10-08T23:56:27.3443936Z Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
2020-10-08T23:56:27.3444849Z 	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:208)
2020-10-08T23:56:27.3445364Z Caused by: java.lang.RuntimeException: Error running SQL job.
2020-10-08T23:56:27.3445932Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeUpdateInternal$4(LocalExecutor.java:498)
2020-10-08T23:56:27.3446773Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:257)
2020-10-08T23:56:27.3447426Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:492)
2020-10-08T23:56:27.3448068Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:415)
2020-10-08T23:56:27.3448666Z 	at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:675)
2020-10-08T23:56:27.3449292Z 	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:328)
2020-10-08T23:56:27.3450015Z 	at java.util.Optional.ifPresent(Optional.java:159)
2020-10-08T23:56:27.3450498Z 	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:216)
2020-10-08T23:56:27.3451030Z 	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:141)
2020-10-08T23:56:27.3451553Z 	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114)
2020-10-08T23:56:27.3452053Z 	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:196)
2020-10-08T23:56:27.3452911Z Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not instantiate JobManager.
2020-10-08T23:56:27.3454084Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-10-08T23:56:27.3454709Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-10-08T23:56:27.3455385Z 	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeUpdateInternal$4(LocalExecutor.java:495)
2020-10-08T23:56:27.3455953Z 	... 10 more
2020-10-08T23:56:27.3456693Z Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not instantiate JobManager.
2020-10-08T23:56:27.3457658Z 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:309)
2020-10-08T23:56:27.3458213Z 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:76)
2020-10-08T23:56:27.3458796Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-10-08T23:56:27.3459363Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-10-08T23:56:27.3477202Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-10-08T23:56:27.3477803Z 	at java.lang.Thread.run(Thread.java:748)
2020-10-08T23:56:27.3478533Z Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not instantiate JobManager.
2020-10-08T23:56:27.3479249Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$4(Dispatcher.java:421)
2020-10-08T23:56:27.3479915Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2020-10-08T23:56:27.3480554Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-10-08T23:56:27.3558586Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-10-08T23:56:27.3559145Z 	... 1 more
2020-10-08T23:56:27.3559602Z Caused by: org.apache.flink.runtime.JobException: Creating the input splits caused an error: Expecting at least one region.
2020-10-08T23:56:27.3560187Z 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:270)
2020-10-08T23:56:27.3560728Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:825)
2020-10-08T23:56:27.3561286Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:240)
2020-10-08T23:56:27.3562017Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:290)
2020-10-08T23:56:27.3562549Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:254)
2020-10-08T23:56:27.3563076Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:235)
2020-10-08T23:56:27.3563574Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:133)
2020-10-08T23:56:27.3564119Z 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:107)
2020-10-08T23:56:27.3564713Z 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:317)
2020-10-08T23:56:27.3565204Z 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:304)
2020-10-08T23:56:27.3565775Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:96)
2020-10-08T23:56:27.3566452Z 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:41)
2020-10-08T23:56:27.3567061Z 	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:141)
2020-10-08T23:56:27.3567649Z 	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:80)
2020-10-08T23:56:27.3568251Z 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$4(Dispatcher.java:408)
2020-10-08T23:56:27.3568637Z 	... 4 more
2020-10-08T23:56:27.3568962Z Caused by: java.io.IOException: Expecting at least one region.
2020-10-08T23:56:27.3569472Z 	at org.apache.flink.connector.hbase1.source.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:225)
2020-10-08T23:56:27.3570079Z 	at org.apache.flink.connector.hbase1.source.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:49)
2020-10-08T23:56:27.3570658Z 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:256)
2020-10-08T23:56:27.3571045Z 	... 18 more
2020-10-08T23:56:27.3571233Z 
2020-10-08T23:56:27.3571648Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:127)
2020-10-08T23:56:27.3572250Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
2020-10-08T23:56:27.3572822Z 	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:221)
2020-10-08T23:56:27.3573545Z 	at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$StandaloneClusterController.submitSQLJob(LocalStandaloneFlinkResource.java:196)
2020-10-08T23:56:27.3574187Z 	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.executeSqlStatements(SQLClientHBaseITCase.java:215)
2020-10-08T23:56:27.3574765Z 	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.testHBase(SQLClientHBaseITCase.java:152)
2020-10-08T23:56:27.3575221Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-08T23:56:27.3575642Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-08T23:56:27.3576144Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-08T23:56:27.3576591Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-08T23:56:27.3577017Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-08T23:56:27.3577526Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-08T23:56:27.3578017Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-08T23:56:27.3578495Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-08T23:56:27.3579084Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-10-08T23:56:27.3579569Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-08T23:56:27.3580041Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-08T23:56:27.3580488Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-08T23:56:27.3580924Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-10-08T23:56:27.3581371Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-08T23:56:27.3582352Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-08T23:56:27.3582732Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-08T23:56:27.3583142Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-08T23:56:27.3583512Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-08T23:56:27.3583845Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-08T23:56:27.3584201Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-08T23:56:27.3584557Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-08T23:56:27.3584894Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-08T23:56:27.3585237Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-08T23:56:27.3585538Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-10-08T23:56:27.3585841Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-10-08T23:56:27.3586160Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-08T23:56:27.3586492Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-08T23:56:27.3586845Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-08T23:56:27.3587199Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-08T23:56:27.3587540Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-08T23:56:27.3587913Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-10-08T23:56:27.3588267Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-08T23:56:27.3588572Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-08T23:56:27.3588888Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-10-08T23:56:27.3589187Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-10-08T23:56:27.3589486Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-08T23:56:27.3589828Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-08T23:56:27.3590279Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-08T23:56:27.3590615Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-08T23:56:27.3590971Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-08T23:56:27.3591316Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-08T23:56:27.3591649Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-10-08T23:56:27.3592072Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-10-08T23:56:27.3592533Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-10-08T23:56:27.3592956Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-10-08T23:56:27.3593386Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-10-08T23:56:27.3593819Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-10-08T23:56:27.3594256Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-08T23:56:27.3594712Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-08T23:56:27.3595186Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-08T23:56:27.3595564Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7307&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
-;;;","09/Oct/20 06:53;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7307&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729;;;","09/Oct/20 06:54;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7307&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","09/Oct/20 14:35;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7315&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","13/Oct/20 12:08;leonard;The test failed when get region information of HBase table, in a strange way that the returned region meta information is unexpected and without any meaningful stack.  

I'll continue investigate.;;;","20/Oct/20 14:12;rmetzger;This CI run reports a green build (I'm experimenting with the CI system), but failed with this error: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8495&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d ;;;","21/Oct/20 03:17;leonard;The same reason with my previous investigation, unfortunately I didn't reproduce this on my many azure builds(I added some log output), [~rmetzger]  Could we add add a little log for  `AbstractTableInputFormat` as a hotfix ?

Once the test failed again, we can obtain more information to help fix it.
{code:java}
Caused by: java.io.IOException: Expecting at least one region.
Oct 20 13:34:40 	at org.apache.flink.connector.hbase1.source.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:225)
Oct 20 13:34:40 	at org.apache.flink.connector.hbase1.source.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:49)
Oct 20 13:34:40 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:234)
Oct 20 13:34:40 	... 18 more
Oct 20 13:34
{code}
{code:java}
// The test failed here with no Exception stack, the Exception is thrown by Flink rather than Hbase side 
@Override
public TableInputSplit[] createInputSplits(final int minNumSplits) throws IOException {
   try {
      initTable();

      // Get the starting and ending row keys for every region in the currently open table
      final Pair<byte[][], byte[][]> keys = table.getRegionLocator().getStartEndKeys();
      if (keys == null || keys.getFirst() == null || keys.getFirst().length == 0) {
         throw new IOException(""Expecting at least one region."");
      }
{code};;;","21/Oct/20 08:04;rmetzger;Seems like your comment is incomplete?

Can you open a small hotfix PR with the change? I'll merge it quickly.;;;","28/Oct/20 17:43;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8500&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","03/Nov/20 18:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8888&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","06/Nov/20 03:04;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9121&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729;;;","10/Nov/20 01:56;godfreyhe;https://dev.azure.com/godfreyhe/godfreyhe-flink/_build/results?buildId=70&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d;;;","10/Nov/20 02:04;leonard;Hi [~godfreyhe]

could you help merge this [PR |https://github.com/apache/flink/pull/13616]? I improve the log information as we discussed with |https://github.com/apache/flink/pull/13616] [~rmetzger]

[https://github.com/apache/flink/pull/13616];;;","10/Nov/20 02:30;godfreyhe;commit: a9a00325109a3060ddc11de94c057025522cd619 improve log and exception message for hbase e2e test SQLClientHBaseITCase;;;","11/Nov/20 11:03;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9440&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","16/Nov/20 19:22;rmetzger;[~Leonard Xu] is the failure I reported after the improved logging has been merged helpful to understand the problem better?;;;","17/Nov/20 01:36;leonard;[~rmetzger] Yes, the added log is helpful, I hope we can fix this by https://github.com/apache/flink/pull/14032;;;","17/Nov/20 02:27;jark;This should have been fixed by FLINK-19863. I would close this issue for now.
We can reopen this issue if it fails again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Table.limit() which is the equivalent of SQL LIMIT,FLINK-18569,13316171,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,aljoscha,aljoscha,10/Jul/20 14:31,17/Jul/20 07:59,13/Jul/23 08:12,17/Jul/20 07:59,,,,,,1.12.0,,,Table SQL / API,Table SQL / Planner,Table SQL / Runtime,,,0,pull-request-available,,,,,"Currently, you can run a SQL query like {{select * FROM (VALUES 'Hello', 'CIAO', 'foo', 'bar') LIMIT 2;}} but you cannot run the equivalent in the Table API.",,aljoscha,dian.fu,fsk119,jark,kezhuw,libenchao,RocMarshal,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 07:59:56 UTC 2020,,,,,,,,,,"0|z0gnkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/20 07:59;twalthr;Fixed in 1.12.0: 46bb8eee3fdc69ec0049c6cb1cc40e9b40e3aaa7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update migration tests in master to cover migration from release-1.11,FLINK-18552,13316073,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,zjwang,zjwang,10/Jul/20 07:14,23/Apr/21 08:35,13/Jul/23 08:12,24/Jul/20 15:11,,,,,,1.11.2,1.12.0,,Tests,,,,,0,pull-request-available,,,,,"We should update the following tests to cover migration from release-1.11:
 * {{CEPMigrationTest}}
 * {{BucketingSinkMigrationTest}}
 * {{FlinkKafkaConsumerBaseMigrationTest}}
 * {{ContinuousFileProcessingMigrationTest}}
 * {{WindowOperatorMigrationTest}}
 * {{StatefulJobSavepointMigrationITCase.scala}}
 * {{StatefulJobWBroadcastStateMigrationITCase.scala}}

 

Refers to https://issues.apache.org/jira/browse/FLINK-13613, there are more migration tests requires to update:
 
* FlinkKafkaProducer011MigrationTest
* FlinkKafkaProducerMigrationOperatorTest
* FlinkKafkaProducerMigrationTest
* StatefulJobSavepointMigrationITCase
* StatefulJobWBroadcastStateMigrationITCase
* TypeSerializerSnapshotMigrationITCase
* AbstractKeyedOperatorRestoreTestBase
* AbstractNonKeyedOperatorRestoreTestBase
* FlinkKinesisConsumerMigrationTest 
 


 

 ",,gaoyunhaii,pnowojski,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15707,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 15:11:19 UTC 2020,,,,,,,,,,"0|z0gmz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/20 06:29;gaoyunhaii;Fix each tests via 
 * CEPMigrationTest:  a1edfafa0c99b04c756be5908aae72391ceddfc5
 * BucketingSinkMigrationTest: 144c78523393579eaea883c6030e3058da4e2382
 * FlinkKafkaConsumerBaseMigrationTest: 5cd8423fbdf08e6188896e6ea869888923f3632c
 * ContinuousFileProcessingMigrationTest: e2a79022fd5e19cb3a6e1da04554109f2aca32d0
 * WindowOperatorMigrationTest: 08e2049164750193778936c684db3a5d9fd4e586
 * StatefulJobSavepointMigrationITCase: bcbeedf917618003ec5e4cb1c162258c7bc05aaa
 * StatefulJobWBroadcastStateMigrationITCase: 0fbea46ac0271dd84fa8acd7f99f449a9a0d458c;;;","24/Jul/20 10:05;pnowojski;Backported to release-1.11 as 99604c6e93~7..99604c6e93;;;","24/Jul/20 10:12;pnowojski;Added missing tests for 1.10 to master branch as 7cf08c096c..13a67e2745 and to release-1.11 as  99604c6e93..0cd46fffcb;;;","24/Jul/20 15:11;pnowojski;Added missing tests for 1.11 to master branch as a49a350d85..740ffb5bfa and to release-1.11 as e55516a040..7cfd72d489;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FunctionITCase.testInvalidUseOfSystemScalarFunction fails,FLINK-18544,13315893,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,dwysakowicz,dwysakowicz,09/Jul/20 13:00,09/Jul/20 15:45,13/Jul/23 08:12,09/Jul/20 15:45,1.11.1,,,,,1.11.1,,,Table SQL / Planner,,,,,0,,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4361&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129

{code}
2020-07-09T09:31:56.3699833Z [INFO] 
2020-07-09T09:31:56.3702121Z [INFO] Results:
2020-07-09T09:31:56.3703431Z [INFO] 
2020-07-09T09:31:56.3709381Z [ERROR] Failures: 
2020-07-09T09:31:56.3710482Z [ERROR]   FunctionITCase.testInvalidUseOfSystemScalarFunction:856 
2020-07-09T09:31:56.3711149Z Expected: exception with message a string containing ""Currently, only table functions can emit rows.""
2020-07-09T09:31:56.3713008Z      but: message was ""SQL validation failed. Function 'MD5' cannot be used as a table function.""
2020-07-09T09:31:56.3713516Z [INFO] 
2020-07-09T09:31:56.3713808Z [ERROR] Tests run: 2661, Failures: 1, Errors: 0, Skipped: 18
2020-07-09T09:31:56.3717340Z [INFO] 
{code}",,dwysakowicz,jark,leonard,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 15:45:00 UTC 2020,,,,,,,,,,"0|z0glv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 15:45;twalthr;Fixed in 1.11.1: 15af990ba4af8adf2c385f09152860399718e5d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamExecutionEnvironment#addSource(SourceFunction, TypeInformation) doesn't use the user defined type information",FLINK-18539,13315848,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,jark,jark,09/Jul/20 09:37,15/Jul/20 03:56,13/Jul/23 08:12,14/Jul/20 01:58,1.11.0,,,,,1.10.2,1.11.1,1.12.0,API / DataStream,,,,,0,pull-request-available,,,,,"
{code:java}
class MySource<T> implements SourceFunction<T>, ResultTypeQueryable<T> {
 TypeInformation getProducedType() {
   return TypeExtractor.createTypeInfo(SourceFunction.class, this.getClass(), 0, null, null);
 } 
}

DataStream ds = tEnv.addSource(new MySource(), Types.ROW(Types.STRING))
{code}

The returned {{TypeInformation}} of {{MySource}} is {{GenericTypeInfo}}, not the user given {{RowTypeInfo}}.


It seems that {{StreamExecutionEnvironment#getTypeInfo}} doesn't use the user given {{typeInfo}} in the highest priority. ",,aljoscha,dian.fu,jark,kezhuw,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 03:00:32 UTC 2020,,,,,,,,,,"0|z0gll4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 09:37;jark;cc [~jqin];;;","13/Jul/20 03:00;jark;- master (1.12.0): 68d83ef7bd43fbd20f51551d7216364b41ace793
- 1.11.1: 38495e7378f6df5eead9a29448e3944f2b3ecbea
- 1.10.2: 2a3b642b1efb957f3d4f20502c40398786ab1469;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition between task acknowledgement and first heartbeat,FLINK-18533,13315759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,dian.fu,dian.fu,09/Jul/20 02:07,23/Jul/20 09:47,13/Jul/23 08:12,23/Jul/20 09:47,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,"FLINK-17075 introduced a reconciliation mechanism for detecting missing/unknown deployments.
For this purpose, we begin tracking deployments after the task submission was acknowledged by the TaskExecutor, and match these against the deployments on the TaskExecutor as reported by the heartbeats.

~~However, {{TaskManagerGateway#submitTask}} is not called in the main thread (so it doesn't spend time on serializating the TaskDeploymentDescriptor), and as such the receival of the acknowledgement is also not running in the main thread.~~

For some reason it appears that heartbeat requests sent after the acknowledge can end up being processed before the acknowledge.
This is pretty easy to reproduce by reverting 6e811e6f21e5baa6bfb1862a51815e1d151c7097 and introducing a sleep into the processing of the task submission ackowledgement within Execution#deploy().

As shown in the AccumulatorLiveITCase, this manifests as a task that was just deployed to being canceled.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56]

{code}
2020-07-08T21:46:15.4438026Z Printing stack trace of Java process 40159
2020-07-08T21:46:15.4442864Z ==============================================================================
2020-07-08T21:46:15.4475676Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-07-08T21:46:15.7672746Z 2020-07-08 21:46:15
2020-07-08T21:46:15.7673349Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-07-08T21:46:15.7673590Z 
2020-07-08T21:46:15.7673893Z ""Attach Listener"" #86 daemon prio=9 os_prio=0 tid=0x00007fef8c025800 nid=0x1b231 runnable [0x0000000000000000]
2020-07-08T21:46:15.7674242Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7674419Z 
2020-07-08T21:46:15.7675150Z ""flink-taskexecutor-io-thread-2"" #85 daemon prio=5 os_prio=0 tid=0x00007fef9c020000 nid=0xb03a waiting on condition [0x00007fefac1f3000]
2020-07-08T21:46:15.7675964Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7676249Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7680997Z 	- parking to wait for  <0x0000000087180a20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7681506Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7682009Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7682666Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7683100Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7683554Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7684013Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7684371Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7684559Z 
2020-07-08T21:46:15.7685213Z ""Flink-DispatcherRestEndpoint-thread-4"" #84 daemon prio=5 os_prio=0 tid=0x00007fef90431800 nid=0x9e49 waiting on condition [0x00007fef7df4a000]
2020-07-08T21:46:15.7685665Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7686052Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7686707Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7687184Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7687721Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7688342Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7688935Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7689579Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7690451Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7690928Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7691317Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7691502Z 
2020-07-08T21:46:15.7692183Z ""Flink-DispatcherRestEndpoint-thread-3"" #83 daemon prio=5 os_prio=0 tid=0x00007fefa01e2800 nid=0x9dc9 waiting on condition [0x00007fef7f1f4000]
2020-07-08T21:46:15.7692636Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7692920Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7693647Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7694105Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7694595Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7695178Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7695746Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7696445Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7696933Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7697394Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7697794Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7697986Z 
2020-07-08T21:46:15.7698687Z ""Flink-DispatcherRestEndpoint-thread-2"" #82 daemon prio=5 os_prio=0 tid=0x00007fefa01e1800 nid=0x9d97 waiting on condition [0x00007fef7e34c000]
2020-07-08T21:46:15.7699367Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7699671Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7700328Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7701040Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7701692Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7702310Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7702916Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7703457Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7704029Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7709397Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7709844Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7710036Z 
2020-07-08T21:46:15.7710763Z ""Thread-23"" #71 prio=5 os_prio=0 tid=0x00007fef81ddc800 nid=0x9d5e waiting on condition [0x00007fef7ebf0000]
2020-07-08T21:46:15.7711170Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7711465Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7712165Z 	- parking to wait for  <0x00000000871812a0> (a java.util.concurrent.CompletableFuture$Signaller)
2020-07-08T21:46:15.7712594Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7713049Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-07-08T21:46:15.7713499Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-07-08T21:46:15.7713957Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-07-08T21:46:15.7714396Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-07-08T21:46:15.7715121Z 	at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:107)
2020-07-08T21:46:15.7715618Z 	at org.apache.flink.test.accumulators.AccumulatorLiveITCase$1.go(AccumulatorLiveITCase.java:148)
2020-07-08T21:46:15.7716212Z 	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:74)
2020-07-08T21:46:15.7716490Z 
2020-07-08T21:46:15.7717251Z ""AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1"" #70 daemon prio=5 os_prio=0 tid=0x00007fefa401e000 nid=0x9d5c waiting on condition [0x00007fef7eff2000]
2020-07-08T21:46:15.7717761Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7718040Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7718688Z 	- parking to wait for  <0x00000000871814b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7719174Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7719714Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7720268Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7720731Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7721215Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7721689Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7722089Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7722279Z 
2020-07-08T21:46:15.7722934Z ""flink-taskexecutor-io-thread-1"" #69 daemon prio=5 os_prio=0 tid=0x00007fef9c18b000 nid=0x9d5b waiting on condition [0x00007fef7e74e000]
2020-07-08T21:46:15.7723481Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7723777Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7724461Z 	- parking to wait for  <0x0000000087180a20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7725025Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7725647Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7726288Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7726757Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7727243Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7727719Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7728118Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7728311Z 
2020-07-08T21:46:15.7728915Z ""pool-3-thread-1"" #68 prio=5 os_prio=0 tid=0x00007fefd8683800 nid=0x9d5a waiting on condition [0x00007fef7e44d000]
2020-07-08T21:46:15.7729425Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7729699Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7730550Z 	- parking to wait for  <0x0000000087181900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7731026Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7731526Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7732122Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-07-08T21:46:15.7732706Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7733225Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7733691Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7734235Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7734622Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7734925Z 
2020-07-08T21:46:15.7735553Z ""Flink-MetricRegistry-thread-1"" #64 daemon prio=5 os_prio=0 tid=0x00007fef9c10a000 nid=0x9d52 waiting on condition [0x00007fef7ecf1000]
2020-07-08T21:46:15.7736123Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7736411Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7737066Z 	- parking to wait for  <0x0000000087181b80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7737547Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7738100Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7738725Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7739426Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7739947Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7740517Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7740970Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7741454Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7741650Z 
2020-07-08T21:46:15.7742217Z ""pool-2-thread-1"" #61 prio=5 os_prio=0 tid=0x00007fefa0087000 nid=0x9d4f waiting on condition [0x00007fef7e84f000]
2020-07-08T21:46:15.7742628Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7742979Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7743629Z 	- parking to wait for  <0x0000000087181dc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7744110Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7744610Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7748719Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-07-08T21:46:15.7749446Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7749975Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7750562Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7751005Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7751365Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7751542Z 
2020-07-08T21:46:15.7752400Z ""jobmanager-future-thread-2"" #58 daemon prio=5 os_prio=0 tid=0x00007fef9c0ff800 nid=0x9d4c waiting on condition [0x00007fef7f2f5000]
2020-07-08T21:46:15.7752842Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7753133Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7753752Z 	- parking to wait for  <0x0000000087182040> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7754233Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7754770Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7755364Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7756121Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7756809Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7757294Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7757786Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7758170Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7758360Z 
2020-07-08T21:46:15.7759187Z ""FlinkCompletableFutureDelayScheduler-thread-1"" #57 daemon prio=5 os_prio=0 tid=0x00007fef9c0f8000 nid=0x9d4b waiting on condition [0x00007fef7f3f6000]
2020-07-08T21:46:15.7759642Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7759910Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7760656Z 	- parking to wait for  <0x0000000087182280> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7761106Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7761610Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7762188Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-07-08T21:46:15.7762857Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7763509Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7763977Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7764465Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7764863Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7765054Z 
2020-07-08T21:46:15.7765798Z ""jobmanager-future-thread-1"" #51 daemon prio=5 os_prio=0 tid=0x00007fefb4019800 nid=0x9d40 waiting on condition [0x00007fef7f0f3000]
2020-07-08T21:46:15.7766860Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7767199Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7767934Z 	- parking to wait for  <0x0000000087182040> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7768427Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7769046Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7769644Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7770228Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7770737Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7771326Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7771772Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7772144Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7772441Z 
2020-07-08T21:46:15.7773057Z ""mini-cluster-io-thread-8"" #46 daemon prio=5 os_prio=0 tid=0x00007fefa003e000 nid=0x9d3a waiting on condition [0x00007fef7f4f7000]
2020-07-08T21:46:15.7773594Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7773888Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7774524Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7775014Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7775545Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7776086Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7776692Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7777163Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7777654Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7778054Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7778243Z 
2020-07-08T21:46:15.7778993Z ""mini-cluster-io-thread-7"" #45 daemon prio=5 os_prio=0 tid=0x00007fefa003c800 nid=0x9d39 waiting on condition [0x00007fef7f5f8000]
2020-07-08T21:46:15.7779432Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7779716Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7780326Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7780804Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7781421Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7781934Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7782378Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7782815Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7783269Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7783628Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7784198Z 
2020-07-08T21:46:15.7785126Z ""mini-cluster-io-thread-6"" #44 daemon prio=5 os_prio=0 tid=0x00007fefa003b000 nid=0x9d38 waiting on condition [0x00007fef7f6f9000]
2020-07-08T21:46:15.7786719Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7787001Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7787771Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7788266Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7788785Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7789334Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7789797Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7790281Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7790770Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7791155Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7791350Z 
2020-07-08T21:46:15.7791985Z ""mini-cluster-io-thread-5"" #43 daemon prio=5 os_prio=0 tid=0x00007fefa4005000 nid=0x9d37 waiting on condition [0x00007fef7f7fa000]
2020-07-08T21:46:15.7792439Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7792718Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7793477Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7794120Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7794908Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7795453Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7795959Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7796446Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7796921Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7798102Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7798308Z 
2020-07-08T21:46:15.7799064Z ""mini-cluster-io-thread-4"" #42 daemon prio=5 os_prio=0 tid=0x00007fef81bcb000 nid=0x9d36 waiting on condition [0x00007fef7fafb000]
2020-07-08T21:46:15.7799503Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7799796Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7800432Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7800925Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7801557Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7802188Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7802641Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7803083Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7803537Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7803909Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7804085Z 
2020-07-08T21:46:15.7804669Z ""mini-cluster-io-thread-3"" #41 daemon prio=5 os_prio=0 tid=0x00007fefa0033800 nid=0x9d35 waiting on condition [0x00007fef7fbfc000]
2020-07-08T21:46:15.7805207Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7805488Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7806232Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7806723Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7807383Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7808642Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7809393Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7809867Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7810356Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7810742Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7810947Z 
2020-07-08T21:46:15.7811680Z ""mini-cluster-io-thread-2"" #40 daemon prio=5 os_prio=0 tid=0x00007fef81bc5800 nid=0x9d34 waiting on condition [0x00007fef7fcfd000]
2020-07-08T21:46:15.7812136Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7812413Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7813079Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7813561Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7814096Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7814648Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7815113Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7815598Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7816075Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7816474Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7816663Z 
2020-07-08T21:46:15.7817337Z ""Flink-DispatcherRestEndpoint-thread-1"" #38 daemon prio=5 os_prio=0 tid=0x00007fef81ba7800 nid=0x9d30 waiting on condition [0x00007fef7fdfe000]
2020-07-08T21:46:15.7817803Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7818256Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7818948Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7819424Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7819954Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7820563Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7821169Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7821709Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7822185Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7822675Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7823066Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7823268Z 
2020-07-08T21:46:15.7823894Z ""mini-cluster-io-thread-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fef81ba6800 nid=0x9d2f waiting on condition [0x00007fef881f5000]
2020-07-08T21:46:15.7824346Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7824625Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7825558Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7826069Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7826571Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7828100Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7830710Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7833058Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7833602Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7833991Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7834183Z 
2020-07-08T21:46:15.7834974Z ""flink-rest-server-netty-boss-thread-1"" #36 daemon prio=5 os_prio=0 tid=0x00007fef81ba2800 nid=0x9d2e runnable [0x00007fef886f6000]
2020-07-08T21:46:15.7837524Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7837854Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-07-08T21:46:15.7838216Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-07-08T21:46:15.7838640Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-07-08T21:46:15.7839059Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-07-08T21:46:15.7839863Z 	- locked <0x0000000087183808> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-07-08T21:46:15.7840536Z 	- locked <0x00000000871837f8> (a java.util.Collections$UnmodifiableSet)
2020-07-08T21:46:15.7841094Z 	- locked <0x00000000871837b0> (a sun.nio.ch.EPollSelectorImpl)
2020-07-08T21:46:15.7841467Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-07-08T21:46:15.7841995Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-07-08T21:46:15.7842624Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-07-08T21:46:15.7843171Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-07-08T21:46:15.7843758Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-07-08T21:46:15.7844391Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-07-08T21:46:15.7844988Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7845195Z 
2020-07-08T21:46:15.7845547Z ""IOManager reader thread #1"" #30 daemon prio=5 os_prio=0 tid=0x00007fefd976f000 nid=0x9d09 waiting on condition [0x00007fef887f7000]
2020-07-08T21:46:15.7845994Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7846271Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7846965Z 	- parking to wait for  <0x0000000087183a70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7847459Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7847978Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7848536Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7849045Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-07-08T21:46:15.7849380Z 
2020-07-08T21:46:15.7849731Z ""IOManager writer thread #1"" #29 daemon prio=5 os_prio=0 tid=0x00007fefd976d000 nid=0x9d08 waiting on condition [0x00007fef888f8000]
2020-07-08T21:46:15.7853026Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7853377Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7854189Z 	- parking to wait for  <0x0000000087183c78> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7854691Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7855215Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7855767Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7856438Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-07-08T21:46:15.7856783Z 
2020-07-08T21:46:15.7857397Z ""Timer-2"" #27 daemon prio=5 os_prio=0 tid=0x00007fefd9736800 nid=0x9d07 in Object.wait() [0x00007fef889f9000]
2020-07-08T21:46:15.7857846Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-07-08T21:46:15.7858155Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7858663Z 	- waiting on <0x0000000087183e80> (a java.util.TaskQueue)
2020-07-08T21:46:15.7859019Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-07-08T21:46:15.7859530Z 	- locked <0x0000000087183e80> (a java.util.TaskQueue)
2020-07-08T21:46:15.7859863Z 	at java.util.TimerThread.run(Timer.java:505)
2020-07-08T21:46:15.7860062Z 
2020-07-08T21:46:15.7860636Z ""Timer-1"" #25 daemon prio=5 os_prio=0 tid=0x00007fefd9734000 nid=0x9d06 in Object.wait() [0x00007fef88afa000]
2020-07-08T21:46:15.7861073Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-07-08T21:46:15.7861394Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7861888Z 	- waiting on <0x0000000087184060> (a java.util.TaskQueue)
2020-07-08T21:46:15.7880889Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-07-08T21:46:15.7884642Z 	- locked <0x0000000087184060> (a java.util.TaskQueue)
2020-07-08T21:46:15.7885023Z 	at java.util.TimerThread.run(Timer.java:505)
2020-07-08T21:46:15.7885225Z 
2020-07-08T21:46:15.7885568Z ""BLOB Server listener at 40413"" #21 daemon prio=5 os_prio=0 tid=0x00007fefd9731000 nid=0x9d05 runnable [0x00007fef88bfb000]
2020-07-08T21:46:15.7885962Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7886274Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-07-08T21:46:15.7886677Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-07-08T21:46:15.7887099Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-07-08T21:46:15.7887488Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-07-08T21:46:15.7887891Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)
2020-07-08T21:46:15.7888333Z 
2020-07-08T21:46:15.7888964Z ""Timer-0"" #22 daemon prio=5 os_prio=0 tid=0x00007fefd9719000 nid=0x9d04 in Object.wait() [0x00007fef88efc000]
2020-07-08T21:46:15.7889410Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-07-08T21:46:15.7889716Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7890222Z 	- waiting on <0x00000000871846a0> (a java.util.TaskQueue)
2020-07-08T21:46:15.7890561Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-07-08T21:46:15.7891090Z 	- locked <0x00000000871846a0> (a java.util.TaskQueue)
2020-07-08T21:46:15.7891423Z 	at java.util.TimerThread.run(Timer.java:505)
2020-07-08T21:46:15.7891621Z 
2020-07-08T21:46:15.7892216Z ""flink-metrics-scheduler-1"" #17 prio=5 os_prio=0 tid=0x00007fefd96de000 nid=0x9d00 waiting on condition [0x00007fefac2f4000]
2020-07-08T21:46:15.7892671Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-07-08T21:46:15.7892986Z 	at java.lang.Thread.sleep(Native Method)
2020-07-08T21:46:15.7893378Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-07-08T21:46:15.7896632Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-07-08T21:46:15.7901619Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-07-08T21:46:15.7902086Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7902278Z 
2020-07-08T21:46:15.7903094Z ""flink-akka.actor.default-dispatcher-3"" #15 prio=5 os_prio=0 tid=0x00007fefd914e800 nid=0x9cfe waiting on condition [0x00007fefac6f6000]
2020-07-08T21:46:15.7904034Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7904361Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7905300Z 	- parking to wait for  <0x0000000087184ce8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-07-08T21:46:15.7905963Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-07-08T21:46:15.7906421Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-07-08T21:46:15.7906846Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-07-08T21:46:15.7907313Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-07-08T21:46:15.7907592Z 
2020-07-08T21:46:15.7908832Z ""flink-akka.actor.default-dispatcher-2"" #14 prio=5 os_prio=0 tid=0x00007fefd9139800 nid=0x9cfb waiting on condition [0x00007fefac7f7000]
2020-07-08T21:46:15.7909581Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7909902Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7917391Z 	- parking to wait for  <0x0000000087184ce8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-07-08T21:46:15.7917891Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-07-08T21:46:15.7918347Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-07-08T21:46:15.7918803Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-07-08T21:46:15.7919099Z 
2020-07-08T21:46:15.7919706Z ""flink-scheduler-1"" #13 prio=5 os_prio=0 tid=0x00007fefd90a3800 nid=0x9cfa waiting on condition [0x00007fefac8f8000]
2020-07-08T21:46:15.7920155Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-07-08T21:46:15.7920447Z 	at java.lang.Thread.sleep(Native Method)
2020-07-08T21:46:15.7920850Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-07-08T21:46:15.7921378Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-07-08T21:46:15.7921924Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-07-08T21:46:15.7922347Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7922537Z 
2020-07-08T21:46:15.7922887Z ""process reaper"" #11 daemon prio=10 os_prio=0 tid=0x00007fef8c039800 nid=0x9cf5 waiting on condition [0x00007fefad336000]
2020-07-08T21:46:15.7923487Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7923790Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7924433Z 	- parking to wait for  <0x00000000871852d8> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-07-08T21:46:15.7924909Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7925392Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-07-08T21:46:15.7925923Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-07-08T21:46:15.7926411Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-07-08T21:46:15.7926861Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-07-08T21:46:15.7927346Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7927830Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7928232Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7928421Z 
2020-07-08T21:46:15.7929070Z ""surefire-forkedjvm-ping-30s"" #10 daemon prio=5 os_prio=0 tid=0x00007fefd8333800 nid=0x9cf2 waiting on condition [0x00007fefaddf4000]
2020-07-08T21:46:15.7929529Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7929832Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7930483Z 	- parking to wait for  <0x000000008003f018> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7930969Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7931521Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7932135Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7932839Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7933385Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7933857Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7934347Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7934731Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7934935Z 
2020-07-08T21:46:15.7935575Z ""surefire-forkedjvm-command-thread"" #9 daemon prio=5 os_prio=0 tid=0x00007fefd831c000 nid=0x9cf1 runnable [0x00007fefc8110000]
2020-07-08T21:46:15.7936010Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7936301Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-07-08T21:46:15.7936656Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-07-08T21:46:15.7937053Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-07-08T21:46:15.7937482Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-07-08T21:46:15.7938078Z 	- locked <0x000000008003f298> (a java.io.BufferedInputStream)
2020-07-08T21:46:15.7938448Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-07-08T21:46:15.7938917Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-07-08T21:46:15.7939441Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-07-08T21:46:15.7939867Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7940058Z 
2020-07-08T21:46:15.7940383Z ""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007fefd820a800 nid=0x9cef runnable [0x0000000000000000]
2020-07-08T21:46:15.7940761Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7940950Z 
2020-07-08T21:46:15.7942892Z ""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007fefd81ff800 nid=0x9cee waiting on condition [0x0000000000000000]
2020-07-08T21:46:15.7943483Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7943655Z 
2020-07-08T21:46:15.7944001Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007fefd81fd000 nid=0x9ced waiting on condition [0x0000000000000000]
2020-07-08T21:46:15.7945021Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7945236Z 
2020-07-08T21:46:15.7945548Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007fefd81fb000 nid=0x9cec runnable [0x0000000000000000]
2020-07-08T21:46:15.7945946Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7946121Z 
2020-07-08T21:46:15.7946503Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007fefd81f9800 nid=0x9ceb waiting on condition [0x0000000000000000]
2020-07-08T21:46:15.7946936Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7947124Z 
2020-07-08T21:46:15.7947437Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fefd81c9800 nid=0x9cea in Object.wait() [0x00007fefc8dfc000]
2020-07-08T21:46:15.7947882Z    java.lang.Thread.State: WAITING (on object monitor)
2020-07-08T21:46:15.7948179Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7948525Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-07-08T21:46:15.7949696Z 	- locked <0x0000000080043288> (a java.lang.ref.ReferenceQueue$Lock)
2020-07-08T21:46:15.7950111Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-07-08T21:46:15.7950533Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-07-08T21:46:15.7950778Z 
2020-07-08T21:46:15.7951106Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fefd81c5000 nid=0x9ce9 in Object.wait() [0x00007fefc8efd000]
2020-07-08T21:46:15.7951551Z    java.lang.Thread.State: WAITING (on object monitor)
2020-07-08T21:46:15.7951862Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7952575Z 	at java.lang.Object.wait(Object.java:502)
2020-07-08T21:46:15.7953820Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-07-08T21:46:15.7954525Z 	- locked <0x0000000080043278> (a java.lang.ref.Reference$Lock)
2020-07-08T21:46:15.7954935Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-07-08T21:46:15.7955185Z 
2020-07-08T21:46:15.7955492Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fefd800b800 nid=0x9ce0 in Object.wait() [0x00007fefe2138000]
2020-07-08T21:46:15.7956693Z    java.lang.Thread.State: WAITING (on object monitor)
2020-07-08T21:46:15.7957487Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7958906Z 	- waiting on <0x0000000087181118> (a org.apache.flink.test.accumulators.AccumulatorLiveITCase$1)
2020-07-08T21:46:15.7959330Z 	at java.lang.Thread.join(Thread.java:1252)
2020-07-08T21:46:15.7959948Z 	- locked <0x0000000087181118> (a org.apache.flink.test.accumulators.AccumulatorLiveITCase$1)
2020-07-08T21:46:15.7960408Z 	at org.apache.flink.core.testutils.CheckedThread.trySync(CheckedThread.java:112)
2020-07-08T21:46:15.7960896Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:100)
2020-07-08T21:46:15.7961352Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:89)
2020-07-08T21:46:15.7961901Z 	at org.apache.flink.test.accumulators.AccumulatorLiveITCase.submitJobAndVerifyResults(AccumulatorLiveITCase.java:178)
2020-07-08T21:46:15.7962593Z 	at org.apache.flink.test.accumulators.AccumulatorLiveITCase.testStreaming(AccumulatorLiveITCase.java:137)
2020-07-08T21:46:15.7963051Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-07-08T21:46:15.7963469Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-07-08T21:46:15.7963954Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-07-08T21:46:15.7964396Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-07-08T21:46:15.7964820Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-07-08T21:46:15.7965332Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-07-08T21:46:15.7965986Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-07-08T21:46:15.7966471Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-07-08T21:46:15.7966960Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-07-08T21:46:15.7967388Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-07-08T21:46:15.7976578Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-07-08T21:46:15.7977052Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-07-08T21:46:15.7996016Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-07-08T21:46:15.7996567Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-07-08T21:46:15.7998186Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-07-08T21:46:15.8000214Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-07-08T21:46:15.8001811Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-07-08T21:46:15.8002898Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-07-08T21:46:15.8003494Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-07-08T21:46:15.8004932Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-07-08T21:46:15.8005359Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-07-08T21:46:15.8006127Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-07-08T21:46:15.8006585Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-07-08T21:46:15.8007100Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-07-08T21:46:15.8007613Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-07-08T21:46:15.8008314Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-07-08T21:46:15.8008844Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-07-08T21:46:15.8009398Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-07-08T21:46:15.8009884Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-07-08T21:46:15.8010358Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-07-08T21:46:15.8010622Z 
2020-07-08T21:46:15.8010875Z ""VM Thread"" os_prio=0 tid=0x00007fefd81bb800 nid=0x9ce8 runnable 
2020-07-08T21:46:15.8011089Z 
2020-07-08T21:46:15.8011384Z ""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007fefd8020000 nid=0x9ce1 runnable 
2020-07-08T21:46:15.8011637Z 
2020-07-08T21:46:15.8011928Z ""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007fefd8022000 nid=0x9ce2 runnable 
2020-07-08T21:46:15.8012181Z 
2020-07-08T21:46:15.8012467Z ""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007fefd8046000 nid=0x9ce6 runnable 
2020-07-08T21:46:15.8012717Z 
2020-07-08T21:46:15.8013007Z ""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007fefd8047800 nid=0x9ce7 runnable 
2020-07-08T21:46:15.8013287Z 
2020-07-08T21:46:15.8013565Z ""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007fefd8028000 nid=0x9ce5 runnable 
2020-07-08T21:46:15.8013829Z 
2020-07-08T21:46:15.8014102Z ""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007fefd8026000 nid=0x9ce4 runnable 
2020-07-08T21:46:15.8034995Z 
2020-07-08T21:46:15.8035322Z ""G1 Concurrent Refinement Thread#2"" os_prio=0 tid=0x00007fefd8024800 nid=0x9ce3 runnable 
2020-07-08T21:46:15.8035581Z 
2020-07-08T21:46:15.8035879Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007fefd820d800 nid=0x9cf0 waiting on condition 
2020-07-08T21:46:15.8036137Z 
2020-07-08T21:46:15.8036337Z JNI global references: 1359
{code}",,dian.fu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17075,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 09:47:07 UTC 2020,,,,,,,,,,"0|z0gl1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 02:19;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0;;;","09/Jul/20 14:19;chesnay;Cancellation of unknown deployments temporarily disabled:
master: 6e811e6f21e5baa6bfb1862a51815e1d151c7097 ;;;","23/Jul/20 09:47;chesnay;master: 11663f5792a41455ee5f1cdf4d53b3170cc04915

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetAvroWriters can not write data to hdfs,FLINK-18530,13315629,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,humengyu,humengyu,08/Jul/20 13:13,15/Jul/20 09:58,13/Jul/23 08:12,15/Jul/20 09:58,1.11.0,,,,,,,,Connectors / FileSystem,,,,,0,,,,,,"I read data from kafka and write to hdfs by StreamingFileSink:
 # in version 1.11.0, ParquetAvroWriters does not work, but it works well in version 1.10.1;
 #  AvroWriters works well in 1.11.0.

{code:java}
public class TestParquetAvroSink {

  @Test
  public void testParquet() throws Exception {
    EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner()
        .inStreamingMode().build();
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);
    env.enableCheckpointing(20000L);

    TableSchema tableSchema = TableSchema.builder().fields(
        new String[]{""id"", ""name"", ""sex""},
        new DataType[]{DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()})
        .build();

    // build a kafka source
    DataStream<Row> rowDataStream = xxxx;

    Schema schema = SchemaBuilder
        .record(""xxx"")
        .namespace(""xxxx"")
        .fields()
        .optionalString(""id"")
        .optionalString(""name"")
        .optionalString(""sex"")
        .endRecord();

    OutputFileConfig config = OutputFileConfig
        .builder()
        .withPartPrefix(""prefix"")
        .withPartSuffix("".ext"")
        .build();

    StreamingFileSink<GenericRecord> sink = StreamingFileSink
        .forBulkFormat(
            new Path(""hdfs://host:port/xxx/xxx/xxx""),
            ParquetAvroWriters.forGenericRecord(schema))
        .withOutputFileConfig(config)
        .withBucketAssigner(new DateTimeBucketAssigner<>(""'pdate='yyyy-MM-dd""))
        .build();

    SingleOutputStreamOperator<GenericRecord> recordDateStream = rowDataStream
        .map(new RecordMapFunction());

    recordDateStream.print();
    recordDateStream.addSink(sink);

    env.execute(""test"");

  }


  @Test
  public void testAvro() throws Exception {
    EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner()
        .inStreamingMode().build();
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);
    env.enableCheckpointing(20000L);

    TableSchema tableSchema = TableSchema.builder().fields(
        new String[]{""id"", ""name"", ""sex""},
        new DataType[]{DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()})
        .build();

    // build a kafka source
    DataStream<Row> rowDataStream = xxxx;

    Schema schema = SchemaBuilder
        .record(""xxx"")
        .namespace(""xxxx"")
        .fields()
        .optionalString(""id"")
        .optionalString(""name"")
        .optionalString(""sex"")
        .endRecord();

    OutputFileConfig config = OutputFileConfig
        .builder()
        .withPartPrefix(""prefix"")
        .withPartSuffix("".ext"")
        .build();

    StreamingFileSink<GenericRecord> sink = StreamingFileSink
        .forBulkFormat(
            new Path(""hdfs://host:port/xxx/xxx/xxx""),
            AvroWriters.forGenericRecord(schema))
        .withOutputFileConfig(config)
        .withBucketAssigner(new DateTimeBucketAssigner<>(""'pdate='yyyy-MM-dd""))
        .build();

    SingleOutputStreamOperator<GenericRecord> recordDateStream = rowDataStream
        .map(new RecordMapFunction());

    recordDateStream.print();
    recordDateStream.addSink(sink);

    env.execute(""test"");

  }

  public static class RecordMapFunction implements MapFunction<Row, GenericRecord> {

    private transient Schema schema;

    @Override
    public GenericRecord map(Row row) throws Exception {
      if (schema == null) {
        schema = SchemaBuilder
            .record(""xxx"")
            .namespace(""xxx"")
            .fields()
            .optionalString(""id"")
            .optionalString(""name"")
            .optionalString(""sex"")
            .endRecord();
      }
      Record record = new Record(schema);
      record.put(""id"", row.getField(0));
      record.put(""name"", row.getField(1));
      record.put(""sex"", row.getField(2));
      return record;
    }
  }
} 
{code}
 ",,humengyu,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-08 13:13:52.0,,,,,,,,,,"0|z0gk94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query Hive table and filter by timestamp partition can fail,FLINK-18529,13315624,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,08/Jul/20 12:22,14/Jul/20 06:07,13/Jul/23 08:12,14/Jul/20 06:07,,,,,,1.11.1,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"The following example
{code}
create table foo (x int) partitioned by (ts timestamp);
select x from foo where timestamp '2020-07-08 13:08:14' = ts;
{code}
fails with
{noformat}
CatalogException: HiveCatalog currently only supports timestamp of precision 9
{noformat}",,libenchao,lirui,lzljs3620320,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 06:07:22 UTC 2020,,,,,,,,,,"0|z0gk80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/20 06:07;lzljs3620320;master: 81f41a6d3fda3902e8ee850d4ab8043a0e9bf763

release-1.11: 273fd847c5cd2ac76e87aa1aa47a8517b78d860b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Table Function type inference fails,FLINK-18520,13315437,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,libenchao,libenchao,07/Jul/20 15:26,11/Aug/20 07:36,13/Jul/23 08:12,09/Jul/20 08:36,1.11.0,,,,,1.11.1,1.12.0,,Table SQL / API,,,,,0,pull-request-available,,,,,"For a simple UDTF like 
{code:java}
public class Split extends TableFunction<String> {
		public Split(){}
		public void eval(String str, String ch) {
			if (str == null || str.isEmpty()) {
				return;
			} else {
				String[] ss = str.split(ch);
				for (String s : ss) {
					collect(s);
				}
			}
		}
	}
{code}

register it using new function type inference {{tableEnv.createFunction(""my_split"", Split.class);}} and using it in a simple query will fail with following exception:

{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 93 to line 1, column 115: No match found for function signature my_split(<CHARACTER>, <CHARACTER>)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:716)
	at com.bytedance.demo.SqlTest.main(SqlTest.java:64)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 93 to line 1, column 115: No match found for function signature my_split(<CHARACTER>, <CHARACTER>)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)
	at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)
	... 7 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature my_split(<CHARACTER>, <CHARACTER>)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)
	... 35 more
{code}

it's reported from user-zh: http://apache-flink.147419.n8.nabble.com/Flink-1-11-SQL-UDTF-No-match-found-for-function-signature-td4553.html
CC [~twalthr]",,ana4,fsk119,godfreyhe,jark,leonard,libenchao,lsy,tiemsn,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 07:36:21 UTC 2020,,,,,,,,,,"0|z0gj3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/20 13:41;twalthr;I opened a PR for this problem. [~libenchao] could you review this?;;;","08/Jul/20 15:32;libenchao;[~twalthr] Thanks for your quick fix, I've reviewed the PR.;;;","09/Jul/20 08:36;twalthr;Fixed in 1.12.0: df4f9bc5b00b14fd1d01b45c20a754d07c1d2ea7
Fixed in 1.11.1: bbfb81df5193c3551b2c17812321facddcfba586;;;","11/Aug/20 03:56;ana4;Has AsyncTableFunction fixed?;;;","11/Aug/20 07:30;twalthr;[~ana4] AsyncTableFunction are a special case of table functions that are currently only used for temporal joins. They still use the old type system. I will open an issue for tracking the progress there.;;;","11/Aug/20 07:36;twalthr;I opened FLINK-18890. But keep in mind that they cannot be used as regular table functions anyway. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate exception to client when execution fails for REST submission,FLINK-18519,13315422,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,kkl0u,kkl0u,07/Jul/20 14:43,08/Jul/20 14:21,13/Jul/23 08:12,08/Jul/20 08:41,1.11.0,,,,,1.11.1,1.12.0,,Runtime / REST,,,,,0,pull-request-available,,,,,"Currently when a user submits an application using the REST api and the execution fails, the exception is logged, but not sent back to the client. This issue aims at propagating the reason back to the client so that it is easier for the user to debug his/her application.",,kkl0u,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 08:41:39 UTC 2020,,,,,,,,,,"0|z0gj08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/20 08:41;kkl0u;Fixed on master with 78b00f65c3cd9c3536e54fd63f240bc89c83e572
and on release-1.11 with 82e603a7d3e24d89c09f6ccdc42e4dcc07232040;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"kubernetes session test failed with ""java.net.SocketException: Broken pipe""",FLINK-18517,13315353,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,dian.fu,dian.fu,07/Jul/20 08:54,10/Aug/20 14:33,13/Jul/23 08:12,29/Jul/20 12:06,1.10.1,,,,,1.10.2,,,Deployment / Kubernetes,Tests,,,,0,pull-request-available,test-stability,,,,"It failed on release-1.10 branch:
https://travis-ci.org/github/apache/flink/jobs/705554778

Exception message:
{code}
020-07-07 01:54:17,173 ERROR org.apache.flink.client.cli.CliFrontend                       - Error while running the command.
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Operation: [get]  for kind: [Service]  with name: [flink-native-k8s-session-1-rest]  in namespace: [default]  failed.
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:670)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:901)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:974)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:974)
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Service]  with name: [flink-native-k8s-session-1-rest]  in namespace: [default]  failed.
	at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64)
	at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:231)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:164)
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.getService(Fabric8FlinkKubeClient.java:299)
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.getRestService(Fabric8FlinkKubeClient.java:240)
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.getRestEndpoint(Fabric8FlinkKubeClient.java:205)
	at org.apache.flink.kubernetes.KubernetesClusterDescriptor.lambda$createClusterClientProvider$0(KubernetesClusterDescriptor.java:88)
	at org.apache.flink.kubernetes.KubernetesClusterDescriptor.retrieve(KubernetesClusterDescriptor.java:118)
	at org.apache.flink.kubernetes.KubernetesClusterDescriptor.retrieve(KubernetesClusterDescriptor.java:59)
	at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.execute(AbstractSessionClusterExecutor.java:63)
	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:962)
	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:108)
	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:58)
	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
	... 11 more
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431)
	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417)
	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:894)
	at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:865)
	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123)
	at org.apache.flink.kubernetes.shadded.okio.Okio$1.write(Okio.java:79)
	at org.apache.flink.kubernetes.shadded.okio.AsyncTimeout$1.write(AsyncTimeout.java:180)
	at org.apache.flink.kubernetes.shadded.okio.RealBufferedSink.flush(RealBufferedSink.java:224)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http2.Http2Writer.settings(Http2Writer.java:203)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http2.Http2Connection.start(Http2Connection.java:515)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http2.Http2Connection.start(Http2Connection.java:505)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.RealConnection.startHttp2(RealConnection.java:298)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.RealConnection.establishProtocol(RealConnection.java:287)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.RealConnection.connect(RealConnection.java:168)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:257)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:126)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
	at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:119)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
	at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
	at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createHttpClient$3(HttpClientUtils.java:112)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
	at org.apache.flink.kubernetes.shadded.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
	at org.apache.flink.kubernetes.shadded.okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:254)
	at org.apache.flink.kubernetes.shadded.okhttp3.RealCall.execute(RealCall.java:92)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:411)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:372)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:337)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:318)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:812)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:220)
	... 28 more
Checking for non-empty .out files...
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.
[FAIL] 'Run kubernetes session test' failed after 1 minutes and 24 seconds! Test exited with exit code 1 and the logs contained errors, exceptions or non-empty .out files
{code}",,dian.fu,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 12:06:57 UTC 2020,,,,,,,,,,"0|z0gikw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/20 08:56;dian.fu;Another instance: [https://travis-ci.org/github/apache/flink/jobs/703303998];;;","21/Jul/20 12:36;dian.fu;https://travis-ci.org/github/apache/flink/jobs/710154463;;;","23/Jul/20 13:09;trohrmann;[~fly_in_gis] do you have an idea what could cause this problem?;;;","24/Jul/20 05:38;wangyang0918;[~dian.fu] [~trohrmann] I think it is same with FLINK-17416. And we need to bump the fabric8 version from 4.5.2 to 4.9.2 in release-1.10 to fix the compatibility issue with jdk 8u252.

I will do it.;;;","24/Jul/20 07:09;trohrmann;Thanks a lot [~fly_in_gis]. Let me know if I can help you with the PR review.;;;","29/Jul/20 12:06;trohrmann;Fixed via 183dfc30b4790a2ee02378170f62ea07ee26aab0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The generated flink-docker file misses flink version in Tags,FLINK-18506,13315282,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,liyu,liyu,07/Jul/20 02:18,10/Aug/20 13:49,13/Jul/23 08:12,07/Jul/20 09:24,,,,,,,,,flink-docker,,,,,0,pull-request-available,,,,,"Changes made in FLINK-17367 removed {{FLINK_VERSION}} from Dockerfile template, but the current [generate-stackbrew-library.sh|https://github.com/apache/flink-docker/blob/31794825ad02db8b0eb961372c74a309a4504bcd/generate-stackbrew-library.sh#L97] is still trying to parse `flink_version` from it, which will cause the generated `library/flink` file missing flink version in `Tags`.

While we could manually work-around the problem, a fix in script is needed.",,jark,liyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 09:24:43 UTC 2020,,,,,,,,,,"0|z0gi54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/20 09:24;chesnay;docker-master: c83ed2f80d3a6ace36205ee92926eef2abbca244;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New table source factory omits unrecognized properties silently,FLINK-18487,13315024,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,libenchao,libenchao,05/Jul/20 08:37,24/Jul/20 02:09,13/Jul/23 08:12,24/Jul/20 02:09,1.11.0,,,,,1.12.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"For the following DDL, we just omits the unrecognized property 'records-per-second'.
{code:sql}
CREATE TABLE MyDataGen (
  int_field int,
  double_field double,
  string_field varchar
) WITH (
  'connector' = 'datagen',
  'records-per-second' = '1'  -- should be rows-per-second
)
{code}
IMO, we should throw Exceptions to tell users that they used a wrong property. 
 CC [~jark] [~twalthr]",,danny0405,dian.fu,fhueske,jark,kezhuw,libenchao,lsy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 02:09:10 UTC 2020,,,,,,,,,,"0|z0ggjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/20 02:10;lzljs3620320;It is true, we can implement a validator for this. But it is not easy to re-use FactoryUtil:
 * Like old schema config option style, datagen source needs option keys related to the fields. like ""fields.*.generator"".
 * FactoryUtil not accept wildcard now.

There is no clear answer in 1.11, but we can move forward in 1.12.;;;","06/Jul/20 02:26;jark;We can have a custom validation logic in {{DataGenTableSourceFactory}}.;;;","06/Jul/20 02:42;danny0405;+1 to have the validation logic in the factory. The FactoryUtil can support validation patterns that are common enough.;;;","06/Jul/20 02:56;jark;Yes. {{FactoryUtil}} is just a helper for common cases. 
Btw, we can still target to fix this in 1.11 along with {{BlackHoleTableSinkFactory}}.;;;","06/Jul/20 03:16;lzljs3620320;We can create a separate validator for {{DataGenTableSourceFactory}}, but just curious, I am wondered why we can not have a WildcardValidatorUtil? 

BTW, BlackHoleTableSinkFactory does not have any option.;;;","06/Jul/20 03:20;jark;If {{WildcardValidatorUtil}} is also commonly required by other connectors, I think we can make it as a public API. But we can start from an internal util first. 

Currently, {{BlackHoleTableSinkFactory}} can also accept arbitrary options and no exception, e.g. {{'a.b.c' = '123'}} is accepted. ;;;","06/Jul/20 03:33;lzljs3620320;I got what are you mean, every connectors should chooses either {{TableFactoryHelper.validate}} or implementing validation by itself, whatever, there should be validation in every connectors.;;;","24/Jul/20 02:09;lzljs3620320;master: 73210cc0f712158ec939ef3ad7dec52a921aad7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kerberized YARN per-job on Docker test failed during unzip jce_policy-8.zip,FLINK-18485,13314967,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dian.fu,dian.fu,04/Jul/20 05:13,08/Jul/20 02:22,13/Jul/23 08:12,08/Jul/20 02:22,1.11.0,1.12.0,,,,1.11.1,1.12.0,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"Instance on 1.11 branch: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4230&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8]
{code:java}
+ curl -LOH Cookie: oraclelicense=accept-securebackup-cookie http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0

100   429  100   429    0     0   1616      0 --:--:-- --:--:-- --:--:--  1616

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0

100  7073  100  7073    0     0  20139      0 --:--:-- --:--:-- --:--:-- 20139
+ unzip jce_policy-8.zip
Archive:  jce_policy-8.zip
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of jce_policy-8.zip or
        jce_policy-8.zip.zip, and cannot find jce_policy-8.zip.ZIP, period.
{code}",,dian.fu,dwysakowicz,klion26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 12:10:29 UTC 2020,,,,,,,,,,"0|z0gg74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/20 05:15;dian.fu;Another instance on master branch: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4229&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b];;;","04/Jul/20 06:02;dian.fu;I tried to download jce_policy-8.zip manually and encountered 404 error.

It seems a temporary technique issue of oracle website. I encountered the following error when trying to download jce_policy-8.zip from oracle website:
{code:java}
This site is experiencing technical difficulty. We are aware of the issue and are working as quick as possible to correct the issue.

We apologize for any inconvenience this may have caused.

To speak with an Oracle sales representative: 1.800.ORACLE1.

To contact Oracle Corporate Headquarters from anywhere in the world: 1.650.506.7000.

To get technical support in the United States: 1.800.633.0738.
{code};;;","05/Jul/20 02:43;dian.fu;jce_policy-8.zip is still not available. It fails several cron tasks.

If this file is still not available, one option for this problem is to use [JDK 8u151+|https://www.oracle.com/java/technologies/javase/8u151-relnotes.html] (current version is 8u131) which allows to set the property *crypto.policy* as ** *unlimited* in the file *java.security*. Then there is no necessary to download jce_policy-8.zip any more.;;;","05/Jul/20 02:51;dian.fu;master:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4239&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4239&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4239&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4239&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4239&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c]

release-1.11:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4240&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4240&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4240&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4240&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334];;;","06/Jul/20 03:16;dian.fu;Similar issue from StackOverflow:  [https://stackoverflow.com/questions/62737641/404-when-trying-to-download-jce-8-policy-from-oracle];;;","06/Jul/20 06:55;dwysakowicz;It fails all the builds consistently. We should definitely do something about it. Updating the java version in the test sounds reasonable to me. Let me try that.;;;","06/Jul/20 07:10;dian.fu;Thanks [~dwysakowicz]. ;;;","06/Jul/20 12:10;dwysakowicz;I updated the java version in:
* master:
** 1b1c343e8964c6400c7c1de3c70212522ba59a64
* 1.11.1:
** 5fa0b2ee8c4ff3cbc604390930154125bd8aed6d

I will wait with closing this task for a green build on master/1.11 branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogSocketExample does not work,FLINK-18477,13314761,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,dwysakowicz,dwysakowicz,02/Jul/20 18:31,13/Jul/20 14:30,13/Jul/23 08:12,13/Jul/20 14:30,1.11.0,,,,,1.11.1,1.12.0,,Examples,Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"The example fails on a fresh build with:
{code}
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to create a source for reading table 'default_catalog.default_database.UserScores'.

Table options are:

'byte-delimiter'='10'
'changelog-csv.column-delimiter'='|'
'connector'='socket'
'format'='changelog-csv'
'hostname'='localhost'
'port'='9999'
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)
Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.UserScores'.

Table options are:

'byte-delimiter'='10'
'changelog-csv.column-delimiter'='|'
'connector'='socket'
'format'='changelog-csv'
'hostname'='localhost'
'port'='9999'
	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)
	at org.apache.flink.table.examples.java.connectors.ChangelogSocketExample.main(ChangelogSocketExample.java:89)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)
	... 8 more
Caused by: org.apache.flink.table.api.TableException: Could not load service provider for factories.
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:346)
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:221)
	at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)
	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118)
	... 31 more
Caused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.table.examples.java.connectors.SocketDynamicTableFactory not found
	at java.util.ServiceLoader.fail(ServiceLoader.java:239)
	at java.util.ServiceLoader.access$300(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:342)
	... 34 more

{code}",,dian.fu,dwysakowicz,jark,leonard,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 14:30:49 UTC 2020,,,,,,,,,,"0|z0gexc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/20 03:56;jark;Hi [~twalthr], do you have time to fix this? I think it would be great if we can fix this in 1.11.1. I can help on this if you don't have time. ;;;","13/Jul/20 08:40;twalthr;Hi Jark, let me fix this today. I also found another issue with the example.;;;","13/Jul/20 14:30;twalthr;Fixed in 1.12.0: e4f431c3785e59792456dbdc7aebc82f20cc5180 f81f3a0ec863198164d45694ce23b21704c61cac
Fixed in 1.11.1: c7509bb244deecd45bb08cc7aa84534f5e5d4efd 5a66b972a267c7fa177bf1a410f8262326f19fbb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-runtime lists ""org.uncommons.maths:uncommons-maths:1.2.2a"" as a bundled dependency, but it isn't",FLINK-18471,13314706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,02/Jul/20 15:00,15/Jul/20 08:07,13/Jul/23 08:12,03/Jul/20 11:06,1.11.0,,,,,1.11.1,,,Build System,,,,,0,pull-request-available,,,,,"This is the relevant section in the NOTICE file

{code}
This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.typesafe.akka:akka-remote_2.11:2.5.21
- io.netty:netty:3.10.6.Final
- org.uncommons.maths:uncommons-maths:1.2.2a
{code}.

The uncommons-maths dependency is not declared anywhere, nor is it included in the binary file.",,jark,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 03 11:06:50 UTC 2020,,,,,,,,,,"0|z0geq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jul/20 15:25;chesnay;Correct, this was a transitive dependency of akka 2.4, but was dropped in 2.5. We can remove the NOTICE entry and the shade-plugin relocation.;;;","03/Jul/20 08:30;rmetzger;Okay, I'll open a PR.;;;","03/Jul/20 11:06;rmetzger;Resolved in 1.11: https://github.com/apache/flink/commit/f4e855cc6f6af0b34456e381e4d5d49ec73e5ff9
Resolved in 1.12: https://github.com/apache/flink/commit/09973500e20e91c773c1f658055ddf82b3b5e30f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte & RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort fail locally,FLINK-18470,13314702,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,02/Jul/20 14:29,03/Jul/20 15:12,13/Jul/23 08:12,03/Jul/20 15:12,1.11.0,1.12.0,,,,1.11.1,1.12.0,,Runtime / State Backends,Tests,,,,0,pull-request-available,,,,,"The tests:
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte

fail locally (in IDE or from cmd with {{mvn clean install}}) with
{code}
java.lang.UnsatisfiedLinkError: org.rocksdb.ReadOptions.newReadOptions()J
	at org.rocksdb.ReadOptions.newReadOptions(Native Method)
	at org.rocksdb.ReadOptions.<init>(ReadOptions.java:16)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIterator(RocksKeyGroupsRocksSingleStateIteratorTest.java:78)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIteratorShort(RocksKeyGroupsRocksSingleStateIteratorTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}",,dwysakowicz,liyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 03 15:12:46 UTC 2020,,,,,,,,,,"0|z0gepc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/20 15:12;dwysakowicz;Fixed in:
* master
** d0d803715b7760d4dd0405ecac0407f4d8a02a71
* 1.11.1
** 5d07c046a45bd273b1d0cb3dd7cd0cb2b942bc00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorITCase.testJobReExecutionAfterTaskExecutorTermination fails with DuplicateJobSubmissionException,FLINK-18468,13314694,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,rmetzger,rmetzger,02/Jul/20 13:35,22/Jul/20 06:04,13/Jul/23 08:12,22/Jul/20 06:04,1.12.0,,,,,1.11.2,1.12.0,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4149&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034
{code}
[ERROR] testJobReExecutionAfterTaskExecutorTermination(org.apache.flink.runtime.taskexecutor.TaskExecutorITCase)  Time elapsed: 1.222 s  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.DuplicateJobSubmissionException: Job has already been submitted.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.testJobReExecutionAfterTaskExecutorTermination(TaskExecutorITCase.java:108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

{code}",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 06:04:34 UTC 2020,,,,,,,,,,"0|z0genk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/20 15:30;trohrmann;The problem of this test case is that we are waiting on the job result and then submit the same job (same {{JobID}}) again. Having received the job result does not mean that the job is completely cleaned up on the cluster which can cause a {{DuplicateJobSubmissionException}}. I'm wondering whether this is actually a valid test case and whether we shouldn't submit the new job under a new {{JobID}}.;;;","22/Jul/20 06:04;trohrmann;Fixed via

master: 1eeb0a2b7562897f5d0c0ea3c4718f16d7284929
1.11.2: 7e5504f01d5b71c79bc686c27b3613d1b30b6fed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog source can't be insert into upsert sink,FLINK-18461,13314588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,jark,jark,02/Jul/20 03:36,24/Feb/21 03:21,13/Jul/23 08:12,03/Jul/20 13:15,,,,,,1.11.1,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"{code:sql}
CREATE TABLE t_pick_order (
      order_no VARCHAR,
      status INT
) WITH (
      'connector' = 'kafka',
      'topic' = 'example',
      'scan.startup.mode' = 'latest-offset',
      'properties.bootstrap.servers' = '172.19.78.32:9092',
      'format' = 'canal-json'
);

CREATE TABLE order_status (
          order_no VARCHAR,
          status INT,
		  PRIMARY KEY (order_no) NOT ENFORCED
) WITH (
          'connector' = 'jdbc',
          'url' = 'jdbc:mysql://xxx:3306/flink_test',
          'table-name' = 'order_status',
          'username' = 'dev',
          'password' = 'xxxx'
);

INSERT INTO order_status SELECT order_no, status FROM t_pick_order ;
{code}

The above queries throw the following exception:

{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Provided trait [BEFORE_AND_AFTER] can't satisfy required trait [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. 
Current node is TableSourceScan(table=[[default_catalog, default_database, t_pick_order]], fields=[order_no, status])
{code}

It is a bug in planner that we didn't fallback to {{BEFORE_AND_AFTER}} trait when {{ONLY_UPDATE_AFTER}} can't be satisfied. This results in Changelog source can't be used to written into upsert sink. ",,hackergin,init,jark,kezhuw,leonard,libenchao,liyu,lsy,trohrmann,twalthr,txhsj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 03 13:15:32 UTC 2020,,,,,,,,,,"0|z0ge00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/20 13:15;jark;- master (1.12.0): e08994588709b30e5d59d3c8302893a53c8a6baf
- 1.11.1: 334f35cbd6da754d8b5b294032cd84c858b1f973;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException in ProcessFunction property object's callback and promise,FLINK-18459,13314566,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jeromexlee,jeromexlee,01/Jul/20 23:34,08/Oct/20 20:26,13/Jul/23 08:12,08/Oct/20 20:26,1.10.0,,,,,,,,Build System,,,,,0,,,,,,"I am developing a Flink application and it will use Cassandra Driver to interact with Cassandra DB. The Cassandra Driver is implemented in Singleton fashion and multiple Flink process functions will interact with it to get data from Cassandra. I also add a future callback to each {{Session.executeAsync}}'s {{ResultSetFuture}}. The app is run on Kubernetes through Docker containers.
 
All dependencies are packaged in a single {{jar}} using {{Bazel}}. Before starting the Flink app, I check all the required classes are in the {{jar}} and are correct and complete. And I use the shaded dependency in order to avoid class loading conflict in JVM. But, when I start and run the Flink app. I keep seeing the following ClassNotFoundException in the Taskmanager logs.

I also notice that these issues are easier to reproduce when giving more resources and parallelism to the Flink app and the process functions. And the issues are most likely happen in the future callback or Promise.
 java.lang.NoClassDefFoundError: com/datastax/driver/core/SessionManager$State
    at com.datastax.driver.core.SessionManager.getState(SessionManager.java:211)
    at io.uhana.cassandra.CassandraDriver.sessionNeedsReconnect(CassandraDriver.java:508)
    at io.uhana.cassandra.CassandraDriver.access$000(CassandraDriver.java:61)
    at io.uhana.cassandra.CassandraDriver$1.onFailure(CassandraDriver.java:518)
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1387)
    at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1015)
    at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:868)
    at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:713)
    at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:230)
    at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:235)
    at com.datastax.driver.core.RequestHandler.access$2600(RequestHandler.java:61)
    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:1011)
    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:647)
    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1262)
    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1180)
    at com.datastax.shaded.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
    at com.datastax.shaded.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
    at com.datastax.shaded.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
    at com.datastax.shaded.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:312)
    at com.datastax.shaded.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:286)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
    at com.datastax.shaded.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
    at com.datastax.driver.core.InboundTrafficMeter.channelRead(InboundTrafficMeter.java:38)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:335)
    at com.datastax.shaded.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1304)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:356)
    at com.datastax.shaded.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
    at com.datastax.shaded.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:921)
    at com.datastax.shaded.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:135)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:646)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:546)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:500)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
    at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
    at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.ClassNotFoundException: com.datastax.driver.core.SessionManager$State
    at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)
    at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:69)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
    ... 49 more
and
ConstantReconnectionPolicy$ConstantSchedule' [enable DEBUG level for full stacktrace] was thrown by a user handler's exceptionCaught() method while handling the following exception:
java.lang.NoClassDefFoundError: com/datastax/shaded/netty/handler/timeout/IdleState
    at com.datastax.shaded.netty.handler.timeout.IdleStateHandler$ReaderIdleTimeoutTask.run(IdleStateHandler.java:493)
    at com.datastax.shaded.netty.handler.timeout.IdleStateHandler$AbstractIdleTask.run(IdleStateHandler.java:466)
    at com.datastax.shaded.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)
    at com.datastax.shaded.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:120)
    at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399)
    at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:464)
    at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
    at com.datastax.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.ClassNotFoundException: com.datastax.shaded.netty.handler.timeout.IdleState
    at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)
    at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:69)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
    ... 9 more {{}}","The environment is:
Flink version is 1.10.0 and using shaded {{netty}}, {{hadoop}}, {{guava}} and {{jackson}}.
Using cassandra-driver-mapping: 3.9.0 and shaded cassandra-driver-core: 3.9.0.",jeromexlee,kezhuw,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 12:55:01 UTC 2020,,,,,,,,,,"0|z0gdv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jul/20 12:55;trohrmann;Hi [~jeromexlee], can you check whether the class {{com.datastax.shaded.netty.handler.timeout.IdleState}} is contained on the classpath? If it is, then a minimal example reproducing the problem would be helpful for further debugging.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompressionFactoryITCase.testWriteCompressedFile fails with ""expected:<1> but was:<2>""",FLINK-18456,13314387,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,dian.fu,dian.fu,30/Jun/20 23:43,25/Aug/21 11:32,13/Jul/23 08:12,25/Aug/21 11:32,1.12.0,1.14.0,,,,1.14.0,,,Connectors / FileSystem,Tests,,,,0,test-stability,,,,,"Instance on the master: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4123&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf]

{code:java}
2020-06-30T07:59:56.3844746Z [INFO] Running org.apache.flink.formats.compress.CompressionFactoryITCase
2020-06-30T08:00:00.3083101Z [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.921 s <<< FAILURE! - in org.apache.flink.formats.compress.CompressionFactoryITCase
2020-06-30T08:00:00.3084778Z [ERROR] testWriteCompressedFile(org.apache.flink.formats.compress.CompressionFactoryITCase)  Time elapsed: 1.191 s  <<< FAILURE!
2020-06-30T08:00:00.3085932Z java.lang.AssertionError: expected:<1> but was:<2>
2020-06-30T08:00:00.3086694Z 	at org.junit.Assert.fail(Assert.java:88)
2020-06-30T08:00:00.3087435Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-06-30T08:00:00.3088250Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-06-30T08:00:00.3089022Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2020-06-30T08:00:00.3090188Z 	at org.apache.flink.formats.compress.CompressionFactoryITCase.validateResults(CompressionFactoryITCase.java:106)
2020-06-30T08:00:00.3091575Z 	at org.apache.flink.formats.compress.CompressionFactoryITCase.testWriteCompressedFile(CompressionFactoryITCase.java:90)
2020-06-30T08:00:00.3092751Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-30T08:00:00.3093687Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-30T08:00:00.3094801Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-30T08:00:00.3095784Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-30T08:00:00.3096737Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-30T08:00:00.3097863Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-30T08:00:00.3099212Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-30T08:00:00.3100380Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-30T08:00:00.3101557Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-06-30T08:00:00.3102957Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-06-30T08:00:00.3104026Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-30T08:00:00.3104859Z 	at java.lang.Thread.run(Thread.java:748)
{code}",,dian.fu,gaoyunhaii,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22710,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 11:32:51 UTC 2021,,,,,,,,,,"0|z0gcrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/20 05:30;dian.fu;Closing this ticket for now as it has not occurred for almost half a year.;;;","11/Aug/21 01:41;xtsong;new instance
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21838&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=25245;;;","25/Aug/21 11:32;gaoyunhaii;Fixed on master via 5390e91bd47219adde15d5d515a4f5baf4231fc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building with JDK 9+ leads to problems on JDK 8,FLINK-18455,13314247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nielsbasjes,nielsbasjes,nielsbasjes,30/Jun/20 09:06,03/Jul/20 08:58,13/Jul/23 08:12,03/Jul/20 08:58,1.10.1,,,,,1.12.0,,,Build System,,,,,0,pull-request-available,,,,,"I was working on some changes in Flink and on my workstation I have also JDK 14 installed.

When Flink is built using JDK > 8 and then run using JDK 8 the problem surfaces that the code crashes with the exception like this:
{code:java}
java.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer;
	at org.apache.flink.core.memory.DataOutputSerializer.wrapAsByteBuffer(DataOutputSerializer.java:65) ~[classes/:?]
	at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.<init>(SpanningRecordSerializer.java:50) ~[classes/:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.<init>(RecordWriter.java:98) ~[classes/:?]
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.<init>(ChannelSelectorRecordWriter.java:50) ~[classes/:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriterBuilder.build(RecordWriterBuilder.java:53) ~[classes/:?]

{code}
This is a problem in the way JDK 9+ generates the code that is incompatible with using the JDK 8 runtime, even if during the build it was indicated that JRE 8 would be the target.

I have found several projects have ran into the exact same problem:
 - https://issues.apache.org/jira/browse/MRESOLVER-67
 - [https://github.com/eclipse/jetty.project/issues/3244]
 - [https://github.com/netty/netty/issues/9880]
 - [https://github.com/apache/felix/pull/114]
 - [https://stackoverflow.com/questions/61267495/exception-in-thread-main-java-lang-nosuchmethoderror-java-nio-bytebuffer-flip] 

-As indicated in the mentioned Jetty ticket the solution is quite simple:-
{quote}-The solution is to cast the {{ByteBuffer}} to {{Buffer}} when calling those methods:-
{code:java}
((Buffer)byteBuffer).position(0);{code}
 -- 
{quote}
 

 

 

 ",,nielsbasjes,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 12:56:24 UTC 2020,,,,,,,,,,"0|z0gbw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/20 10:01;nielsbasjes;This seems like the better solution:

[https://github.com/eclipse/jetty.project/issues/3244#issuecomment-495322586]

Essentially when building with JDK 9+ specify ""release 8"" should fix this issue.;;;","30/Jun/20 10:56;nielsbasjes;Looking at the actual Flink code I found that 
 
In a JDK 8 build both source and target are set to Java 8
[https://github.com/apache/flink/blob/d735d8cd8e5d9fae5322001099097581822453ae/pom.xml#L109]
     <java.version>1.8</java.version>
 
[https://github.com/apache/flink/blob/d735d8cd8e5d9fae5322001099097581822453ae/pom.xml#L115]
    <maven.compiler.source>${java.version}</maven.compiler.source>    <maven.compiler.target>${java.version}</maven.compiler.target>

 
 
In a JDK 11 build a profile is activated that overrides it to Java 11
[https://github.com/apache/flink/blob/d735d8cd8e5d9fae5322001099097581822453ae/pom.xml#L938]
  <profile>
     <id>java11</id>
     <activation>
         <jdk>11</jdk>
[https://github.com/apache/flink/blob/d735d8cd8e5d9fae5322001099097581822453ae/pom.xml#L1004]
      <artifactId>maven-compiler-plugin</artifactId>
      <configuration>
          <source>11</source>
          <target>11</target>
 
However I have Java 14 (and the 'java11' profile is only activated at the EXACT version of java 11)  so it stays at source and target 1.8 but does not specify the ""release 8"" setting. ... which causes the problems I see.;;;","01/Jul/20 11:36;nielsbasjes;Debugging the profile activation turns out to be needlessly hard.
The input used by maven to determine the activation ""<jdk>"" is the *system property* {{java.version}}.
This is however hidden because a *property* {{java.version}} is created.

Put this in https://issues.apache.org/jira/browse/FLINK-18458;;;","01/Jul/20 20:13;chesnay;Java 11 profile is now also used for later versions:
master: ba92b3b8b02e099c8aab4b2b23a37dca4558cabd;;;","02/Jul/20 12:56;trohrmann;Can this ticket be closed [~chesnay]?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack overflow of AggregateITCase#testAggregationCodeSplit  on Azure,FLINK-18453,13314201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,kevin.cyj,kevin.cyj,30/Jun/20 04:14,10/Jul/20 02:26,13/Jul/23 08:12,10/Jul/20 02:26,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,test-stability,,,,"Here is some log:

```
[ERROR] testAggregationCodeSplit[LocalGlobal=OFF, MiniBatch=OFF, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase) Time elapsed: 8.167 s <<< ERROR!
Caused by: java.lang.StackOverflowError 
 at org.codehaus.janino.CodeContext.extract16BitValue(CodeContext.java:720) 
 at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:561) 
 at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:557)
``` 
The whole log: [https://dev.azure.com/kevin-flink/flink/_build/results?buildId=30&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=5129dea2-618b-5c74-1b8f-9ec63a37a8a6]",,aljoscha,jark,kevin.cyj,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 02:25:56 UTC 2020,,,,,,,,,,"0|z0gbm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/20 10:42;aljoscha;Another instance: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=255&view=logs&j=a1590513-d0ea-59c3-3c7b-aad756c48f25&t=5129dea2-618b-5c74-1b8f-9ec63a37a8a6&l=14637;;;","09/Jul/20 12:27;jark;It seems that the generated code is too large. What about to reduce the number of aggregates and use a smaller {{TableConfig#setMaxGeneratedCodeLength(??)}} (but not {{1}} length)? [~libenchao] ;;;","09/Jul/20 12:52;libenchao;[~jark] It's reasonable. I also don't like current design for the time it costs.;;;","10/Jul/20 02:25;libenchao;Fixed via e162bd7e9064032c0fd5f2035e1a23249f3ca5c2 (1.12.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix StateMigrationException because RetractableTopNFunction#ComparatorWrapper might be incompatible,FLINK-18452,13314196,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,kyledong,kyledong,30/Jun/20 03:40,19/Nov/21 15:12,13/Jul/23 08:12,23/Nov/20 03:29,1.10.0,1.10.1,1.11.0,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"We found that in SQL jobs using ""Top-N"" functionality provided by the blink planner, the job state cannot be retrieved because of ""incompatible"" state serializers (in fact they are compatible).

The error log is displayed like below
{panel:title=taskmanager.log}
2020-06-30 09:19:32.089 [Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[appkey, serverid], orderBy=[quantity DESC], select=[appkey, serverid,  quantity]) (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task  - Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[appkey, serverid], orderBy=[quantity DESC], select=[appkey, serverid, oid, quantity]) (1/1) (bd4d2e4327efac57dc70e220b8de460b) switched from RUNNING to FAILED.
java.lang.RuntimeException: Error while getting state
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:62)
        at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:144)
        at org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.open(RetractableTopNFunction.java:115)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.KeyedProcessOperator.open(KeyedProcessOperator.java:57)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:990)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.StateMigrationException: The new state serializer cannot be incompatible.
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:543)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:491)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:652)
        at org.apache.flink.runtime.state.KeyedStateFactory.createInternalState(KeyedStateFactory.java:47)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory.createStateAndWrapWithTtlIfEnabled(TtlStateFactory.java:72)
        at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getOrCreateKeyedState(AbstractKeyedStateBackend.java:279)
        at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getPartitionedState(AbstractKeyedStateBackend.java:328)
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getPartitionedState(DefaultKeyedStateStore.java:124)
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:60)
        ... 13 more{panel}
 
After careful debugging, it is found to be an issue with the compatibility check of type serializers.
 
In short, during checkpointing, Flink serializes _SortedMapSerializer_ by creating a _SortedMapSerializerSnapshot_ object, and the original comparator is encapsulated within the object (here we call it _StreamExecSortComparator$579_).
 
At restoration, the object is read and restored as normal. However, during the construction of RetractableTopNFunction instance, another Comparator is provided by Flink as an argument (we call it _StreamExecSortComparator$626_), and it is later used in the _ValueStateDescriptor_ which acts like a key to the state store.
 
Here comes the problem: when the newly-restored Flink program tries to access state (_getState_) through the previously mentioned _ValueStateDescriptor_, the State Backend firstly detects whether the provided comparator in state descriptor is compatible with the one in snapshot, eventually the logic goes to the _equals_ method at _RetractableTopNFunction.ComparatorWrapper_ class.
 
In the equals method, here is a code snippet:
{code:java}
return generatedRecordComparator.getClassName().equals(oGeneratedComparator.getClassName()) &&
      generatedRecordComparator.getCode().equals(oGeneratedComparator.getCode()) &&
      Arrays.equals(generatedRecordComparator.getReferences(), oGeneratedComparator.getReferences());
{code}
After debugging, we found that the class name of comparator within snapshot is _StreamExecSortComparator$579_, and the class name of comparator provided in the new job is _StreamExecSortComparator$626_, hence this method always returns false, even though actually they are indeed compatible (acts the same). Also, because the code in each generator is generated independently, the corresponding varaibles within the two comparators are highly likely to be different (_isNullA$581_ vs _isNullA$682_).
 
Hence we believe that the implementation of equals method has serious flaws, and should be addressed in later releases.",,jark,kezhuw,kyledong,leonard,libenchao,lipeidian,lsy,shenlang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/20 03:39;kyledong;c2ebeac8aadebad0dffa5cc255d45190594c5b2a84bda020dd30bf24b9169702.png;https://issues.apache.org/jira/secure/attachment/13006706/c2ebeac8aadebad0dffa5cc255d45190594c5b2a84bda020dd30bf24b9169702.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 23 03:29:53 UTC 2020,,,,,,,,,,"0|z0gbl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/20 03:45;kyledong;Hi [~qingru.zhang], would you please be so kind to look at this issue, as you are the original author for the code. Thank you very much.;;;","30/Jun/20 06:05;jark;I think this is an issue in {{ComparatorWrapper#equals}}, we shouldn't compare the generated class name and code, because they are not consistent even in the same Flink version. We should compare the sort key fields (the meata information used to generate the comparator). ;;;","30/Jun/20 06:31;kyledong;Thank you [~jark] for the reply, and I agree that the meta info given to _org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator#gen_ is more suitable for the comparison (maybe there are other classes that have the same problem).

I am quite interested in solving this issue, could you please assign this ticket to me? Thank you very much : );;;","30/Jun/20 07:30;kyledong;One important thing to note here is backward compatibility. For example, for _GeneratedRecordComparator_ instances created in current or earlier Flink versions, AFAIK they do not have the proper meta info needed to compare with the new one with the meta info.

In order to maintain compatibility, a hacky approach is to extract relevant fields from the generated code text, however, this is error-prone and could possibly only be used as a fallback approach if no other methods are available.

Or maybe we could add a new class (like _GeneratedRecordComparatorV2_) with the required meta info, and ""migrate"" the current implementation to the new one if needed. In this way, we could enumerate all of the comparator code generation implementations in existing Flink versions, and provide a robust migration plan.;;;","01/Jul/20 02:12;jark;Hi [~kyledong], currently, SQL/Table doesn't provide state compatible across major versions. So I think we don't need to care about it. 

But I'm not sure whether it is worth to fix in minor versions. If we need to fix it in minor versions, we may need a complex solution. ;;;","13/Jul/20 09:19;kyledong;Yes, I agree that we can fix this bug in next major release, i.e. 1.12.0, as currently there are no other user reports on this problem, it seems not to be that urgent to change existing class structure.;;;","17/Nov/20 02:48;jark;I will take this issue. ;;;","23/Nov/20 03:29;jark;Fixed in master (1.12.0): 49029a0a690e0810d238c76446c05e2ec191b91a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The operator name select: (ip, ts, count, environment.access AS access, environment.brand AS brand, sid, params.adid AS adid, eventid) exceeded the 80 characters length limit and was truncated",FLINK-18443,13313894,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mzz_q,mzz_q,29/Jun/20 01:54,21/Jul/20 02:08,13/Jul/23 08:12,21/Jul/20 02:08,1.9.0,,,,,1.10.0,,,API / DataStream,,,,,0,,,,,,"*Schema:*

{code:java}
.withSchema(new Schema()
 .field(""ip"", Types.STRING())
 .field(""ts"", Types.STRING())
 .field(""procTime"", Types.SQL_TIMESTAMP).proctime()
 .field(""environment"", schemaEnvironment)
 .field(""advs"", ObjectArrayTypeInfo.getInfoFor(new Array[Row](0).getClass, 
  Types.ROW(Array(""count"",""sid"", ""eventid"",""params""), 
  Array[TypeInformation[_]](Types.STRING(),Types.STRING(), 
  Types.STRING(),Types.ROW(Array(""adid"",""adtype"",""ecpm""),Array[TypeInformation[_]] (Types.STRING(),Types.STRING(),Types.STRING()))))))
)
{code}

*when execute this sql*:

{code:java}
val sql =
      """"""
        |SELECT
        |ip,
        |ts,
        |params.ad,
        |params.adtype,
        |eventid,
        |procTime
        |FROM aggs_test
        |CROSS JOIN UNNEST(advs) AS t (`count`,sid, eventid,params)
        |"""""".stripMargin
{code}

*I got a warning，and the console keeps brushing this warning,no normal printout*

{code:java}
09:38:38,694 WARN  org.apache.flink.metrics.MetricGroup                          - The operator name correlate: table(explode($cor0.advs)), select: ip, ts, procTime, advs, sid, eventid, params exceeded the 80 characters length limit and was truncated.

{code}

*But after I change it to this way, although I occasionally brush this Warn, it can be output normally。I change the 'params' type from Types.ROW to Types.STRING*。

{code:java}
 .field(""advs"", ObjectArrayTypeInfo.getInfoFor(new Array[Row](0).getClass, Types.ROW(Array(""count"", ""sid"", ""eventid"", ""params""),
          Array[TypeInformation[_]](Types.STRING(), Types.STRING(), Types.STRING(), Types.STRING()))))
{code}









",,dian.fu,godfreyhe,jark,libenchao,mzz_q,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 03:23:55 UTC 2020,,,,,,,,,,"0|z0g9ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/20 01:59;mzz_q;I tried to read the source code，But it didn't help me。Because of the parameter METRICS_OPERATOR_NAME_MAX_LENGTH  is final。

{code:java}
static final int METRICS_OPERATOR_NAME_MAX_LENGTH = 80;
public OperatorMetricGroup getOrAddOperator(OperatorID operatorID, String name) {
		if (name != null && name.length() > METRICS_OPERATOR_NAME_MAX_LENGTH) {
			LOG.warn(""The operator name {} exceeded the {} characters length limit and was truncated."", name, METRICS_OPERATOR_NAME_MAX_LENGTH);
			name = name.substring(0, METRICS_OPERATOR_NAME_MAX_LENGTH);
		}
		OperatorMetricGroup operator = new OperatorMetricGroup(this.registry, this, operatorID, name);
		// unique OperatorIDs only exist in streaming, so we have to rely on the name for batch operators
		final String key = operatorID + name;

		synchronized (this) {
			OperatorMetricGroup previous = operators.put(key, operator);
			if (previous == null) {
				// no operator group so far
				return operator;
			} else {
				// already had an operator group. restore that one.
				operators.put(key, previous);
				return previous;
			}
		}
	}

{code}

;;;","08/Jul/20 03:23;mzz_q;I replaced Types with DataTypes, the problem has solved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ROW_NUMBER function: ROW/RANGE not allowed with RANK, DENSE_RANK or ROW_NUMBER functions",FLINK-18440,13313814,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,shenlang,shenlang,28/Jun/20 04:03,14/Jul/20 02:14,13/Jul/23 08:12,14/Jul/20 02:14,1.11.0,,,,,1.11.1,1.12.0,,Table SQL / Runtime,,,,30/Jul/20 00:00,0,pull-request-available,,,,,"When I run flink sql ,the flink sql like this:

create view test as select  name, eat ,sum(age) as cnt from test_source group by  name,eat;

create view resultsssss as select *, ROW_NUMBER() OVER (PARTITION BY name ORDER BY cnt DESC) as row_num from test;

create table sink (
name varchar,
eat varchar,
cnt bigint
)
with(
'connector' = 'print'
);

insert into sink select name,eat , cnt from resultsssss where  row_num <= 3 ;



The same sql code I could run success in flink 1.10, now I change the flink version into flink 1.11, it throw the exception.

Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 124 to line 1, column 127: ROW/RANGE not allowed with RANK, DENSE_RANK or ROW_NUMBER functions
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$ToRelContextImpl.expandView(FlinkPlannerImpl.scala:204)
	at org.apache.calcite.plan.ViewExpanders$1.expandView(ViewExpanders.java:52)
	at org.apache.flink.table.planner.catalog.SqlCatalogViewTable.convertToRel(SqlCatalogViewTable.java:58)
	at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.expand(ExpandingPreparingTable.java:59)
	at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.toRel(ExpandingPreparingTable.java:55)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)


",,danny0405,felixzheng,jark,JinxinTang,libenchao,lsy,shenlang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/20 10:55;JinxinTang;image-2020-06-28-18-55-43-692.png;https://issues.apache.org/jira/secure/attachment/13006619/image-2020-06-28-18-55-43-692.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 02:14:29 UTC 2020,,,,,,,,,,"0|z0g980:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/20 05:32;lsy;[~shenlang]，I also encountered this problem when create view use row_number function, I have study calcite source code, it 'SqlWindow#validate' method throw this error, so I guess calcite don't support create view use row_ number over partition by order by  desc grammar, because there's no explicit lower bound and upper bound. if you use following sql:

 
{code:java}
create view order_source as select sum(amount) over (partition by order_id, order_goods_id order by proctime ROWS BETWEEN unbounded PRECEDING AND CURRENT ROW) from dm_trd_order_goods
{code}
 it will not throw exception, I think this is calcite's problem.

CC [~jark], what do you think about it?;;;","28/Jun/20 05:41;shenlang;Hi [~lsy],
I find that in the end my sql would translate into like this :
SELECT `test`.`name`, `test`.`eat`, `test`.`cnt`, ROW_NUMBER() OVER (PARTITION BY `test`.`name` ORDER BY `test`.`cnt` DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS `row_num`
FROM `default_catalog`.`default_database`.`test` AS `test`.

And at the same time , I add the ROWS BETWEEN unbounded PRECEDING AND CURRENT ROW into my sql , It aslo throw the exception like this :
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: ROW/RANGE not allowed with RANK, DENSE_RANK or ROW_NUMBER functions
;;;","28/Jun/20 06:01;jark;I reproduced this problem with adding the following test in {{org.apache.flink.table.planner.plan.stream.sql.TableScanTest}}


{code:scala}
 @Test
  def test(): Unit = {
    util.addTable(
      """"""
        |CREATE TABLE test_source (
        |  name STRING,
        |  eat STRING,
        |  age BIGINT
        |) WITH (
        |  'connector' = 'values'
        |)
      """""".stripMargin)
    util.tableEnv.executeSql(""create view test as select name, eat ,sum(age) as cnt from test_source group by name,eat"")
    util.tableEnv.executeSql(""create view resultsssss as select *, ROW_NUMBER() OVER (PARTITION BY name ORDER BY cnt DESC) as row_num from test"")
    util.addTable(
      s""""""
         |create table sink (
         |name varchar,
         |eat varchar,
         |cnt bigint
         |)
         |with(
         |'connector' = 'print'
         |)
         |"""""".stripMargin
    )
    util.verifyPlanInsert(""insert into sink select name,eat , cnt from resultsssss where row_num <= 3"")
  }
{code}

I guess this might be a problem related to Calcite. cc [~danny0405], could you help to have a look at this?
;;;","28/Jun/20 10:55;JinxinTang;!image-2020-06-28-18-55-43-692.png!

Hope this could help :);;;","03/Jul/20 10:20;JinxinTang;cc [~jark] Thank you for your test code and suggestion, I have just open a PR for this, could you please help me review :);;;","10/Jul/20 10:52;danny0405;After some code research, i found that the SqlWindow bounds state was mutated during sql-to-rel conversion in Calcite release-1.22, this has been fixed in https://issues.apache.org/jira/browse/CALCITE-3877.

See this logic in SqlValidatorImpl#resolveWindow of 1.22 Calcite:

{code:java}
    if (populateBounds) {
      window.populateBounds();
    }
{code}
The flag from SqlToRelConverter is always true.

I can figure out 2 ways to fix this problem:
1. We can resolve this problem when upgrade to Calcite 1.23 or 1.24.
2. In SqlToOperationConverter#convertCreateView, keep the validated sql string before it is converted.

I would fire a fix for 2 soon ~;;;","10/Jul/20 11:07;danny0405;I have fired a fix in https://github.com/apache/flink/pull/12868.;;;","14/Jul/20 02:14;jark;Fixed in 
- master (1.12.0): 9723cb2090cff1c68e878282600fcd06c4f34125
- 1.11.1: 677021cc1ca872dd81fed8cd5e5aea834a3b5236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update sql client jar url in docs,FLINK-18439,13313812,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,28/Jun/20 03:23,29/Jun/20 02:28,13/Jul/23 08:12,29/Jun/20 02:28,1.11.0,,,,,1.11.0,,,Documentation,,,,,0,pull-request-available,,,,,"the sql client jar url should be：

[https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch6] ...

but current is ：

[https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-elasticsearch6...]

 ",,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 02:28:22 UTC 2020,,,,,,,,,,"0|z0g97k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/20 02:28;jark;- master (1.12.0): f809d8a9374ef3a2be2be355bd1c78d81bc89d75
- 1.11.0: 4882cc6ddbe1353a7c167e00000662162da0b3fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not select fields with JdbcCatalog,FLINK-18434,13313439,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,25/Jun/20 12:49,10/Jul/20 07:31,13/Jul/23 08:12,10/Jul/20 07:31,1.11.0,,,,,1.11.1,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"A query which selects fields from a table will fail if we set the PostgresCatalog as default.

Steps to reproduce:
 # Create postgres catalog and set it as default
 # Create any table (in any catalog)
 # Query that table with {{SELECT field FROM t}} (Important it must be a field name not '{{*}}'
 #  The query will fail

Stack trace:
{code}
org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement.
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:100) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:91) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:257) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:211) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
Caused by: java.lang.UnsupportedOperationException
	at org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.getFunction(AbstractJdbcCatalog.java:261) ~[?:?]
	at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:570) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$2(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
	at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:370) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.makeNullaryCall(SqlValidatorImpl.java:1754) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5987) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6154) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6140) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectExpr(SqlValidatorImpl.java:5574) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:452) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4255) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
{code}

The problem is that Calcite will try to check first if there is a built-in function with that name that allows calls without parenthesis. Therefore it will query the {{FunctionCatalog}} for that function. The logic in {{org.apache.flink.table.catalog.FunctionCatalog#lookupFunction}} is such that it will call {{JdbcCatalog#getFunction}} in the end, which in case of {{AbstractJdbcCatalog}} throws {{UnsupportedOperationException}}.
",,dwysakowicz,felixzheng,jark,leonard,libenchao,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 07:31:21 UTC 2020,,,,,,,,,,"0|z0g6ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/20 15:11;jark;Thanks [~dwysakowicz]. I think this is a design flaw of {{Catalog}} interface. Maybe this is the time to have separate {{ReadOnlyCatalog}}. 

For temporary fix, I think the {{FunctionCatalog}} should call {{Catalog#functionExists(..)}} first, and {{JdbcCatalog#functionExists(..)}} should always return false. WDYT?;;;","26/Jun/20 07:10;dwysakowicz;I completely agree this shows one of the flaws of the Catalog interface. I don't think though having a {{ReadableOnlyCatalog}} would necessarily help in this particular case. In the end the {{Catalog#getFunction}} is a read operation. Personally what I don't like about the Catalog interface is that there is so many different responsibilities boundled into one interface  (tables/views/functions/partitions/statistics/...). Honestly though I really don't know how to proceed with it, and if we can do sth about it. It is a new API that was introduced only recently.

As for a fix for that particular issue the easiest thing we can do is to throw {{FunctionNotExistException}} instead of {{UnsupportedOperationException}}. Which imo is another design flaw as it is controlling the application flaw with exceptions. I'd rather like to see this method return an Optional instead. Simply switching to {{Catalog#functionExists}} would not help, as this method also throws {{UnsupportedOperationException}} in {{JdbcCatalog}}.;;;","10/Jul/20 07:31;dwysakowicz;Fixed in:
* master
** f2b66081ad87142137e57377d68a0c0e5edf6285
* 1.11.1
** 4c9a2d405d98e96e5bf5387f71616c58deaf9751;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add default method for CheckpointListener.notifyCheckpointAborted(checkpointId),FLINK-18429,13313279,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,24/Jun/20 15:35,28/Jun/20 09:36,13/Jul/23 08:12,24/Jun/20 18:47,,,,,,1.11.0,,,API / DataStream,,,,,0,pull-request-available,,,,,"The {{CheckpointListener}} interface is implemented by many users. Adding a new method {{notifyCheckpointAborted(long)}} to the interface without a default method breaks many user programs.

We should turn this method into a default method:
  - Avoid breaking programs
  - It is in practice less relevant for programs to react to checkpoints being aborted then to being completed. The reason is that on completion you often want to commit side-effects, while on abortion you frequently do not do anything, but let the next successful checkpoint commit all changes up to then.

*Original Confusion*

There was confusion about this originally, going back to a comment by myself suggesting this should not be a default method, incorrectly thinking of it as an internal interface: https://github.com/apache/flink/pull/8693#issuecomment-542834147

See also clarification email on the mailing list:
{noformat}
About the ""notifyCheckpointAborted()"":

When I wrote that comment, I was (apparently wrongly) assuming we were talking about an
internal interface here, because the ""abort"" signal was originally only intended to cancel the
async part of state backend checkpoints.

I just realized that this is exposed to users - and I am actually with Thomas on this one. The
""CheckpointListener"" is a very public interface that many users implement. The fact that it is
tagged ""@PublicEvolving"" is somehow not aligned with reality. So adding the method here will
in reality break lots and lots of user programs.

I think also in practice it is much less relevant for user applications to react to aborted checkpoints.
Since the notifications there can not be relied upon (if there is a task failure concurrently) users
always have to follow the ""newer checkpoint subsumes older checkpoint"" contract, so the abort
method is probably rarely relevant.

This is something we should change, in my opinion.
{noformat}",,sewen,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18431,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 28 09:36:52 UTC 2020,,,,,,,,,,"0|z0g5xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 16:22;sewen;With apologies to [~yunta] for the confusion I caused here.;;;","24/Jun/20 18:47;sewen;Fixed in
  - 1.11.0 via a60a4a9c71a2648176ca46a25230920674130d01
  - 1.12.0 (master) via 4776813cc335080dbe8684f51c3aa0f7f1d774d0;;;","28/Jun/20 09:36;yunta;[~sewen] thanks for your kind notification and glad to see this reverted to not influence more users. If I could insist more on my original implementation for this interface, this hot fix could be avoided.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecutionEnvironment#continuousSource() method should be renamed to source(),FLINK-18428,13313240,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,24/Jun/20 12:32,24/Jun/20 18:49,13/Jul/23 08:12,24/Jun/20 18:49,,,,,,1.11.0,,,API / DataStream,,,,,0,pull-request-available,,,,,"The current code in master did not follow the [latest FLIP-27 discussion|[https://lists.apache.org/thread.html/r54287e9c9880916560bb97c38962db7b4d326056a7b23a9d0416e6a7%40%3Cdev.flink.apache.org%3E]], where we should have a \{{StreamExecutionEnvironment#source()}} instead of  \{{StreamExecutionEnvironment#continuousSource()}}. The concern was that the latter would mislead the users to think that the execution is always in streaming mode while it actually depends on the boundedness of the source.",,becket_qin,sewen,trohrmann,wanglijie,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 18:49:13 UTC 2020,,,,,,,,,,"0|z0g5oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 18:49;sewen;Fixed in
  - 1.11.0 via 369d4a6ce46dddfd2a14ad66ac2e189bd4827158
  - 1.12.0 (master) via 49b5103299374641662d66b5165441b532206b71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job failed under java 11,FLINK-18427,13313195,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,simahao,simahao,24/Jun/20 08:27,31/Oct/20 07:12,13/Jul/23 08:12,31/Oct/20 07:12,1.10.0,,,,,,,,Runtime / Configuration,Runtime / Network,,,,0,,,,,,"flink version:1.10.0

deployment mode:cluster

os:linux redhat7.5

Job parallelism:greater than 1

My job run normally under java 8, but failed under java 11.Excpetion info like below,netty send message failed.In addition, I found job would failed when task was distributed on multi node, if I set job's parallelism = 1, job run normally under java 11 too.

 

2020-06-24 09:52:162020-06-24 09:52:16org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: Sending the partition request to '/170.0.50.19:33320' failed. at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient$1.operationComplete(NettyPartitionRequestClient.java:124) at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient$1.operationComplete(NettyPartitionRequestClient.java:115) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:474) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111) at org.apache.flink.shaded.netty4.io.netty.util.internal.PromiseNotificationUtil.tryFailure(PromiseNotificationUtil.java:64) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.notifyOutboundHandlerException(AbstractChannelHandlerContext.java:818) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:718) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:708) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.access$1700(AbstractChannelHandlerContext.java:56) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1102) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1149) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1073) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:416) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:515) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.io.IOException: Error while serializing message: PartitionRequest(8059a0b47f7ba0ff814ea52427c584e7@6750c1170c861176ad3ceefe9b02f36e:0:2) at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageEncoder.write(NettyMessage.java:177) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:716) ... 11 moreCaused by: java.io.IOException: java.lang.OutOfMemoryError: Direct buffer memory at org.apache.flink.runtime.io.network.netty.NettyMessage$PartitionRequest.write(NettyMessage.java:497) at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageEncoder.write(NettyMessage.java:174) ... 12 moreCaused by: java.lang.OutOfMemoryError: Direct buffer memory at java.base/java.nio.Bits.reserveMemory(Bits.java:175) at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118) at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317) at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:772) at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:748) at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:245) at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:215) at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:147) at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:342) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178) at org.apache.flink.runtime.io.network.netty.NettyMessage.allocateBuffer(NettyMessage.java:148) at org.apache.flink.runtime.io.network.netty.NettyMessage.allocateBuffer(NettyMessage.java:111) at org.apache.flink.runtime.io.network.netty.NettyMessage.access$200(NettyMessage.java:59) at org.apache.flink.runtime.io.network.netty.NettyMessage$PartitionRequest.write(NettyMessage.java:482) ... 13 more",,Echo Lee,sewen,simahao,xtsong,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/20 05:49;simahao;image-2020-06-29-13-49-17-756.png;https://issues.apache.org/jira/secure/attachment/13006638/image-2020-06-29-13-49-17-756.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 31 07:07:26 UTC 2020,,,,,,,,,,"0|z0g5ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 15:42;sewen;Thanks for reporting this.

I think the reason is that under Java 11, Netty will allocate memory from the pool of Java Direct Memory and is affected by the MaxDirectMemory limit,
Under Java 8, it allocates native memory and is not affected by that setting.

For Flink 1.10.0 you probably need an increased values for ""Framework Offheap Memory"" in higher parallelism cases.
This documentation page has more details: https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_setup.html#configure-off-heap-memory-direct-or-native

Flink 1.11.0 should reduce the Netty memory usage significantly, by directly reading into Flink's managed memory on the receiver side. [~zjwang], [~pnowojski] would know more details about this. ;;;","25/Jun/20 15:05;zjwang;Fully agree with [~sewen]'s above analysis.

In addition, we also planned to back port this improvement https://issues.apache.org/jira/browse/FLINK-15962 to release-1.10 later, then it can also help to reduce the netty direct memory overhead somehow. ;;;","26/Jun/20 08:23;sewen;[~simahao] Could you confirm that increasing Framework Offheap Memory solves this?
;;;","27/Jun/20 08:05;simahao;Thanks for [~sewen]'s reply, because Chinese Dragon Boat Festival, I will try to adjust '[{{taskmanager.memory.task.off-heap.size}}|https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#taskmanager-memory-task-off-heap-size]' on 29th June , then I will tell you the result.;;;","29/Jun/20 02:00;simahao;[~sewen], according to your suggestion, I adjuested the off-heap setting, job run normally.Thank you.

In addition, I have two questions, .
 # What's the difference between 'taskmanager.memory.task.off-heap.size' and 'taskmanager.memory.framework.off-heap.size'? I found that either 'task' or 'framework' setting is ok for my job, which one should I use?
 # According to webUI->Task Manager->Metrics,there are some memory metircs information, I want to know, Outside JVM(Type=Direct) is task's off-heap?If so, how to understand Capacity field?If not, where could I monitor the off-heap usage rate?

!image-2020-06-29-13-49-17-756.png!;;;","20/Oct/20 06:37;zjwang;[~xintongsong] might be more familiar with the concerns of memory setting [~simahao] proposed above.;;;","20/Oct/20 08:27;xtsong;Thanks for puling me in, [~zjwang].
Sorry for the late response, [~simahao].

bq. 1. What's the difference between 'taskmanager.memory.task.off-heap.size' and 'taskmanager.memory.framework.off-heap.size'? I found that either 'task' or 'framework' setting is ok for my job, which one should I use?

For the current version, there's practically no differences between task/framework memory. They are prepared for future optimization, where we plan to allow dynamically slicing task managers' memory to slots (currently it is sliced into fixed number and size of slots). Then, task memory can be sliced for task execution, while framework memory will be reserved for task manager's framework.

Usually we recommend users to only set the task memory, because framework memory is quite stable and Flink already come up with good default values for them. However, in your case, the memory consumption comes from Netty which is definitely part of Flink's framework. That's why Stephan and Zhijiang suggested you to increase the framework memory.

Again, there's practically no differences between task/framework memory for the current version. However, tuning the right configuration would help reduce efforts when upgrading to future versions。

bq. 2. According to webUI->Task Manager->Metrics,there are some memory metircs information, I want to know, Outside JVM(Type=Direct) is task's off-heap?If so, how to understand Capacity field?If not, where could I monitor the off-heap usage rate?

Please ignore these metrics here. These's metrics are directly retrieved from MXBeans, and does not corresponds to Flink's memory configurations well. The community is already aware of how confusing and misleading these metrics could be, and is working on an improvement for this web ui page. Sorry for the inconvenience. ;;;","31/Oct/20 07:07;simahao;Thanks for [~xintongsong]'s reply and everyone's help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompatible deprecated key type for registration cluster options ,FLINK-18426,13313194,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,24/Jun/20 08:26,04/Aug/20 09:53,13/Jul/23 08:12,24/Jun/20 14:32,1.11.0,,,,,1.11.0,1.12.0,,Runtime / Configuration,Runtime / Coordination,,,,0,pull-request-available,,,,,"With FLINK-15827 we deprecated unused {{TaskManagerOptions}}. As part of this deprecation, we added them as deprecated keys for a couple of {{ClusterOptions}}. The problem is that the deprecated keys are of type {{Duration}} whereas the valid options are of type {{Long}}. Hence, the system will fail if a deprecated config option has been configured because it cannot be parsed as a long.

In order to solve the problem, I propose to remove the deprecated keys from the new {{ClusterOptions}}.",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 14:32:31 UTC 2020,,,,,,,,,,"0|z0g5eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 14:32;trohrmann;Fixed via

1.12.0: ca534016f90efd348bc9596671f067f99ff3ae19
1.11.1: 62c7265522fcf1b708b4906d5d74e40188a80f28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericArrayData cannot convert object arrays to primitive arrays,FLINK-18425,13313189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,24/Jun/20 08:09,24/Jun/20 13:10,13/Jul/23 08:12,24/Jun/20 13:10,,,,,,1.11.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,GenericArrayData.toBooleanArray throws a cast exception if it is backed by an object array.,,jark,libenchao,RocMarshal,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 13:10:42 UTC 2020,,,,,,,,,,"0|z0g5dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 13:10;twalthr;Fixed in 1.12.0: 7def95b6c006f0407731be8ec139b54454dc3f97
Fixed in 1.11.0: a05972f98d3b939857d0c0911f68d97186d1cada;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch (v6.3.1) sink end-to-end test instable,FLINK-18421,13313151,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,dian.fu,dian.fu,24/Jun/20 02:04,23/Jul/20 14:22,13/Jul/23 08:12,23/Jul/20 14:22,1.11.1,1.12.0,,,,1.11.2,1.12.0,,Connectors / ElasticSearch,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3983&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a

{code}
2020-06-23T22:28:10.5540446Z [FAIL] 'Elasticsearch (v6.3.1) sink end-to-end test' failed after 0 minutes and 36 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

exceptions in the log:
{code}
(1/1) (69721d998f4b68253d1e59f7f9065def) switched from DEPLOYING to RUNNING.
2020-06-23T22:28:10.5206189Z 2020-06-23 22:28:05,844 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (69721d998f4b68253d1e59f7f9065def) switched from RUNNING to FINISHED.
2020-06-23T22:28:10.5207672Z 2020-06-23 22:28:05,852 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Elasticsearch 6.x end to end sink test example (59aa77d1ad30f5bedd2759ecfe2bb870) switched from state RUNNING to FINISHED.
2020-06-23T22:28:10.5208679Z 2020-06-23 22:28:05,852 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 59aa77d1ad30f5bedd2759ecfe2bb870.
2020-06-23T22:28:10.5209478Z 2020-06-23 22:28:05,852 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2020-06-23T22:28:10.5210355Z 2020-06-23 22:28:05,861 WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Error encountered during shutdown
2020-06-23T22:28:10.5211371Z java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6a7696d6 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3c0b9ab1[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 1]
2020-06-23T22:28:10.5212310Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_252]
2020-06-23T22:28:10.5212864Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_252]
2020-06-23T22:28:10.5213404Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_252]
2020-06-23T22:28:10.5213939Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_252]
2020-06-23T22:28:10.5214480Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252]
2020-06-23T22:28:10.5339896Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) [?:1.8.0_252]
2020-06-23T22:28:10.5340629Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:594) [?:1.8.0_252]
2020-06-23T22:28:10.5341291Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_252]
2020-06-23T22:28:10.5341799Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_252]
2020-06-23T22:28:10.5342271Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_252]
2020-06-23T22:28:10.5342825Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_252]
2020-06-23T22:28:10.5343516Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_252]
2020-06-23T22:28:10.5344122Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_252]
2020-06-23T22:28:10.5344635Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_252]
2020-06-23T22:28:10.5345088Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
2020-06-23T22:28:10.5345861Z Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6a7696d6 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3c0b9ab1[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 1]
2020-06-23T22:28:10.5346852Z 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_252]
2020-06-23T22:28:10.5347483Z 	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_252]
2020-06-23T22:28:10.5348048Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_252]
2020-06-23T22:28:10.5348626Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_252]
2020-06-23T22:28:10.5349208Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_252]
2020-06-23T22:28:10.5349749Z 	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_252]
2020-06-23T22:28:10.5351000Z 	at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:62) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-06-23T22:28:10.5351695Z 	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_252]
2020-06-23T22:28:10.5352353Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_252]
2020-06-23T22:28:10.5352712Z 	... 12 more
{code}",,dian.fu,pnowojski,rmetzger,RocMarshal,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 14:22:05 UTC 2020,,,,,,,,,,"0|z0g554:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 02:13;dian.fu;Another instance on the master branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3983&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","07/Jul/20 02:02;dian.fu;Similar exception stack for ""Quickstarts Scala nightly end-to-end test"". It also throw the same exception stack during shutdown: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4276&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","21/Jul/20 12:31;dian.fu;Another instance for ""Quickstarts Scala nightly end-to-end test"":
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4677&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9;;;","22/Jul/20 14:58;rmetzger;I guess this is rather an issue of the CheckpointCoordinator ?
These tests are checking the log output for exceptions, and failing if they find some (""Found error in log files; printing first 500 lines; see full logs for details:"")

[~roman_khachatryan][~pnowojski] can you advise?;;;","22/Jul/20 16:09;roman;Yes [~rmetzger], you are right.

I created a PR to fix this.;;;","23/Jul/20 14:22;pnowojski;merged commit da210e0 into apache:master, d89baa37dd into release-1.11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLClientHBaseITCase.testHBase failed with ""ArgumentError: wrong number of arguments (0 for 1)""",FLINK-18420,13313143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,dian.fu,dian.fu,24/Jun/20 01:52,24/Jun/20 08:51,13/Jul/23 08:12,24/Jun/20 08:51,1.12.0,,,,,1.12.0,,,Connectors / HBase,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3983&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=6caf31d6-847a-526e-9624-468e053467d6

{code}
[INFO] Running org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-06-23T23:07:01.9393979Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 43.277 s <<< FAILURE! - in org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-06-23T23:07:01.9397602Z [ERROR] testHBase(org.apache.flink.tests.util.hbase.SQLClientHBaseITCase)  Time elapsed: 43.276 s  <<< ERROR!
2020-06-23T23:07:01.9398196Z java.io.IOException: 
2020-06-23T23:07:01.9399343Z Process execution failed due error. Error output:OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2020-06-23T23:07:01.9400131Z WARNING: An illegal reflective access operation has occurred
2020-06-23T23:07:01.9401440Z WARNING: Illegal reflective access by org.jruby.java.invokers.RubyToJavaInvoker (file:/tmp/junit3131433123777334326/hbase/lib/jruby-complete-1.6.8.jar) to method java.lang.Object.registerNatives()
2020-06-23T23:07:01.9402282Z WARNING: Please consider reporting this to the maintainers of org.jruby.java.invokers.RubyToJavaInvoker
2020-06-23T23:07:01.9403191Z WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
2020-06-23T23:07:01.9403798Z WARNING: All illegal access operations will be denied in a future release
2020-06-23T23:07:01.9404516Z ArgumentError: wrong number of arguments (0 for 1)
2020-06-23T23:07:01.9405477Z   method_added at file:/tmp/junit3131433123777334326/hbase/lib/jruby-complete-1.6.8.jar!/builtin/javasupport/core_ext/object.rb:10
2020-06-23T23:07:01.9406654Z   method_added at file:/tmp/junit3131433123777334326/hbase/lib/jruby-complete-1.6.8.jar!/builtin/javasupport/core_ext/object.rb:129
2020-06-23T23:07:01.9407831Z        Pattern at file:/tmp/junit3131433123777334326/hbase/lib/jruby-complete-1.6.8.jar!/builtin/java/java.util.regex.rb:2
2020-06-23T23:07:01.9408979Z         (root) at file:/tmp/junit3131433123777334326/hbase/lib/jruby-complete-1.6.8.jar!/builtin/java/java.util.regex.rb:1
2020-06-23T23:07:01.9409598Z        require at org/jruby/RubyKernel.java:1062
2020-06-23T23:07:01.9410469Z         (root) at file:/tmp/junit3131433123777334326/hbase/lib/jruby-complete-1.6.8.jar!/builtin/java/java.util.regex.rb:42
2020-06-23T23:07:01.9411122Z         (root) at /tmp/junit3131433123777334326/hbase/bin/../bin/hirb.rb:38
2020-06-23T23:07:01.9411481Z 
2020-06-23T23:07:01.9411996Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:127)
2020-06-23T23:07:01.9412745Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
2020-06-23T23:07:01.9413515Z 	at org.apache.flink.tests.util.hbase.LocalStandaloneHBaseResource.executeHBaseShell(LocalStandaloneHBaseResource.java:188)
2020-06-23T23:07:01.9414502Z 	at org.apache.flink.tests.util.hbase.LocalStandaloneHBaseResource.executeHBaseShell(LocalStandaloneHBaseResource.java:179)
2020-06-23T23:07:01.9415198Z 	at org.apache.flink.tests.util.hbase.LocalStandaloneHBaseResource.createTable(LocalStandaloneHBaseResource.java:158)
2020-06-23T23:07:01.9415865Z 	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.testHBase(SQLClientHBaseITCase.java:117)
2020-06-23T23:07:01.9416428Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-23T23:07:01.9416990Z 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-23T23:07:01.9417635Z 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-23T23:07:01.9419058Z 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2020-06-23T23:07:01.9420497Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-23T23:07:01.9421198Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-23T23:07:01.9421729Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-23T23:07:01.9422220Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-23T23:07:01.9422716Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-06-23T23:07:01.9423270Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-23T23:07:01.9423743Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-23T23:07:01.9424409Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-23T23:07:01.9424797Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-23T23:07:01.9425165Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-23T23:07:01.9425525Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-23T23:07:01.9426581Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-23T23:07:01.9427164Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-23T23:07:01.9427599Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-23T23:07:01.9427990Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-23T23:07:01.9428625Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-23T23:07:01.9429051Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-23T23:07:01.9429435Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-23T23:07:01.9429860Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-23T23:07:01.9430242Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-23T23:07:01.9430606Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-23T23:07:01.9430969Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-06-23T23:07:01.9431423Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-06-23T23:07:01.9431797Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-23T23:07:01.9432200Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-23T23:07:01.9432842Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-23T23:07:01.9433256Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-23T23:07:01.9457247Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-23T23:07:01.9457625Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-23T23:07:01.9457986Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-06-23T23:07:01.9458436Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-06-23T23:07:01.9458926Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-06-23T23:07:01.9459396Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-06-23T23:07:01.9459853Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-06-23T23:07:01.9460299Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-06-23T23:07:01.9460785Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-23T23:07:01.9461411Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-23T23:07:01.9461898Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-23T23:07:01.9462483Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dian.fu,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 08:49:16 UTC 2020,,,,,,,,,,"0|z0g53c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/20 02:36;leonard;Thanks for the report.

The test failed in JDK11 profile, the reason is we only support HBase 1.4.3 which is not supported JDK11
 I'd like to fix this, [~dian.fu] could help assign the ticket to me ?

[1] [https://hbase.apache.org/book.html#basic.prerequisites]
 [2] https://issues.apache.org/jira/browse/HBASE-22972;;;","24/Jun/20 02:39;dian.fu;[~Leonard Xu] Thanks for taking this issue. Have assigned this issue to you.;;;","24/Jun/20 08:49;jark;Fixed in master (1.12.0): b6b54810c5c00732229f175a79b5d94e46750244;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not create a catalog from user jar,FLINK-18419,13313068,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,23/Jun/20 14:36,10/Jul/20 07:30,13/Jul/23 08:12,10/Jul/20 07:30,1.11.0,,,,,1.11.1,1.12.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The {{CREATE CATALOG}} statement does not work if the catalog implementation comes from the user classloader. The problem is that {{org.apache.flink.table.planner.operations.SqlToOperationConverter#convertCreateCatalog}} uses the {{SqlToOperationConverter}} classloader.

We should use {{Thread.currentThread().getContextClassloader()}} for now.

One of the ways to reproduce it is try to create e.g. a postgres catalog with the {{flink-connector-jdbc}} passed as an additional jar to {{sql--client}}",,dwysakowicz,godfreyhe,jark,kezhuw,leonard,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 07:30:30 UTC 2020,,,,,,,,,,"0|z0g4mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/20 07:30;dwysakowicz;Fixed in:
* master
** 8158421c988247220b25af2b06b3488d90d3eb3f..69ef4b7cffb446cca655f7c426a49d9fbb3a8cc3
* 1.11.1
** fba633f2a8c5477c537991c88b412f79e8d54f33..1aa92e897e7d71a1063b87ee08496edac6e4b633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcFullTest failed to compile on JDK11,FLINK-18412,13312940,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,dian.fu,dian.fu,23/Jun/20 02:21,23/Jun/20 02:57,13/Jul/23 08:12,23/Jun/20 02:57,1.11.0,1.12.0,,,,1.11.0,,,Connectors / JDBC,Tests,,,,0,test-stability,,,,,"master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3928&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=946871de-358d-5815-3994-8175615bc253
release-1.11: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=946871de-358d-5815-3994-8175615bc253
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6

{code}
2020-06-22T20:19:50.2157534Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2158031Z [ERROR] COMPILATION ERROR : 
2020-06-22T20:19:50.2158826Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2159987Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[137,51] cannot find symbol
2020-06-22T20:19:50.2160676Z   symbol:   variable f1
2020-06-22T20:19:50.2161236Z   location: variable tuple2 of type java.lang.Object
2020-06-22T20:19:50.2163372Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[136,33] incompatible types: cannot infer functional interface descriptor for org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.StatementExecutorFactory<org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor>
2020-06-22T20:19:50.2164788Z [INFO] 2 errors 
2020-06-22T20:19:50.2165569Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2166430Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2167374Z [INFO] Reactor Summary:
2020-06-22T20:19:50.2167713Z [INFO] 
2020-06-22T20:19:50.2168486Z [INFO] force-shading ...................................... SUCCESS [  5.905 s]
2020-06-22T20:19:50.2169067Z [INFO] flink .............................................. SUCCESS [ 10.173 s]
2020-06-22T20:19:50.2169978Z [INFO] flink-annotations .................................. SUCCESS [  1.637 s]
2020-06-22T20:19:50.2170980Z [INFO] flink-test-utils-parent ............................ SUCCESS [  0.117 s]
2020-06-22T20:19:50.2171877Z [INFO] flink-test-utils-junit ............................. SUCCESS [  1.224 s]
2020-06-22T20:19:50.2172896Z [INFO] flink-metrics ...................................... SUCCESS [  0.101 s]
2020-06-22T20:19:50.2173788Z [INFO] flink-metrics-core ................................. SUCCESS [  1.726 s]
2020-06-22T20:19:50.2175058Z [INFO] flink-core ......................................... SUCCESS [ 29.372 s]
2020-06-22T20:19:50.2175982Z [INFO] flink-java ......................................... SUCCESS [  5.577 s]
2020-06-22T20:19:50.2176868Z [INFO] flink-queryable-state .............................. SUCCESS [  0.085 s]
2020-06-22T20:19:50.2177760Z [INFO] flink-queryable-state-client-java .................. SUCCESS [  1.619 s]
2020-06-22T20:19:50.2178600Z [INFO] flink-filesystems .................................. SUCCESS [  0.105 s]
2020-06-22T20:19:50.2179500Z [INFO] flink-hadoop-fs .................................... SUCCESS [ 20.792 s]
2020-06-22T20:19:50.2180402Z [INFO] flink-runtime ...................................... SUCCESS [01:51 min]
2020-06-22T20:19:50.2181462Z [INFO] flink-scala ........................................ SUCCESS [ 36.797 s]
2020-06-22T20:19:50.2182326Z [INFO] flink-mapr-fs ...................................... SUCCESS [  0.848 s]
2020-06-22T20:19:50.2183372Z [INFO] flink-filesystems :: flink-fs-hadoop-shaded ........ SUCCESS [  4.422 s]
2020-06-22T20:19:50.2184407Z [INFO] flink-s3-fs-base ................................... SUCCESS [  2.085 s]
2020-06-22T20:19:50.2185259Z [INFO] flink-s3-fs-hadoop ................................. SUCCESS [  6.051 s]
2020-06-22T20:19:50.2186131Z [INFO] flink-s3-fs-presto ................................. SUCCESS [ 10.325 s]
2020-06-22T20:19:50.2186990Z [INFO] flink-swift-fs-hadoop .............................. SUCCESS [ 22.021 s]
2020-06-22T20:19:50.2187820Z [INFO] flink-oss-fs-hadoop ................................ SUCCESS [  6.407 s]
2020-06-22T20:19:50.2188686Z [INFO] flink-azure-fs-hadoop .............................. SUCCESS [  8.868 s]
2020-06-22T20:19:50.2189526Z [INFO] flink-optimizer .................................... SUCCESS [ 10.922 s]
2020-06-22T20:19:50.2190385Z [INFO] flink-streaming-java ............................... SUCCESS [ 14.119 s]
2020-06-22T20:19:50.2191563Z [INFO] flink-clients ...................................... SUCCESS [  2.558 s]
2020-06-22T20:19:50.2192425Z [INFO] flink-test-utils ................................... SUCCESS [  1.837 s]
2020-06-22T20:19:50.2193609Z [INFO] flink-runtime-web .................................. SUCCESS [02:01 min]
2020-06-22T20:19:50.2194615Z [INFO] flink-examples ..................................... SUCCESS [  0.174 s]
2020-06-22T20:19:50.2195284Z [INFO] flink-examples-batch ............................... SUCCESS [ 12.889 s]
2020-06-22T20:19:50.2195902Z [INFO] flink-connectors ................................... SUCCESS [  0.109 s]
2020-06-22T20:19:50.2196513Z [INFO] flink-hadoop-compatibility ......................... SUCCESS [  6.164 s]
2020-06-22T20:19:50.2197043Z [INFO] flink-state-backends ............................... SUCCESS [  0.075 s]
2020-06-22T20:19:50.2197591Z [INFO] flink-statebackend-rocksdb ......................... SUCCESS [  4.125 s]
2020-06-22T20:19:50.2198116Z [INFO] flink-tests ........................................ SUCCESS [ 36.488 s]
2020-06-22T20:19:50.2198658Z [INFO] flink-streaming-scala .............................. SUCCESS [ 33.694 s]
2020-06-22T20:19:50.2199392Z [INFO] flink-hcatalog ..................................... SUCCESS [  7.401 s]
2020-06-22T20:19:50.2199957Z [INFO] flink-table ........................................ SUCCESS [  0.073 s]
2020-06-22T20:19:50.2200496Z [INFO] flink-table-common ................................. SUCCESS [  5.219 s]
2020-06-22T20:19:50.2201146Z [INFO] flink-table-api-java ............................... SUCCESS [  3.072 s]
2020-06-22T20:19:50.2201699Z [INFO] flink-table-api-java-bridge ........................ SUCCESS [  1.363 s]
2020-06-22T20:19:50.2202221Z [INFO] flink-table-api-scala .............................. SUCCESS [ 11.035 s]
2020-06-22T20:19:50.2202978Z [INFO] flink-table-api-scala-bridge ....................... SUCCESS [  9.510 s]
2020-06-22T20:19:50.2203548Z [INFO] flink-sql-parser ................................... SUCCESS [  6.408 s]
2020-06-22T20:19:50.2204152Z [INFO] flink-libraries .................................... SUCCESS [  0.080 s]
2020-06-22T20:19:50.2204832Z [INFO] flink-cep .......................................... SUCCESS [  3.582 s]
2020-06-22T20:19:50.2205395Z [INFO] flink-table-planner ................................ SUCCESS [02:02 min]
2020-06-22T20:19:50.2205980Z [INFO] flink-sql-parser-hive .............................. SUCCESS [  2.555 s]
2020-06-22T20:19:50.2206691Z [INFO] flink-table-runtime-blink .......................... SUCCESS [  6.373 s]
2020-06-22T20:19:50.2207523Z [INFO] flink-table-planner-blink .......................... SUCCESS [02:42 min]
2020-06-22T20:19:50.2208335Z [INFO] flink-metrics-jmx .................................. SUCCESS [  0.559 s]
2020-06-22T20:19:50.2209071Z [INFO] flink-formats ...................................... SUCCESS [  0.069 s]
2020-06-22T20:19:50.2209852Z [INFO] flink-json ......................................... SUCCESS [  1.230 s]
2020-06-22T20:19:50.2210584Z [INFO] flink-connector-kafka-base ......................... SUCCESS [  4.715 s]
2020-06-22T20:19:50.2211540Z [INFO] flink-avro ......................................... SUCCESS [  4.357 s]
2020-06-22T20:19:50.2212347Z [INFO] flink-csv .......................................... SUCCESS [  1.086 s]
2020-06-22T20:19:50.2213216Z [INFO] flink-connector-kafka-0.10 ......................... SUCCESS [  4.828 s]
2020-06-22T20:19:50.2214121Z [INFO] flink-connector-kafka-0.11 ......................... SUCCESS [  5.160 s]
2020-06-22T20:19:50.2214850Z [INFO] flink-connector-elasticsearch-base ................. SUCCESS [ 10.191 s]
2020-06-22T20:19:50.2215579Z [INFO] flink-connector-elasticsearch5 ..................... SUCCESS [ 16.121 s]
2020-06-22T20:19:50.2216348Z [INFO] flink-connector-elasticsearch6 ..................... SUCCESS [ 13.925 s]
2020-06-22T20:19:50.2217097Z [INFO] flink-connector-elasticsearch7 ..................... SUCCESS [ 14.855 s]
2020-06-22T20:19:50.2217870Z [INFO] flink-connector-hbase .............................. SUCCESS [ 26.486 s]
2020-06-22T20:19:50.2218627Z [INFO] flink-hadoop-bulk .................................. SUCCESS [  0.804 s]
2020-06-22T20:19:50.2219401Z [INFO] flink-orc .......................................... SUCCESS [  1.600 s]
2020-06-22T20:19:50.2220154Z [INFO] flink-orc-nohive ................................... SUCCESS [  1.258 s]
2020-06-22T20:19:50.2221047Z [INFO] flink-parquet ...................................... SUCCESS [  5.085 s]
2020-06-22T20:19:50.2221830Z [INFO] flink-connector-hive ............................... SUCCESS [ 29.718 s]
2020-06-22T20:19:50.2222595Z [INFO] flink-connector-jdbc ............................... FAILURE [ 42.673 s]
2020-06-22T20:19:50.2223455Z [INFO] flink-connector-rabbitmq ........................... SKIPPED
2020-06-22T20:19:50.2224297Z [INFO] flink-connector-twitter ............................ SKIPPED
2020-06-22T20:19:50.2225011Z [INFO] flink-connector-nifi ............................... SKIPPED
2020-06-22T20:19:50.2225705Z [INFO] flink-connector-cassandra .......................... SKIPPED
2020-06-22T20:19:50.2226427Z [INFO] flink-connector-filesystem ......................... SKIPPED
2020-06-22T20:19:50.2227146Z [INFO] flink-connector-kafka .............................. SKIPPED
2020-06-22T20:19:50.2228045Z [INFO] flink-connector-gcp-pubsub ......................... SKIPPED
2020-06-22T20:19:50.2228760Z [INFO] flink-connector-kinesis ............................ SKIPPED
2020-06-22T20:19:50.2229459Z [INFO] flink-sql-connector-elasticsearch7 ................. SKIPPED
2020-06-22T20:19:50.2230163Z [INFO] flink-connector-base ............................... SKIPPED
2020-06-22T20:19:50.2230956Z [INFO] flink-sql-connector-elasticsearch6 ................. SKIPPED
2020-06-22T20:19:50.2231671Z [INFO] flink-sql-connector-kafka-0.10 ..................... SKIPPED
2020-06-22T20:19:50.2232353Z [INFO] flink-sql-connector-kafka-0.11 ..................... SKIPPED
2020-06-22T20:19:50.2233168Z [INFO] flink-sql-connector-kafka .......................... SKIPPED
2020-06-22T20:19:50.2234041Z [INFO] flink-sql-connector-hive-1.2.2 ..................... SKIPPED
2020-06-22T20:19:50.2234926Z [INFO] flink-sql-connector-hive-2.2.0 ..................... SKIPPED
2020-06-22T20:19:50.2235647Z [INFO] flink-sql-connector-hive-2.3.6 ..................... SKIPPED
2020-06-22T20:19:50.2236328Z [INFO] flink-sql-connector-hive-3.1.2 ..................... SKIPPED
2020-06-22T20:19:50.2236998Z [INFO] flink-avro-confluent-registry ...................... SKIPPED
2020-06-22T20:19:50.2237717Z [INFO] flink-sequence-file ................................ SKIPPED
2020-06-22T20:19:50.2238397Z [INFO] flink-compress ..................................... SKIPPED
2020-06-22T20:19:50.2239109Z [INFO] flink-sql-orc ...................................... SKIPPED
2020-06-22T20:19:50.2239793Z [INFO] flink-sql-parquet .................................. SKIPPED
2020-06-22T20:19:50.2240474Z [INFO] flink-examples-streaming ........................... SKIPPED
2020-06-22T20:19:50.2241300Z [INFO] flink-examples-table ............................... SKIPPED
2020-06-22T20:19:50.2241996Z [INFO] flink-examples-build-helper ........................ SKIPPED
2020-06-22T20:19:50.2242836Z [INFO] flink-examples-streaming-twitter ................... SKIPPED
2020-06-22T20:19:50.2243548Z [INFO] flink-examples-streaming-state-machine ............. SKIPPED
2020-06-22T20:19:50.2244395Z [INFO] flink-examples-streaming-gcp-pubsub ................ SKIPPED
2020-06-22T20:19:50.2245069Z [INFO] flink-container .................................... SKIPPED
2020-06-22T20:19:50.2245787Z [INFO] flink-queryable-state-runtime ...................... SKIPPED
2020-06-22T20:19:50.2246509Z [INFO] flink-mesos ........................................ SKIPPED
2020-06-22T20:19:50.2247208Z [INFO] flink-kubernetes ................................... SKIPPED
2020-06-22T20:19:50.2247921Z [INFO] flink-yarn ......................................... SKIPPED
2020-06-22T20:19:50.2248641Z [INFO] flink-gelly ........................................ SKIPPED
2020-06-22T20:19:50.2249329Z [INFO] flink-gelly-scala .................................. SKIPPED
2020-06-22T20:19:50.2250039Z [INFO] flink-gelly-examples ............................... SKIPPED
2020-06-22T20:19:50.2250869Z [INFO] flink-external-resources ........................... SKIPPED
2020-06-22T20:19:50.2251554Z [INFO] flink-external-resource-gpu ........................ SKIPPED
2020-06-22T20:19:50.2252277Z [INFO] flink-metrics-dropwizard ........................... SKIPPED
2020-06-22T20:19:50.2253101Z [INFO] flink-metrics-graphite ............................. SKIPPED
2020-06-22T20:19:50.2253833Z [INFO] flink-metrics-influxdb ............................. SKIPPED
2020-06-22T20:19:50.2254742Z [INFO] flink-metrics-prometheus ........................... SKIPPED
2020-06-22T20:19:50.2255401Z [INFO] flink-metrics-statsd ............................... SKIPPED
2020-06-22T20:19:50.2256077Z [INFO] flink-metrics-datadog .............................. SKIPPED
2020-06-22T20:19:50.2256755Z [INFO] flink-metrics-slf4j ................................ SKIPPED
2020-06-22T20:19:50.2257446Z [INFO] flink-cep-scala .................................... SKIPPED
2020-06-22T20:19:50.2258126Z [INFO] flink-table-uber ................................... SKIPPED
2020-06-22T20:19:50.2259021Z [INFO] flink-table-uber-blink ............................. SKIPPED
2020-06-22T20:19:50.2259740Z [INFO] flink-python ....................................... SKIPPED
2020-06-22T20:19:50.2260417Z [INFO] flink-sql-client ................................... SKIPPED
2020-06-22T20:19:50.2261262Z [INFO] flink-state-processor-api .......................... SKIPPED
2020-06-22T20:19:50.2261940Z [INFO] flink-ml-parent .................................... SKIPPED
2020-06-22T20:19:50.2262773Z [INFO] flink-ml-api ....................................... SKIPPED
2020-06-22T20:19:50.2263475Z [INFO] flink-ml-lib ....................................... SKIPPED
2020-06-22T20:19:50.2264295Z [INFO] flink-ml-uber ...................................... SKIPPED
2020-06-22T20:19:50.2264976Z [INFO] flink-scala-shell .................................. SKIPPED
2020-06-22T20:19:50.2265828Z [INFO] flink-dist ......................................... SKIPPED
2020-06-22T20:19:50.2266560Z [INFO] flink-yarn-tests ................................... SKIPPED
2020-06-22T20:19:50.2267234Z [INFO] flink-end-to-end-tests ............................. SKIPPED
2020-06-22T20:19:50.2267933Z [INFO] flink-cli-test ..................................... SKIPPED
2020-06-22T20:19:50.2268619Z [INFO] flink-parent-child-classloading-test-program ....... SKIPPED
2020-06-22T20:19:50.2269313Z [INFO] flink-parent-child-classloading-test-lib-package ... SKIPPED
2020-06-22T20:19:50.2269984Z [INFO] flink-dataset-allround-test ........................ SKIPPED
2020-06-22T20:19:50.2270699Z [INFO] flink-dataset-fine-grained-recovery-test ........... SKIPPED
2020-06-22T20:19:50.2271519Z [INFO] flink-datastream-allround-test ..................... SKIPPED
2020-06-22T20:19:50.2272199Z [INFO] flink-batch-sql-test ............................... SKIPPED
2020-06-22T20:19:50.2273062Z [INFO] flink-stream-sql-test .............................. SKIPPED
2020-06-22T20:19:50.2273762Z [INFO] flink-bucketing-sink-test .......................... SKIPPED
2020-06-22T20:19:50.2274588Z [INFO] flink-distributed-cache-via-blob ................... SKIPPED
2020-06-22T20:19:50.2275278Z [INFO] flink-high-parallelism-iterations-test ............. SKIPPED
2020-06-22T20:19:50.2275976Z [INFO] flink-stream-stateful-job-upgrade-test ............. SKIPPED
2020-06-22T20:19:50.2276675Z [INFO] flink-queryable-state-test ......................... SKIPPED
2020-06-22T20:19:50.2277352Z [INFO] flink-local-recovery-and-allocation-test ........... SKIPPED
2020-06-22T20:19:50.2278076Z [INFO] flink-elasticsearch5-test .......................... SKIPPED
2020-06-22T20:19:50.2278778Z [INFO] flink-elasticsearch6-test .......................... SKIPPED
2020-06-22T20:19:50.2279483Z [INFO] flink-quickstart ................................... SKIPPED
2020-06-22T20:19:50.2280173Z [INFO] flink-quickstart-java .............................. SKIPPED
2020-06-22T20:19:50.2280982Z [INFO] flink-quickstart-scala ............................. SKIPPED
2020-06-22T20:19:50.2281719Z [INFO] flink-quickstart-test .............................. SKIPPED
2020-06-22T20:19:50.2282416Z [INFO] flink-confluent-schema-registry .................... SKIPPED
2020-06-22T20:19:50.2283272Z [INFO] flink-stream-state-ttl-test ........................ SKIPPED
2020-06-22T20:19:50.2284102Z [INFO] flink-sql-client-test .............................. SKIPPED
2020-06-22T20:19:50.2284793Z [INFO] flink-streaming-file-sink-test ..................... SKIPPED
2020-06-22T20:19:50.2285536Z [INFO] flink-state-evolution-test ......................... SKIPPED
2020-06-22T20:19:50.2286243Z [INFO] flink-rocksdb-state-memory-control-test ............ SKIPPED
2020-06-22T20:19:50.2287011Z [INFO] flink-end-to-end-tests-common ...................... SKIPPED
2020-06-22T20:19:50.2287749Z [INFO] flink-metrics-availability-test .................... SKIPPED
2020-06-22T20:19:50.2288440Z [INFO] flink-metrics-reporter-prometheus-test ............. SKIPPED
2020-06-22T20:19:50.2289231Z [INFO] flink-heavy-deployment-stress-test ................. SKIPPED
2020-06-22T20:19:50.2290158Z [INFO] flink-connector-gcp-pubsub-emulator-tests .......... SKIPPED
2020-06-22T20:19:50.2291011Z [INFO] flink-streaming-kafka-test-base .................... SKIPPED
2020-06-22T20:19:50.2291740Z [INFO] flink-streaming-kafka-test ......................... SKIPPED
2020-06-22T20:19:50.2292476Z [INFO] flink-streaming-kafka011-test ...................... SKIPPED
2020-06-22T20:19:50.2293323Z [INFO] flink-streaming-kafka010-test ...................... SKIPPED
2020-06-22T20:19:50.2294177Z [INFO] flink-plugins-test ................................. SKIPPED
2020-06-22T20:19:50.2294858Z [INFO] dummy-fs ........................................... SKIPPED
2020-06-22T20:19:50.2295612Z [INFO] another-dummy-fs ................................... SKIPPED
2020-06-22T20:19:50.2296338Z [INFO] flink-tpch-test .................................... SKIPPED
2020-06-22T20:19:50.2297112Z [INFO] flink-streaming-kinesis-test ....................... SKIPPED
2020-06-22T20:19:50.2298047Z [INFO] flink-elasticsearch7-test .......................... SKIPPED
2020-06-22T20:19:50.2298793Z [INFO] flink-end-to-end-tests-common-kafka ................ SKIPPED
2020-06-22T20:19:50.2299551Z [INFO] flink-tpcds-test ................................... SKIPPED
2020-06-22T20:19:50.2300251Z [INFO] flink-netty-shuffle-memory-control-test ............ SKIPPED
2020-06-22T20:19:50.2301104Z [INFO] flink-python-test .................................. SKIPPED
2020-06-22T20:19:50.2301876Z [INFO] flink-statebackend-heap-spillable .................. SKIPPED
2020-06-22T20:19:50.2302551Z [INFO] flink-contrib ...................................... SKIPPED
2020-06-22T20:19:50.2303405Z [INFO] flink-connector-wikiedits .......................... SKIPPED
2020-06-22T20:19:50.2304247Z [INFO] flink-fs-tests ..................................... SKIPPED
2020-06-22T20:19:50.2305061Z [INFO] flink-docs ......................................... SKIPPED
2020-06-22T20:19:50.2305813Z [INFO] flink-walkthroughs ................................. SKIPPED
2020-06-22T20:19:50.2306505Z [INFO] flink-walkthrough-common ........................... SKIPPED
2020-06-22T20:19:50.2307228Z [INFO] flink-walkthrough-table-java ....................... SKIPPED
2020-06-22T20:19:50.2307920Z [INFO] flink-walkthrough-table-scala ...................... SKIPPED
2020-06-22T20:19:50.2308641Z [INFO] flink-walkthrough-datastream-java .................. SKIPPED
2020-06-22T20:19:50.2309341Z [INFO] flink-walkthrough-datastream-scala ................. SKIPPED
2020-06-22T20:19:50.2310060Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2310483Z [INFO] BUILD FAILURE
2020-06-22T20:19:50.2311235Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2311680Z [INFO] Total time: 17:47 min
2020-06-22T20:19:50.2312305Z [INFO] Finished at: 2020-06-22T20:19:50+00:00
2020-06-22T20:19:50.3937346Z [INFO] Final Memory: 326M/1212M
2020-06-22T20:19:50.3938874Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.3953129Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-jdbc_2.11: Compilation failure: Compilation failure:
2020-06-22T20:19:50.3954979Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[137,51] cannot find symbol
2020-06-22T20:19:50.3955665Z [ERROR] symbol:   variable f1
2020-06-22T20:19:50.3956090Z [ERROR] location: variable tuple2 of type java.lang.Object
2020-06-22T20:19:50.3958074Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[136,33] incompatible types: cannot infer functional interface descriptor for org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.StatementExecutorFactory<org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor>
2020-06-22T20:19:50.3959865Z [ERROR] -> [Help 1]
2020-06-22T20:19:50.3960153Z [ERROR] 
2020-06-22T20:19:50.3961449Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-22T20:19:50.3962383Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-22T20:19:50.3962995Z [ERROR] 
2020-06-22T20:19:50.3963580Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-22T20:19:50.3964490Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
2020-06-22T20:19:50.3964969Z [ERROR] 
2020-06-22T20:19:50.3965374Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-22T20:19:50.3966310Z [ERROR]   mvn <goals> -rf :flink-connector-jdbc_2.11
{code}",,dian.fu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17544,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 02:57:46 UTC 2020,,,,,,,,,,"0|z0g3u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/20 02:31;dian.fu;cc [~fsk119];;;","23/Jun/20 02:57;jark;Thanks for reporting this [~dian.fu].;;;","23/Jun/20 02:57;jark;- master (1.12.0): de8a25ae597845f688376bde51dda5e419bf2085
- 1.11.0: 515aff8195beeb88ba9c4de555f5085b562800ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in SlotPoolImpl.maybeRemapOrphanedAllocation if no pending request is registered,FLINK-18407,13312799,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,trohrmann,trohrmann,22/Jun/20 09:44,23/Jun/20 12:36,13/Jul/23 08:12,23/Jun/20 12:36,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,The method {{SlotPoolImp.maybeRemapOrphanedAllocation}} can throw a NPE if no pending request is registered under a given {{AllocationID}} and if there is no connection established to the {{ResourceManager}}.,,nicholasjiang,trohrmann,wind_ljy,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 12:36:33 UTC 2020,,,,,,,,,,"0|z0g2zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/20 11:23;nicholasjiang;[~trohrmann], do you mean that get pending request according to requestIdOfAllocatedSlot maybe null? If the pending request is null, this would cause the NPE.;;;","22/Jun/20 11:26;zhuzh;[~nicholasjiang] it's a potential problem because resourceManagerGateway is nullable.;;;","23/Jun/20 12:36;zhuzh;Fixed via d04d1f4a4bf41d26d28835d7a7551408274187c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that only exactly once checkpointing can be unaligned,FLINK-18403,13312785,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,AHeise,22/Jun/20 08:06,22/Jun/21 14:07,13/Jul/23 08:12,22/Jun/20 15:57,1.11.0,1.12.0,,,,1.11.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"Currently, it's possible to configure at least once and unaligned checkpoint enabled with undesired side-effect.

We should sanitize the configuration; potentially with warnings.",,AHeise,pnowojski,stevenz3wu,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 15:57:05 UTC 2020,,,,,,,,,,"0|z0g2wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/20 15:57;pnowojski;Merged to master as 755025c365, release-1.11 as 6b4d170b78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableResult#print can not print the result of unbounded stream query,FLINK-18399,13312618,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,godfreyhe,godfreyhe,20/Jun/20 15:15,23/Jun/20 16:07,13/Jul/23 08:12,23/Jun/20 16:07,1.11.0,,,,,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"In current implementation of PrintUtils, all result will be collected to local memory to compute column width first. this can works fine with batch query and bounded stream query. but for unbounded stream query, the result will be endless, so the result will be never printed. To solve this, we can use fix-length strategy, and print a row immediately once the row is accessed.",,aljoscha,godfreyhe,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 15:03:31 UTC 2020,,,,,,,,,,"0|z0g1vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/20 15:03;jark;Fixed in 
 - master (1.12.0): 1414bac01817fe522bbe254c1394a1e78e12062e
 - 1.11.0: 8d9f6df60d4c01d3410c763ff3508d50f78658ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Jekyll to 4.0.1,FLINK-18381,13312461,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,19/Jun/20 12:39,22/Jun/20 09:37,13/Jul/23 08:12,22/Jun/20 09:37,,,,,,1.11.0,,,Documentation,,,,,0,,,,,,"Building the docs with Ruby 2.7 and Jekyll 4.0.0 spits out a lot of warnings, see [https://github.com/jekyll/jekyll/issues/7947]. Updating to 4.0.1 fixes this.",,aljoscha,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 09:37:35 UTC 2020,,,,,,,,,,"0|z0g0wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/20 09:37;aljoscha;master: ea9ce92a273853616c9ae1237f1882990aa6bbc2

release-1.11: 29daf12aa20662cc1a2305413a4de5450e373180;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogManager checks for CatalogTableImpl instead of CatalogTable,FLINK-18378,13312445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,fhueske,fhueske,19/Jun/20 11:10,22/Jun/20 12:04,13/Jul/23 08:12,22/Jun/20 12:04,1.11.0,,,,,1.11.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"The {{CatalogManager}} checks for {{CatalogTableImpl}} instead of {{CatalogTable}} to decide whether to resolve the table schema. See https://github.com/apache/flink/blob/release-1.11/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java#L369

Resolving the table schema adjusts the type of fields which are referenced by watermarks, i.e., changes their type from {{TIMESTAMP(3)}} to {{TIMESTAMP(3) ROWTIME}}. If table schema is not properly resolved some queries involving time attributes will fail during type validation.

However, {{CatalogTableImpl}} is an internal implementation of the public {{CatalogTable}} interface. Hence, external {{Catalog}} implementations will not work with {{CatalogTableImpl}} but rather {{CatalogTable}} and hence might fail to work correctly with queries that involve event-time attributes.",,dwysakowicz,fhueske,jark,libenchao,nicholasjiang,twalthr,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 12:04:02 UTC 2020,,,,,,,,,,"0|z0g0t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/20 11:44;jark;Yes. I think we should fix this. cc [~godfreyhe];;;","19/Jun/20 12:37;dwysakowicz;I don't think it is that simple. Event though it would work.

This requires adding the {{copy(TableSchema)}} method to the public {{CatalogTable}} interface. I am not convinced if we should do that. Instead I want to check if we can pass the resolved schema as part of {{TableLookupResult}}.;;;","19/Jun/20 12:53;fhueske;Is it necessary to preserve the original table type? 
Wouldn't it be possible to create a new {{CatalogTableImpl}} with the resolved schema and all other fields (properties, comment, partitionKeys, ...) and return it?

Btw. what's the purpose of the check? Is it to distinguish between tables and views?;;;","19/Jun/20 13:16;dwysakowicz;First of all, I have to admit I missed it during the review, therefore I am not 100% sure why we need the check there.

We should not change the implementation of the {{CatalogTable}}. Theoretically {{Catalog}} can return custom implementation that it might need later on to instantiate the {{DynamicTableSource/Sink}}. That was the case for {{Hive}} some time ago (Now I see it is no longer the case}}. Something like:

{code}
Catalog cat = ...
CatalogBaseTable table = cat.getTable(); // instance of MyCustomTable
....
DynamicTableSourceFactory factory = (DynamicTableSourceFactory) cat.getFactory();

// where the factory does:

DynamicTableSource createDynamicTableSource(Context context) {
  CatalogTable table = context.getCatalogTable();
  if (!(table instanceof MyCustomTable)) {
      throw new IllegalArgumentException(""...."");
  }
}

{code}

It is a good questions what do we want to do for views. Views should not have any computed columns, but they might have the watermark specification imo. So we should probably resolve them too. Which makes the check somewhat unnecessary.

I think we have two options:
1.  we add {{CatalogBaseTable#copy(TableSchema)}} and then do what [~nicholasjiang] was suggesting:
{code}
private CatalogBaseTable resolveTableSchema(CatalogBaseTable table) {
   if (!(table instanceof CatalogTable)) {
     return table;
  }
  CatalogTable catalogTable = (CatalogTable) table;
  TableSchema newTableSchema = schemaResolver.resolve(catalogTable.getSchema());
  return catalogTable.copy(newTableSchema);
}
{code} 

2. we do not add the {{copy}} method, but we pass the resolvedSchema as part of the {{TableLookup}};;;","19/Jun/20 13:24;fhueske;[~dwysakowicz] Thanks for the explanation! (y);;;","19/Jun/20 14:16;twalthr;In my opinion, this issue is related to FLINK-17793 so we might fix both in one design. However, we can temporarily fix this issue first.;;;","22/Jun/20 11:25;nicholasjiang;[~dwysakowicz] Thanks for the explanation!;;;","22/Jun/20 12:04;dwysakowicz;Fixed in
* master
** dc102ecc4aef38fd7ddeaeeec86e16d10a609098
* 1.11
** cc6258c2876f76b68d9516508325af03a3e39c3e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException can happen in SlotPoolImpl#maybeRemapOrphanedAllocation,FLINK-18372,13312357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,19/Jun/20 05:11,22/Jun/20 15:51,13/Jul/23 08:12,22/Jun/20 15:51,1.12.0,,,,,1.12.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"NullPointerException can happen in SlotPoolImpl#maybeRemapOrphanedAllocation, which indicates a bug.

https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/8189/logs/115

6:07:07,950 [flink-akka.actor.default-dispatcher-7] WARN  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Slot offering to JobManager failed. Freeing the slots and returning them to the ResourceManager.
java.lang.NullPointerException: null
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.maybeRemapOrphanedAllocation(SlotPoolImpl.java:599) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFulfillSlotRequestOrMakeAvailable(SlotPoolImpl.java:564) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.offerSlot(SlotPoolImpl.java:701) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.offerSlots(SlotPoolImpl.java:625) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.offerSlots(JobMaster.java:541) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
16:07:07,977 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=0.5000000000000000, taskHeapMemory=64.000mb (67108864 bytes), taskOffHeapMemory=0 bytes, managedMemory=2.000mb (2097152 bytes), networkMemory=1.563mb (1638400 bytes)}, allocationId: 4dcfe78bb09fcf1117bd0be11c039df9, jobId: 00a771c28c805577994e752b25bef01c).",,azagrebin,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 15:51:57 UTC 2020,,,,,,,,,,"0|z0g09k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/20 09:39;chesnay;[~zhuzh] Are we sure this can only happen in 1.12?;;;","19/Jun/20 10:04;zhuzh;[~chesnay] I have not identified the root cause. But at least for this specific NPE problem, it will only happen in master branch at the moment.;;;","19/Jun/20 10:11;azagrebin;There is no SlotPoolImpl#maybeRemapOrphanedAllocation in 1.11.

I think the problem can be that `allocationIdOfRequest` to remap can be null if `pendingRequest` to fulfil is from `waitingForResourceManager` so there is nothing to remap. the original `pendingRequest` of offered slot has to be moved to `waitingForResourceManager` on the place of the fulfilled one.;;;","19/Jun/20 10:14;zhuzh;Yes the cause should be that a fulfilled request is not in `SlotPoolImpl#pendingRequests` yet. However, I think it should have been so it is a bit weird.

I find these log lines which seem weird because a task slot request is fulfilled even before the JM is connected to a RM.


{code:java}
2020-06-18T16:11:52.6078290Z 16:07:07,821 6321 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster [] - Connecting to ResourceManager akka.tcp://flink@127.0.0.1:42309/user/rpc/resourcemanager_0(98b55e72529e946e2d02579002f34274)
2020-06-18T16:11:52.6079359Z 16:07:07,873 6373 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.jobmaster.JobMaster [] - Resolved ResourceManager address, beginning registration
2020-06-18T16:11:52.6080396Z 16:07:07,876 6376 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService [] - Starting ZooKeeperLeaderRetrievalService /leader/00a771c28c805577994e752b25bef01c/job_manager_lock.
2020-06-18T16:11:52.6081789Z 16:07:07,876 6376 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 834ef81e30da53bdb392b11805574dc1@akka.tcp://flink@127.0.0.1:42309/user/rpc/jobmanager_2 for job 00a771c28c805577994e752b25bef01c.
2020-06-18T16:11:52.6083414Z 16:07:07,907 6407 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 834ef81e30da53bdb392b11805574dc1@akka.tcp://flink@127.0.0.1:42309/user/rpc/jobmanager_2 for job 00a771c28c805577994e752b25bef01c.
2020-06-18T16:11:52.6085074Z 16:07:07,916 6416 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - DataSource (at testJobManagerFailure(JobManagerHAProcessFailureRecoveryITCase.java:165) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (d37b2d59a796947e855620e4a6b9c4a3) switched from SCHEDULED to DEPLOYING.
2020-06-18T16:11:52.6086831Z 16:07:07,917 6417 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying DataSource (at testJobManagerFailure(JobManagerHAProcessFailureRecoveryITCase.java:165) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (attempt #0) to b89abcc35a8876d889ddc6f6f87127ef @ 66481b3fda78 (dataPort=46239)
2020-06-18T16:11:52.6088244Z 16:07:07,953 6453 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster [] - JobManager successfully registered at ResourceManager, leader id: 98b55e72529e946e2d02579002f34274.
{code}
;;;","19/Jun/20 13:06;trohrmann;Where did you get the logs from [~zhuzh]? They seem not to be the logs from the failed build.

I think it can happen that the {{JobMaster}} receives slots from the {{TaskExecutor}} but has not connected to the {{ResourceManager}} yet. If the {{TaskExecutor}} has still allocated slots for a job and the {{JobMaster}} gains leadership, then the {{TaskExecutor}} will offer the slots to the {{JobMaster}} w/o it being connected to the {{ResourceManager}}.;;;","19/Jun/20 13:54;zhuzh;The logs are from https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/8189/logs/115.
Thanks for the explanation [~trohrmann]! I think you are right that this can happen if the TM established connection with the new JM before the previous slots timeout. That's also why this problem was revealed in this HA test.
So it's possible that we directly fulfill requests in {{SlotPoolImpl#waitingForResourceManager}}. There would be no orphaned allocation in this case. We can fix this issue by simply check whether the orphaned allocation is null before remapping it.;;;","22/Jun/20 15:51;zhuzh;Fixed via 6d52d0492f9c6655ac95cc34460231376001efa9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentITCase.testStatementSetWithSameSinkTableNames failed on azure,FLINK-18369,13312251,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,pnowojski,pnowojski,18/Jun/20 14:35,10/Jul/20 07:06,13/Jul/23 08:12,10/Jul/20 07:06,1.12.0,,,,,1.12.0,,,Table SQL / Legacy Planner,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3778&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c

{noformat}
Caused by: java.io.IOException: Output path '/tmp/junit6162277837132243514/junit3149129531927352947.tmp' could not be initialized. Canceling task...
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:229)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:88)
	at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:205)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

{noformat}
[ERROR] Errors: 
[ERROR]   TableEnvironmentITCase.testStatementSetWithSameSinkTableNames:557 » Execution ...
[INFO] 
[ERROR] Tests run: 810, Failures: 0, Errors: 1, Skipped: 13
{noformat}
",,dwysakowicz,godfreyhe,jark,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17383,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 07:06:56 UTC 2020,,,,,,,,,,"0|z0fzm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/20 04:32;jark;cc [~godfreyhe];;;","19/Jun/20 05:14;godfreyhe;[~pnowojski] is there any failure info ?;;;","19/Jun/20 06:48;pnowojski;Sorry, I don't have anything more [~godfreyhe]. I've attached the url to all information that I had from a build failure.;;;","19/Jun/20 06:55;pnowojski;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3793&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=6568c985-5fcc-5b89-1ebd-0385b8088b14;;;","19/Jun/20 09:20;dwysakowicz;Hi, [~godfreyhe]
I did a small investigation on it. I think the problem is the usage of {{writeAsText}} /{{TextOutputFormat}} writing twice to the same path from a single {{Job}}.

After we switched in https://issues.apache.org/jira/browse/FLINK-17383 from a Collection to Cluster execution mode, the job is executed truly parallelized (before every operation was effectively sequential).

I am not 100% sure what is the purpose of this test, but I see that we do not care about the actual results. Maybe it would be ok to use a different sink, if the purpose is to check the sinks will get suffixed with an id?;;;","19/Jun/20 10:38;dwysakowicz;I can confirm that you cannot have a plan with two {{DataSet#writeAsTest(...)}} that write to the same path:

A code like below has exact same problem:
{code}
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
DataSource<Integer> ds1 = env.fromElements(1, 2, 3);
DataSource<Integer> ds2 = env.fromElements(1, 2, 3);

String resultPath = tempFolder.newFile().toURI().toString();

ds1.writeAsText(resultPath, FileSystem.WriteMode.OVERWRITE);
ds2.writeAsText(resultPath, FileSystem.WriteMode.OVERWRITE);
env.execute();
{code};;;","19/Jun/20 10:51;dwysakowicz;I disabled this test temporarily on master via: 497a18e509258d181a1cd92be9900b061ec3ccdd;;;","09/Jul/20 07:26;godfreyhe;[~dwysakowicz] you are right. For {{INSERT}} operation, the field names of {{TableResult}}'s schema is sink table names, This requires the names are unique. This test just verifies the case of the same sink names and does not check the execution result.;;;","10/Jul/20 07:06;dwysakowicz;Fixed in e5bf5dc8e9c42a9d1e63c4025a71f8bdb29a50dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest.createHDFS fails with ""Running in secure mode, but config doesn't have a keytab"" ",FLINK-18368,13312248,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,rmetzger,rmetzger,18/Jun/20 14:29,19/Jun/20 08:51,13/Jul/23 08:12,19/Jun/20 08:51,1.12.0,,,,,1.11.0,,,FileSystems,Tests,,,,0,test-stability,,,,,https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8184&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa,,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 08:51:37 UTC 2020,,,,,,,,,,"0|z0fzlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/20 15:08;sewen;Another instance:

https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/61/logs/91
;;;","18/Jun/20 15:09;sewen;This is probably caused by some other test not properly cleaning up. There are probably some Kerberos configs lying around so the test HDFS-Minicluster cluster starts in Kerberized mode.

 ;;;","18/Jun/20 15:38;sewen;Hmmm, one strange thing is that the stack traces in both linked test runs do not correspond to Hadoop 2.4.1 code, which should be the dependency.

I am wondering if there could be some dependency mixup for whatever reason?
;;;","18/Jun/20 15:48;sewen;The problem happens every time the {{HadoopUtilsTest}} runs first, because that test does not clean up after itself.
Will push a simple fix.

The fact that the stack traces don't line up with the version I would assume to run is strange, but it looks like this is unrelated to this issues.;;;","18/Jun/20 17:18;rmetzger;Thanks a lot for looking into this immediately.
The reason why the stack traces don't line up is because we are using Hadoop 2.8.3 by default on CI.;;;","19/Jun/20 08:51;sewen;Fixed in 
  - 1.11.0 via c7cd54188f073bcca056508fe0aa894826e34f0b
  - 1.12.0 (master) via c8e9d0da032f2d7dd094096d606ab6cc3e368b19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink History Server doesn't show correctly table of Completed jobs when there are no archived jobs are in the archive directory,FLINK-18360,13312166,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jvimr,jvimr,jvimr,18/Jun/20 08:57,23/Jun/20 09:10,13/Jul/23 08:12,23/Jun/20 09:10,1.10.0,,,,,1.12.0,,,Runtime / REST,,,,,0,pull-request-available,,,,,"When the directory defined in ${historyserver.archive.fs.dir} is empty (=there are no archived completed jobs), the History Server UI fails to show the Completed Jobs table.

It should show the empty table with correct column headers and ""No data"" icon.

This is due to file ${historyserver.web.tmpdir}/jobs/overview.json not being created. This file is fetched by the web UI from url `/jobs/overview`.

This file is correctly created and populated by the History Server if the directory defined in ${historyserver.archive.fs.dir} contains any job.

The situation when the Completed Jobs table is not populated normally indicates that the History Server stars up and processes the jobs in the archive, so the user should wait. 
This happened to us few times, as we waited for the HS to finish the archived jobs processing just to find out after hours that the HS has in fact nothing to show.

 

The fix is simple, by removing `
{code:java}
if (!events.isEmpty()){code}
condition around 
{code:java}
updateJobOverview(webOverviewDir, webDir){code}
in the [HistoryServerArchiveFetcher.java|[https://github.com/apache/flink/blob/master/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java#L288]]

 ",,jvimr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/20 08:43;jvimr;flink-hs-correct-no-jobs.png;https://issues.apache.org/jira/secure/attachment/13005935/flink-hs-correct-no-jobs.png","18/Jun/20 08:43;jvimr;flink-hs-no-data-shown.png;https://issues.apache.org/jira/secure/attachment/13005934/flink-hs-no-data-shown.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 09:10:31 UTC 2020,,,,,,,,,,"0|z0fz34:",9223372036854775807,Histrory server show correctly Completed Jobs table when there are no archived jobs,,,,,,,,,,,,,,,,,,,"18/Jun/20 09:01;jvimr;the fix (with test) is available as a [Pull Request|[https://github.com/apache/flink/pull/12704]];;;","23/Jun/20 09:10;chesnay;master: efcc5434dd744ad547999e970c00142a9b20e914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.core.execution.DefaultExecutorServiceLoader not thread safe,FLINK-18352,13312078,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,mklein0,mklein0,17/Jun/20 23:30,23/Jun/20 06:30,13/Jul/23 08:12,22/Jun/20 14:26,1.10.0,,,,,1.10.2,1.11.0,1.12.0,Client / Job Submission,,,,,1,pull-request-available,,,,,"The singleton nature of the  *org.apache.flink.core.execution.DefaultExecutorServiceLoader* class is not thread-safe due to the fact that *java.util.ServiceLoader* class is not thread-safe.

[https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html#Concurrency]

 

This can result in *ServiceLoader* class entering into an inconsistent state for processes which attempt to self-heal. This then requires bouncing the process/container in the hopes the race condition does not re-occur.

[https://stackoverflow.com/questions/60391499/apache-flink-cannot-find-compatible-factory-for-specified-execution-target-lo]

 

Additionally the following stack traces have been seen when using a *org.apache.flink.streaming.api.environment.RemoteStreamEnvironment* instances.
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 2
    at sun.misc.CompoundEnumeration.nextElement(CompoundEnumeration.java:61)
    at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
    at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
    at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
    at org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:60)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1724)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1706)
{code}
 
{code:java}
java.util.NoSuchElementException: null
    at sun.misc.CompoundEnumeration.nextElement(CompoundEnumeration.java:59)
    at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
    at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
    at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
    at org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:60)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1724)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1706)
{code}
The workaround for using the ***StreamExecutionEnvironment* implementations is to write a custom implementation of *DefaultExecutorServiceLoader* which is thread-safe and pass that to the *StreamExecutionEnvironment* constructors.",,aljoscha,kkl0u,mklein0,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18411,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 06:29:48 UTC 2020,,,,,,,,,,"0|z0fyjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/20 14:26;kkl0u;Fixed on master with b05ab524836c1c07d128610248fe372c8d697974
on release-1.11 with eeeff7a5fa8ec0acb67bac8eb7dbd9a4165a91e7
and on release-1.10 with dcd7574daccfbbacf99d101a3f0f852e332e92a7;;;","23/Jun/20 06:29;pnowojski;This caused https://issues.apache.org/jira/browse/FLINK-18411;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoteInputChannel should checkError before checking partitionRequestClient,FLINK-18348,13311957,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wind_ljy,wind_ljy,wind_ljy,17/Jun/20 12:33,23/Jun/20 15:57,13/Jul/23 08:12,23/Jun/20 15:56,1.11.0,1.12.0,,,,1.11.0,,,Runtime / Network,,,,,0,pull-request-available,,,,,"The error will be set and \{{partitionRequestClient}} will be a null value if a remote channel fails to request the partition at the beginning. And the task will fail [here|https://github.com/apache/flink/blob/2150533ac0b2a6cc00238041853bbb6ebf22cee9/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java#L172] when the task thread trying to fetch data from channels.

And then we get error:


{code:java}
java.lang.IllegalStateException: Queried for a buffer before requesting a queue.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:172) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:637) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:615) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNext(SingleInputGate.java:598) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
{code}

But the root cause is the {{PartitionConnectionException}} we set when requesting the partition.",,libenchao,pnowojski,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 15:56:48 UTC 2020,,,,,,,,,,"0|z0fxsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 12:39;wind_ljy;After visiting the codes, I think we can safely remove the {{checkState(partitionRequestClient != null)}} in {{getNextBuffer()}} because it can only be null value when the task fails to request partitions, which is already covered in {{checkError()}}.

Update: it seems that this check in other functions like {{resumeConsumption()}} can also be removed safely.;;;","17/Jun/20 17:46;pnowojski;[~wind_ljy] is this another instance of the same problem?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3722&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{noformat}
[FAIL] 'Streaming File Sink s3 end-to-end test' failed after 15 minutes and 18 seconds! Test exited with exit code 1
{noformat}

{noformat}
java.lang.IllegalStateException: Queried for a buffer before requesting a queue.
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:172) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:637) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:615) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:603) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:105) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:79) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:158) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:345) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:558) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]{noformat}
;;;","17/Jun/20 17:55;pnowojski;[~wind_ljy] are you sure you have noticed this problem on Flink 1.10.1? I think this change was caused by FLINK-16536 via adding this method:

{code:java}
	private void internalRequestPartitions() {
		for (InputChannel inputChannel : inputChannels.values()) {
			try {
				inputChannel.requestSubpartition(consumedSubpartitionIndex);
			} catch (Throwable t) {
				inputChannel.setError(t);
				return;
			}
		}
	}
{code}

-Upgrading to blocker-. Upgrading to critical. I think this is not a blocker, as this is just hiding some connection issue with this {{IllegalStateException}}. [~zjwang], what do you think?
;;;","18/Jun/20 03:37;zjwang;[~pnowojski] FLINK-16536 might cause this issue, but there was also another place to cause it in [link|https://github.com/apache/flink/blob/2150533ac0b2a6cc00238041853bbb6ebf22cee9/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyPartitionRequestClient.java#L121]. So this issue is not a new one brought by release-1.11, not blocker issue as well.

[~wind_ljy] Regarding the solution, I think the conservative way is to reverse the calls between `checkError` and `checkState`. `checkState` might be still reasonable to guard the logics in some cases. E.g. if we have some logic bugs to miss `requestPartition` in advance, then the `checkState` can help locate such issue.;;;","18/Jun/20 03:59;wind_ljy;[~pnowojski] Sorry for choosing wrong affected version :(.  I actually met this problem on release-1.11 branch when testing some features on my own.

[~zjwang] I'm not sure that {{RemoteInputChannel.onError(Throwable)}} will cause this problem because client is already not null when requesting a new partition. I agree that we can keep this to prevent some unexpected changes. Should I submit a PR to reverse the calls now?
;;;","18/Jun/20 08:23;pnowojski;I think I agree with [~wind_ljy] that the place you pointed out [~zjwang] is not affecting this bug/illegal state exception. So this is on a verge of being release blocker.

If you can work on this [~wind_ljy] that would be great. Just please note that we would need a fix for this very shortly. Could you also provide a unit test for this exact scenario? For example via injecting {{ConnectionManager}} that would throw an {{ExpectedTestException}}?;;;","18/Jun/20 09:06;wind_ljy;[~pnowojski] Okay. I'll submit a PR today.;;;","18/Jun/20 10:03;wind_ljy;[~pnowojski] I've submitted a PR, please take a look. Thanks! :);;;","18/Jun/20 10:53;zjwang;[~wind_ljy] You are right, my previous mentioned usages will not cause this issue and it is indeed brought by FLIINK-16536 only.;;;","23/Jun/20 15:56;pnowojski;merged to master as a32c99d2f1 release-1.11 as 52fa6abdb7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building Flink Walkthrough Table Java 0.1 COMPILATION ERROR,FLINK-18341,13311874,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sjwiesman,pnowojski,pnowojski,17/Jun/20 05:05,24/Jul/20 17:22,13/Jul/23 08:12,24/Jul/20 17:22,1.11.1,1.12.0,,,,1.11.2,1.12.0,,Table SQL / API,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3652&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334

{noformat}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-22294375765/flink-walkthrough-table-java/src/main/java/org/apache/flink/walkthrough/SpendReport.java:[23,46] cannot access org.apache.flink.table.api.bridge.java.BatchTableEnvironment
  bad class file: /home/vsts/work/1/.m2/repository/org/apache/flink/flink-table-api-java-bridge_2.11/1.12-SNAPSHOT/flink-table-api-java-bridge_2.11-1.12-SNAPSHOT.jar(org/apache/flink/table/api/bridge/java/BatchTableEnvironment.class)
    class file has wrong version 55.0, should be 52.0
    Please remove or make sure it appears in the correct subdirectory of the classpath.

(...)

[FAIL] 'Walkthrough Table Java nightly end-to-end test' failed after 0 minutes and 4 seconds! Test exited with exit code 1

{noformat}
",,dian.fu,pnowojski,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18194,,,,,,,,,,,,,,FLINK-17497,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 17:22:32 UTC 2020,,,,,,,,,,"0|z0fxa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 15:35;sjwiesman;I have an open PR - FLINK-18194 - to delete this code. I'll try and up its priority since its now causing test instability. ;;;","17/Jun/20 15:40;pnowojski;Thanks [~sjwiesman]. Let me know if you will need to find some reviewer for the change.;;;","23/Jun/20 19:37;sjwiesman;Fixed in master: afebdc2f19a7e5439d9550bd8ffba609fba9a7dc;;;","21/Jul/20 02:32;dian.fu;Anther instance on the release-1.11 nightly:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4655&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994;;;","22/Jul/20 02:06;dian.fu;Another instance on 1.11:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4698&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994;;;","22/Jul/20 02:06;dian.fu;cc [~sjwiesman];;;","23/Jul/20 02:31;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4744&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994;;;","23/Jul/20 02:36;dian.fu;Hi [~sjwiesman] , I take a quick look at this issue and think that the reason it fails on 1.11 is because you forgot to cherry-pick the following commit(there are two commits in the original [PR|https://github.com/apache/flink/pull/12592]) to 1.11 branch: https://github.com/apache/flink/commit/57267f277339f7ba3790c8e0110cdb77e9593073

Could you confirm that?;;;","23/Jul/20 02:39;dian.fu;I reopen it for now. We can close it after fixing it in 1.11.;;;","23/Jul/20 15:48;sjwiesman;HI [~dian.fu]

You are absolutly correct. I will do that now. ;;;","23/Jul/20 17:05;sjwiesman;It was slightly more than just a cherry pick so I have opened PR's . ;;;","24/Jul/20 17:22;sjwiesman;Fixed in master: d88f9b1f9577291b32b472387f6981059522d9e2
release-1.11: 2777ecd6b7adc67cb0f01523a2f55688aaaf21d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB tests crash the JVM on CI,FLINK-18338,13311790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,chesnay,chesnay,16/Jun/20 18:02,22/Jun/20 08:06,13/Jul/23 08:12,17/Jun/20 17:37,1.11.0,,,,,1.11.0,,,Runtime / State Backends,Tests,,,,0,test-stability,,,,,"Something about {{pure virtual method called}}.

Seen this twice in separate PRs.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3615&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3632&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1",,klion26,liyu,pnowojski,rmetzger,yunta,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17800,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 08:06:54 UTC 2020,,,,,,,,,,"0|z0fwrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 07:29;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3657&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

This is how maven fails:
{code}
2020-06-17T03:22:58.7173637Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-statebackend-rocksdb_2.11: There are test failures.
2020-06-17T03:22:58.7175307Z [ERROR] 
2020-06-17T03:22:58.7176949Z [ERROR] Please refer to /__w/3/s/flink-state-backends/flink-statebackend-rocksdb/target/surefire-reports for the individual test results.
2020-06-17T03:22:58.7178591Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-06-17T03:22:58.7179590Z [ERROR] ExecutionException Error occurred in starting fork, check output in log
2020-06-17T03:22:58.7180750Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log
2020-06-17T03:22:58.7182154Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-06-17T03:22:58.7183460Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
2020-06-17T03:22:58.7184827Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
2020-06-17T03:22:58.7186293Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-06-17T03:22:58.7187522Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-06-17T03:22:58.7188983Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-06-17T03:22:58.7190325Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-06-17T03:22:58.7191589Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-06-17T03:22:58.7192828Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-06-17T03:22:58.7194029Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-06-17T03:22:58.7195159Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-06-17T03:22:58.7196547Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-06-17T03:22:58.7197897Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-06-17T03:22:58.7199359Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-06-17T03:22:58.7200670Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-06-17T03:22:58.7201762Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-06-17T03:22:58.7202764Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-06-17T03:22:58.7203716Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-06-17T03:22:58.7204717Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-06-17T03:22:58.7205620Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-06-17T03:22:58.7206495Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-17T03:22:58.7207455Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-17T03:22:58.7208657Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-17T03:22:58.7209729Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-17T03:22:58.7210726Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-06-17T03:22:58.7211875Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-06-17T03:22:58.7213014Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-06-17T03:22:58.7214212Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-06-17T03:22:58.7215465Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log
2020-06-17T03:22:58.7216750Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:622)
2020-06-17T03:22:58.7217947Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-06-17T03:22:58.7219191Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
2020-06-17T03:22:58.7220376Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
2020-06-17T03:22:58.7221429Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-17T03:22:58.7222477Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-17T03:22:58.7223602Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-17T03:22:58.7224621Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-06-17T03:22:58.7225637Z [ERROR] -> [Help 1]
2020-06-17T03:22:58.7226066Z [ERROR] 
{code}

This is what the JVM debugging files have to say
{code}
# Created at 2020-06-17T03:21:56.823
pure virtual method called

# Created at 2020-06-17T03:21:56.823
terminate called without an active exception

# Created at 2020-06-17T03:21:59.079
Aborted (core dumped)
{code}
(The coredump is available)

From the core dump I got
{code}
(gdb) where
#0  0x00007ff6d1cdf428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007ff6d1ce102a in __GI_abort () at abort.c:89
#2  0x00007ff6d042784d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007ff6d04256b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007ff6d0425701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007ff6d042623f in __cxa_pure_virtual () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007ff5536200d5 in rocksdb::DBImpl::CloseHelper() () from /tmp/junit2957569837385648710/junit7227349420023790038/librocksdbjni-linux64.so
#7  0x00007ff55362b17b in rocksdb::DBImpl::~DBImpl() () from /tmp/junit2957569837385648710/junit7227349420023790038/librocksdbjni-linux64.so
#8  0x00007ff55362b451 in rocksdb::DBImpl::~DBImpl() () from /tmp/junit2957569837385648710/junit7227349420023790038/librocksdbjni-linux64.so
#9  0x00007ff6ba4b4470 in ?? ()
#10 0x0000000085721e58 in ?? ()
#11 0x00007ff6ba2e2544 in ?? ()
#12 0x00007ff5dccad930 in ?? ()
#13 0x0000000085721938 in ?? ()
#14 0x00007ff6ba450f60 in ?? ()
#15 0x0000000000000000 in ?? ()
{code};;;","17/Jun/20 07:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3653&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=66b5c59a-0094-561d-0e44-b149dfdd586d;;;","17/Jun/20 07:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3653&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=b33fdd4f-3de5-542e-2624-5d53167bb672;;;","17/Jun/20 07:49;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3653&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708;;;","17/Jun/20 07:50;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3652&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034;;;","17/Jun/20 07:50;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3652&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada;;;","17/Jun/20 07:54;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3630&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0;;;","17/Jun/20 07:59;chesnay;2 changes have been made recently in the RocksDB module; [~rmetzger] could you check whether reverting any of these resolves the issue?;;;","17/Jun/20 08:14;rmetzger;This CI run only runs the ""core"" module (where the issue occurs), with the two recent changes (from FLINK-17800) reverted: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8176&view=results;;;","17/Jun/20 10:39;liyu;bq. This CI run only runs the ""core"" module (where the issue occurs), with the two recent changes (from FLINK-17800) reverted: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8176&view=results

[~rmetzger] Just for double check, could we also try this ""core"" module test multiple times with changes from FLINK-17800? Thanks.

PS. [~yunta] has already downloaded the coredump file and now debugging.;;;","17/Jun/20 11:11;rmetzger;Thank you for looking into this. [~yunta] Can I assign you to the ticket?

[~liyu] I'm running FLINK-17800 also multiple times to make sure I can reproduce the issue: So far w/o luck.;;;","17/Jun/20 11:26;rmetzger;With the revert, I had 16 core stages passing.
Without the revert, I had the error once out of 9 successful runs (with 2 unrelated failures).
This means: I can reproduce the issue on the Azure-hosted personal accounts, and the probability that FLINK-17800 causes the issue is high.
Once I have more runs confirming the revert, shall we merge it?;;;","17/Jun/20 11:47;liyu;Thanks for the efforts [~rmetzger]. +1 on merging the revert according to the testing result.;;;","17/Jun/20 11:48;liyu;Reassigning to [~yunta] w/ offline confirmation.;;;","17/Jun/20 12:36;rmetzger;I reverted the change and reopened FLINK-17800.

I'll leave it to you whether you want to close this ticket now or not.;;;","17/Jun/20 17:37;pnowojski;Thanks [~rmetzger] for fixing the build stability. Closing this issue.;;;","22/Jun/20 08:06;yunta;I have figured out why this happened and this [success CI|https://dev.azure.com/myasuka/flink/_build/results?buildId=157&view=results] of multi core modules could also prove it.

The root cause: newly added test {{RocksDBStateMisuseOptionTest}} forgets to dispose {{RocksDBKeyedStateBackend}}.

Code below with frocksdbjni of 5.17.2-artisans-2.0 could reproduce this:
{code:java}
NativeLibraryLoader.getInstance().loadLibrary(""/tmp/rocksdb-lib"");

List<ColumnFamilyHandle> cf = new ArrayList<>(1);
try (DBOptions options = new DBOptions().setCreateIfMissing(true);
     ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();
     RocksDB rocksdb = RocksDB.open(options,
             ""/tmp/rocksdb-2"",
             Collections.singletonList(new ColumnFamilyDescriptor(""default"".getBytes(), columnFamilyOptions)),
             cf)) {
    rocksdb.put(ByteBuffer.allocate(4).array(), ByteBuffer.allocate(4).array());
}
{code}
RocksDB-java use [#finalize|https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#memory-management] to release C++ object when Java starts GC. However, if we do not destroy column family handle before destroying RocksDB, the [assert|https://github.com/dataArtisans/frocksdb/blob/49bc897d5d768026f1eb816d960c1f2383396ef4/db/column_family.cc#L1238] would fail at [versions reset|https://github.com/dataArtisans/frocksdb/blob/49bc897d5d768026f1eb816d960c1f2383396ef4/db/db_impl.cc#L515] when DB closing and we cannot ensure the order of GC, that's why sometimes the CI would fail.

I'll create a new PR to fix FLINK-17800 and avoid this problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointFailureManager forgets failed checkpoints after a successful one,FLINK-18336,13311755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,16/Jun/20 15:36,02/Jul/20 09:53,13/Jul/23 08:12,02/Jul/20 09:53,,,,,,1.12.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"To my understanding, failure shouldn't be counted more than once for a single checkpoint.

However, after a successful checkpoint, all previous failures are cleared.

So this test will currently fail:

 
{code:java}
TestFailJobCallback callback = new TestFailJobCallback();
CheckpointFailureManager failureManager = new CheckpointFailureManager(2, callback);

failureManager.handleJobLevelCheckpointException(new CheckpointException(CHECKPOINT_EXPIRED), 1L);
failureManager.handleJobLevelCheckpointException(new CheckpointException(CHECKPOINT_EXPIRED), 2L);

failureManager.handleCheckpointSuccess(2L);
failureManager.handleJobLevelCheckpointException(new CheckpointException(CHECKPOINT_EXPIRED), 3L);
failureManager.handleJobLevelCheckpointException(new CheckpointException(CHECKPOINT_EXPIRED), 4L);

// shouldn't be counted because 1L has already failed:
failureManager.handleJobLevelCheckpointException(new CheckpointException(CHECKPOINT_EXPIRED), 1L); 

assertEquals(0, callback.getInvokeCounter());{code}
 ",,roman,sewen,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 09:53:35 UTC 2020,,,,,,,,,,"0|z0fwk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 21:34;sewen;What is the problem exactly? So far it was always the expected behavior that after a successful checkpoint, the failed ones are forgotton (failure counter reset).;;;","18/Jun/20 08:03;roman;What if after a successful checkpoint, failureManager is notified about a failure that it has already seen?

I've added an example unit test in the description to demonstrate this better.;;;","18/Jun/20 11:49;sewen;This means a failure for a checkpoint that is already subsumed by a newer checkpoint (higher checkpoint ID) ?

In that case, should the failure manager maintain something like ""latest successful checkpoint"" and ignore all failures / messages from older (subsumed) checkpoint attempts?

That way we would preserve the notion of tracking ""consecutive checkpoint failures"" and we would not need to keep a lot of information from earlier checkpoints.;;;","18/Jun/20 13:26;roman;> This means a failure for a checkpoint that is already subsumed by a newer checkpoint (higher checkpoint ID) ?

Yes, and the other case is decline of an older savepoint (which I believe are not subsumed).

 

> In that case, should the failure manager maintain something like ""latest successful checkpoint"" and ignore all failures / messages from older (subsumed) checkpoint attempts?

It should work and it's quite simple and efficient.;;;","23/Jun/20 17:37;roman;I published a PR with this change: [https://github.com/apache/flink/pull/12751];;;","02/Jul/20 09:53;zjwang;Merged in master: 06af98a6e0247dbf1b08f5caf6ca67f2a56dd8e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 NotifyCheckpointAbortedITCase.testNotifyCheckpointAborted time outs,FLINK-18335,13311753,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,pnowojski,pnowojski,16/Jun/20 15:27,29/Jun/21 03:03,13/Jul/23 08:12,29/Jun/21 03:03,1.12.0,1.13.0,,,,,,,Runtime / Checkpointing,Tests,,,,0,auto-deprioritized-critical,auto-unassigned,pull-request-available,stale-major,test-stability,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3582&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0


{noformat}
[ERROR] Errors: 
[ERROR]   NotifyCheckpointAbortedITCase.testNotifyCheckpointAborted:182->verifyAllOperatorsNotifyAborted:195->Object.wait:502->Object.wait:-2 » TestTimedOut
{noformat}

CC [~yunta]
",,dian.fu,pnowojski,rmetzger,trohrmann,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 29 03:03:12 UTC 2021,,,,,,,,,,"0|z0fwjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/20 02:26;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3928&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=baf26b34-3c6a-54e8-f93f-cf269b32f802;;;","08/Jul/20 02:06;dian.fu;Another instance on 1.11: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4315&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=94b2a454-a9e3-5226-421d-758b172639ef];;;","14/Jul/20 03:05;dian.fu;Another case on master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4464&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","22/Jul/20 15:00;rmetzger;What's the status of this ticket? Since it is a critical, reoccurring build instability, we should address it soon.;;;","23/Jul/20 02:49;yunta;[~rmetzger] I cannot produce this locally and I also try to trigger CI of tests more than 40+ times ( https://dev.azure.com/myasuka/flink/_build/results?buildId=183&view=results and more), however, still cannot reproduce it.

Since notify checkpoint abort message is sent asynchronously, there existed the possibility for time out. I think we could create another PR to increase the time out and add necessary logs so that we could know what's wrong if meet this error next time, what do you think?;;;","23/Jul/20 15:26;rmetzger;Thanks for looking into this!
Increasing the timeout and adding additional debugging information both sounds good to me!;;;","07/Sep/20 08:21;rmetzger;[~yunta] shall we close this ticket? It seems that the issue is not occurring anymore. Maybe it has been fixed in the meantime?;;;","08/Sep/20 03:55;yunta;[~rmetzger], I am preparing a PR to implement what I commented before, and we could close this ticket once that one merged.;;;","08/Sep/20 06:23;rmetzger;Okay, that's even better. Thanks.;;;","15/Sep/20 09:15;rmetzger;[~yunta] added more debugging logs to the test: https://github.com/apache/flink/commit/168ba42585b03902a027f63dcd1d7aaad152bd12

Please re-open the ticket once the test has failed again.;;;","31/Mar/21 10:11;trohrmann;Another instance occurred: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15809&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4423;;;","16/Apr/21 10:54;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 22:53;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","20/May/21 10:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","27/May/21 23:05;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","28/Jun/21 10:44;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Jun/21 12:25;yunta;I think we might better go to https://issues.apache.org/jira/browse/FLINK-20816 for further discussion if problem still existed. [~trohrmann];;;","28/Jun/21 14:49;trohrmann;If the failed test did not include the fixes of FLINK-20816 and if we are sure that these fixes fix the problem, then I am fine closing this ticket [~yunta].;;;","29/Jun/21 03:03;yunta;I think the failed case on March did not include the fix and I will close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UnsignedTypeConversionITCase failed caused by MariaDB4j ""Asked to waitFor Program""",FLINK-18333,13311737,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,jark,jark,16/Jun/20 14:25,18/Sep/20 06:22,13/Jul/23 08:12,18/Sep/20 05:20,1.12.0,,,,,1.12.0,,,Connectors / JDBC,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8173&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa
{code}
2020-06-16T08:23:26.3013987Z [INFO] Running org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase
2020-06-16T08:23:30.2252334Z Tue Jun 16 08:23:30 UTC 2020 Thread[main,5,main] java.lang.NoSuchFieldException: DEV_NULL
2020-06-16T08:23:31.2907920Z ----------------------------------------------------------------
2020-06-16T08:23:31.2913806Z Tue Jun 16 08:23:30 UTC 2020:
2020-06-16T08:23:31.2914839Z Booting Derby version The Apache Software Foundation - Apache Derby - 10.14.2.0 - (1828579): instance a816c00e-0172-bc39-e4b1-00000e4ce818 
2020-06-16T08:23:31.2915845Z on database directory memory:/__w/1/s/flink-connectors/flink-connector-jdbc/target/test with class loader sun.misc.Launcher$AppClassLoader@677327b6 
2020-06-16T08:23:31.2916637Z Loaded from file:/__w/1/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar
2020-06-16T08:23:31.2916968Z java.vendor=Private Build
2020-06-16T08:23:31.2917461Z java.runtime.version=1.8.0_242-8u242-b08-0ubuntu3~16.04-b08
2020-06-16T08:23:31.2922200Z user.dir=/__w/1/s/flink-connectors/flink-connector-jdbc/target
2020-06-16T08:23:31.2922516Z os.name=Linux
2020-06-16T08:23:31.2922709Z os.arch=amd64
2020-06-16T08:23:31.2923086Z os.version=4.15.0-1083-azure
2020-06-16T08:23:31.2923316Z derby.system.home=null
2020-06-16T08:23:31.2923616Z derby.stream.error.field=org.apache.flink.connector.jdbc.JdbcTestBase.DEV_NULL
2020-06-16T08:23:31.2924790Z Database Class Loader started - derby.database.classpath=''
2020-06-16T08:23:37.4354243Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.133 s - in org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase
2020-06-16T08:23:38.1880075Z [INFO] Running org.apache.flink.connector.jdbc.table.JdbcTableSourceITCase
2020-06-16T08:23:41.3718038Z Tue Jun 16 08:23:41 UTC 2020 Thread[main,5,main] java.lang.NoSuchFieldException: DEV_NULL
2020-06-16T08:23:41.4383244Z ----------------------------------------------------------------
2020-06-16T08:23:41.4401761Z Tue Jun 16 08:23:41 UTC 2020:
2020-06-16T08:23:41.4402797Z Booting Derby version The Apache Software Foundation - Apache Derby - 10.14.2.0 - (1828579): instance a816c00e-0172-bc3a-103b-00000e4b0610 
2020-06-16T08:23:41.4403758Z on database directory memory:/__w/1/s/flink-connectors/flink-connector-jdbc/target/test with class loader sun.misc.Launcher$AppClassLoader@677327b6 
2020-06-16T08:23:41.4404581Z Loaded from file:/__w/1/.m2/repository/org/apache/derby/derby/10.14.2.0/derby-10.14.2.0.jar
2020-06-16T08:23:41.4404945Z java.vendor=Private Build
2020-06-16T08:23:41.4405497Z java.runtime.version=1.8.0_242-8u242-b08-0ubuntu3~16.04-b08
2020-06-16T08:23:41.4406048Z user.dir=/__w/1/s/flink-connectors/flink-connector-jdbc/target
2020-06-16T08:23:41.4406303Z os.name=Linux
2020-06-16T08:23:41.4406494Z os.arch=amd64
2020-06-16T08:23:41.4406878Z os.version=4.15.0-1083-azure
2020-06-16T08:23:41.4407097Z derby.system.home=null
2020-06-16T08:23:41.4407415Z derby.stream.error.field=org.apache.flink.connector.jdbc.JdbcTestBase.DEV_NULL
2020-06-16T08:23:41.5287219Z Database Class Loader started - derby.database.classpath=''
2020-06-16T08:23:46.4567063Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 23.729 s <<< FAILURE! - in org.apache.flink.connector.jdbc.table.UnsignedTypeConversionITCase
2020-06-16T08:23:46.4575785Z [ERROR] org.apache.flink.connector.jdbc.table.UnsignedTypeConversionITCase  Time elapsed: 23.729 s  <<< ERROR!
2020-06-16T08:23:46.4576490Z ch.vorburger.exec.ManagedProcessException: An error occurred while running a command: create database if not exists `test`;
2020-06-16T08:23:46.4577193Z 	at ch.vorburger.mariadb4j.DB.run(DB.java:300)
2020-06-16T08:23:46.4577537Z 	at ch.vorburger.mariadb4j.DB.run(DB.java:265)
2020-06-16T08:23:46.4577861Z 	at ch.vorburger.mariadb4j.DB.run(DB.java:269)
2020-06-16T08:23:46.4578212Z 	at ch.vorburger.mariadb4j.DB.createDB(DB.java:308)
2020-06-16T08:23:46.4578611Z 	at ch.vorburger.mariadb4j.junit.MariaDB4jRule.initDB(MariaDB4jRule.java:55)
2020-06-16T08:23:46.4579084Z 	at ch.vorburger.mariadb4j.junit.MariaDB4jRule.before(MariaDB4jRule.java:50)
2020-06-16T08:23:46.4579547Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
2020-06-16T08:23:46.4579987Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-16T08:23:46.4580440Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-16T08:23:46.4580850Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-16T08:23:46.4581244Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-16T08:23:46.4581697Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-16T08:23:46.4582207Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-16T08:23:46.4583149Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-16T08:23:46.4583724Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-16T08:23:46.4584271Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-16T08:23:46.4584832Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-16T08:23:46.4585362Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-16T08:23:46.4585967Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-16T08:23:46.4587306Z Caused by: ch.vorburger.exec.ManagedProcessException: Asked to waitFor Program [/tmp/MariaDB4j/base/bin/mysql, --socket=/tmp/MariaDB4j.42725.sock] (in working directory /tmp/MariaDB4j/base), but it was never even start()'ed!
2020-06-16T08:23:46.4588006Z 	at ch.vorburger.exec.ManagedProcess.assertWaitForIsValid(ManagedProcess.java:478)
2020-06-16T08:23:46.4588500Z 	at ch.vorburger.exec.ManagedProcess.waitForExitMaxMsWithoutLog(ManagedProcess.java:439)
2020-06-16T08:23:46.4588978Z 	at ch.vorburger.exec.ManagedProcess.waitForExit(ManagedProcess.java:418)
2020-06-16T08:23:46.4589379Z 	at ch.vorburger.mariadb4j.DB.run(DB.java:298)
2020-06-16T08:23:46.4589645Z 	... 18 more
2020-06-16T08:23:46.4600716Z 
2020-06-16T08:23:47.8874730Z [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.698 s - in org.apache.flink.connector.jdbc.table.JdbcTableSourceITCase
2020-06-16T08:23:48.2451996Z [INFO] Running org.apache.flink.connector.jdbc.table.JdbcUpsertTableSinkITCase
2020-06-16T08:23:48.9339094Z [INFO] Running org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase
2020-06-16T08:24:06.7722329Z [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.526 s - in org.apache.flink.connector.jdbc.table.JdbcUpsertTableSinkITCase
2020-06-16T08:24:07.8245499Z [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.889 s - in org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase
2020-06-16T08:24:08.1984427Z [INFO] 
2020-06-16T08:24:08.1984773Z [INFO] Results:
2020-06-16T08:24:08.1984960Z [INFO] 
2020-06-16T08:24:08.1985153Z [ERROR] Errors: 
2020-06-16T08:24:08.1991115Z [ERROR]   UnsignedTypeConversionITCase.org.apache.flink.connector.jdbc.table.UnsignedTypeConversionITCase Â» ManagedProcess
{code}",,dian.fu,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 06:22:14 UTC 2020,,,,,,,,,,"0|z0fwg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 14:26;jark;There is a trick may solve this problem shared in https://github.com/vorburger/MariaDB4j/issues/233#issuecomment-588573085;;;","07/Aug/20 01:49;dian.fu;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5259&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407;;;","11/Aug/20 02:26;dian.fu;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5372&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407];;;","11/Aug/20 08:01;leonard;It seems a mariadb4j's unstable issue bu have no solution after simple investigation, I'd like and some retry logic to avoid this unstable problem.

HDYT? [~jark] [~dian.fu];;;","11/Aug/20 08:01;leonard;I'd like to take this ticket.;;;","11/Aug/20 08:07;dian.fu;Thanks [~Leonard Xu] I have assigned the ticket to you.;;;","25/Aug/20 07:39;jark;Hi [~Leonard Xu], what is the status of this issue now? ;;;","25/Aug/20 07:42;leonard;[~jark]

Thanks for the reminder, I'll open the PR soon.;;;","07/Sep/20 12:07;jark;Fixed in master (1.12.0): ad4246b623243d710d5da7f153437426d8de0c4c;;;","07/Sep/20 12:07;jark;I merged the #13244 pull request. Let's close this ticket if we don't see the failures in one or more weeks. ;;;","18/Sep/20 05:20;dian.fu;Thanks [~Leonard Xu] [~jark] for the fix and review. This issue doesn't occur for more than one week. I'm closing this ticket!;;;","18/Sep/20 06:13;jark;Thanks [~dian.fu];;;","18/Sep/20 06:22;leonard;Thanks [~dian.fu] [~jark] :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive NOTICE issues,FLINK-18331,13311720,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,16/Jun/20 13:14,17/Jun/20 09:05,13/Jul/23 08:12,17/Jun/20 09:05,1.11.0,,,,,1.11.0,,,Connectors / Hive,,,,,0,,,,,,1.2/2.2 NOTICE entries are not sorted alphabetically.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 09:05:21 UTC 2020,,,,,,,,,,"0|z0fwc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 09:05;chesnay;master: efc4c0cbecb6c7282cb17bc53818188caf5d7ced
1.11: dc8b61bd9cc3bb0e22100eea14756c649ef1f32d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python NOTICE issues,FLINK-18330,13311719,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dian.fu,chesnay,chesnay,16/Jun/20 13:12,17/Jun/20 12:29,13/Jul/23 08:12,17/Jun/20 12:29,1.11.0,,,,,1.11.0,,,API / Python,Build System,,,,0,pull-request-available,,,,,"beam-runners-core-java / beam-vendor-bytebuddy bundled but not listed
protobuf-java-util listed but not bundled

The NOTICE file additionally lists various dependencies that are bundled by beam. While this is fine, the lack of separation makes verification difficult.

These would be the entries for directly bundled dependencies:
{code}
This project bundles the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.fasterxml.jackson.core:jackson-annotations:2.10.1
- com.fasterxml.jackson.core:jackson-core:2.10.1
- com.fasterxml.jackson.core:jackson-databind:2.10.1
- com.google.flatbuffers:flatbuffers-java:1.9.0
- io.netty:netty-buffer:4.1.44.Final
- io.netty:netty-common:4.1.44.Final
- joda-time:joda-time:2.5
- org.apache.arrow:arrow-format:0.16.0
- org.apache.arrow:arrow-memory:0.16.0
- org.apache.arrow:arrow-vector:0.16.0
- org.apache.beam:beam-model-fn-execution:2.19.0
- org.apache.beam:beam-model-job-management:2.19.0
- org.apache.beam:beam-model-pipeline:2.19.0
- org.apache.beam:beam-runners-core-construction-java:2.19.0
- org.apache.beam:beam-runners-java-fn-execution:2.19.0
- org.apache.beam:beam-sdks-java-core:2.19.0
- org.apache.beam:beam-sdks-java-fn-execution:2.19.0
- org.apache.beam:beam-vendor-grpc-1_21_0:0.1
- org.apache.beam:beam-vendor-guava-26_0-jre:0.1
- org.apache.beam:beam-vendor-sdks-java-extensions-protobuf:2.19.0

This project bundles the following dependencies under the BSD license.
See bundled license files for details

- net.sf.py4j:py4j:0.10.8.1
- com.google.protobuf:protobuf-java:3.7.1

This project bundles the following dependencies under the MIT license. (https://opensource.org/licenses/MIT)
See bundled license files for details.

- net.razorvine:pyrolite:4.13
{code}

These are the ones that are (supposedly) bundled by beam, which would need additional verification:
{code}
The bundled Apache Beam dependencies bundle the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.google.api.grpc:proto-google-common-protos:1.12.0
- com.google.code.gson:gson:2.7
- com.google.guava:guava:26.0-jre
- io.grpc:grpc-auth:1.21.0
- io.grpc:grpc-core:1.21.0
- io.grpc:grpc-context:1.21.0
- io.grpc:grpc-netty:1.21.0
- io.grpc:grpc-protobuf:1.21.0
- io.grpc:grpc-stub:1.21.0
- io.grpc:grpc-testing:1.21.0
- io.netty:netty-buffer:4.1.34.Final
- io.netty:netty-codec:4.1.34.Final
- io.netty:netty-codec-http:4.1.34.Final
- io.netty:netty-codec-http2:4.1.34.Final
- io.netty:netty-codec-socks:4.1.34.Final
- io.netty:netty-common:4.1.34.Final
- io.netty:netty-handler:4.1.34.Final
- io.netty:netty-handler-proxy:4.1.34.Final
- io.netty:netty-resolver:4.1.34.Final
- io.netty:netty-transport:4.1.34.Final
- io.netty:netty-transport-native-epoll:4.1.34.Final
- io.netty:netty-transport-native-unix-common:4.1.34.Final
- io.netty:netty-tcnative-boringssl-static:2.0.22.Final
- io.opencensus:opencensus-api:0.21.0
- io.opencensus:opencensus-contrib-grpc-metrics:0.21.0

The bundled Apache Beam dependencies bundle the following dependencies under the BSD license.
See bundled license files for details

- com.google.protobuf:protobuf-java-util:3.7.1
- com.google.auth:google-auth-library-credentials:0.13.0
{code}",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 12:29:43 UTC 2020,,,,,,,,,,"0|z0fwc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 12:29;dian.fu;master: dc84538930d79e67144d1198d37cb5bc54e070fc
release-1.11: d94219da94d4ebe868c2b69405b263694959288c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dist NOTICE issues,FLINK-18329,13311715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,16/Jun/20 13:08,17/Jun/20 09:05,13/Jul/23 08:12,17/Jun/20 09:04,1.10.0,,,,,1.10.2,1.11.0,,Build System,Runtime / Coordination,,,,0,,,,,,akka-actor version incorrect 2.5.1 -> 2.5.21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 09:04:50 UTC 2020,,,,,,,,,,"0|z0fwb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 09:04;chesnay;master: 630728a80934ba3f819b9096d54f31e351696d39
1.11: 17e471d241573a50fa855a6b78fbf0ebd98585e2
1.10: b1fdf6dda30d981f3aa6ba2ea9c5cb8a978fd8e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner NOTICE issues,FLINK-18328,13311714,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,16/Jun/20 13:07,17/Jun/20 09:05,13/Jul/23 08:12,17/Jun/20 09:05,1.11.0,,,,,1.11.0,,,Build System,Table SQL / Planner,,,,0,,,,,,"not actually bundled: asm, json-smart, accessors-smart, joda-time",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 09:05:51 UTC 2020,,,,,,,,,,"0|z0fwaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 09:05;chesnay;master: 05f2c40d01c0b4b526e406604bbceb52ac864c0f
1.11: 73c0a55a31cc4370a91ad1236b0d923d29cf689e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes NOTICE issues,FLINK-18326,13311712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Jun/20 13:06,17/Jun/20 09:05,13/Jul/23 08:12,17/Jun/20 09:05,1.11.0,,,,,1.11.0,,,Build System,,,,,0,,,,,,"snakeyaml: 1.23 -> 1.24
logging-interceptor: 3.12.0 -> 3.12.6
generex is actually excluded",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 09:05:39 UTC 2020,,,,,,,,,,"0|z0fwag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 09:05;chesnay;master: 7a6a82f229c3bbd5da0d8d9d46e576b6229c87f9
1.11: 8649bde18a2c00351f730d35de358dca034c27b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE may throw when use SqlDataTypeSpec#getNullable,FLINK-18325,13311687,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Aron.tao,Aron.tao,Aron.tao,16/Jun/20 10:54,04/Nov/20 02:17,13/Jul/23 08:12,04/Nov/20 02:17,,,,,,1.12.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"SqlMapTypeNameSpec#unparse calls SqlDataTypeSpec#getNullable, and ""getNullable"" may throw NPE
{code:java}
if (!keyType.getNullable()) { 
  writer.keyword(""NOT NULL""); 
}
{code}
 

See in  SqlDataTypeSpec 
{code:java}
/** Whether data type allows nulls.
 *
 * <p>Nullable is nullable! Null means ""not specified"". E.g.
 * {@code CAST(x AS INTEGER)} preserves the same nullability as {@code x}.
 */
private Boolean nullable;
{code}
 

This API is from calcite, and all callers will determine if it is null:

!image-2020-06-16-18-53-17-462.png|width=488,height=114!

 

 ",,Aron.tao,jark,leonard,libenchao,lsy,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/20 10:53;Aron.tao;image-2020-06-16-18-53-17-462.png;https://issues.apache.org/jira/secure/attachment/13005782/image-2020-06-16-18-53-17-462.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 02:17:02 UTC 2020,,,,,,,,,,"0|z0fw4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 10:57;Aron.tao;Not only org.apache.flink.sql.parser.type.SqlMapTypeNameSpec#unparse, other method will call this too, I'll give a pr.;;;","16/Jun/20 12:51;ykt836;Thanks [~Aron.tao], assigning this to you.;;;","02/Jul/20 06:01;Aron.tao;Can anybody take a look at this PR?;;;","04/Nov/20 02:17;jark;Fixed in master: 9652b6db4130de15dc8b9889b8ceba832bc41ba7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unstable ExecutorNotifierTest#testExceptionInHandler.,FLINK-18322,13311660,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,16/Jun/20 08:34,16/Jun/20 08:58,13/Jul/23 08:12,16/Jun/20 08:58,,,,,,1.11.0,,,Connectors / Common,,,,,0,pull-request-available,,,,,The {{ExecutorNotifierTest#testExceptionInHandler()}} fails intermittently because the {{UncaughtExceptionHandler}} may fire after the {{ExecutorService}} has  shutdown. The fix is to ensure only check the exception after the {{UncaughtExceptionHandler}} has fired.,,becket_qin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 08:58:09 UTC 2020,,,,,,,,,,"0|z0fvyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 08:58;becket_qin;Merged.

master: 8e2af35314384eead43ae2582664404433ac60ab
release-1.11: 24ab44cbb5e675d868a6fceb3fb25185156f426a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-sql-connector-hive modules should merge hive-exec dependencies,FLINK-18320,13311655,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Jun/20 08:09,28/Jun/20 01:57,13/Jul/23 08:12,23/Jun/20 10:54,,,,,,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"Since hive-exec is a bundle jar, we should merge the bundle dependencies from hive-exec.",,libenchao,lirui,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 10:54:10 UTC 2020,,,,,,,,,,"0|z0fvxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 08:44;rmetzger;The maven shade plugin lists the following included dependencies
{code}
[INFO] Including org.apache.flink:flink-connector-hive_2.11:jar:1.12-SNAPSHOT in the shaded jar.
[INFO] Including org.apache.hive:hive-exec:jar:1.2.2 in the shaded jar.
[INFO] Including org.apache.hive:hive-metastore:jar:1.2.2 in the shaded jar.
[INFO] Including org.apache.thrift:libfb303:jar:0.9.2 in the shaded jar.
[INFO] Including org.apache.orc:orc-core:jar:nohive:1.4.3 in the shaded jar.
[INFO] Including io.airlift:aircompressor:jar:0.8 in the shaded jar.
{code}

However, the final jar contains at least
{code}
./minlog-1.2.jar
./reflectasm-1.07-shaded.jar
./objenesis-1.2.jar
{code}
which are not mentioned in our {{META-INF/NOTICE}}.;;;","16/Jun/20 12:23;lirui;{{minlog}}, {{reflectasm}} and {{objenesis}} are introduced by {{kryo}} which itself is also a uber jar.;;;","23/Jun/20 10:54;chesnay;master:
3611fd74b1f9344191fc2d7626c9be667a56899f
637d83e0c7216a0918cfd1e68f4c40fa7f2c63a6
1.11:
bd6c1b2730024ea87f21eff34f0a862154e8e54c
e9afcf09e0152a8379756b74cea3dde8a8e8ec30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lack LICENSE.protobuf in flink-sql-orc,FLINK-18319,13311654,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Jun/20 08:07,18/Jun/20 03:39,13/Jul/23 08:12,18/Jun/20 03:39,1.11.0,,,,,1.11.0,,,Connectors / ORC,,,,,0,pull-request-available,,,,,flink-sql-orc bundle protobuf but not include LICENSE.protobuf.,,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18317,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 03:39:41 UTC 2020,,,,,,,,,,"0|z0fvxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/20 03:39;lzljs3620320;master: ec9f68dff32626e62b2b28e457dd49f7f359c9eb

release-1.11: 0dafcf8792220dbe5b77544261f726a566b054f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong documentation in Kafka SQL Connector page,FLINK-18314,13311611,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,shen_dijie,shen_dijie,16/Jun/20 03:08,16/Jun/20 16:30,13/Jul/23 08:12,16/Jun/20 13:06,1.11.0,1.12.0,,,,1.11.0,,,Documentation,,,,,0,,,,,,"In this page [https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html.|https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html]

Maven dependency should be flink-connector-kafka-0.11 instead of flink-connector-kafka-011, which is missing `.` 

flink-connector-kafka-010_2.11 has the same problem.

I read the source code, the content of kafka.md is wrong.

In the same page,DDL should be 

{{`properties.bootstrap.servers` instead of }}{{properties.bootstrap.server.}}

{{when i used}} {{properties.bootstrap.server,i got a exception :}}

Caused by: org.apache.flink.table.api.ValidationException: One or more required options are missing.

Missing required options are:

properties.bootstrap.servers",,jark,libenchao,liufangliang,shen_dijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 13:06:38 UTC 2020,,,,,,,,,,"0|z0fvo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 03:12;shen_dijie;sorry ,i change the wrong state . i mean it's not fixed now 
 ;;;","16/Jun/20 12:24;liufangliang;+1 ,

a small mistake in kafka.md  and kafka.zh.md  ,  as following 
{code:java}
'properties.bootstrap.server' = 'localhost:9092'
{code}
The correct one should be like this
{code:java}
'properties.bootstrap.servers' = 'localhost:9092'
{code}
cc [~jark]  [~shen_dijie] , can you assign it to me ?

 ;;;","16/Jun/20 12:31;jark;Thanks for reporting this [~shen_dijie] [~liufangliang], I will quickly fix this with other documentation PRs. ;;;","16/Jun/20 12:33;liufangliang;okey , thanks for [~jark] your reply .;;;","16/Jun/20 13:06;jark;- master (1.12.0): 2bb108377a964dea4ef7119c48f281ede067e88f
- 1.11.0: e811e2b4dc5b8637353f53b7bb19b82bdec8d1be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingKafkaITCase stalls indefinitely,FLINK-18311,13311597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,dian.fu,dian.fu,16/Jun/20 01:53,16/Jun/20 17:54,13/Jul/23 08:12,16/Jun/20 13:57,1.11.0,1.12.0,,,,1.11.0,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,,"CI: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3537&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee]

{code}
020-06-15T21:01:59.0792207Z [INFO] org.apache.flink:flink-sql-connector-kafka-0.10_2.11:1.11-SNAPSHOT:jar already exists in /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/target/dependencies
2020-06-15T21:01:59.0793580Z [INFO] org.apache.flink:flink-sql-connector-kafka-0.11_2.11:1.11-SNAPSHOT:jar already exists in /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/target/dependencies
2020-06-15T21:01:59.0794931Z [INFO] org.apache.flink:flink-sql-connector-kafka_2.11:1.11-SNAPSHOT:jar already exists in /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/target/dependencies
2020-06-15T21:01:59.0795686Z [INFO] 
2020-06-15T21:01:59.0796403Z [INFO] --- maven-surefire-plugin:2.22.1:test (end-to-end-tests) @ flink-end-to-end-tests-common-kafka ---
2020-06-15T21:01:59.0869911Z [INFO] 
2020-06-15T21:01:59.0871981Z [INFO] -------------------------------------------------------
2020-06-15T21:01:59.0874203Z [INFO]  T E S T S
2020-06-15T21:01:59.0875086Z [INFO] -------------------------------------------------------
2020-06-15T21:02:00.0134000Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-06-15T21:45:33.4889677Z ##[error]The operation was canceled.
2020-06-15T21:45:33.4902658Z ##[section]Finishing: Run e2e tests
2020-06-15T21:45:33.5058601Z ##[section]Starting: Cache Maven local repo
2020-06-15T21:45:33.5164621Z ==============================================================================
2020-06-15T21:45:33.5164972Z Task         : Cache
2020-06-15T21:45:33.5165250Z Description  : Cache files between runs
2020-06-15T21:45:33.5165497Z Version      : 2.0.1
2020-06-15T21:45:33.5165769Z Author       : Microsoft Corporation
2020-06-15T21:45:33.5166079Z Help         : https://aka.ms/pipeline-caching-docs
2020-06-15T21:45:33.5166442Z ==============================================================================
2020-06-15T21:45:34.0475096Z ##[section]Finishing: Cache Maven local repo
2020-06-15T21:45:34.0502436Z ##[section]Starting: Checkout flink-ci/flink-mirror@release-1.11 to s
2020-06-15T21:45:34.0506976Z ==============================================================================
2020-06-15T21:45:34.0507297Z Task         : Get sources
2020-06-15T21:45:34.0507642Z Description  : Get sources from a repository. Supports Git, TfsVC, and SVN repositories.
2020-06-15T21:45:34.0507965Z Version      : 1.0.0
2020-06-15T21:45:34.0508198Z Author       : Microsoft
2020-06-15T21:45:34.0508559Z Help         : [More Information](https://go.microsoft.com/fwlink/?LinkId=798199)
2020-06-15T21:45:34.0508934Z ==============================================================================
2020-06-15T21:45:34.3924966Z Cleaning any cached credential from repository: flink-ci/flink-mirror (GitHub)
2020-06-15T21:45:34.3990430Z ##[section]Finishing: Checkout flink-ci/flink-mirror@release-1.11 to s
2020-06-15T21:45:34.4049857Z ##[section]Starting: Finalize Job
2020-06-15T21:45:34.4086754Z Cleaning up task key
2020-06-15T21:45:34.4087951Z Start cleaning up orphan processes.
2020-06-15T21:45:34.4481307Z Terminate orphan process: pid (11772) (java)
2020-06-15T21:45:34.4548480Z Terminate orphan process: pid (12132) (java)
2020-06-15T21:45:34.4632331Z Terminate orphan process: pid (30726) (bash)
2020-06-15T21:45:34.4660351Z Terminate orphan process: pid (30728) (bash)
2020-06-15T21:45:34.4710124Z Terminate orphan process: pid (68958) (java)
2020-06-15T21:45:34.4751577Z Terminate orphan process: pid (119102) (java)
2020-06-15T21:45:34.4800161Z Terminate orphan process: pid (129546) (sh)
2020-06-15T21:45:34.4830588Z Terminate orphan process: pid (129548) (java)
2020-06-15T21:45:34.4833955Z ##[section]Finishing: Finalize Job
2020-06-15T21:45:34.4877321Z ##[section]Finishing: e2e_ci
{code}",,aljoscha,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15362,FLINK-17327,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 17:54:17 UTC 2020,,,,,,,,,,"0|z0fvlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 01:58;dian.fu;release-1.11: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3534&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3537&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3542&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee]

master:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3536&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3541&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee;;;","16/Jun/20 02:12;dian.fu;It seems caused by the PR: [https://github.com/apache/flink/pull/12589]

cc [~aljoscha];;;","16/Jun/20 06:42;rmetzger;All the builds are failing with this. Upgrading to blocker;;;","16/Jun/20 07:31;rmetzger;Shall we disable the test via hotfix?;;;","16/Jun/20 07:41;chesnay;I'd rather revert the version bump.;;;","16/Jun/20 13:57;aljoscha;master:
- f01cac8d5e7e4b47034c7d6b518cdac2bc8806c2
- ab7b8c45ce21d9c823e8e4214a9ad68bf79a511b

release-1.11:
- f2a2b5aed2751d055932efb813ba809cee304692
- 635098515631544899ac1e7a7a6a2ddf2e9023be;;;","16/Jun/20 17:54;rmetzger;In retrospect, I believe we should have reverted the version bump to have more CI stability during the day.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to satisfy the node-sass dependency when compiling flink-runtime-web?,FLINK-18306,13311495,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,appleyuchi,appleyuchi,15/Jun/20 13:27,17/Jun/20 23:26,13/Jul/23 08:12,17/Jun/20 23:26,1.12.0,,,,,,,,Runtime / Web Frontend,,,,,0,,,,,,"2 commands in flink-runtime-web 's pom.xml,they are

*npm ci --cache-max=0 --no-save*

*npm run build*

－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－

npm ci need the v4.11.0 in package-lock.json

when compiling,

*it tell me that it need {color:#172b4d}node-sass/v4.11.0/linux-x64-72_binding.node.{color}*

 

{color:#172b4d}*The author of node-sass has already deleted linux-x64-72_binding.node*{color}

{color:#172b4d}[https://github.com/sass/node-sass/issues/2653]{color}

 

{color:#172b4d}list of node-sass/v4.11.0{color}

{color:#172b4d}[https://github.com/sass/node-sass/releases/tag/v4.11.0]{color}

 

*{color:#172b4d}Question:{color}*

*{color:#172b4d}how to satisfy the requirement above when compiling flink-runtime-web?{color}*

 ",,appleyuchi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18288,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-15 13:27:55.0,,,,,,,,,,"0|z0fuz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem connector doesn't flush part files after rolling interval,FLINK-18303,13311483,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,15/Jun/20 12:38,17/Jun/20 02:10,13/Jul/23 08:12,17/Jun/20 02:10,,,,,,1.11.0,,,Connectors / FileSystem,Table SQL / Ecosystem,,,,0,,,,,,"I have set ""execution.checkpointing.interval"" to ""5s"" and ""sink.rolling-policy.time-interval"" to ""2s"". However, it still take 60 seconds to see the first part file. 

This can be reproduced by the following code in SQL CLI:

{code:sql}
CREATE TABLE CsvTable (
  event_timestamp STRING,
  `user` STRING,
  message STRING,
  duplicate_count BIGINT,
  constant STRING
) WITH (
  'connector' = 'filesystem',
  'path' = '$RESULT',
  'format' = 'csv',
  'sink.rolling-policy.time-interval' = '2s'
);

INSERT INTO CsvTable -- read from Kafka Avro, and write into Filesystem Csv
SELECT AvroTable.*, RegReplace('Test constant folding.', 'Test', 'Success') AS constant
FROM AvroTable;
{code}

This is found when I migrate SQLClientKafkaITCase to use DDL (FLINK-18086).",,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18086,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 02:10:41 UTC 2020,,,,,,,,,,"0|z0fuwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 02:20;lzljs3620320;First, I don't think this is a technical bug, but it can be a user oriented bug.

The options control the ""see the part file"" for csv,json are:
 # rolling-policy.time-interval
 # bucket check interval
 # checkpoint interval

As long as one of the three options is not set small, it will not achieve the desired effect.;;;","17/Jun/20 02:10;jark;- master (1.12.0): e2db1dcc6a5a60210a4452849048b915cf794aaf
- 1.11.0: 6b8eaa3fa83fb1fd30efe11ed0dd090ca25bcaf3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql client uses wrong class loader when execute INSERT statements,FLINK-18302,13311482,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,jark,jark,15/Jun/20 12:33,17/Jun/20 02:10,13/Jul/23 08:12,17/Jun/20 02:10,,,,,,1.11.0,,,Table SQL / Client,,,,,0,,,,,,"Sql-client when execute INSERT statements does not use the user class loader from ExecutionContext. This makes it impossible to run queries with UDF in it if the dependencies are added with {{--jar}} flag.

This can be reproduced when I migrate {{SQLClientKafkaITCase}} to use DDL (FLINK-18086). 

It give exception:

{code}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)
Caused by: java.lang.RuntimeException: Error running SQL job.
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:595)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:515)
	at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:596)
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:315)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:212)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)
Caused by: java.lang.RuntimeException: Could not execute program.
	at org.apache.flink.table.client.gateway.local.ProgramDeployer.deploy(ProgramDeployer.java:84)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:592)
	... 8 more
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
	at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:96)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:51)
	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:217)
	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:154)
	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:109)
	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:850)
	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:52)
	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:43)
	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:55)
	at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.execute(AbstractSessionClusterExecutor.java:57)
	at org.apache.flink.table.client.gateway.local.ProgramDeployer.deploy(ProgramDeployer.java:82)
	... 9 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
	... 21 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 24 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 13, Column 30: Cannot determine simple type name ""org""
{code}


The reason is that we only wrap classloader for the inner {{applyUpdate}}, but not the outer {{executeUpdateInternal}}. However, {{ProgramDeployer#deploy()}} in {{executeUpdateInternal}} will invoke code compile which throws this exception.",,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18086,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 02:10:36 UTC 2020,,,,,,,,,,"0|z0fuw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 02:10;jark;- master (1.12.0): ca1f7642c7120800f1ac93575dcdfa0223c097a4
- 1.11.0: 75e265c7dcb2805a94a74cc7fe08acdc826ddf36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client doesn't support ALTER VIEW,FLINK-18300,13311450,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,15/Jun/20 09:58,18/Jun/20 06:36,13/Jul/23 08:12,18/Jun/20 03:35,,,,,,1.12.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,lirui,lsy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 03:54:14 UTC 2020,,,,,,,,,,"0|z0fup4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/20 03:35;lzljs3620320;master: 626dd39e449749a8c207820d0c45b555db235c54;;;","18/Jun/20 03:47;lzljs3620320;Looks like we should have a complete e2e testing for SQL-CLI and all DDLs with both default and hive dialects.

It is still very easy to forget somethings for SQL-CLI and finally occurred bugs.;;;","18/Jun/20 03:54;lirui;[~lzljs3620320] I did e2e tests for hive dialect while I was working on FLINK-17965. I didn't test for views because FLINK-17113 wasn't resolved at that time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client: setting execution.type to invalid value shuts down the session,FLINK-18297,13311444,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,alpinegizmo,alpinegizmo,15/Jun/20 09:25,30/Jun/20 13:07,13/Jul/23 08:12,30/Jun/20 13:07,1.10.0,,,,,1.10.2,1.11.0,1.12.0,Table SQL / Client,,,,,0,,,,,,"It's annoying to lose all of one's work in a session because of a typo.

 

Flink SQL> SET execution.type=foo;

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Invalid configuration entry.

at org.apache.flink.table.client.config.entries.ConfigEntry.<init>(ConfigEntry.java:41)

at org.apache.flink.table.client.config.entries.ExecutionEntry.<init>(ExecutionEntry.java:112)

at org.apache.flink.table.client.config.entries.ExecutionEntry.enrich(ExecutionEntry.java:375)

at org.apache.flink.table.client.config.Environment.enrich(Environment.java:295)

at org.apache.flink.table.client.gateway.local.LocalExecutor.setSessionProperty(LocalExecutor.java:284)

at org.apache.flink.table.client.cli.CliClient.callSet(CliClient.java:370)

at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:262)

at java.util.Optional.ifPresent(Optional.java:159)

at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200)

at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:125)

at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104)

at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)

Caused by: org.apache.flink.table.api.ValidationException: Unknown value for property 'type'.

Supported values are [streaming, batch] but was: foo

at org.apache.flink.table.descriptors.DescriptorProperties.lambda$validateEnum$34(DescriptorProperties.java:1254)

at org.apache.flink.table.descriptors.DescriptorProperties.validateOptional(DescriptorProperties.java:1520)

at org.apache.flink.table.descriptors.DescriptorProperties.validateEnum(DescriptorProperties.java:1247)

at org.apache.flink.table.descriptors.DescriptorProperties.validateEnumValues(DescriptorProperties.java:1266)

at org.apache.flink.table.client.config.entries.ExecutionEntry.validate(ExecutionEntry.java:123)

at org.apache.flink.table.client.config.entries.ConfigEntry.<init>(ConfigEntry.java:39)

... 11 more

 

Shutting down the session...

done.",,alpinegizmo,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16217,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 13:05:21 UTC 2020,,,,,,,,,,"0|z0funs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/20 13:05;leonard;Hello, [~alpinegizmo] 
 Looks like this issue has been fixed in FLINK-16217 , you can try 1.10.2, 1.11.0, 1.12.0 

Flink SQL> SET execution.type=foo;
 [ERROR] Could not execute SQL statement. Reason:
 org.apache.flink.table.api.ValidationException: Unknown value for property 'type'.
 Supported values are [streaming, batch] but was: foo

Flink SQL>

I close this ticket, please feel free to reopen if you have any question ^_^ ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink s3 end-to-end test stalls,FLINK-18291,13311390,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,15/Jun/20 06:34,16/Jun/20 14:30,13/Jul/23 08:12,16/Jun/20 14:30,1.12.0,,,,,1.12.0,,,FileSystems,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3444&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2020-06-12T21:55:57.6277963Z Number of produced values 10870/60000
2020-06-12T21:57:10.5467073Z Number of produced values 22960/60000
2020-06-12T21:58:01.0025226Z Number of produced values 59650/60000
2020-06-12T21:58:52.5624619Z Number of produced values 60000/60000
2020-06-12T21:58:53.2407133Z Cancelling job 9412dcb358631ab461a3a1e851417b9e.
2020-06-12T21:58:54.0819168Z Cancelled job 9412dcb358631ab461a3a1e851417b9e.
2020-06-12T21:58:54.1097745Z Waiting for job (9412dcb358631ab461a3a1e851417b9e) to reach terminal state CANCELED ...
2020-06-13T00:00:35.0502923Z ##[error]The operation was canceled.
2020-06-13T00:00:35.0522780Z ##[section]Finishing: Run e2e tests
{code}",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 14:30:29 UTC 2020,,,,,,,,,,"0|z0fubs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 06:35;rmetzger;I guess the first step is to introduce a timeout to get proper logs;;;","16/Jun/20 14:30;rmetzger;Timeout added: https://github.com/apache/flink/commit/fc6f37240970d2ba1d47105e679a62757f08d2cc
Please reopen if the error happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests are crashing with exit code 239,FLINK-18290,13311386,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,rmetzger,rmetzger,15/Jun/20 05:52,22/Jun/20 03:00,13/Jul/23 08:12,17/Jun/20 17:35,1.11.0,,,,,1.11.0,,,Build System / Azure Pipelines,Runtime / Checkpointing,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3467&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8]
Kafka011ProducerExactlyOnceITCase
 
{code:java}
2020-06-15T03:24:28.4677649Z [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
2020-06-15T03:24:28.4692049Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-connector-kafka-0.11_2.11: There are test failures.
2020-06-15T03:24:28.4692585Z [ERROR] 
2020-06-15T03:24:28.4693170Z [ERROR] Please refer to /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire-reports for the individual test results.
2020-06-15T03:24:28.4693928Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-06-15T03:24:28.4694423Z [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4696762Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4698486Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4699066Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4699458Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4699960Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4700849Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4703760Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4705501Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4706297Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4706592Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4706895Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4707386Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-06-15T03:24:28.4708053Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-06-15T03:24:28.4708908Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-06-15T03:24:28.4709720Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-06-15T03:24:28.4710497Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-06-15T03:24:28.4711448Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-06-15T03:24:28.4712395Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-06-15T03:24:28.4712997Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-06-15T03:24:28.4713524Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-06-15T03:24:28.4714079Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-06-15T03:24:28.4714560Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-06-15T03:24:28.4715096Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-06-15T03:24:28.4715672Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-06-15T03:24:28.4716445Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-06-15T03:24:28.4717024Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-06-15T03:24:28.4717478Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-06-15T03:24:28.4717939Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-06-15T03:24:28.4718378Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-06-15T03:24:28.4718852Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-06-15T03:24:28.4719230Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-06-15T03:24:28.4719676Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-15T03:24:28.4720309Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-15T03:24:28.4720882Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-15T03:24:28.4721339Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-15T03:24:28.4721888Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-06-15T03:24:28.4722658Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-06-15T03:24:28.4723430Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-06-15T03:24:28.4724062Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-06-15T03:24:28.4724657Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4726770Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4728582Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4729202Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4729612Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4730247Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4730781Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-06-15T03:24:28.4731292Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-06-15T03:24:28.4731829Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-06-15T03:24:28.4732353Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-06-15T03:24:28.4732792Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-15T03:24:28.4733235Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-15T03:24:28.4733718Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-15T03:24:28.4734170Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-06-15T03:24:28.4734682Z [ERROR] -> [Help 1]
2020-06-15T03:24:28.4734859Z [ERROR] 
2020-06-15T03:24:28.4735312Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-15T03:24:28.4735927Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-15T03:24:28.4736439Z [ERROR] 
2020-06-15T03:24:28.4736952Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-15T03:24:28.4737706Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2020-06-15T03:24:28.4738167Z [ERROR] 
2020-06-15T03:24:28.4738553Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-15T03:24:28.4739663Z [ERROR]   mvn <goals> -rf :flink-connector-kafka-0.11_2.11
2020-06-15T03:24:29.0980029Z MVN exited with EXIT CODE: 1.
{code}

This could be a CI environment issue...
When did it start?",,dian.fu,pnowojski,rmetzger,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18375,,,,,,,,,,,,,FLINK-17768,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 17:35:18 UTC 2020,,,,,,,,,,"0|z0fuaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 06:01;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3467&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323
ZooKeeperHighAvailabilityITCase

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3477&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20
KafkaProducerAtLeastOnceITCase;;;","15/Jun/20 06:03;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3460&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29
GroupWindowITCase;;;","15/Jun/20 06:04;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3460&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5
KafkaProducerExactlyOnceITCase
;;;","15/Jun/20 06:05;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3460&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=39857031-7f0c-5fd5-d730-a19c5794f839
SortITCase;;;","15/Jun/20 06:05;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3460&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc
UnalignedCheckpointCompatibilityITCase;;;","15/Jun/20 06:08;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3460&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185

This one doesn't include the failed test name. The dump file contains this 
{code}
# Created at 2020-06-14T21:18:00.714
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.Reader.read(Reader.java:100)
	at java.util.Scanner.readInput(Scanner.java:804)
	at java.util.Scanner.findWithinHorizon(Scanner.java:1685)
	at java.util.Scanner.hasNextLine(Scanner.java:1500)
	at org.apache.maven.surefire.booter.PpidChecker$ProcessInfoConsumer.execute(PpidChecker.java:354)
	at org.apache.maven.surefire.booter.PpidChecker.unix(PpidChecker.java:190)
	at org.apache.maven.surefire.booter.PpidChecker.isProcessAlive(PpidChecker.java:123)
	at org.apache.maven.surefire.booter.ForkedBooter$2.run(ForkedBooter.java:214)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


# Created at 2020-06-14T21:18:00.720
System.exit() or native command error interrupted process checker.
java.lang.IllegalStateException: error [STOPPED] to read process 1262
	at org.apache.maven.surefire.booter.PpidChecker.checkProcessInfo(PpidChecker.java:145)
	at org.apache.maven.surefire.booter.PpidChecker.isProcessAlive(PpidChecker.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter$2.run(ForkedBooter.java:214)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code};;;","15/Jun/20 06:08;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3460&view=logs&j=d47ab8d2-10c7-5d9e-8178-ef06a797a0d8&t=10585c27-e00d-55bc-cd53-59cb93661f3f
AggregateITCase;;;","15/Jun/20 06:09;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3461&view=logs&j=d47ab8d2-10c7-5d9e-8178-ef06a797a0d8&t=dbd54e26-95e0-584b-4a47-190a8df6e3ac
JoinITCase;;;","15/Jun/20 06:11;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3451&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=39857031-7f0c-5fd5-d730-a19c5794f839
AggregateITCase;;;","15/Jun/20 06:11;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3451&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19
KafkaProducerAtLeastOnceITCase;;;","15/Jun/20 06:12;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3451&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53;;;","15/Jun/20 06:30;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3444&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0;;;","15/Jun/20 06:36;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3435&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=398fcd67-68ce-5e3e-515b-9e466296fd73;;;","15/Jun/20 06:36;rmetzger;Kafka011ProducerExactlyOnceITCase
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3435&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051;;;","15/Jun/20 06:37;rmetzger;OverWindowITCase
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3435&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=f2460a7f-3a8f-5c32-3bac-d566722e4e62;;;","15/Jun/20 06:39;rmetzger;WindowAggregateITCase
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3434&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=39857031-7f0c-5fd5-d730-a19c5794f839;;;","15/Jun/20 06:40;rmetzger;SemiAntiJoinStreamITCase
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3434&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53;;;","15/Jun/20 06:44;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3416&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323
different message:
{code}
2020-06-12T14:58:58.4925378Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-tests: There are test failures.
2020-06-12T14:58:58.4926990Z [ERROR] 
2020-06-12T14:58:58.4928598Z [ERROR] Please refer to /__w/1/s/flink-tests/target/surefire-reports for the individual test results.
2020-06-12T14:58:58.4930518Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-06-12T14:58:58.4931819Z [ERROR] ExecutionException Error occurred in starting fork, check output in log
2020-06-12T14:58:58.4933542Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log
2020-06-12T14:58:58.4935484Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-06-12T14:58:58.4937639Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-06-12T14:58:58.4939378Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-06-12T14:58:58.4940913Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-06-12T14:58:58.4942684Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-06-12T14:58:58.4944675Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-06-12T14:58:58.4946481Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-06-12T14:58:58.4948262Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-06-12T14:58:58.4949908Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-06-12T14:58:58.4951418Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-06-12T14:58:58.4953124Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-06-12T14:58:58.4954936Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-06-12T14:58:58.4956744Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-06-12T14:58:58.4958642Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-06-12T14:58:58.4960451Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-06-12T14:58:58.4961947Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-06-12T14:58:58.4963411Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-06-12T14:58:58.4964883Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-06-12T14:58:58.4966147Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-06-12T14:58:58.4967332Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-06-12T14:58:58.4968546Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-12T14:58:58.4969835Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-12T14:58:58.4971426Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-12T14:58:58.4973515Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-12T14:58:58.4974976Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-06-12T14:58:58.4976522Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-06-12T14:58:58.4978170Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-06-12T14:58:58.4979664Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-06-12T14:58:58.4981240Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log
2020-06-12T14:58:58.4983173Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:622)
2020-06-12T14:58:58.4985026Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-06-12T14:58:58.4986722Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-06-12T14:58:58.4988461Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-06-12T14:58:58.4989846Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-12T14:58:58.4991482Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-12T14:58:58.4992958Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-12T14:58:58.4994232Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-06-12T14:58:58.4995347Z [ERROR] -> [Help 1]
2020-06-12T14:58:58.4995798Z [ERROR] 
2020-06-12T14:58:58.4996912Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-12T14:58:58.4998474Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-12T14:58:58.4999315Z [ERROR] 
2020-06-12T14:58:58.5000415Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-12T14:58:58.5001882Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}

No additional info in dumpstream;;;","15/Jun/20 07:22;rmetzger;First occurrence of the issue was build 20200612.19 (build ID 3416)
First occurrence of the exact issue with exit code was build 20200612.26 (id 3434).

This commit (https://github.com/flink-ci/flink-mirror/commit/c004b119ef28dc7387935d8d3a4dbf296cc5f661) introduces a System.exit(-17) in the checkpoint coordinator. 256 - 17 = 239. Coincidence? Introducing a {{System.exit(-17);}} into any test will lead to exactly the failure reported here.

This seems to be the reason why System.exit() gets called (from 20200612.19):
{code}
14:56:33,906 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka://flink/user/rpc/taskmanager_28.
14:56:33,887 [jobmanager-future-thread-7] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'jobmanager-future-thread-7' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1db3e21b rejected from java.util.concurrent.ScheduledThreadPoolExecutor@198c75f2[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 20]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_242]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609) [?:1.8.0_242]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_242]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1db3e21b rejected from java.util.concurrent.ScheduledThreadPoolExecutor@198c75f2[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 20]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_242]
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_242]
	at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:62) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_242]
	... 10 more
{code}

Seems to be the same in the .26 run:
{code}
20:51:18,122 [mini-cluster-io-thread-26] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - JobManager for job 63af744539889f9a6bf731aa05b02e97 with leader id 93f8812403b7e711da29465d96a74439 lost leadership.
20:51:18,122 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 63af744539889f9a6bf731aa05b02e97.
20:51:18,121 [    Checkpoint Timer] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'Checkpoint Timer' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@59fa0f36 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cf89a6d[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 7]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) [?:1.8.0_242]
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:594) [?:1.8.0_242]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_242]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_242]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@59fa0f36 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cf89a6d[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 7]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_242]
	at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_242]
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_242]
	at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:62) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_242]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_242]
	... 12 more
20:51:18,123 [PermanentBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
{code};;;","15/Jun/20 07:41;trohrmann;This looks a bit as if the {{CheckpointCoordinator}} still tries to enqueue some tasks after the {{timer}} has been shut down. This happens once the {{ExecutionGraph}} transitions into a terminal state and after {{CheckpointCoordinator.shutdown()}} is called in {{ExecutionGraph.onTerminalState}}.

[~pnowojski], [~roman_khachatryan], [~SleePy] what exactly is the contract of {{CheckpointCoordinator.shutdown()}}? If the {{CheckpointCoordinator}} could still have some concurrent actions running after {{shutdown}}, then we cannot directly call {{ExecutorService.shutdownNow()}}.

As a quick fix, we can remove {{FutureUtils.assertNoException}} but in the long run we should properly manage the lifecycle of the {{CheckpointCoordinator}} and its concurrent tasks.;;;","15/Jun/20 08:48;roman;I think the issue is in calling timer.shutdownNow (right after coordinator.shutdown) which fails all incomplete futures.

As a fix, I propose to
 # replace {{FutureUtils.assertNoException with a call to failureManager}}
 # {{make failureManager }}{{decide to fail the job or to log the error if the system is shutting down}};;;","16/Jun/20 06:00;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3542&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129;;;","16/Jun/20 06:06;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3541&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29;;;","17/Jun/20 17:35;pnowojski;Merged to master as f85dfbb12d release-1.11 as 4a6825b843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF notify checkpoint aborted interface not work,FLINK-18289,13311382,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,yunta,yunta,15/Jun/20 04:15,18/Jun/20 09:23,13/Jul/23 08:12,18/Jun/20 09:23,1.11.0,,,,,1.11.0,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,,"Current {{notifyCheckpointAborted}} is not overridden in {{AbstractUdfStreamOperator}} and lead to even user override {{CheckpointListener}}, that part of code would not be called.",,pnowojski,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 09:23:27 UTC 2020,,,,,,,,,,"0|z0fua0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/20 09:23;pnowojski;merged commit 4183aac into apache:master
merged commit 3e1b2b7a33 into release-1.11

(If RC2 passes, the fix version will have to be updated from 1.11.0, to 1.11.1);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WEB UI failure in Flink1.12,FLINK-18288,13311379,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,appleyuchi,appleyuchi,15/Jun/20 03:05,16/Jun/20 15:21,13/Jul/23 08:12,16/Jun/20 15:18,1.12.0,,,,,,,,,,,,,0,,,,,," 

 

①build command:

*mvn clean install -T 2C  -DskipTests -Dskip.npm -Dmaven.compile.fork=true*

 

②use flink-conf.yaml  from 1.1o.1  in 1.12

masters:

Desktop:8082

 

slaves:

Desktop
Laptop

③$FLINK_HOME/bin/start-cluster.sh

 

 

④open browser in:

Desktop:8082

{""errors"":[""Unable to load requested file /index.html.""]}

 

 

 

 ",,appleyuchi,qingyue,vthinkxie,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18306,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 15:08:09 UTC 2020,,,,,,,,,,"0|z0fu9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 04:21;wangyang0918;You should not skip the npm build. Please remove the {{-Dskip.npm}}, and have a try again.;;;","15/Jun/20 05:31;appleyuchi;[INFO] ----------------< org.apache.flink:flink-sequence-file >----------------
[INFO] ----------------< org.apache.flink:flink-hcatalog_2.11 >----------------
[INFO] Building flink-hcatalog 1.11.0 [84/183]
[INFO] Building flink-sequence-file 1.11.0 [83/183]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] Excluding org.apache.flink:flink-java:jar:1.11.0 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-lang3:jar:3.3.2 from the shaded jar.
[WARNING] *****************************************************************
[WARNING] * Your build is requesting parallel execution, but project *
[WARNING] * contains the following plugin(s) that have goals not marked *
[WARNING] * as @threadSafe to support parallel building. *
[WARNING] * While this /may/ work fine, please look for plugin updates *
[WARNING] * and/or request plugins be made thread-safe. *
[INFO] Excluding org.apache.commons:commons-math3:jar:3.5 from the shaded jar.
[WARNING] * If reporting an issue, report it against the plugin in *
[WARNING] * question, not against maven-core *
[WARNING] *****************************************************************
[INFO] Excluding org.apache.flink:flink-scala_2.11:jar:1.11.0 from the shaded jar.
[WARNING] The following plugins are not marked @threadSafe in flink-sequence-file:
[INFO] Excluding org.apache.flink:flink-shaded-asm-7:jar:7.1-10.0 from the shaded jar.
[WARNING] org.commonjava.maven.plugins:directory-maven-plugin:0.1
[WARNING] Enable debug to see more precisely which goals are not marked @threadSafe.
[INFO] Excluding org.scala-lang:scala-reflect:jar:2.11.12 from the shaded jar.
[WARNING] *****************************************************************
[WARNING] *****************************************************************
[WARNING] * Your build is requesting parallel execution, but project *
[WARNING] * contains the following plugin(s) that have goals not marked *
[WARNING] * as @threadSafe to support parallel building. *
[WARNING] * While this /may/ work fine, please look for plugin updates *
[WARNING] * and/or request plugins be made thread-safe. *
[WARNING] * If reporting an issue, report it against the plugin in *
[INFO] Excluding org.scala-lang:scala-library:jar:2.11.12 from the shaded jar.
[WARNING] * question, not against maven-core *
[WARNING] *****************************************************************
[INFO] Excluding org.scala-lang:scala-compiler:jar:2.11.12 from the shaded jar.
[WARNING] The following plugins are not marked @threadSafe in flink-hcatalog:
[WARNING] org.commonjava.maven.plugins:directory-maven-plugin:0.1
[INFO] Excluding org.scala-lang.modules:scala-xml_2.11:jar:1.0.5 from the shaded jar.
[WARNING] org.scalastyle:scalastyle-maven-plugin:1.0.0
[INFO] Excluding org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1 from the shaded jar.
[WARNING] Enable debug to see more precisely which goals are not marked @threadSafe.
[INFO] Excluding org.apache.flink:flink-clients_2.11:jar:1.11.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-runtime_2.11:jar:1.11.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-queryable-state-client-java:jar:1.11.0 from the shaded jar.
[WARNING] *****************************************************************
[INFO] Excluding org.apache.flink:flink-hadoop-fs:jar:1.11.0 from the shaded jar.
[INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-netty:jar:4.1.39.Final-10.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-jackson:jar:2.10.1-10.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-zookeeper-3:jar:3.4.10-10.0 from the shaded jar.
[INFO] Excluding org.javassist:javassist:jar:3.24.0-GA from the shaded jar.
[INFO] Excluding com.typesafe.akka:akka-actor_2.11:jar:2.5.21 from the shaded jar.
[INFO] Excluding com.typesafe:config:jar:1.3.0 from the shaded jar.
[INFO] Excluding org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0 from the shaded jar.
[INFO] Excluding com.typesafe.akka:akka-remote_2.11:jar:2.5.21 from the shaded jar.
[INFO] Excluding io.netty:netty:jar:3.10.6.Final from the shaded jar.
[INFO] Excluding com.typesafe.akka:akka-stream_2.11:jar:2.5.21 from the shaded jar.
[INFO] Excluding org.reactivestreams:reactive-streams:jar:1.0.2 from the shaded jar.
[INFO] 
[INFO] Excluding com.typesafe:ssl-config-core_2.11:jar:0.3.7 from the shaded jar.
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ flink-sequence-file ---
[INFO] Excluding com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21 from the shaded jar.
[INFO] Excluding com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21 from the shaded jar.
[INFO] Deleting /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target
[INFO] Excluding org.clapper:grizzled-slf4j_2.11:jar:1.3.2 from the shaded jar.
[INFO] Excluding com.github.scopt:scopt_2.11:jar:3.5.0 from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.4 from the shaded jar.
[INFO] Excluding com.twitter:chill_2.11:jar:0.7.6 from the shaded jar.
[INFO] Excluding com.twitter:chill-java:jar:0.7.6 from the shaded jar.
[INFO] Excluding org.lz4:lz4-java:jar:1.5.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-optimizer_2.11:jar:1.11.0 from the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.3.1 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-streaming-java_2.11:jar:1.11.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-core:jar:1.11.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-annotations:jar:1.11.0 from the shaded jar.
[INFO] 
[INFO] Excluding org.apache.flink:flink-metrics-core:jar:1.11.0 from the shaded jar.
[INFO] --- maven-checkstyle-plugin:2.17:check (validate) @ flink-sequence-file ---
[INFO] Excluding com.esotericsoftware.kryo:kryo:jar:2.24.0 from the shaded jar.
[INFO] Excluding com.esotericsoftware.minlog:minlog:jar:1.2 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-compress:jar:1.20 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-guava:jar:18.0-10.0 from the shaded jar.
[INFO] Excluding org.apache.logging.log4j:log4j-slf4j-impl:jar:2.12.1 from the shaded jar.
[INFO] Excluding org.apache.logging.log4j:log4j-api:jar:2.12.1 from the shaded jar.
[INFO] Excluding org.apache.logging.log4j:log4j-core:jar:2.12.1 from the shaded jar.
[INFO] Including org.apache.flink:force-shading:jar:1.11.0 in the shaded jar.
[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.15 from the shaded jar.
[INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.
[INFO] Excluding org.objenesis:objenesis:jar:2.1 from the shaded jar.
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0.jar with /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-shaded.jar
[INFO] Replacing original test artifact with shaded test artifact.
[INFO] Replacing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-tests.jar with /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-shaded-tests.jar
[INFO] Dependency-reduced POM written at: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/dependency-reduced-pom.xml
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-maven-version) @ flink-sequence-file ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-maven) @ flink-sequence-file ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-jackson) @ flink-sequence-file ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (forbid-log4j-1) @ flink-sequence-file ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-sequence-file ---
[INFO] 
[INFO] --- directory-maven-plugin:0.1:highest-basedir (directories) @ flink-sequence-file ---
[INFO] Highest basedir set to: /home/appleyuchi/桌面/Flink超线程测试/flink
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ flink-sequence-file ---
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ flink-sequence-file ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ flink-sequence-file ---
[INFO] Compiling 3 source files to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/classes
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (rename) @ flink-examples-batch_2.11 ---
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ flink-sequence-file ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ flink-sequence-file ---
[INFO] Compiling 2 source files to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ flink-sequence-file ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ flink-sequence-file ---
[INFO] Building jar: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.1:shade (shade-flink) @ flink-sequence-file ---
[INFO] Including org.apache.flink:force-shading:jar:1.11.0 in the shaded jar.
[INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.
[INFO] No artifact matching filter io.netty:netty
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0.jar with /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0-shaded.jar
[INFO] Replacing original test artifact with shaded test artifact.
[INFO] Replacing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0-tests.jar with /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0-shaded-tests.jar
[INFO] Dependency-reduced POM written at: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/dependency-reduced-pom.xml
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (integration-tests) @ flink-sequence-file ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ flink-sequence-file ---
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-sequence-file/1.11.0/flink-sequence-file-1.11.0.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/dependency-reduced-pom.xml to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-sequence-file/1.11.0/flink-sequence-file-1.11.0.pom
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-formats/flink-sequence-file/target/flink-sequence-file-1.11.0-tests.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-sequence-file/1.11.0/flink-sequence-file-1.11.0-tests.jar
[INFO] Executing tasks

main:
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
 [copy] Copying 1 file to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (integration-tests) @ flink-examples-batch_2.11 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- scalastyle-maven-plugin:1.0.0:check (default) @ flink-examples-batch_2.11 ---
[WARNING] testSourceDirectory is not specified or does not exist value=/home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/src/test/scala
Saving to outputFile=/home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/scalastyle-output.xml
Processed 12 file(s)
Found 0 errors
Found 0 warnings
Found 0 infos
Finished in 80 ms
[INFO] 
[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ flink-examples-batch_2.11 ---
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/dependency-reduced-pom.xml to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0.pom
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-KMeans.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-KMeans.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-TransitiveClosure.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-TransitiveClosure.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-ConnectedComponents.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-ConnectedComponents.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-EnumTriangles.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-EnumTriangles.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-PageRank.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-PageRank.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-WebLogAnalysis.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-WebLogAnalysis.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-WordCount.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-WordCount.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-DistCp.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-DistCp.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-examples/flink-examples-batch/target/flink-examples-batch_2.11-1.11.0-tests.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-examples-batch_2.11/1.11.0/flink-examples-batch_2.11-1.11.0-tests.jar
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ flink-hcatalog_2.11 ---
[INFO] Deleting /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target
[INFO] 
[INFO] --- maven-checkstyle-plugin:2.17:check (validate) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-maven-version) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-maven) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-jackson) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (forbid-log4j-1) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- directory-maven-plugin:0.1:highest-basedir (directories) @ flink-hcatalog_2.11 ---
[INFO] Highest basedir set to: /home/appleyuchi/桌面/Flink超线程测试/flink
[INFO] 
[INFO] --- build-helper-maven-plugin:3.1.0:add-source (add-source) @ flink-hcatalog_2.11 ---
[INFO] Source directory: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ flink-hcatalog_2.11 ---
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ flink-hcatalog_2.11 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-hcatalog_2.11 ---
[INFO] /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/main/java:-1: info: compiling
[INFO] /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/main/scala:-1: info: compiling
[INFO] Compiling 3 source files to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/classes at 1592198798982
[INFO] 
[INFO] > fsevents@1.2.7 install /home/appleyuchi/桌面/Flink超线程测试/flink/flink-runtime-web/web-dashboard/node_modules/fsevents
[INFO] > node install
[INFO] 
[INFO] prepare-compile in 0 s
[INFO] compile in 12 s
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ flink-hcatalog_2.11 ---
[INFO] Compiling 2 source files to /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/classes
[INFO] /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/main/java/org/apache/flink/hcatalog/HCatInputFormatBase.java: Some input files use unchecked or unsafe operations.
[INFO] /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/main/java/org/apache/flink/hcatalog/HCatInputFormatBase.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- build-helper-maven-plugin:3.1.0:add-test-source (add-test-source) @ flink-hcatalog_2.11 ---
[INFO] Test Source directory: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/test/scala added.
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ flink-hcatalog_2.11 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.2:testCompile (scala-test-compile) @ flink-hcatalog_2.11 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ flink-hcatalog_2.11 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ flink-hcatalog_2.11 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ flink-hcatalog_2.11 ---
[INFO] Building jar: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.1:shade (shade-flink) @ flink-hcatalog_2.11 ---
[INFO] Excluding org.apache.flink:flink-hadoop-compatibility_2.11:jar:1.11.0 from the shaded jar.
[INFO] Excluding org.apache.hive.hcatalog:hcatalog-core:jar:0.12.0 from the shaded jar.
[INFO] Excluding com.google.guava:guava:jar:11.0.2 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-cli:jar:0.12.0 from the shaded jar.
[INFO] Excluding jline:jline:jar:0.9.94 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-service:jar:0.12.0 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-shims:jar:0.12.0 from the shaded jar.
[INFO] Excluding org.apache.thrift:libthrift:jar:0.9.0 from the shaded jar.
[INFO] Excluding commons-logging:commons-logging-api:jar:1.0.4 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-metastore:jar:0.12.0 from the shaded jar.
[INFO] Excluding org.antlr:antlr:jar:3.4 from the shaded jar.
[INFO] Excluding org.antlr:antlr-runtime:jar:3.4 from the shaded jar.
[INFO] Excluding org.antlr:stringtemplate:jar:3.2.1 from the shaded jar.
[INFO] Excluding antlr:antlr:jar:2.7.7 from the shaded jar.
[INFO] Excluding org.antlr:ST4:jar:4.0.4 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-serde:jar:0.12.0 from the shaded jar.
[INFO] Excluding org.mockito:mockito-all:jar:1.8.2 from the shaded jar.
[INFO] Excluding org.apache.thrift:libfb303:jar:0.9.0 from the shaded jar.
[INFO] Excluding org.apache.avro:avro-mapred:jar:1.7.1 from the shaded jar.
[INFO] Excluding org.apache.avro:avro-ipc:jar:1.7.1 from the shaded jar.
[INFO] Excluding org.apache.velocity:velocity:jar:1.7 from the shaded jar.
[INFO] Excluding org.mortbay.jetty:servlet-api:jar:2.5-20081211 from the shaded jar.
[INFO] Excluding com.jolbox:bonecp:jar:0.7.1.RELEASE from the shaded jar.
[INFO] Excluding commons-pool:commons-pool:jar:1.5.4 from the shaded jar.
[INFO] Excluding org.datanucleus:datanucleus-api-jdo:jar:3.2.1 from the shaded jar.
[INFO] Excluding org.datanucleus:datanucleus-core:jar:3.2.2 from the shaded jar.
[INFO] Excluding org.datanucleus:datanucleus-rdbms:jar:3.2.1 from the shaded jar.
[INFO] Excluding javax.jdo:jdo-api:jar:3.0.1 from the shaded jar.
[INFO] Excluding javax.transaction:jta:jar:1.1 from the shaded jar.
[INFO] Excluding org.apache.derby:derby:jar:10.4.2.0 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-common:jar:0.12.0 from the shaded jar.
[INFO] Excluding org.apache.hive:hive-exec:jar:0.12.0 from the shaded jar.
[INFO] Excluding org.iq80.snappy:snappy:jar:0.2 from the shaded jar.
[INFO] Excluding com.googlecode.javaewah:JavaEWAH:jar:0.3.2 from the shaded jar.
[INFO] Excluding javolution:javolution:jar:5.5.1 from the shaded jar.
[INFO] Excluding org.codehaus.jackson:jackson-mapper-asl:jar:1.8.8 from the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.3.1 from the shaded jar.
[INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
[INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
[INFO] Excluding org.mortbay.jetty:jetty:jar:6.1.26 from the shaded jar.
[INFO] Excluding org.mortbay.jetty:jetty-util:jar:6.1.26 from the shaded jar.
[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
[INFO] Excluding org.apache.httpcomponents:httpclient:jar:4.5.3 from the shaded jar.
[INFO] Excluding org.apache.httpcomponents:httpcore:jar:4.4.6 from the shaded jar.
[INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
[INFO] Excluding commons-configuration:commons-configuration:jar:1.7 from the shaded jar.
[INFO] Excluding commons-digester:commons-digester:jar:1.8.1 from the shaded jar.
[INFO] Excluding commons-beanutils:commons-beanutils:jar:1.8.3 from the shaded jar.
[INFO] Excluding org.codehaus.jackson:jackson-core-asl:jar:1.8.8 from the shaded jar.
[INFO] Excluding org.apache.avro:avro:jar:1.8.2 from the shaded jar.
[INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.7 from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.4 from the shaded jar.
[INFO] Excluding org.tukaani:xz:jar:1.5 from the shaded jar.
[INFO] Excluding com.google.protobuf:protobuf-java:jar:2.5.0 from the shaded jar.
[INFO] Excluding org.apache.zookeeper:zookeeper:jar:3.4.10 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-compress:jar:1.20 from the shaded jar.
[INFO] Excluding io.netty:netty:jar:3.6.2.Final from the shaded jar.
[INFO] Including org.apache.flink:force-shading:jar:1.11.0 in the shaded jar.
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0.jar with /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0-shaded.jar
[INFO] Replacing original test artifact with shaded test artifact.
[INFO] Replacing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0-tests.jar with /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0-shaded-tests.jar
[INFO] Dependency-reduced POM written at: /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/dependency-reduced-pom.xml
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (integration-tests) @ flink-hcatalog_2.11 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- scalastyle-maven-plugin:1.0.0:check (default) @ flink-hcatalog_2.11 ---
[WARNING] testSourceDirectory is not specified or does not exist value=/home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/src/test/scala
Saving to outputFile=/home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/scalastyle-output.xml
Processed 1 file(s)
Found 0 errors
Found 0 warnings
Found 0 infos
Finished in 22 ms
[INFO] 
[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ flink-hcatalog_2.11 ---
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-hcatalog_2.11/1.11.0/flink-hcatalog_2.11-1.11.0.jar
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/dependency-reduced-pom.xml to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-hcatalog_2.11/1.11.0/flink-hcatalog_2.11-1.11.0.pom
[INFO] Installing /home/appleyuchi/桌面/Flink超线程测试/flink/flink-connectors/flink-hcatalog/target/flink-hcatalog_2.11-1.11.0-tests.jar to /home/appleyuchi/bigdata/apache-maven-3.5.4/jar_warehouse/org/apache/flink/flink-hcatalog_2.11/1.11.0/flink-hcatalog_2.11-1.11.0-tests.jar
*{color:#de350b}[ERROR] Aborted (core dumped){color}*
[INFO] 
[INFO] > node-sass@4.11.0 install /home/appleyuchi/桌面/Flink超线程测试/flink/flink-runtime-web/web-dashboard/node_modules/node-sass
[INFO] > node scripts/install.js
[INFO] 
[INFO] Downloading binary from https://github.com/sass/node-sass/releases/download/v4.11.0/linux-x64-64_binding.node;;;","15/Jun/20 07:42;wangyang0918;It seems that the mvn failed with core dump. Are you compiling Flink with maven 3.2.5? If you want to dig futhure, you need to analyze the core dump file.;;;","15/Jun/20 12:59;appleyuchi;runtime-web has only two commands:

*npm ci* and *npm run build*

the former need node-sass  v4.11 linux-x64-72_binding.node

 

if you input *npm ci* you'll find it not downloadable.

node-sass/v4.11.0/linux-x64-72_binding.node

has already been deleted by the author of node-sass.

 

*The reason why you can build it successfully is that*

*you have history file of  node-sass/v4.11.0/linux-x64-72_binding.node*

 

*if you use Ubuntu19.10/20.04,you'll fail.*;;;","16/Jun/20 04:04;wangyang0918;I am not familiar with npm. cc [~vthinkxie], could you please have a look?;;;","16/Jun/20 04:20;appleyuchi;*----------------------------------some basics----------------------------------*

*npm -> package.json/package-lock.json*

is just like

*mvn -> pom.xml*

 

*npm->node.js*

is just like

*mvn->java*

#-----------------------------*npm dependency file relation*------------------------------------

npm ci -cache-max=0 --nosave (package-lock.json)

npm run build(package.json)

*#------------------------------------isolated node.js environment-----------------------------------*

*npm build an* *isolated node.js environment* *in Flink*

node.js　version is v10.9.0

 

*maybe you will say:why don't you compile it separately?*

Because My Ubuntu Desktop is 19.10,

nvm with node.js(v10.9.0)is incompatible with the Anular needed.

#----------------------------*-the structure of Flink Building*-*---*----------------------------------

There are two kinds of buid in the building of Flink

They mix *npm build* into *mvn build.*

 

 

 

 ;;;","16/Jun/20 05:17;vthinkxie;[~appleyuchi]

plz note that :
 # `node-sass/v4.11.0/linux-x64-72_binding.node` is not the static file, the file position and name is determined by your npm and node version dynamically, which means that it will always have an error if you have installed *unmatched* node or npm version yourself. That is why *it works fine* in every other' and ci environment except yours.(just imagine that you have installed many different versions of maven and want them to work together in the same command line, which is impossible.)
 # the node and npm version are managed by maven-frontend-plugin in the flink-runtime-web/pom.xml in isolated folders(flink-runtime-web/web-dashboard/node and flink-runtime-web/web-dashboard/node_modules), if you want to manage it yourself(try to run npm ci or npm install yourself), *DON'T DO THIS* until you have full knowledge of npm and node package, it would mess up the isolated env of node and npm. 
 # plz have a try to remove all your `flink-runtime-web/web-dashboard/node` and `flink-runtime-web/web-dashboard/node_modules` caches in the flink-runtime-web/web-dashboard and have a try again.

 ;;;","16/Jun/20 06:39;appleyuchi;Thanks for your replies.

some supplement:

*v4.11.0 is written in pacakge-lock.json by official developer,*

 

*the sub-version number  ""linux-x64-72_binding""* of node-sass

is newest of node-sass in history.

it's common sense for an dependency-manage-software to download the newest dependency.

 

*So,It's not dynamic,**they're not affected by my npm/nodejs,*

 

*your description are off the point,*

*you descrition is about package-compatibility,*

*not about package-availability(version is written by official developer)*

#---------------------------------------------------------------------------------------------------------------

for point 2 and 3 in your replies.

*how can you try this in ubuntu19.10?*

*it will cause ""core dump"" if you don't change npm and node.js version set by official develop*

 

 

 

 

 ;;;","16/Jun/20 15:08;appleyuchi;首先对两位表示感谢,感谢你们在工作时间回答我的问题.

[~vthinkxie] [~fly_in_gis]

感谢[~fly_in_gis] 那天在午夜两点回答的问题.

十分感谢!

 

Final Solution(独立编译flink-runtime-web):

[https://yuchi.blog.csdn.net/article/details/106796295]

这个方案可以使用用户指定的node版本以及高级版本的ubuntu个人环境中进行编译.

 

删除flink-runtime-web/pom.xml中的所有executions中的npm编译语句.

然后到根目录进行整体mvn编译即可

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No appenders could be found for logger ,FLINK-18284,13311243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,appleyuchi,appleyuchi,13/Jun/20 07:58,14/Jun/20 13:10,13/Jul/23 08:12,14/Jun/20 13:10,,,,,,,,,Scala Shell,,,,,0,,,,,,"start-scala-shell.sh local
Starting Flink Shell:
log4j:WARN No appenders could be found for logger (org.apache.flink.configuration.GlobalConfiguration).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

 

what's the configuration to fix this warning,it's annoying.

PS:
configuration in log4j.properties did not work.",,appleyuchi,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 12:30:31 UTC 2020,,,,,,,,,,"0|z0ftf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/20 15:09;appleyuchi;release-1.11.0-rc1

For Flink 1.10.1
 ①$FLINK_HOME/conf remain flink-conf.yaml,delete other files.
 ②$FLINK_HOME/bin   delete start-scala-shell.sh

from release-1.11.0-rc1
 ③git clone [http://github.com/flink]（release-1.11.0-rc1）
 ④cd flink-dist/src/main/flink-bin/conf（release-1.11.0-rc1）
 ⑤cp * $FLINK_HOME/conf（from release-1.11.0-rc1 to local 1.10.1）
 ⑥cp flink-scala-shell/start-script/start-scala-shell.sh $FLINK_HOME/bin（from release-1.11.0-rc1 to local 1.10.1）


Test:
 ⑦$FLINK_HOME/bin/start-scala-shell.sh local(warning remains)
 log4j:WARN No appenders could be found for logger (org.apache.flink.configuration.GlobalConfiguration).
 log4j:WARN Please initialize the log4j system properly.
 log4j:WARN See [http://logging.apache.org/log4j/1.2/faq.html#noconfig] for more info.

Modification:
 add to log4j.properties

log4j.rootLogger=INFO, console
 log4j.appender.console=org.apache.log4j.ConsoleAppender
 log4j.appender.console.layout=org.apache.log4j.PatternLayout
 log4j.appender.console.layout.ConversionPattern=%d\{dd/MM/yyyy HH:mm:ss.SSS} %5p [%-10c] %m%n

Reference:
 [https://riptutorial.com/apache-flink/example/29954/logging-configuration];;;","13/Jun/20 15:24;appleyuchi;*修复效果如下:*

(Python3.6) appleyuchi@Desktop:~$ start-scala-shell.sh local
Starting Flink Shell:
13/06/2020 23:23:51.327 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.rpc.address, Desktop
13/06/2020 23:23:51.327 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.rpc.port, 6123
13/06/2020 23:23:51.327 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.heap.size, 2048m
13/06/2020 23:23:51.327 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: taskmanager.memory.process.size, 1728m
13/06/2020 23:23:51.328 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: taskmanager.numberOfTaskSlots, 2
13/06/2020 23:23:51.328 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: parallelism.default, 2
13/06/2020 23:23:51.328 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.execution.failover-strategy, region
13/06/2020 23:23:51.328 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: rest.port, 8082
13/06/2020 23:23:51.329 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: rest.address, Desktop
13/06/2020 23:23:51.329 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: web.submit.enable, true
13/06/2020 23:23:51.329 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.archive.fs.dir, hdfs://Desktop:9000/completed-jobs/
13/06/2020 23:23:51.329 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.web.address, Desktop
13/06/2020 23:23:51.329 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.web.port, 18082
13/06/2020 23:23:51.330 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.archive.fs.dir, hdfs://Desktop:9000/completed-jobs/
13/06/2020 23:23:51.330 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.archive.fs.refresh-interval, 10000
13/06/2020 23:23:51.330 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: env.java.home, /home/appleyuchi/Java/jdk1.8.0_131
13/06/2020 23:23:51.400 WARN [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The resource configuration option Key: 'taskmanager.memory.process.size' , default: null (fallback keys: []) is set but it will have no effect for local execution, only the following options matter for the resource configuration: [Key: 'taskmanager.memory.process.size' , default: null (fallback keys: []), Key: 'taskmanager.memory.flink.size' , default: null (fallback keys: []), Key: 'taskmanager.memory.framework.heap.size' , default: 128 mb (fallback keys: []), Key: 'taskmanager.memory.framework.off-heap.size' , default: 128 mb (fallback keys: []), Key: 'taskmanager.memory.jvm-metaspace.size' , default: 256 mb (fallback keys: []), Key: 'taskmanager.memory.jvm-overhead.min' , default: 192 mb (fallback keys: []), Key: 'taskmanager.memory.jvm-overhead.max' , default: 1 gb (fallback keys: []), Key: 'taskmanager.memory.jvm-overhead.fraction' , default: 0.1 (fallback keys: [])]
13/06/2020 23:23:51.400 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The configuration option Key: 'taskmanager.cpu.cores' , default: null (fallback keys: []) required for local execution is not set, setting it to its default value 1.7976931348623157E308
13/06/2020 23:23:51.400 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The configuration option Key: 'taskmanager.memory.task.heap.size' , default: null (fallback keys: []) required for local execution is not set, setting it to its default value 9223372036854775807 bytes
13/06/2020 23:23:51.400 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The configuration option Key: 'taskmanager.memory.task.off-heap.size' , default: 0 bytes (fallback keys: []) required for local execution is not set, setting it to its default value 9223372036854775807 bytes
13/06/2020 23:23:51.401 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The configuration option Key: 'taskmanager.memory.network.min' , default: 64 mb (fallback keys: [\{key=taskmanager.network.memory.min, isDeprecated=true}]) required for local execution is not set, setting it to its default value 64 mb
13/06/2020 23:23:51.401 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The configuration option Key: 'taskmanager.memory.network.max' , default: 1 gb (fallback keys: [\{key=taskmanager.network.memory.max, isDeprecated=true}]) required for local execution is not set, setting it to its default value 64 mb
13/06/2020 23:23:51.401 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils] The configuration option Key: 'taskmanager.memory.managed.size' , default: null (fallback keys: [\{key=taskmanager.memory.size, isDeprecated=true}]) required for local execution is not set, setting it to its default value 128 mb
13/06/2020 23:23:51.414 INFO [org.apache.flink.runtime.minicluster.MiniCluster] Starting Flink Mini Cluster
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.rpc.address, Desktop
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.rpc.port, 6123
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.heap.size, 2048m
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: taskmanager.memory.process.size, 1728m
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: taskmanager.numberOfTaskSlots, 2
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: parallelism.default, 2
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.execution.failover-strategy, region
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: rest.port, 8082
13/06/2020 23:23:51.416 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: rest.address, Desktop
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: web.submit.enable, true
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.archive.fs.dir, hdfs://Desktop:9000/completed-jobs/
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.web.address, Desktop
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.web.port, 18082
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.archive.fs.dir, hdfs://Desktop:9000/completed-jobs/
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: historyserver.archive.fs.refresh-interval, 10000
13/06/2020 23:23:51.417 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: env.java.home, /home/appleyuchi/Java/jdk1.8.0_131
13/06/2020 23:23:51.419 INFO [org.apache.flink.runtime.minicluster.MiniCluster] Starting Metrics Registry
13/06/2020 23:23:51.460 INFO [org.apache.flink.runtime.metrics.MetricRegistryImpl] No metrics reporter configured, no metrics will be exposed/reported.
13/06/2020 23:23:51.460 INFO [org.apache.flink.runtime.minicluster.MiniCluster] Starting RPC Service(s)
13/06/2020 23:23:51.731 INFO [akka.event.slf4j.Slf4jLogger] Slf4jLogger started
13/06/2020 23:23:51.799 INFO [org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils] Trying to start actor system at :0
13/06/2020 23:23:51.838 INFO [akka.event.slf4j.Slf4jLogger] Slf4jLogger started
13/06/2020 23:23:51.851 INFO [akka.remote.Remoting] Starting remoting
13/06/2020 23:23:51.958 INFO [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://flink-metrics@192.168.0.103:36023]
13/06/2020 23:23:51.967 INFO [org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils] Actor system started at akka.tcp://flink-metrics@192.168.0.103:36023
13/06/2020 23:23:51.972 INFO [org.apache.flink.runtime.rpc.akka.AkkaRpcService] Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/MetricQueryService .
13/06/2020 23:23:52.058 INFO [org.apache.flink.runtime.minicluster.MiniCluster] Starting high-availability services
13/06/2020 23:23:52.071 INFO [org.apache.flink.runtime.blob.BlobServer] Created BLOB server storage directory /tmp/blobStore-2eb23fd1-6af2-4ef4-9990-c7026b398cdc
13/06/2020 23:23:52.075 INFO [org.apache.flink.runtime.blob.BlobServer] Started BLOB server at 0.0.0.0:43421 - max concurrent requests: 50 - max backlog: 1000
13/06/2020 23:23:52.078 INFO [org.apache.flink.runtime.blob.PermanentBlobCache] Created BLOB cache storage directory /tmp/blobStore-e2d31c1c-ef19-4fd9-9f79-f0317a327f01
13/06/2020 23:23:52.079 INFO [org.apache.flink.runtime.blob.TransientBlobCache] Created BLOB cache storage directory /tmp/blobStore-00bc8023-74d0-4f3d-9a74-dd31aaf49177
13/06/2020 23:23:52.080 INFO [org.apache.flink.runtime.minicluster.MiniCluster] Starting 1 TaskManger(s)
13/06/2020 23:23:52.081 INFO [org.apache.flink.runtime.taskexecutor.TaskManagerRunner] Starting TaskManager with ResourceID: 3cf127c5-8005-4b60-b728-7e52ca0293a5
13/06/2020 23:23:52.088 INFO [org.apache.flink.runtime.taskexecutor.TaskManagerServices] Temporary file directory '/tmp': total 136 GB, usable 61 GB (44.85% usable)
13/06/2020 23:23:52.090 INFO [org.apache.flink.runtime.io.disk.FileChannelManagerImpl] FileChannelManager uses directory /tmp/flink-io-48f2f98e-0fb4-44f6-92e3-f37b58f7fd05 for spill files.
13/06/2020 23:23:52.097 INFO [org.apache.flink.runtime.io.disk.FileChannelManagerImpl] FileChannelManager uses directory /tmp/flink-netty-shuffle-0dcc5d28-4052-4486-b596-a70702c14cee for spill files.
13/06/2020 23:23:52.131 INFO [org.apache.flink.runtime.io.network.buffer.NetworkBufferPool] Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
13/06/2020 23:23:52.136 INFO [org.apache.flink.runtime.io.network.NettyShuffleEnvironment] Starting the network environment and its components.
13/06/2020 23:23:52.137 INFO [org.apache.flink.runtime.taskexecutor.KvStateService] Starting the kvState service and its components.
13/06/2020 23:23:52.143 INFO [org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration] Messages have a max timeout of 10000 ms
13/06/2020 23:23:52.166 INFO [org.apache.flink.runtime.rpc.akka.AkkaRpcService] Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/taskmanager_0 .
13/06/2020 23:23:52.175 INFO [org.apache.flink.runtime.taskexecutor.JobLeaderService] Start job leader service.
13/06/2020 23:23:52.176 INFO [org.apache.flink.runtime.filecache.FileCache] User file cache uses directory /tmp/flink-dist-cache-4a2ebd2f-6749-4a68-9807-9b8b00f7815a
13/06/2020 23:23:52.188 INFO [org.apache.flink.configuration.Configuration] Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
13/06/2020 23:23:52.215 INFO [org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint] Starting rest endpoint.
13/06/2020 23:23:52.376 INFO [org.apache.flink.runtime.webmonitor.WebMonitorUtils] Determined location of main cluster component log file: /home/appleyuchi/bigdata/flink-1.10.1/log/flink-appleyuchi-scala-shell-local-Desktop.log
13/06/2020 23:23:52.376 INFO [org.apache.flink.runtime.webmonitor.WebMonitorUtils] Determined location of main cluster component stdout file: /home/appleyuchi/bigdata/flink-1.10.1/log/flink-appleyuchi-scala-shell-local-Desktop.out
13/06/2020 23:23:52.492 INFO [org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint] Rest endpoint listening at Desktop:8082
13/06/2020 23:23:52.493 INFO [org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService] Proposing leadership to contender http://Desktop:8082
13/06/2020 23:23:52.495 INFO [org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint] Web frontend listening at http://Desktop:8082.
13/06/2020 23:23:52.496 INFO [org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint] http://Desktop:8082 was granted leadership with leaderSessionID=7d7abb46-cfe8-4e9b-b040-061c57fede32
13/06/2020 23:23:52.496 INFO [org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService] Received confirmation of leadership for leader http://Desktop:8082 , session=7d7abb46-cfe8-4e9b-b040-061c57fede32
13/06/2020 23:23:52.503 INFO [org.apache.flink.runtime.rpc.akka.AkkaRpcService] Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/resourcemanager .
13/06/2020 23:23:52.522 INFO [org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService] Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
13/06/2020 23:23:52.523 INFO [org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService] Proposing leadership to contender LeaderContender: StandaloneResourceManager
13/06/2020 23:23:52.525 INFO [org.apache.flink.runtime.resourcemanager.StandaloneResourceManager] ResourceManager akka://flink/user/resourcemanager was granted leadership with fencing token b39e28df051dee22e5aaabc9a22345b7
13/06/2020 23:23:52.526 INFO [org.apache.flink.runtime.minicluster.MiniCluster] Flink Mini Cluster started successfully
13/06/2020 23:23:52.528 INFO [org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl] Starting the SlotManager.

Starting local Flink cluster (host: localhost, port: 8082).

13/06/2020 23:23:52.530 INFO [org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess] Start SessionDispatcherLeaderProcess.
13/06/2020 23:23:52.530 INFO [org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService] Received confirmation of leadership for leader akka://flink/user/resourcemanager , session=e5aaabc9-a223-45b7-b39e-28df051dee22

Connecting to Flink cluster (host: localhost, port: 8082).

13/06/2020 23:23:52.533 INFO [org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess] Recover all persisted job graphs.
13/06/2020 23:23:52.533 INFO [org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess] Successfully recovered 0 persisted job graphs.
13/06/2020 23:23:52.539 INFO [org.apache.flink.runtime.rpc.akka.AkkaRpcService] Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/dispatcher .
13/06/2020 23:23:52.542 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutor] Connecting to ResourceManager akka://flink/user/resourcemanager(b39e28df051dee22e5aaabc9a22345b7).
13/06/2020 23:23:52.557 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutor] Resolved ResourceManager address, beginning registration
13/06/2020 23:23:52.557 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutor] Registration at ResourceManager attempt 1 (timeout=100ms)
13/06/2020 23:23:52.569 INFO [org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService] Received confirmation of leadership for leader akka://flink/user/dispatcher , session=9928799b-9ad4-4223-939b-da0599dbd704
13/06/2020 23:23:52.569 INFO [org.apache.flink.runtime.resourcemanager.StandaloneResourceManager] Registering TaskManager with ResourceID 3cf127c5-8005-4b60-b728-7e52ca0293a5 (akka://flink/user/taskmanager_0) at ResourceManager
13/06/2020 23:23:52.573 INFO [org.apache.flink.runtime.taskexecutor.TaskExecutor] Successful registration at resource manager akka://flink/user/resourcemanager under registration id 7d155a856666cd66d0b8a08edcf9514b.

▒▓██▓██▒
 ▓████▒▒█▓▒▓███▓▒
 ▓███▓░░ ▒▒▒▓██▒ ▒
 ░██▒ ▒▒▓▓█▓▓▒░ ▒████
 ██▒ ░▒▓███▒ ▒█▒█▒
 ░▓█ ███ ▓░▒██
 ▓█ ▒▒▒▒▒▓██▓░▒░▓▓█
 █░ █ ▒▒░ ███▓▓█ ▒█▒▒▒
 ████░ ▒▓█▓ ██▒▒▒ ▓███▒
 ░▒█▓▓██ ▓█▒ ▓█▒▓██▓ ░█░
 ▓░▒▓████▒ ██ ▒█ █▓░▒█▒░▒█▒
 ███▓░██▓ ▓█ █ █▓ ▒▓█▓▓█▒
 ░██▓ ░█░ █ █▒ ▒█████▓▒ ██▓░▒
 ███░ ░ █░ ▓ ░█ █████▒░░ ░█░▓ ▓░
 ██▓█ ▒▒▓▒ ▓███████▓░ ▒█▒ ▒▓ ▓██▓
 ▒██▓ ▓█ █▓█ ░▒█████▓▓▒░ ██▒▒ █ ▒ ▓█▒
 ▓█▓ ▓█ ██▓ ░▓▓▓▓▓▓▓▒ ▒██▓ ░█▒
 ▓█ █ ▓███▓▒░ ░▓▓▓███▓ ░▒░ ▓█
 ██▓ ██▒ ░▒▓▓███▓▓▓▓▓██████▓▒ ▓███ █
▓███▒ ███ ░▓▓▒░░ ░▓████▓░ ░▒▓▒ █▓
█▓▒▒▓▓██ ░▒▒░░░▒▒▒▒▓██▓░ █▓
██ ▓░▒█ ▓▓▓▓▒░░ ▒█▓ ▒▓▓██▓ ▓▒ ▒▒▓
▓█▓ ▓▒█ █▓░ ░▒▓▓██▒ ░▓█▒ ▒▒▒░▒▒▓█████▒
 ██░ ▓█▒█▒ ▒▓▓▒ ▓█ █░ ░░░░ ░█▒
 ▓█ ▒█▓ ░ █░ ▒█ █▓
 █▓ ██ █░ ▓▓ ▒█▓▓▓▒█░
 █▓ ░▓██░ ▓▒ ▓█▓▒░░░▒▓█░ ▒█
 ██ ▓█▓░ ▒ ░▒█▒██▒ ▓▓
 ▓█▒ ▒█▓▒░ ▒▒ █▒█▓▒▒░░▒██
 ░██▒ ▒▓▓▒ ▓██▓▒█▒ ░▓▓▓▓▒█▓
 ░▓██▒ ▓░ ▒█▓█ ░░▒▒▒
 ▒▓▓▓▓▓▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░▓▓ ▓░▒█░

F L I N K - S C A L A - S H E L L

NOTE: Use the prebound Execution Environments and Table Environment to implement batch or streaming programs.

Batch - Use the 'benv' and 'btenv' variable

* val dataSet = benv.readTextFile(""/path/to/data"")
 * dataSet.writeAsText(""/path/to/output"")
 * benv.execute(""My batch program"")
 *
 * val batchTable = btenv.fromDataSet(dataSet)
 * btenv.registerTable(""tableName"", batchTable)
 * val result = btenv.sqlQuery(""SELECT * FROM tableName"").collect
 HINT: You can use print() on a DataSet to print the contents or collect()
 a sql query result back to the shell.

Streaming - Use the 'senv' and 'stenv' variable

* val dataStream = senv.fromElements(1, 2, 3, 4)
 * dataStream.countWindowAll(2).sum(0).print()
 *
 * val streamTable = stenv.fromDataStream(dataStream, 'num)
 * val resultTable = streamTable.select('num).where('num % 2 === 1 )
 * resultTable.toAppendStream[Row].print()
 * senv.execute(""My streaming program"")
 HINT: You can only print a DataStream to the shell in local mode.

scala>;;;","13/Jun/20 17:51;yunta;[~appleyuchi] Thanks for your enthusiasm to contribute!
However, I have several suggestions:
# Please not use Chinese in JIRA tickets as Flink has developers across the world and not everyone could read Chinese.
# I think your solution is not correct as it print too many useless logs in console, e.g. ""13/06/2020 23:23:51.327 INFO [org.apache.flink.configuration.GlobalConfiguration] Loading configuration property: jobmanager.rpc.address, Desktop""
# Flink-1.11.0 has jumped to log4j2 (see FLINK-15672), and you can try code at release-1.11 branch to build a new clean flink-dist directory to see whether your problems still existed. From my side, I noticed the annoying outputs has gone.;;;","14/Jun/20 01:45;appleyuchi;Thanks for your replies

1.you may know that last official fix is only for start-scala-shell.sh,not for log4j.properties.

[https://github.com/apache/flink/pull/7371]

 

are you sure your log4j.propertires is left as default when you test it?

 

2.This is already 1.11 code from newest github.

start-scala-shell.sh->xxx.xml->log4j.properties,

this is the flow.

3. this warning can be caused by no relation to whether it's  log4j1 or log4j2.

*only 3 parts of a software:*

*configuration file, startscript and jars from mvn with pom.xml*

I use the default ""configuration file, startscript"" from 1.11.

*so the only difference between mine and yours is from*  *jars from mvn with pom.xml?*

*it's strange that the developers will write log4j settings  both in src code and configuration file in separated parts.*

*it they did not write them in separated parts, why do I need to build it?*

 

*I can not build it ,so much errors if I use default pom.xml provided by flink1.11.*;;;","14/Jun/20 02:10;appleyuchi;quotation from yours:

""I think your solution is not correct as it print too many useless logs in console, e.g. ""13/06/2020 23:23:51.327 INFO ""

*you can suppress it after you set ""WARN level"" in log4j.properties and these INFO will disappear.*

*This issue and modification is not about ""warning level"",but misconfiguration,*

 

 

*And could you release 1.9.4 in [https://flink.apache.org/downloads.html]*

*which fix this bug？*

*Thanks*;;;","14/Jun/20 03:48;appleyuchi;according to your  *suggestion 3*:

*""Flink-1.11.0 has jumped to log4j2 (see FLINK-15672),you can try code at release-1.11 branch to build a new clean flink-dist directory to see whether your problems still existed""*

 

here's the result from newest flink repository:

 

(Python3.6) appleyuchi@Desktop:flink$ find ./ -name ""*log4j.properties*""
 ./tools/ci/log4j.properties
 *./flink-dist*/src/main/flink-bin/conf/*log4j.properties*
 ./flink-docs/src/main/resources/*log4j.properties*

(Python3.6) appleyuchi@Desktop:flink$ find ./ -name ""*log4j2.properties*""
 ./flink-examples/flink-examples-streaming/src/main/resources/*log4j2.properties*
 ./flink-examples/flink-examples-batch/src/main/resources/*log4j2.properties*
 ./flink-examples/flink-examples-table/src/main/resources/*log4j2.properties*
 ./flink-libraries/flink-gelly-examples/src/main/resources/*log4j2.properties*
 ./flink-quickstart/flink-quickstart-java/src/main/resources/archetype-resources/src/main/resources/*log4j2.properties*
 ./flink-quickstart/flink-quickstart-scala/src/main/resources/archetype-resources/src/main/resources/*log4j2.properties*
 ./flink-walkthroughs/flink-walkthrough-table-java/src/main/resources/archetype-resources/src/main/resources/*log4j2.properties*
 ./flink-walkthroughs/flink-walkthrough-table-scala/src/main/resources/archetype-resources/src/main/resources/*log4j2.properties*
 ./flink-walkthroughs/flink-walkthrough-datastream-scala/src/main/resources/archetype-resources/src/main/resources/*log4j2.properties*
 ./flink-walkthroughs/flink-walkthrough-datastream-java/src/main/resources/archetype-resources/src/main/resources/*log4j2.properties*

 

*log4j2.properties is no relation to flink-dist,*

*look forward to your new suggestions。*

 

 ;;;","14/Jun/20 08:01;yunta;[~appleyuchi]
Feel a bit surprised that you replied so many comments but not ever build your own Flink-1.11 from starch. 

I think commands below could help you:

{code:bash}
git clone -b release-1.11 https://github.com/apache/flink.git
cd flink
mvn clean package -pl flink-dist -am -DskipTests -Dcheckstyle.skip  -Pskip-webui-build
{code}

The reason why I mentioned log4j2 is because previous annoying outputs would only occur with log4j1.

I rebuild Flink-1.11 and verified on my local mac and CentOS, the annoying outputs have gone.
;;;","14/Jun/20 11:06;appleyuchi;log4j1 is fixed in above [https://github.com/apache/flink/pull/7371],why did it not work?;;;","14/Jun/20 11:07;appleyuchi;skip web ui is not right.;;;","14/Jun/20 12:30;appleyuchi;I have built it,but how to build web ui?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch6DynamicSink#asSummaryString() return identifier typo,FLINK-18277,13311137,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhongqishang,zhongqishang,zhongqishang,12/Jun/20 13:13,13/Apr/21 20:41,13/Jul/23 08:12,14/Jun/20 14:16,,,,,,1.11.0,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,"identifier Spelling mistakes
`org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink#asSummaryString`
{code:java}
	@Override
	public String asSummaryString() {
		return ""Elasticsearch7"";
	}
{code}
",,jark,leonard,zhongqishang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 14:48:42 UTC 2020,,,,,,,,,,"0|z0fsrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/20 13:17;zhongqishang;[~jark] can you assign to me and I will be familiar with the submission process.;;;","12/Jun/20 15:05;jark;Good catch!;;;","14/Jun/20 14:48;jark;Fixed in
 - master (1.12.0): da33f940a42aa72d27ca1af6b84290c74ea9a9c1
 - 1.11.0: 823b02319d33a83a3621fedac165bd250c33d155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystemLookupFunction can fail if the file gets updated/deleted while cache is reloaded,FLINK-18272,13311072,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,12/Jun/20 07:25,18/Jun/20 03:33,13/Jul/23 08:12,18/Jun/20 03:33,,,,,,1.11.0,,,FileSystems,,,,,0,pull-request-available,,,,,,,libenchao,lirui,lsy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 03:33:47 UTC 2020,,,,,,,,,,"0|z0fsd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 09:11;lirui;When we hit some exceptions to load the table, it's hard to tell whether it's because the table is being updated (in which case we should wait a little bit and retry), or it's because there's some problem with the underlying files (in which case the job should be aborted).
I'll add some logic to retry the load in {{FileSystemLookupFunction}}. Hopefully that'll mitigate the issue.;;;","15/Jun/20 09:18;lzljs3620320;Currently lookup join implementations like HBase, JDBC, also implement retry + sleep, it is a good way to avoid failure when there are some accidental problems in external system.;;;","18/Jun/20 03:33;lzljs3620320;master: 40621c733c1647c5745d2795eaa32b6cc7dae3ae

release-1.11: 80fa0f5c5b8600f4b386487f267bde80b882bd07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct Table API in Temporal table docs,FLINK-18268,13311045,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,12/Jun/20 03:52,16/Jun/20 04:17,13/Jul/23 08:12,16/Jun/20 04:17,1.11.0,,,,,1.11.0,,,Documentation,,,,,0,pull-request-available,,,,,"see user's feedback[1]:

The  *getTableEnvironment* method has been dropped, but the documentation still use it
{code:java}
 val tEnv = TableEnvironment.getTableEnvironment(env)
{code}
[1][http://apache-flink.147419.n8.nabble.com/flink-TableEnvironment-can-not-call-getTableEnvironment-api-tt3871.html]

 ",,dwysakowicz,leonard,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 04:17:57 UTC 2020,,,,,,,,,,"0|z0fs74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/20 06:33;dwysakowicz;Could you please elaborate what is this issue about? The attached link is in Chinese.;;;","12/Jun/20 06:51;leonard;Sure, updated the description.;;;","12/Jun/20 06:58;dwysakowicz;Thanks!;;;","16/Jun/20 04:17;lzljs3620320;master: fea20adef1ab5f722e418c1a89600e0786208469

release-1.11: 9f1053fdac461b0b7ec3597c5cd6b8262bcdfc9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hidden files should be ignored when the filesystem table searches for partitions,FLINK-18265,13311037,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,lzljs3620320,lzljs3620320,12/Jun/20 03:04,15/Jun/20 01:55,13/Jul/23 08:12,15/Jun/20 01:55,,,,,,1.11.0,,,Connectors / FileSystem,Table SQL / API,,,,0,pull-request-available,,,,,"If there are some hidden files in the path of filesystem partitioned table, query this table will occur:
{code:java}
Caused by: org.apache.flink.table.api.TableException: Partition keys are: [j], incomplete partition spec: {}
        at org.apache.flink.table.filesystem.FileSystemTableSource.toFullLinkedPartSpec(FileSystemTableSource.java:209) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.filesystem.FileSystemTableSource.access$800(FileSystemTableSource.java:62) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.filesystem.FileSystemTableSource$1.lambda$getPaths$0(FileSystemTableSource.java:174) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_152]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) ~[?:1.8.0_152]
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438) ~[?:1.8.0_152]
        at org.apache.flink.table.filesystem.FileSystemTableSource$1.getPaths(FileSystemTableSource.java:177) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code}
Hidden files should be ignored when the filesystem table searches for partitions. This is not correct partition.",,godfreyhe,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 01:55:13 UTC 2020,,,,,,,,,,"0|z0fs5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 01:55;lzljs3620320;master:

777a1931ca0445ef4b956e00c565a097e1e4c168

3916da093fadd25bf54417f3be5e91f4720d90e5

release-1.11: 

e66279c1b683dabffe233b37066c42cd21450c59

89c224e12c198d35a303487c4a2ab73e968214e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-orc and flink-parquet have invalid NOTICE file,FLINK-18261,13310959,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,rmetzger,rmetzger,11/Jun/20 17:51,16/Jun/20 02:14,13/Jul/23 08:12,16/Jun/20 02:14,1.11.0,,,,,1.11.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"flink-orc provides a {{-jar-with-dependencies.jar}} variant which ships binaries.
However, these binaries are not documented in {{META-INF/NOTICE}}.
There are two similar files in that directory (NOTICE from force-shading and NOTICE.txt from Commons Lang). 

There is a NOTICE file that looks valid, but it is in {{META-INF/services}}.


I assume this has been introduced in FLINK-17460.",,leonard,libenchao,lzljs3620320,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17460,,,,,,,,,,FLINK-18256,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 02:14:02 UTC 2020,,,,,,,,,,"0|z0fro0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 17:53;rmetzger;It's probably a good idea to use the maven-shade plugin for building the fat jar: It will make it easier to create and validate the NOTICE file.;;;","12/Jun/20 01:54;lzljs3620320;Hi [~rmetzger], thanks for reporting. But maven-shade can not create a new jar? Maybe we need create two new modules: {{flink-sql-format-orc}} and {{flink-sql-format-parquet}} for creating bundled jars? What do you think?;;;","12/Jun/20 02:19;lzljs3620320;Considering that SQL layer dependencies can be reduced, for example, ""parquet-avro"" does not need to be bundled in. I create a flink-sql-parquet to create bundled jar.;;;","16/Jun/20 02:14;lzljs3620320;master: 6388336b0c56589c3a77e38f8fd16f582e2d947c

release-1.11: 040de969fbf30072fc2ef2f0e6eac3e89570f625;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HeartbeatManagerTest.testHeartbeatCluster unstable,FLINK-18259,13310918,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,rmetzger,rmetzger,11/Jun/20 13:13,12/Jun/20 13:48,13/Jul/23 08:12,12/Jun/20 13:48,1.12.0,,,,,1.11.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3265&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.209 s <<< FAILURE! - in org.apache.flink.runtime.heartbeat.HeartbeatManagerTest
[ERROR] testHeartbeatCluster(org.apache.flink.runtime.heartbeat.HeartbeatManagerTest)  Time elapsed: 0.218 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.testHeartbeatCluster(HeartbeatManagerTest.java:236)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 13:48:27 UTC 2020,,,,,,,,,,"0|z0frew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 16:29;trohrmann;I think the problem is that the heartbeat timeout for this test is too low (100ms). On slow machines it can take apparently more time until a heartbeat is successfully sent. I propose to increase the timeouts a bit.;;;","12/Jun/20 13:48;trohrmann;Fixed via

master: 4d8f55e14de053c99621c5cfd3c4cf13a9c4ad02
1.11.0: d6364cec088d5b3439da7077b053120e12880769;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop dependencies are wrongly bundled into flink-orc,FLINK-18256,13310900,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,11/Jun/20 11:16,12/Jun/20 06:36,13/Jul/23 08:12,12/Jun/20 06:36,1.11.0,1.12.0,,,,1.11.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"The {{hadoop-common}} and {{hadoop-hdfs}} have been bundled into the {{flink-orc_2.11-1.11-SNAPSHOT-jar-with-dependencies.jar}}. Since we suggest our users to put this jar into lib directory or add dependency in their pom to build fat jar, it might be incorrect to have hadoop here. Flink now is not hadoop free, we have some specific hadoop version classes in the released jars.

 

> How to fix?

Let flink-orc depends on {{hadoop-common}} and {{hadoop-hdfs}} directly and make them provided.

 ",,aljoscha,rmetzger,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18261,,,,,,FLINK-17978,,FLINK-11086,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 06:36:27 UTC 2020,,,,,,,,,,"0|z0fraw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 11:17;wangyang0918;[~lzljs3620320] Could you assign this ticket to me? I will attach a PR to fix this.;;;","11/Jun/20 14:09;aljoscha;We should only assign fixVersion = 1.11.0, not 1.12.0, because 1.12.0 technically never had the bug if we fix it before releasing. Even though there are now two separate branches.;;;","12/Jun/20 02:09;wangyang0918;[~aljoscha], thanks for your kindly reminding.;;;","12/Jun/20 06:36;rmetzger;Merged to master in [https://github.com/apache/flink/commit/241962751e83fc1ad7e2c02516c081d6c5f8aabd]

merged to release-1.11 in [https://github.com/apache/flink/commit/2f4956f715d97a1fa474e470517b25300f78cdcf];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Savepoint (file, async, no parallelism change) end-to-end test fails with unaligned checkpoint enabled",FLINK-18252,13310858,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,AHeise,11/Jun/20 08:37,22/Jun/21 14:06,13/Jul/23 08:12,12/Jun/20 14:51,,,,,,1.11.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,https://dev.azure.com/arvidheise0209/arvidheise/_build/results?buildId=298&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=2f5b54d0-1d28-5b01-d344-aa50ffe0cdf8,,AHeise,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 14:51:22 UTC 2020,,,,,,,,,,"0|z0fr1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/20 14:51;pnowojski;merged commit f981316 into apache:release-1.11
merged commit dd3717e into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test: TableITCase.testCollectWithClose:122 expected:<CANCELED> but was:<CANCELLING>,FLINK-18247,13310824,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,rmetzger,rmetzger,11/Jun/20 06:35,12/Jun/20 01:57,13/Jul/23 08:12,12/Jun/20 01:57,1.11.0,1.12.0,,,,1.11.0,,,Table SQL / API,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3202&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

{code}
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.159 s <<< FAILURE! - in org.apache.flink.table.api.TableITCase
[ERROR] testCollectWithClose[TableEnvironment:isStream=false](org.apache.flink.table.api.TableITCase)  Time elapsed: 0.567 s  <<< FAILURE!
java.lang.AssertionError: expected:<CANCELED> but was:<CANCELLING>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.table.api.TableITCase.testCollectWithClose(TableITCase.scala:122)

{code}",,godfreyhe,lzljs3620320,nicholasjiang,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 01:57:18 UTC 2020,,,,,,,,,,"0|z0fqu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 07:11;godfreyhe;Thanks for reporting this, {{assertEquals(JobStatus.CANCELED, tableResult.getJobClient.get().getJobStatus().get())}} in {{TableITCase#testCollectWithClose}} is not rigorous.  The expected job status should be not {{RUNNING}}, instead of must be {{CANCELED}};;;","11/Jun/20 07:12;nicholasjiang;[~rmetzger], because job status may not be CANCELED when iterator is closing. [~godfreyhe] could fix this test case.;;;","11/Jun/20 07:12;godfreyhe;I would like to fix this;;;","11/Jun/20 10:38;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3244&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29;;;","11/Jun/20 12:26;lzljs3620320;master: 0b0a6bc60b04e9b838db0e63ae17d0d4885888ae

[~godfreyhe] please create a PR for release-1.11 (There are some conflicts);;;","12/Jun/20 01:57;lzljs3620320;release-1.11: bf5e7d6ac5256dec01a772ebccbbcfa8c971ae81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink e2e fails with java version mismatch on JDK11 nightly build,FLINK-18246,13310823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,rmetzger,rmetzger,11/Jun/20 06:28,12/Jun/20 07:29,13/Jul/23 08:12,12/Jun/20 07:29,1.12.0,,,,,1.12.0,,,API / Python,Build System,Tests,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3213&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=679407b1-ea2c-5965-2c8d-1467777fff88

{code}
Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)
No taskexecutor daemon (pid: 123813) is running anymore on fv-az670.
No standalonesession daemon to stop on host fv-az670.

{code}
",,dian.fu,rmetzger,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 07:29:30 UTC 2020,,,,,,,,,,"0|z0fqts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 06:28;rmetzger;This could also be a problem of the Azure setup, not sure.;;;","11/Jun/20 09:12;zhongwei;Hi [~rmetzger]. The cause of the test failure is that the docker image used in the PyFlink yarn per-job tests is using Java 8. It is the same as FLINK-13719. I think we could temporarily disable the PyFlink e2e test when running on jdk11, and enable it after resolving FLINK-13719.;;;","12/Jun/20 07:29;dian.fu;merged to master via bfcd800b41a5f018f7941137de40de1fa5d39811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom OptionsFactory settings seem to have no effect on RocksDB,FLINK-18242,13310627,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyu,nkruber,nkruber,10/Jun/20 11:25,21/Jun/20 04:59,13/Jul/23 08:12,21/Jun/20 04:59,1.10.0,1.10.1,1.11.0,,,1.10.2,1.11.0,1.12.0,Runtime / State Backends,,,,,0,pull-request-available,,,,," When I configure a custom {{OptionsFactory}} for RocksDB like this (similarly by specifying it via the {{state.backend.rocksdb.options-factory}} configuration):
{code:java}
Configuration globalConfig = GlobalConfiguration.loadConfiguration();
String checkpointDataUri = globalConfig.getString(CheckpointingOptions.CHECKPOINTS_DIRECTORY);
RocksDBStateBackend stateBackend = new RocksDBStateBackend(checkpointDataUri);
stateBackend.setOptions(new DefaultConfigurableOptionsFactoryWithLog());
env.setStateBackend((StateBackend) stateBackend);{code}
it seems to be loaded
{code:java}
2020-06-10 12:54:20,720 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend  - Using predefined options: DEFAULT.
2020-06-10 12:54:20,721 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend  - Using application-defined options factory: DefaultConfigurableOptionsFactoryWithLog{DefaultConfigurableOptionsFactory{configuredOptions={}}}. {code}
but it seems like none of the options defined in there is actually used. Just as an example, my factory does set the info log level to {{INFO_LEVEL}} but this is what you will see in the created RocksDB instance:
{code:java}
> cat /tmp/flink-io-c95e8f48-0daa-4fb9-a9a7-0e4fb42e9135/*/db/OPTIONS*|grep info_log_level
  info_log_level=HEADER_LEVEL
  info_log_level=HEADER_LEVEL{code}
Together with the bug from FLINK-18241, it seems I cannot re-activate the RocksDB log that we disabled in FLINK-15068. FLINK-15747 was aiming at changing that particular configuration, but the problem seems broader since {{setDbLogDir()}} was actually also ignored and Flink itself does not change that setting.",,klion26,liyu,nkruber,sewen,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 11:18;nkruber;DefaultConfigurableOptionsFactoryWithLog.java;https://issues.apache.org/jira/secure/attachment/13005362/DefaultConfigurableOptionsFactoryWithLog.java",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 21 04:59:56 UTC 2020,,,,,,,,,,"0|z0fpmo:",9223372036854775807,"After FLINK-18242, the deprecated `OptionsFactory` and `ConfigurableOptionsFactory` classes are removed (not applicable for release-1.10), please use `RocksDBOptionsFactory` and `ConfigurableRocksDBOptionsFactory` instead. Please also recompile your application codes if any class extending `DefaultConfigurableOptionsFactory`.",,,,,,,,,,,,,,,,,,,"11/Jun/20 03:28;yunta;[~NicoK] The root cause is current {{[RocksDBResourceContainer|https://github.com/apache/flink/blob/88cc44afbfb5267e2674ecb735561365e735d2b0/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBResourceContainer.java#L90]}} would only deal with new interface {{DefaultConfigurableOptionsFactory#createDBOptions(DBOptions currentOptions, Collection<AutoCloseable> handlesToClose)}}.
As you only override the deprecated interface, that's why the new options not take effect.

I think this bug would only occur if user extends {{DefaultConfigurableOptionsFactory}} as we migrate {{DefaultConfigurableOptionsFactory}} to new {{RocksDBOptionsFactory}} but still keep previous interfaces. Not sure whether extending {{DefaultConfigurableOptionsFactory}} has been ‘abused’ among users.;;;","11/Jun/20 04:37;liyu;Thanks for the analysis [~yunta], but you may have neglected below code path in `RocksDBStateBackend#setOptions`:
{code}
	public void setOptions(OptionsFactory optionsFactory) {
		this.rocksDbOptionsFactory = optionsFactory instanceof RocksDBOptionsFactory
				? (RocksDBOptionsFactory) optionsFactory
				: new RocksDBOptionsFactoryAdapter(optionsFactory);
	}
{code}

While in `RocksDBOptionsFactoryAdaptor` we have below method:
{code}
	public DBOptions createDBOptions(DBOptions currentOptions, Collection<AutoCloseable> handlesToClose) {
		return optionsFactory.createDBOptions(currentOptions);
	}
{code}

Please notice that we also have below instructions in `RocksDBOptionsFactory` javadoc:
{code}
	/**
	 * Do not override these methods, they are only to maintain interface compatibility with
	 * prior versions. They will be removed in one of the next versions.
	 */
	@Override
	default DBOptions createDBOptions(DBOptions currentOptions) {
		return createDBOptions(currentOptions, new ArrayList<>());
	}
{code}

And since the usage mentioned in description here actually breaks the above rule, IMHO it's a user error.

However, we should take some actions to prevent this from happening again, possible ones including:
1. Make `DefaultConfigurableOptionsFactory` a final class. This may break the compilation of existing application code, but it's way better than having the problem mentioned here.
2. Write more explicit notice and suggestions in our documentation.;;;","11/Jun/20 10:25;nkruber;Thanks for the analysis - after changing to the new interface, the options factory did indeed work as expected.

It seems that I got into this situation by compiling against Flink 1.9 and executing in 1.10: Since I did not bundle the {{DefaultOptionsFactory}} class from 1.9, at runtime Flink would only see the 1.10 file and interfaces and thus missed my {{createDBOptions(DBOptions currentOptions)}}. Changing to compile against 1.10 did not help me noticing the interface change since the old code was still valid.

IMHO, this silent API change may be a problem (for some users): since there are new getters/setters for the new {{RocksDBOptionsFactory}} and there is the {{RocksDBOptionsFactoryAdapter}}, it would not have to inherit from {{OptionsFactory}} to keep backwards compatibility. This may be a different story for {{DefaultOptionsFactory}} (which may have needed both interfaces and then special handling, or a {{final createDBOptions(DBOptions currentOptions)}}, or just live with the API change) but neither of the mentioned classes has an API annotation, not even {{@PublicEvolving}}.

# I would be against making {{DefaultConfigurableOptionsFactory}} final as this does not solve anything and the class may actually be useful if you can tune parameters via the cluster config and only need some additional manual settings that are not exposed via config.
# just adding more documentation would probably not have helped here, at least not for old code;;;","11/Jun/20 11:18;liyu;I agree that making `DefaultConfigurableOptionsFactory` final is not a thorough solution. After a second thought, we should consider completely separating `RocksDBOptionsFactory` and `OptionsFactory`. The only problem would be how to make `RocksDBStateBackend#setOptions` backward compatible (accepting an `OptionsFactory` and assign to a `RocksDBOptionsFactory` variable) - I think another {{OptionsFactory->RocksDBOptionsFactory}} adapter could make it work.

Let me prepare a PR to better Illustrate the idea.;;;","15/Jun/20 16:12;sewen;I see this is a thing we need to fix in 1.10.x.

For 1.11 / 1.12 what do you think about dropping the {{OptionsFactory}} and only go ahead with the {{RocksDBOptionsFactory}}?
That would be less confusing for the users and we had the {{OptionsFactory}} deprecated in the previous release.
;;;","16/Jun/20 04:37;liyu;bq. For 1.11 / 1.12 what do you think about dropping the OptionsFactory and only go ahead with the RocksDBOptionsFactory?

+1, let me prepare the PRs separately.;;;","17/Jun/20 14:03;liyu;Merged into master via 5ed371fb17ee842240dbb886bf5fdefab6d9db62;;;","18/Jun/20 01:48;liyu;Merged into release-1.11 via e13146f80114266aa34c9fe9f3dc27e87f7a7649;;;","21/Jun/20 04:59;liyu;Merged into release-1.10 via fd91affd8040123e2f757b24859b7dad33e09532

Closing the JIRA since all work done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom OptionsFactory in user code not working when configured via flink-conf.yaml,FLINK-18241,13310626,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,nkruber,nkruber,10/Jun/20 11:15,12/Jun/20 06:42,13/Jul/23 08:12,12/Jun/20 06:42,1.10.0,1.10.1,,,,1.10.2,1.11.0,,Runtime / Configuration,Runtime / State Backends,,,,0,pull-request-available,,,,,"It seems like Flink 1.10 broke custom {{OptionsFactory}} definitions via the {{state.backend.rocksdb.options-factory}} configuration if the implementation resides in the user-code jar file. This is particularly bad to debug RocksDB issues since we disabled its (ever-growing) LOG file in FLINK-15068.

If you look at the stack trace from the error below, you will notice, that {{StreamExecutionEnvironment}} is not provided with a user-code classloader and will us the one of its own class which is the parent loader that does not know about our {{OptionsFactory}}. This exact same code was working with Flink 1.9.3.

(I believe putting the custom {{OptionsFactory}} into a separate jar file inside Flink's lib folder may be a workaround but that should ideally not be needed).
{code:java}
2020-06-09 16:18:59,409 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Could not start cluster entrypoint StandaloneJobClusterEntryPoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneJobClusterEntryPoint.
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:192) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:525) [flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.main(StandaloneJobClusterEntryPoint.java:116) [flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
Caused by: org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:261) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        ... 2 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Could not retrieve the JobGraph.
        at org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcessFactoryFactory.createFactory(JobDispatcherLeaderProcessFactoryFactory.java:57) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.createDispatcherRunner(DefaultDispatcherRunnerFactory.java:51) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:196) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        ... 2 more
Caused by: org.apache.flink.util.FlinkException: Could not create the JobGraph from the provided user code jar.
        at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.retrieveJobGraph(ClassPathJobGraphRetriever.java:114) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcessFactoryFactory.createFactory(JobDispatcherLeaderProcessFactoryFactory.java:55) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.createDispatcherRunner(DefaultDispatcherRunnerFactory.java:51) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:196) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        ... 2 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.util.DynamicCodeLoadingException: Cannot find configured options factory class: com.ververica.DefaultConfigurableOptionsFactoryWithLog
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.OptimizerPlanEnvironment.getPipeline(OptimizerPlanEnvironment.java:80) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:108) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:58) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.retrieveJobGraph(ClassPathJobGraphRetriever.java:104) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcessFactoryFactory.createFactory(JobDispatcherLeaderProcessFactoryFactory.java:55) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.createDispatcherRunner(DefaultDispatcherRunnerFactory.java:51) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:196) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        ... 2 more
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.util.DynamicCodeLoadingException: Cannot find configured options factory class: com.ververica.DefaultConfigurableOptionsFactoryWithLog
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.<init>(RocksDBStateBackend.java:376) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configure(RocksDBStateBackend.java:394) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:47) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:32) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.state.StateBackendLoader.loadStateBackendFromConfig(StateBackendLoader.java:154) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.loadStateBackend(StreamExecutionEnvironment.java:767) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:750) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:218) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:190) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamPlanEnvironment.<init>(StreamPlanEnvironment.java:38) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createStreamExecutionEnvironment(StreamExecutionEnvironment.java:1871) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1859) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at com.ververica.flinktraining.exercises.troubleshoot.TroubledStreamingJobUtils.createConfiguredEnvironment(TroubledStreamingJobUtils.java:40) ~[?:?]
        at com.ververica.flinktraining.solutions.troubleshoot.TroubledStreamingJobSolution43.main(TroubledStreamingJobSolution43.java:40) ~[?:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.OptimizerPlanEnvironment.getPipeline(OptimizerPlanEnvironment.java:80) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:108) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:58) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.retrieveJobGraph(ClassPathJobGraphRetriever.java:104) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcessFactoryFactory.createFactory(JobDispatcherLeaderProcessFactoryFactory.java:55) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.createDispatcherRunner(DefaultDispatcherRunnerFactory.java:51) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:196) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        ... 2 more
Caused by: org.apache.flink.util.DynamicCodeLoadingException: Cannot find configured options factory class: com.ververica.DefaultConfigurableOptionsFactoryWithLog
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configureOptionsFactory(RocksDBStateBackend.java:605) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.<init>(RocksDBStateBackend.java:370) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configure(RocksDBStateBackend.java:394) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:47) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:32) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.state.StateBackendLoader.loadStateBackendFromConfig(StateBackendLoader.java:154) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.loadStateBackend(StreamExecutionEnvironment.java:767) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:750) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:218) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:190) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamPlanEnvironment.<init>(StreamPlanEnvironment.java:38) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createStreamExecutionEnvironment(StreamExecutionEnvironment.java:1871) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1859) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at com.ververica.flinktraining.exercises.troubleshoot.TroubledStreamingJobUtils.createConfiguredEnvironment(TroubledStreamingJobUtils.java:40) ~[?:?]
        at com.ververica.flinktraining.solutions.troubleshoot.TroubledStreamingJobSolution43.main(TroubledStreamingJobSolution43.java:40) ~[?:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.OptimizerPlanEnvironment.getPipeline(OptimizerPlanEnvironment.java:80) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:108) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:58) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.retrieveJobGraph(ClassPathJobGraphRetriever.java:104) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcessFactoryFactory.createFactory(JobDispatcherLeaderProcessFactoryFactory.java:55) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.createDispatcherRunner(DefaultDispatcherRunnerFactory.java:51) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:196) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        ... 2 more
Caused by: java.lang.ClassNotFoundException: com.ververica.DefaultConfigurableOptionsFactoryWithLog
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_252]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_252]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_252]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_252]
        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_252]
        at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_252]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configureOptionsFactory(RocksDBStateBackend.java:594) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.<init>(RocksDBStateBackend.java:370) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configure(RocksDBStateBackend.java:394) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:47) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.createFromConfig(RocksDBStateBackendFactory.java:32) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.state.StateBackendLoader.loadStateBackendFromConfig(StateBackendLoader.java:154) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.loadStateBackend(StreamExecutionEnvironment.java:767) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:750) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:218) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:190) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamPlanEnvironment.<init>(StreamPlanEnvironment.java:38) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createStreamExecutionEnvironment(StreamExecutionEnvironment.java:1871) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1859) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at com.ververica.flinktraining.exercises.troubleshoot.TroubledStreamingJobUtils.createConfiguredEnvironment(TroubledStreamingJobUtils.java:40) ~[?:?]
        at com.ververica.flinktraining.solutions.troubleshoot.TroubledStreamingJobSolution43.main(TroubledStreamingJobSolution43.java:40) ~[?:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.OptimizerPlanEnvironment.getPipeline(OptimizerPlanEnvironment.java:80) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:108) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:58) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.retrieveJobGraph(ClassPathJobGraphRetriever.java:104) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcessFactoryFactory.createFactory(JobDispatcherLeaderProcessFactoryFactory.java:55) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.createDispatcherRunner(DefaultDispatcherRunnerFactory.java:51) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:196) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:220) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:174) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:173) ~[flink-dist_2.12-1.10.1-stream1.jar:1.10.1-stream1]{code}
I attached the {{OptionsFactory}} that I used to show the error. (just put it into the user code jar and configure
{code:java}
state.backend.rocksdb.options-factory: com.ververica.DefaultConfigurableOptionsFactoryWithLog {code}",,aljoscha,dwysakowicz,klion26,liyu,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 11:07;nkruber;DefaultConfigurableOptionsFactoryWithLog.java;https://issues.apache.org/jira/secure/attachment/13005361/DefaultConfigurableOptionsFactoryWithLog.java",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 06:42:46 UTC 2020,,,,,,,,,,"0|z0fpmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 11:49;nkruber;I just tried with Flink 1.11-SNAPSHOT and the problem seems to be fixed there, however, I have the same problem as described in FLINK-18242 (I'll update that one and extend the description);;;","11/Jun/20 12:43;dwysakowicz;The problem is that the StreamPlanEnvironment/OptimizerPlanEnvironment does not set user class loader correctly.  Those are used when submitting a job through web ui and in per job mode in 1.10. 

In 1.11 they are not used any longer, but we use a ContextEnvironment which sets the classloader correctly and thus it works.  (they are still used for {{flink info}} which I think will also fail in 1.11)

In 1.9 it works because there is no logic that would read it when constructing the environment. It uses a completely different path which uses the correct classloader on the cluster side.

As a side note all versions work correctly if we use the cli (flink run)

I will prepare patches for 1.10/1.11/master for setting the correct classloader.

;;;","12/Jun/20 06:42;dwysakowicz;Fixed:
* master
** 039a74afe572a03c3cef5dd3643e9e2e52e5b4e5
* 1.11.0
** a6557f66435973b65967af7dd1d893f748f7feae
* 1.10.2
** c2e2c6eb0694f32cdb88bf45a2d00aae0b2e9c72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct modulus function usage in documentation or allow % operator,FLINK-18240,13310623,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,jark,jark,10/Jun/20 10:53,05/Jul/20 03:15,13/Jul/23 08:12,05/Jul/20 02:22,,,,,,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,,"In the documentation: https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/queries.html#scan-projection-and-filter

There is an example:

{code}
SELECT * FROM Orders WHERE a % 2 = 0
{code}

But % operator is not allowed in Flink:


{code:java}
org.apache.calcite.sql.parser.SqlParseException: Percent remainder '%' is not allowed under the current SQL conformance level
{code}

Either we correct the documentation to use {{MOD}} function, or allow % operator. 

This is reported in user-zh ML: http://apache-flink.147419.n8.nabble.com/FLINK-SQL-td3822.html",,felixzheng,fsk119,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18486,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 05 02:22:20 UTC 2020,,,,,,,,,,"0|z0fpls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 11:03;libenchao;`%` is introduced in SQL:2011, in calcite only `BABEL`/`LENIENT`/`MYSQL_5` conformance supports it.

In Flink, we have our own conformance, IMO, we can support it by changing `FlinkSqlConformance#isPercentRemainderAllowed`;;;","10/Jun/20 11:58;jark;If % is included in SQL standard, I think it makes sense to allow it. Which setion mentions % operator in SQL:2011? ;;;","10/Jun/20 12:16;libenchao;I found it in wiki page[1]. I haven't got one complete copy of SQL:2011 standard.
After simply changing it in `FlinkSqlConformance`, the calcite will throw other exceptions. Maybe it's not so easy to open it.

[1] https://en.wikipedia.org/wiki/Modulo_operation;;;","10/Jun/20 12:56;leonard;Hi, [~jark] [~libenchao] 

Percent sign has been supported in calcite 1.14.0 (2017-10-01) [1] by our old friend [~sunjincheng121], I think we can support in Flink， maybe  `FlinkSqlConformance` and `FlinkSqlOperatorTable` and some others need to adapt.

[1]https://issues.apache.org/jira/browse/CALCITE-1897;;;","12/Jun/20 02:59;libenchao;[~Leonard Xu] I think you are right, we can support it this way. +1 to support it.;;;","02/Jul/20 12:52;fsk119;Hi [~jark] I'm very interested in this issue. Could you assign it to me? I'm willing to solve it.;;;","05/Jul/20 02:22;libenchao;Fixed via 4b9f9fe35256462851dbb8e0076114c32e345243 (1.12.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kubernetes e2e test fails with ""Kubernetes 1.18.3 requires conntrack to be installed in root's path"" ",FLINK-18239,13310619,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,rmetzger,rmetzger,10/Jun/20 10:43,10/Jun/20 16:53,13/Jul/23 08:12,10/Jun/20 16:53,1.11.0,1.12.0,,,,1.11.0,,,Deployment / Kubernetes,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3133&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
{code}
Docker version 19.03.11, build 42e35e61f3
docker-compose version 1.26.0, build d4451659
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
Starting minikube ...
* minikube v1.11.0 on Ubuntu 16.04
* Using the none driver based on user configuration
X Sorry, Kubernetes 1.18.3 requires conntrack to be installed in root's path
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
Command: start_kubernetes_if_not_running failed. Retrying...
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
Starting minikube ...
* minikube v1.11.0 on Ubuntu 16.04
* Using the none driver based on user configuration
X Sorry, Kubernetes 1.18.3 requires conntrack to be installed in root's path
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
Command: start_kubernetes_if_not_running failed. Retrying...
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
Starting minikube ...
* minikube v1.11.0 on Ubuntu 16.04
* Using the none driver based on user configuration
X Sorry, Kubernetes 1.18.3 requires conntrack to be installed in root's path
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
* There is no local cluster named ""minikube""
  - To fix this, run: ""minikube start""
Command: start_kubernetes_if_not_running failed. Retrying...
Command: start_kubernetes_if_not_running failed 3 times.
Could not start minikube. Aborting...
Debugging failed Kubernetes test:
Currently existing Kubernetes resources
The connection to the server localhost:8080 was refused - did you specify the right host or port?
The connection to the server localhost:8080 was refused - did you specify the right host or port?
Flink logs:

{code}",,aljoscha,rmetzger,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 16:53:23 UTC 2020,,,,,,,,,,"0|z0fpkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 10:43;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3134&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 10:46;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3140&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 10:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3140&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 10:48;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3144&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 10:52;rmetzger;Looks like K8s has upgraded from v1.17.3 to v1.18.3;;;","10/Jun/20 11:30;aljoscha;You mean we upgraded? Or it upgraded by itself? 😅;;;","10/Jun/20 11:31;aljoscha;This isn't even an instability, I think all my builds have this.

cc [~fly_in_gis];;;","10/Jun/20 11:39;chesnay;Let's just pin the version in {{flink-end-to-end-tests/test-scripts/common_kubernetes.sh:setup_kubernetes_for_linux}};;;","10/Jun/20 11:58;wangyang0918;It seems that the azure environment has changed. And it provides a default minikube with version v1.11.0. Before this we are using v1.8.2.

I will try to pin the minikube version for the current K8s e2e tests.;;;","10/Jun/20 12:10;rmetzger;bq. You mean we upgraded? Or it upgraded by itself? 😅

We are always testing against the latest stable version. If the K8s project decides to change that version, our test is affected as well.;;;","10/Jun/20 12:14;chesnay;[~fly_in_gis] How long do you think that will take? If it takes longer we may instead want to disable the kubernetes tests in the meantime.;;;","10/Jun/20 12:25;wangyang0918;[~chesnay] I will have a hotfix right now. But i think running the e2e tests needs more than 2 hour. We could disable the K8s e2e tests first and when it is fixed i will re-enable them.

 

[~rmetzger] Actually, the minikube version does not change in our test. However, the azure provides a new version by default. Currently, we choose to use the installed minikube and skip downloading. I will fix it via always using the installed version.;;;","10/Jun/20 14:50;rmetzger;What's the status? What you can do is change the order of e2e tests so that the K8s tests run first. Then you have a result in 40 minutes.;;;","10/Jun/20 16:01;wangyang0918;[~rmetzger] I have attached a PR for this issue. And the K8s e2e tests could pass.

 

[https://github.com/apache/flink/pull/12585]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3200&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","10/Jun/20 16:53;rmetzger;Thanks. I merged it to master: https://github.com/apache/flink/commit/3dedb85000d722620d65ef41e791a8e1d1a910e2
and release-1.11: https://github.com/apache/flink/commit/ea5aae232865d7fe16041a5ecd448760ae4abecc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoteChannelThroughputBenchmark deadlocks,FLINK-18238,13310595,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,pnowojski,pnowojski,10/Jun/20 08:30,17/Jun/20 13:12,13/Jul/23 08:12,17/Jun/20 13:12,1.11.0,,,,,1.11.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"In the last couple of days {{RemoteChannelThroughputBenchmark.remoteRebalance}} deadlocked for the second time:

http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/6019/

",,klion26,liyu,pnowojski,roman,yunta,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-8871,,,,,,,,,,,,,,,"10/Jun/20 14:49;pnowojski;consoleText_remote_benchmark_deadlock.txt;https://issues.apache.org/jira/secure/attachment/13005417/consoleText_remote_benchmark_deadlock.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 13:12:57 UTC 2020,,,,,,,,,,"0|z0fpfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 03:23;zjwang;[~kevin.cyj] already finalized the root cause which was introduced by FLINK-8871. 

When the checkpoint coordinator received the aborted checkpoint RPC from one task, it will send the abort RPC call to all the remaining tasks to avoid useless checkpoint happen.

If the respective checkpoint has not performed on SubtaskCheckpointCoordinatorImpl side, it will store this aborted id to exit the later checkpoint directly. But the downstream side still waits for barrier alignment and can not receive any checkpoint barrier or CancelCheckpointMarker any more from this aborted upstream. Then it will cause gradually backpressure until completely deadlock.   

One possible option is to broadcast CancelCheckpointMarker to downstream side when the upstream already aborted the checkpoint from RPC call, then the downstream can end the alignment immediately. But one side effect is that the CancelCheckpointMarker might be broadcasted twice, one is via RPC trigger and anther is via data stream in CheckpointBarrierHandler.;;;","15/Jun/20 06:54;pnowojski;Thanks a lot [~kevin.cyj] for investigating this.
{quote}
But the downstream side still waits for barrier alignment and can not receive any checkpoint barrier or CancelCheckpointMarker any more from this aborted upstream.
{quote}
Why is the downstream task not receiving the checkpoint aborted RPC call? Is it because it was sent before this task started or something like that?;;;","15/Jun/20 06:56;zjwang;The downstream task can receive the abort rpc call from coordinator, but it can not touch the `CheckpointBarrierHandler` to end the alignment and it only works on `StreamTask` with sub task coordinator.;;;","15/Jun/20 07:02;yunta;Thanks for [~kevin.cyj] for investigating this.

[~pnowojski], I think this is because current implementation would not emit checkpoint barrier or CancelCheckpointMarker downstream if it found the checkpoint has been aborted (see [SubtaskCheckpointCoordinatorImpl code|https://github.com/apache/flink/blob/35f95f5ac02c6014cdc8ef714ca66ad7e2cfdd5b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SubtaskCheckpointCoordinatorImpl.java#L240-L252]).

I compare the implementation of current Flink with our internal Blink and noticed that we would broadcast barrier and then return to ignore follow-up sync and async phase in Blink. I think that's why we did not meet this problem internally.;;;","15/Jun/20 07:33;pnowojski;{quote}
The downstream task can receive the abort rpc call from coordinator, but it can not touch the `CheckpointBarrierHandler` to end the alignment and it only works on `StreamTask` with sub task coordinator.
{quote}
Ok I get it. It would be I think OK to cancel the alignment directly from the {{SubtaskCheckpointCoordinator}}, but as I wrote above, it might open up possibilities for some race conditions with task not started.
{quote}
One possible option is to broadcast CancelCheckpointMarker to downstream side when the upstream already aborted the checkpoint from RPC call, then the downstream can end the alignment immediately. But one side effect is that the CancelCheckpointMarker might be broadcasted twice, one is via RPC trigger and anther is via data stream in CheckpointBarrierHandler.
{quote}
This could be handled by checking if the checkpoint has already been aborted or not.

But now that I think about it, what if there are multiple ongoing checkpoints in the job graph: N, N+1, ..., N +5. What if checkpoint N+5 fails somewhere at the head of the job graph, while other are still flowing through the job graph? Without the abort RPC call, if a task completed checkpoints N, .., N+4, it broadcasted checkpoint barriers for those checkpoints and then failed for checkpoint N+5, those cancellation markers from this Task wouldn't be processed by downstream Tasks (because of alignment and blocked channels) that are still waiting for alignment on checkpoints N, ..., N+4. So checkpoints N, ..., N+4 could complete normally.

With the abort RPC call, cancellations can overtake the pending checkpoint barriers, so in the before mentioned scenario, we would cancel all checkpoints, from N to N+5. I'm not sure if this can happen on master as it is without broadcasting {{CancelCheckpointMarker}}, but I think it could happen with broadcasting.;;;","15/Jun/20 09:27;zjwang;bq. Ok I get it. It would be I think OK to cancel the alignment directly from the SubtaskCheckpointCoordinator, but as I wrote above, it might open up possibilities for some race conditions with task not started.

bq. This could be handled by checking if the checkpoint has already been aborted or not.

I guess we can not cancel the alignment easily now when receiving the abort rpc call, since the SubtaskCheckpointCoordinator can not access the component of CheckpointBarrierHandler directly. Also we could not avoid duplicated broadcast since these two components work separately.

bq. With the abort RPC call, cancellations can overtake the pending checkpoint barriers, so in the before mentioned scenario, we would cancel all checkpoints, from N to N+5. I'm not sure if this can happen on master as it is without broadcasting CancelCheckpointMarker, but I think it could happen with broadcasting.

I think we need to finalize the semantic of aborting checkpoint from global coordinator firstly. If the checkpoint N+5 is aborted from coordinator, does that mean all the pending checkpoints smaller than N+5 should also be aborted as well?
 The previous semantic is for precise abort for the indicated checkpoint id from rpc call. If we changed this behavior to abort all the preceding pending checkpoint, we might need to think more and redefine the semantic/behavior.

Anyway I think it might be a separate topic to discuss. Now we can firstly confirm how to cancel the dedicated checkpoint to keep the previous behavior.;;;","15/Jun/20 09:39;roman;RPC abort notifications were introduced in 1.11 but we've encountered this issue in 1.10, right? ;;;","15/Jun/20 09:50;zjwang;bq. RPC abort notifications were introduced in 1.11 but we've encountered this issue in 1.10, right? 

The motivation for introducing abort notification is for saving the efforts of clearing the invalid checkpoint, and it did in 1.11. After this improvement we prevented the barrier broadcasting to downstream side, so it encountered the deadlock issue now. This issue is not in 1.10.;;;","15/Jun/20 11:20;yunta;I think the clear solution is to broadcast {{CancelCheckpointMarker}} downside. However, I'm not very sure whether it's okay to process {{CancelCheckpointMarker}} twice on task side.

Maybe I could prepare a PR without UTs first to see how much work need to broadcast {{CancelCheckpointMarker}} downside.;;;","15/Jun/20 11:53;pnowojski;[~yunta] as I wrote above, broadcasting {{CancelCheckpointMarker}} downstream can probably cause checkpoint failures of previous checkpoints, that would complete successfully otherwise.

Current semantic of {{CancelCheckpointMarker(N)}} is that it aborts all checkpoints <= N. To preserve the previous behaviour, we would need more complicate logic, either in processing {{CancelCheckpointMarker}} or in broadcasting it (we could postpone broadcasting).

Maybe it's fine to change the previous behaviour, but that wasn't the intention of the FLINK-8871.

Either way, I think it would be safer to revert this change and re-do it on master branch for 1.12, as who knows what other problems can pop up even in the simplest solution:
{quote}
the clear solution is to broadcast CancelCheckpointMarker downside. 
{quote} 
and we are now blocking 1.11 release.
{quote}
since the SubtaskCheckpointCoordinator can not access the component of CheckpointBarrierHandler directly
{quote}
[~zjwang] this would be easy to solve, assuming that we would want to go this direction.;;;","15/Jun/20 13:29;yunta;[~pnowojski] Thanks for your clarification.

From my point of view, broadcasting checkpoint barrier downside, which is our internal Flink did, could solve this problem. In other words, we need to change the order of current step-0 after step-2:

{code:java}
// Step (2): Send the checkpoint barrier downstream
operatorChain.broadcastEvent(
	new CheckpointBarrier(metadata.getCheckpointId(), metadata.getTimestamp(), options),
	options.isUnalignedCheckpoint());

......

// Step (0): Record the last triggered checkpointId.
lastCheckpointId = metadata.getCheckpointId();
if (checkAndClearAbortedStatus(metadata.getCheckpointId())) {
	LOG.info(""Checkpoint {} has been notified as aborted, would not trigger any checkpoint."", metadata.getCheckpointId());
	return;
}
{code}

;;;","15/Jun/20 13:29;yunta;If we have to choose to revert code, I think https://issues.apache.org/jira/browse/FLINK-18074 cannot be directly reverted.;;;","16/Jun/20 10:22;pnowojski;Copying the result of an online discussion: we decided to go with broadcasting checkpoint cancellation markers from {{SubtaskCheckpointCoordinatorImpl#checkpointState}} in the case when {{notifyCheckpointAborted}} RPC call was received before it the checkpoint was triggered. This guarantees that downstream tasks will always eventually stop the alignment. 

We could further optimise the process by cancelling the ongoing alignment of the task, once it receives {{notifyCheckpointAborted}} RPC, but that would require some more extensive changes that we do not need to do right now.;;;","17/Jun/20 13:12;zjwang;Master: 07772bdb9abc0bcd3b3c8869f6abdc8088bd7cea

Release-1.11: e7c634f223c0a2c180ca5abe14a4661115f0afe6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException when reading filesystem partitioned table with stream mode,FLINK-18237,13310591,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,10/Jun/20 08:12,11/Jun/20 11:51,13/Jul/23 08:12,11/Jun/20 11:51,,,,,,1.11.0,,,Connectors / FileSystem,Table SQL / API,,,,0,pull-request-available,,,,,"IllegalArgumentException when reading filesystem partitioned table with stream mode.
{code:java}
Caused by: java.lang.IllegalArgumentException: FileInputFormats with multiple paths are not supported yet.
        at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.<init>(ContinuousFileMonitoringFunction.java:126) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.<init>(ContinuousFileMonitoringFunction.java:110) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createFileInput(StreamExecutionEnvironment.java:1513) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1480) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.createInput(StreamExecLegacyTableSourceScan.scala:200) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.PhysicalLegacyTableSourceScan.getSourceTransformation(PhysicalLegacyTableSourceScan.scala:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:98) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlan(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code}
When reading filesystem partitioned table, there will maybe many directories to read, but {{ContinuousFileMonitoringFunction}} not support multiple paths, we should not use it.",,kkl0u,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 11:51:04 UTC 2020,,,,,,,,,,"0|z0fpeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 08:15;kkl0u;Hi [~lzljs3620320], could you post a bit more details about the problem and also provide the stacktrace of the exception so that we also know what is happening? This will also help us track the problems in case another user in the mailing list sees something similar.;;;","10/Jun/20 08:18;lzljs3620320;Hi [~kkl0u], I written comments in PR, I will copy them to Jira comments.;;;","11/Jun/20 07:02;kkl0u;Thanks a lot [~lzljs3620320]!;;;","11/Jun/20 11:51;lzljs3620320;master: 35e2fca5c9b59a87a6f1a17628daf75d3de575b0

release-1.11: ca9bd803a6cc9424040a5c5f834ee9e5efaa0fb1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink elasticsearch IT test ElasticsearchSinkTestBase.runElasticsearchSink* verify it not right,FLINK-18236,13310575,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,10/Jun/20 07:17,18/Jun/20 01:44,13/Jul/23 08:12,17/Jun/20 14:43,1.10.0,,,,,1.12.0,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,"we can see there are diffirent tests

runElasticsearchSinkTest

runElasticsearchSinkCborTest

runElasticsearchSinkSmileTest

runElasticSearchSinkTest

etc.

And use SourceSinkDataTestKit.verifyProducedSinkData(client, index) to ensure the correctness of results. But all of them use the same index.

That is to say, if the second unit test sink doesn't send successfully. they are also equal when use verifyProducedSinkData

 ",,jackylau,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 01:44:19 UTC 2020,,,,,,,,,,"0|z0fpb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 06:15;jackylau;Hi [~jark] could you please spend some of time reviewing this issue and PR?;;;","17/Jun/20 14:40;rmetzger;Thanks a lot for your contribution. Please check the contribution guide in Flink: https://flink.apache.org/contributing/contribute-code.html
We usually ask contributors to first get a confirmation in JIRA (by being assigned to the ticket) for their contribution, to make sure the contribution is relevant.

In this case, I have the feeling that the contribution is not based on an actual failure, rather on ""searching through the code for a contribution opportunity""
Quote from the contribution guide:
{quote}Is this a contribution just for the sake of getting a commit in an open source project (fixing typos, style changes merely for taste reasons){quote}

Such changes have the potential to be ignored because the take away very limited contributor capacities. If you want to work on something that has a chance to get some review attention, I would recommend working on tickets labelled with the {{starter}} tag. Alternatively, flink-statefun is currently looking to add support for more language SDKs.

I will now merge the PR because it does not break anything, needs no review, and will make the error reporting a tiny bit better.
;;;","17/Jun/20 14:43;rmetzger;Merged to master in https://github.com/apache/flink/commit/6623ef1843b693b222d182b99c5b94b5d4512222;;;","18/Jun/20 01:44;jackylau;Hi [~rmetzger], Thanks . I found this from the page you attach.

*Pull requests belonging to unassigned Jira tickets or not authored by assignee will not be reviewed or merged by the community.*

And i am not searching through the code for a contribution opportunity. I found this, becasue i am working on elastcisearch source connector [https://cwiki.apache.org/confluence/display/FLINK/FLIP-127:+Support+Elasticsearch+Source+Connector]. So I read the code carefully and found that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorSubmissionTest unstable,FLINK-18233,13310565,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,rmetzger,rmetzger,10/Jun/20 06:53,12/Jun/20 13:48,13/Jul/23 08:12,12/Jun/20 13:48,1.12.0,,,,,1.11.0,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3116&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
2020-06-10T04:55:48.9840787Z [ERROR] Tests run: 11, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 16.96 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest
2020-06-10T04:55:48.9842063Z [ERROR] testFailingScheduleOrUpdateConsumers(org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest)  Time elapsed: 10.866 s  <<< ERROR!
2020-06-10T04:55:48.9842771Z org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
2020-06-10T04:55:48.9843232Z 	at sun.misc.Unsafe.allocateMemory(Native Method)
2020-06-10T04:55:48.9843569Z 	at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:127)
2020-06-10T04:55:48.9843928Z 	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
2020-06-10T04:55:48.9847080Z 	at org.apache.flink.core.memory.MemorySegmentFactory.allocateDirectMemory(MemorySegmentFactory.java:143)
2020-06-10T04:55:48.9847779Z 	at org.apache.flink.core.memory.MemorySegmentFactory.allocateUnpooledOffHeapMemory(MemorySegmentFactory.java:131)
2020-06-10T04:55:48.9848321Z 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.<init>(NetworkBufferPool.java:122)
2020-06-10T04:55:48.9848929Z 	at org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.createNettyShuffleEnvironment(NettyShuffleServiceFactory.java:116)
2020-06-10T04:55:48.9849547Z 	at org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.build(NettyShuffleEnvironmentBuilder.java:144)
2020-06-10T04:55:48.9850142Z 	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.createShuffleEnvironment(TaskSubmissionTestEnvironment.java:259)
2020-06-10T04:55:48.9850770Z 	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.access$100(TaskSubmissionTestEnvironment.java:84)
2020-06-10T04:55:48.9851496Z 	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment$Builder.lambda$build$0(TaskSubmissionTestEnvironment.java:357)
2020-06-10T04:55:48.9852092Z 	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment$Builder$$Lambda$63/238383730.get(Unknown Source)
2020-06-10T04:55:48.9852509Z 	at java.util.Optional.orElseGet(Optional.java:267)
2020-06-10T04:55:48.9853118Z 	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment$Builder.build(TaskSubmissionTestEnvironment.java:355)
2020-06-10T04:55:48.9853753Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.testFailingScheduleOrUpdateConsumers(TaskExecutorSubmissionTest.java:545)
2020-06-10T04:55:48.9854252Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-10T04:55:48.9854625Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-10T04:55:48.9855148Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-10T04:55:48.9855544Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-10T04:55:48.9855948Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-10T04:55:48.9856395Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-10T04:55:48.9856860Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-10T04:55:48.9857320Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-10T04:55:48.9857800Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-06-10T04:55:48.9858318Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-06-10T04:55:48.9858743Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-10T04:55:48.9859070Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-10T04:55:48.9859357Z 
2020-06-10T04:55:48.9859763Z [ERROR] testUpdateTaskInputPartitionsFailure(org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest)  Time elapsed: 0.949 s  <<< ERROR!
2020-06-10T04:55:48.9860432Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Failure while stopping RpcEndpoint taskmanager_0.
2020-06-10T04:55:48.9860972Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-06-10T04:55:48.9861684Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-06-10T04:55:48.9862256Z 	at org.apache.flink.runtime.rpc.RpcUtils.terminateRpcEndpoint(RpcUtils.java:83)
2020-06-10T04:55:48.9862811Z 	at org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.close(TaskSubmissionTestEnvironment.java:269)
2020-06-10T04:55:48.9863489Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.testUpdateTaskInputPartitionsFailure(TaskExecutorSubmissionTest.java:457)
2020-06-10T04:55:48.9863968Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-10T04:55:48.9864355Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-10T04:55:48.9864821Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-10T04:55:48.9865245Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-10T04:55:48.9865645Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-10T04:55:48.9866106Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-10T04:55:48.9866569Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-10T04:55:48.9867012Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-10T04:55:48.9867424Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-10T04:55:48.9867795Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-10T04:55:48.9868141Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-10T04:55:48.9868511Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-10T04:55:48.9868914Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-10T04:55:48.9869373Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-10T04:55:48.9869794Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-10T04:55:48.9870312Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-10T04:55:48.9870886Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-10T04:55:48.9871579Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-10T04:55:48.9872212Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-10T04:55:48.9872760Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-10T04:55:48.9873424Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-10T04:55:48.9874094Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-10T04:55:48.9874571Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-10T04:55:48.9875084Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-10T04:55:48.9875575Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-10T04:55:48.9876094Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-10T04:55:48.9876555Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-10T04:55:48.9876974Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-10T04:55:48.9877467Z Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Failure while stopping RpcEndpoint taskmanager_0.
2020-06-10T04:55:48.9877981Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:518)
2020-06-10T04:55:48.9878478Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:176)
2020-06-10T04:55:48.9878903Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-06-10T04:55:48.9879288Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-06-10T04:55:48.9879835Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-06-10T04:55:48.9880316Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-06-10T04:55:48.9880721Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-06-10T04:55:48.9881198Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-06-10T04:55:48.9881578Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-06-10T04:55:48.9881957Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-06-10T04:55:48.9882322Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-06-10T04:55:48.9882677Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-06-10T04:55:48.9883075Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-06-10T04:55:48.9883418Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-06-10T04:55:48.9883726Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-06-10T04:55:48.9884096Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-06-10T04:55:48.9884506Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-06-10T04:55:48.9884985Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-10T04:55:48.9885413Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-10T04:55:48.9885757Z Caused by: java.lang.IllegalStateException
2020-06-10T04:55:48.9886103Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179)
2020-06-10T04:55:48.9886519Z 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:416)
2020-06-10T04:55:48.9886974Z 	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:216)
2020-06-10T04:55:48.9887445Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:514)
2020-06-10T04:55:48.9887768Z 	... 18 more
2020-06-10T04:55:48.9887885Z 
2020-06-10T04:55:49.7419345Z [INFO] Running org.apache.flink.runtime.taskexecutor.DefaultJobLeaderServiceTest
{code}
",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 13:48:29 UTC 2020,,,,,,,,,,"0|z0fp8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 13:47;trohrmann;The test failure seems to be twofold:

{{TaskExectuorSubmissionTest.testFailingScheduleOrUpdateConsumers}} fails because it cannot allocate fast enough direct byte buffers (32MB in 10s). It needs slightly more than 10s.

{{TaskExecutorSubmissionTest.testUpdateTaskInputPartitionsFailure}} fails because of an {{IllegalStateException}}. This is most likely cause by https://github.com/apache/flink/pull/12399/files#r438132048 (FLINK-15687).

For fixing this issue I propose to increase the timeouts of {{TaskExecutorSubmissionTest}}. The latter problem will be addressed as part of FLINK-15687.;;;","12/Jun/20 13:48;trohrmann;Fixed via

master: e026274882b21f647f71febd65623e0a2c71d553
1.11.0: 180533c668778bd032732b21e8c24fe0e664518b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive streaming source can not work when inserting new multiple records partition with start offset,FLINK-18232,13310559,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,10/Jun/20 06:24,11/Jun/20 03:16,13/Jul/23 08:12,11/Jun/20 02:32,,,,,,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"* Hive streaming source HiveMapredSplitReader reuse row in a wrong way, if change reuse row instance, will loose partition fields.
 * When converting Flink File split to Hadoop File split, length should not be -1.
 * DirectoryMonitorDiscovery should convert DFS modificationTime to UTC time mills.
 * {{HiveTableSource.createStreamSourceForNonPartitionTable}} should use local zone mills instead of UTC mills because {{ContinuousFileMonitoringFunction}} use local zone mills.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18077,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 03:16:25 UTC 2020,,,,,,,,,,"0|z0fp7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 02:32;lzljs3620320;master: f59b8b479199aa55776b3cca13b325223db984b0

release-1.11: a80f7690367e9a07b19321f3b34d8fdba17295b8;;;","11/Jun/20 03:06;ykt836;this kind of Jira title and commit message show too less information. I would rather split these issues into 3 jiras.;;;","11/Jun/20 03:16;lzljs3620320;Hi [~ykt836], You are right, I changed title to explain test case failure. At least, should split into multiple commits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
official example stuck in flink shell(local mode),FLINK-18227,13310535,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,appleyuchi,appleyuchi,10/Jun/20 03:39,11/Jun/20 11:49,13/Jul/23 08:12,11/Jun/20 11:49,1.10.1,,,,,,,,Scala Shell,,,,,0,,,,,,"*1.*start-scala-shell.sh local

then I type the example from the log after I start flink shell(local)

and get stuck finally,no result in the picture I uploaded.

 

 

*2.*Maybe the above question is not regarded as issue,but ""support"",

um,is there any other website except stackoverflow to propose a question?

I want to try flink shell,but almost no material in China's website.

my question is more than one,*but the stackoverflow has many limitations.*

*for example:*

①I can't propose more than one question in an hour.

②If I propose several questions got downvote,my stackoverflow account will be banned.",,appleyuchi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 03:44;appleyuchi;卡住.png;https://issues.apache.org/jira/secure/attachment/13005296/%E5%8D%A1%E4%BD%8F.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 03:44:34 UTC 2020,,,,,,,,,,"0|z0fp28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 03:42;appleyuchi;!卡住.png!;;;","10/Jun/20 03:44;appleyuchi;!卡住.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceManager requests unnecessary new workers if previous workers are allocated but not registered.,FLINK-18226,13310534,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,xtsong,xtsong,10/Jun/20 03:30,14/Jun/20 00:26,13/Jul/23 08:12,14/Jun/20 00:26,1.11.0,,,,,1.11.0,,,Deployment / Kubernetes,Deployment / YARN,Runtime / Coordination,,,0,pull-request-available,,,,,"h2. Problem

Currently on Kubernetes & Yarn deployment, the ResourceManager compares *pending workers requested from Kubernetes/Yarn* against *pending workers required by SlotManager*, for deciding whether new workers should be requested in case of a worker failure.
 * {{KubernetesResourceManager#requestKubernetesPodIfRequired}}
 * {{YarnResourceManager#requestYarnContainerIfRequired}}

*Pending workers requested from Kubernetes/Yarn* is decreased when the worker is allocated, *before the worker is actually started and registered*.
 * Decreased in {{ActiveResourceManager#notifyNewWorkerAllocated}}, which is called in
 * {{KubernetesResourceManager#onAdded}}
 * {{YarnResourceManager#onContainersOfResourceAllocated}}

On the other hand, *pending workers required by SlotManager* is derived from the number of pending slots inside SlotManager, which is decreased *when the new workers/slots are registered*.
 * {{SlotManagerImpl#registerSlot}}

Therefore, if a worker {{w1}} is failed after another worker {{w2}} is allocated but before {{w2}} is registered, the ResourceManager will request an unnecessary new worker for {{w2}}.

h2. Impact

Normally, the extra worker should be released soon after allocated. But in cases where the Kubernetes/Yarn cluster does not have enough resources, it might create more and more pending pods/containers.

It's even more severe for Kubernetes, because {{KubernetesResourceManager#onAdded}} only suggest that the pod spec has been successfully added to the cluster, but the pod may not actually been allocated due to lack of resources. Imagine there are {{N}} pending pods, a failure of a running pod means requesting another {{N}} new pods.

In a session cluster, such pending pods could take long to be cleared even after all jobs in the session cluster have terminated.",,guoyangze,Ming Li,rmetzger,trohrmann,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17976,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 00:26:00 UTC 2020,,,,,,,,,,"0|z0fp20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 03:39;xtsong;Here are my proposal for solving this problem.
 * We should decrease *pending workers requested from Kubernetes/Yarn* when the new worker is registered, rather than allocated.
 * We would need to match the registered workers with the pending ones, wrt their resource spec. To avoid passing {{WorkerResourceSpec}} all the way to the {{TaskExecutor}} and register it back, we can record the mapping between {{ResourceID}} and {{WorkerResourceSpec}} when the workers are allocated, and find out the {{WorkerResourceSpec}} from the {{ResourceID}} when the worker is registered.;;;","10/Jun/20 03:39;xtsong;cc [~trohrmann];;;","10/Jun/20 08:52;trohrmann;Thanks for reporting this issue [~xintongsong]. Your analysis and your solution proposal sound good to me. Please go ahead and provide a fix. I will review it. 

Since this issue should have existed before, I will decrease the priority of this issue to critical.;;;","10/Jun/20 12:28;xtsong;Thanks [~trohrmann]. I'll try to open a PR asap. Still dealing with the test cases.

Regarding the priority, it is true that this issue existed before. But I think our changes in 1.11.0 have made this worse. Previously, for each worker failure, we will request at most one more new worker. Now we are trying to request all the pending workers. ;;;","10/Jun/20 13:04;trohrmann;This is true, then let's try to fix it before the next RC is created. Then it doesn't matter whether it is a blocker or critical ;-) Increasing the severity to blocker for the time being.;;;","11/Jun/20 08:51;xtsong;Some updates on this ticket.

I ran into some YARN test failures. Turns out we cannot simply move the decreasing of pending worker count from on-worker-allocated to on-worker-registered. The problem is that, {{YarnResourceManager}} relies on this counter for deciding which worker to start.

A yarn container resource {{c1}} (e.g., <4GB>) might corresponds to multiple worker resource spec {{w1}}, {{w2}} (e.g., <1GB heap, 3GB off-heap> & <3GB heap, 1GB off-heap>). Say if the number of pending workers is <w1 : 1, w2 : 1>, that means the number of pending containers <c1 : 2>. When a container of resource {{c1}} is allocated, YarnRM choose one from {{w1}} and {{w2}} and starts a TM accordingly. Let's say it picks {{w1}}. Then the pending workers become <w1 : 0, w2 :1> & <c1 : 1>. In this way, when another container of {{c1}} is allocated, YarnRM knows it should starts a TM with {{w2}} rather than {{w1}}.

If we do not decrease the counter on worker registered rather than allocated, then YarnRM won't be able to know which worker resource spec should the TM be started with when the second container is allocated before the first one registered.;;;","11/Jun/20 09:01;xtsong;One way to solve the problem is to have two counters, for
* pending workers not allocated
* pending workers allocated not registered

The former could be used for deciding which worker resource spec the TM should be start with on Yarn, while the latter could be used for deciding whether to request a new worker on a failure of exist worker.

I think this might be a feasible fix for the 1.11.0 release. However, it relies on each deployment-specific RM to call {{notifyWorkerRequested/Allocated/Registered/AllocationFailed/Terminated}} methods at proper timing, which is kind of crumbly. Ideally, we should have the common {{ActiveResourceManager}} to decide common workflow, leaving interfaces like {{requestNewWorker}} or {{onWorkerAllocated}} for the deployment-specific RMs to fill in. Of course this requires more efforts and could be a follow up issue.

[~trohrmann] WDYT?;;;","11/Jun/20 12:20;trohrmann;Yes this sounds like a good plan [~xintongsong]. 

Maybe we manage to allocate some time in the 1.12 release to clean the {{ResourceManager}} up and put the right abstractions in place. I think you are right that we are running into many of these problems as often as we have different {{ResourceManager}} implementations.

For now a quick fix for the release would be good enough.;;;","14/Jun/20 00:26;xtsong;Fixed via
* master: 4f333e54fdc54956f97f7ea5031a63a5239f2c8c
* 1.11.0: 2af0d4b001ef2e4ecd83eeb1abbfe920dc631977;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroSerializer does not correctly instantiate GenericRecord,FLINK-18223,13310452,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicusX,nicusX,nicusX,09/Jun/20 18:33,15/Jun/20 10:19,13/Jul/23 08:12,15/Jun/20 10:16,1.10.1,,,,,1.10.2,1.11.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,AVRO,pull-request-available,,,,"{{AvroSerializer.createInstance()}} simply calls {{InstantiationUtil.instantiate(type)}} to create a new instance, also when type is GenericRecord.

This fails with an exception, because a GenericRecord must be instantiated through {{GenericRecordBuilder}} but {{InstantiationUtil}} is not aware of it.
{code:java}
The class 'org.apache.avro.generic.GenericRecord' is not instantiable: The class is not a proper class. It is either abstract, an interface, or a primitive type.{code}
This can be proven with this test
{code:java}
@Test
public void shouldInstantiateGenericRecord() {
    org.apache.avro.Schema SCHEMA = new org.apache.avro.Schema.Parser().parse(""{\""type\"":\""record\"",\""name\"":\""Dummy\"",\""namespace\"":\""dummy\"",\""fields\"":[{\""name\"":\""something\"",\""type\"":{\""type\"":\""string\"",\""avro.java.string\"":\""String\""}}]}"");
    AvroSerializer<GenericRecord> serializer = new AvroSerializer<>(GenericRecord.class, SCHEMA);

    serializer.createInstance();
}
{code}
 ",,aljoscha,dwysakowicz,gaoyunhaii,maguowei,nicusX,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 10:16:35 UTC 2020,,,,,,,,,,"0|z0fojs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 09:00;aljoscha;Hi [~nicusX], could you maybe open a PR with a fix?;;;","10/Jun/20 11:46;nicusX;This is what I'm trying to do, [~aljoscha]. I have the fix ready and tested.
But I cannot make the full Flink test suite (mvn clean verify) pass. It looks like it's randomly failing on completely unrelated modules, while the same tests are not failing if I run them in IntelliJ :-?;;;","10/Jun/20 13:41;twalthr;in most of the cases randomly failing modules can be avoided by running a full `mvn clean install -Pfast` for the entire Flink codebase before;;;","10/Jun/20 16:17;nicusX;I submitted a PR but the bot complains the ticket has not been assigned.
Not sure what's the process for a simple bugfix like this;;;","10/Jun/20 19:02;dwysakowicz;Everything is fine. I assigned the ticket to you. It is just a warning to remind that a consensus on the JIRA issue should be reached first, before opening a PR.  But you did that.
Also the CI failure is unrelated to your change. There is a fix for the unstable test. I will rerun the CI for you.;;;","15/Jun/20 10:16;dwysakowicz;Fixed in:
* master
** f1e421197195c63bb570be76125de4db72dbefbf
* 1.11.0
** b1834221961162c37b14e3185c1249535fbd32ec
* 1.10.2
** 2a381c926c649c9f282ee02331996ce6499a2b60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Avro Confluent Schema Registry nightly end-to-end test"" unstable with ""Kafka cluster did not start after 120 seconds""",FLINK-18222,13310438,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,09/Jun/20 16:52,04/Nov/20 11:43,13/Jul/23 08:12,01/Sep/20 13:26,1.11.0,1.12.0,,,,1.11.3,1.12.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-06-09T15:16:48.1427795Z ==============================================================================
2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'
2020-06-09T15:16:48.1429204Z ==============================================================================
2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-48143298170
2020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz
2020-06-09T15:16:48.3214487Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-06-09T15:16:48.3215154Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-06-09T15:16:48.3215597Z 
2020-06-09T15:16:48.3528820Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:49.3421526Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:50.3415678Z   8 35.8M    8 2960k    0     0  2896k      0  0:00:12  0:00:01  0:00:11 2896k
2020-06-09T15:16:51.3406836Z  23 35.8M   23 8544k    0     0  4226k      0  0:00:08  0:00:02  0:00:06 4225k
2020-06-09T15:16:51.6553485Z  70 35.8M   70 25.2M    0     0  8550k      0  0:00:04  0:00:03  0:00:01 8548k
2020-06-09T15:16:51.6555606Z 100 35.8M  100 35.8M    0     0  10.7M      0  0:00:03  0:00:03 --:--:-- 10.7M
2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz
2020-06-09T15:16:51.9880242Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-06-09T15:16:51.9880983Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-06-09T15:16:51.9914252Z 
2020-06-09T15:16:52.3398614Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:53.3399552Z   9  398M    9 39.5M    0     0   111M      0  0:00:03 --:--:--  0:00:03  111M
2020-06-09T15:16:53.9149276Z  47  398M   47  188M    0     0   139M      0  0:00:02  0:00:01  0:00:01  138M
2020-06-09T15:16:53.9150980Z 100  398M  100  398M    0     0   206M      0  0:00:01  0:00:01 --:--:--  206M
2020-06-09T15:17:04.3565942Z Waiting for broker...
2020-06-09T15:17:12.4215170Z Waiting for broker...
2020-06-09T15:17:14.3012835Z Waiting for broker...
2020-06-09T15:17:16.1965074Z Waiting for broker...
2020-06-09T15:17:18.1102274Z Waiting for broker...
2020-06-09T15:17:19.9929632Z Waiting for broker...
2020-06-09T15:17:21.8607172Z Waiting for broker...
2020-06-09T15:17:23.7802949Z Waiting for broker...
2020-06-09T15:17:25.6695260Z Waiting for broker...
2020-06-09T15:17:27.5536417Z Waiting for broker...
2020-06-09T15:17:29.4327778Z Waiting for broker...
2020-06-09T15:17:31.3203091Z Waiting for broker...
2020-06-09T15:17:33.1987150Z Waiting for broker...
2020-06-09T15:17:35.0694860Z Waiting for broker...
2020-06-09T15:17:36.9595576Z Waiting for broker...
2020-06-09T15:17:38.9243558Z Waiting for broker...
2020-06-09T15:17:40.7984064Z Waiting for broker...
2020-06-09T15:17:42.6676095Z Waiting for broker...
2020-06-09T15:17:44.5628797Z Waiting for broker...
2020-06-09T15:17:46.4374532Z Waiting for broker...
2020-06-09T15:17:48.3086761Z Waiting for broker...
2020-06-09T15:17:50.1574336Z Waiting for broker...
2020-06-09T15:17:52.0432952Z Waiting for broker...
2020-06-09T15:17:53.9406541Z Waiting for broker...
2020-06-09T15:17:55.8162052Z Waiting for broker...
2020-06-09T15:17:57.7090015Z Waiting for broker...
2020-06-09T15:17:59.5747770Z Waiting for broker...
2020-06-09T15:18:01.4601854Z Waiting for broker...
2020-06-09T15:18:03.3332039Z Waiting for broker...
2020-06-09T15:18:05.2210453Z Waiting for broker...
2020-06-09T15:18:07.1133675Z Waiting for broker...
2020-06-09T15:18:09.0132417Z Waiting for broker...
2020-06-09T15:18:10.8769511Z Waiting for broker...
2020-06-09T15:18:12.7601639Z Waiting for broker...
2020-06-09T15:18:14.6389770Z Waiting for broker...
2020-06-09T15:18:16.5210725Z Waiting for broker...
2020-06-09T15:18:18.4088216Z Waiting for broker...
2020-06-09T15:18:20.2732225Z Waiting for broker...
2020-06-09T15:18:22.1558390Z Waiting for broker...
2020-06-09T15:18:24.0400570Z Waiting for broker...
2020-06-09T15:18:25.9134038Z Waiting for broker...
2020-06-09T15:18:27.7922350Z Waiting for broker...
2020-06-09T15:18:29.6748679Z Waiting for broker...
2020-06-09T15:18:31.5340996Z Waiting for broker...
2020-06-09T15:18:33.3998472Z Waiting for broker...
2020-06-09T15:18:35.2718135Z Waiting for broker...
2020-06-09T15:18:37.1426082Z Waiting for broker...
2020-06-09T15:18:39.1282264Z Waiting for broker...
2020-06-09T15:18:41.0029183Z Waiting for broker...
2020-06-09T15:18:42.8700037Z Waiting for broker...
2020-06-09T15:18:44.7531621Z Waiting for broker...
2020-06-09T15:18:46.6465173Z Waiting for broker...
2020-06-09T15:18:48.9504192Z Waiting for broker...
2020-06-09T15:18:50.4165383Z Waiting for broker...
2020-06-09T15:18:52.2931688Z Waiting for broker...
2020-06-09T15:18:54.1669857Z Waiting for broker...
2020-06-09T15:18:56.0238505Z Waiting for broker...
2020-06-09T15:18:57.8931143Z Waiting for broker...
2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:
{code}
There's a lot of log output I didn't analyze yet.",,dian.fu,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 11:43:14 UTC 2020,,,,,,,,,,"0|z0fogo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 05:55;rmetzger;Another case https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3477&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","26/Jul/20 02:36;dian.fu;Another instance on master:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4903&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529];;;","01/Sep/20 13:26;rmetzger;Merged to master in https://github.com/apache/flink/commit/3aee1490e07fd9b4eddd240fc17b7ed4fd191bba;;;","17/Sep/20 02:30;dian.fu;Instance on 1.11:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6568&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873]

[~rmetzger] What about also back-porting the fix to 1.11?;;;","17/Sep/20 05:26;rmetzger;I backported the fix to ""release-1.11"" in https://github.com/apache/flink/commit/5ad541120f18a1dd34736b5fd18b349b456fc96a.;;;","17/Sep/20 08:54;dian.fu;[~rmetzger] Thanks a lot! (y);;;","04/Nov/20 11:43;dian.fu;Seems another instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8962&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
config.sh#extractExecutionResults does not fail on wrong input,FLINK-18217,13310390,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,09/Jun/20 12:41,11/Jun/20 13:04,13/Jul/23 08:12,11/Jun/20 13:04,,,,,,1.11.0,,,Deployment / Scripts,,,,,0,pull-request-available,,,,,"If the BashJavaUtils fail, and {{config.sh#extractExecutionResults}} is called with the exception output, which does not contain the expected lines, then it will not detect an error.

{code}
execution_results=$(echo ""${output}"" | grep ${EXECUTION_PREFIX})
num_lines=$(echo ""${execution_results}"" | wc -l)
    if [[ ${num_lines} -ne ${expected_lines} ]]; then
{code}

Apparently, even if {{execution_results}} is empty, {{$(echo ""${execution_results}"" | wc -l)}} returns 1.",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,FLINK-18152,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 13:04:41 UTC 2020,,,,,,,,,,"0|z0fo60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 13:04;chesnay;master: 6216906400b4d1c0730868b1f68d2087bc70bec5
1.11: 1fb76645c2f6b670ebf1bcc4ae758bdd6b52bf46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BashJavaUtils logging message should include log level,FLINK-18215,13310372,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/20 11:40,10/Jun/20 10:31,13/Jul/23 08:12,10/Jun/20 10:31,,,,,,1.11.0,,,Runtime / Configuration,,,,,0,pull-request-available,,,,,"The BashJavaUtils currently log things like this:
{code}
[] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
{code}

This means that users cannot differentiate between info/warn/error messages.
An example where this might be helpful is FLINK-18214.",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 10:31:08 UTC 2020,,,,,,,,,,"0|z0fo20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/20 10:31;chesnay;master: 26063234aa4de64c660405a623d5d8e84d0e3023
1.11: e444d1ff87148cc75231c79b6de39543844f6af8 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect warning if jobstore.cache-size exceeds heap size,FLINK-18214,13310371,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,chesnay,chesnay,09/Jun/20 11:39,11/Jun/20 07:37,13/Jul/23 08:12,11/Jun/20 07:37,,,,,,1.11.0,,,Runtime / Configuration,,,,,0,pull-request-available,,,,,"The logging parameters are mixed up.

{code}
The configured or derived JVM heap memory size (jobstore.cache-size: 128.000mb (134217728 bytes)) is less than the configured or default size of the job store cache (jobmanager.memory.heap.size: 1.000gb (1073741825 bytes))
{code}

",,azagrebin,nicholasjiang,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18157,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 07:37:27 UTC 2020,,,,,,,,,,"0|z0fo1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 11:44;nicholasjiang;[~chesnay] This bug has already fixed in [Jobstore size check compares against offHeapMemory title|https://github.com/apache/flink/pull/12516].;;;","09/Jun/20 11:52;chesnay;Related: if we exceed the heap memory we should probably fail early.;;;","09/Jun/20 11:54;chesnay;[~nicholasjiang] Yes, I think you're right.;;;","09/Jun/20 12:04;nicholasjiang;[~chesnay], therefore, if this pull request is merged, could this issue close?;;;","09/Jun/20 12:13;chesnay;[~nicholasjiang] Not necessarily; it might also make sense to fail early in this case.;;;","09/Jun/20 14:56;azagrebin;The size of the cached jobs cannot really be calculated. Hence we only estimate the size of the jobs.

The job cache size is more of a soft threshold anyways and we cannot guarantee that we use at most this amount of memory.

So the earlier agreement was to not fail but warn.

Therefore, I suggest to close the issue.;;;","09/Jun/20 15:17;chesnay;Then the option is rather misleading; the description explicitly says {{""The job store cache size in bytes which is used to keep completed jobs in memory.""}} and the current error message just reinforces this wrong interpretation.
It also doesn't mention what the consequences are; will this just lead to some jobs being evicted earlier? (no) can this crash the cluster? (answer: *yes*).

The documentation is then also wrong, on both counts to boost:
{{The Job cache resides in the JVM Heap. It can be configured by jobstore.cache-size which must be less than the configured or derived JVM Heap size.}}

How good of an estimate is it for actual jobs? For what size of a job is the estimate correct?

As is stands, a user who uses this option will assume that this provides some form of assurance that the cache will not impact the job execution. But this is simply not the case.

I appreciate the difficulty of nailing down the cache size of in-memory objects, but if we can't do that reliably, we have to document that.;;;","09/Jun/20 15:32;azagrebin;True, we can improve the docs. I guess I also got into the same trap of option description.;;;","10/Jun/20 15:02;azagrebin;The offline conclusion ([~chesnay] [~trohrmann] [~azagrebin] ) is to remove the check as it may be inconclusive and confusing for the users.
The job cache size option does not strictly limit the real size and stays an advanced emergency mean.

Therefore, I will submit a PR to remove the related code from JobManagerProcessUtils, tests and memory tune guide.;;;","11/Jun/20 07:37;azagrebin;merged into master by 1adfd582c5ca8d90b71557d38463eddddcf94c0b

merged into 1.11 by 3ca7fe0b69b906ab54d3cdbe79643421536321e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Init lookup join failed when use udf on lookup table,FLINK-18212,13310359,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,liuyufei,liuyufei,09/Jun/20 10:11,19/Aug/20 02:12,13/Jul/23 08:12,19/Aug/20 02:12,1.10.1,,,,,1.10.3,1.11.2,1.12.0,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Throw exception 

{code}
Caused by: scala.MatchError: (CONCAT(_UTF-16LE'Hello', $2),_UTF-16LE'Hello,Jark':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") (of class scala.Tuple2)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.org$apache$flink$table$planner$plan$nodes$common$CommonLookupJoin$$extractConstantField(CommonLookupJoin.scala:617)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.extractConstantFieldsFromEquiCondition(CommonLookupJoin.scala:607)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.analyzeLookupKeys(CommonLookupJoin.scala:567)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.<init>(CommonLookupJoin.scala:129)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.<init>(StreamExecLookupJoin.scala:49)
{code}

SQL:

{code:sql}
SELECT
  T.id, T.len, T.content, D.name 
FROM 
  T JOIN userTable for system_time as of T.proctime AS D 
ON T.id = D.id 
WHERE 
  add(T.id, D.id) > 3 AND add(T.id, 2) > 3 AND CONCAT('Hello', D.name) = 'Hello,Jark'
{code}

When use function a RexCall can't match RexInputRef and cause this error, myabe shoud add condition""{{case _ => return}}"" to skip this.
",,jark,leonard,libenchao,liuyufei,tartarus,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 02:46:34 UTC 2020,,,,,,,,,,"0|z0fnz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/20 14:15;zhuzh;[~lzljs3620320] would you help to take a look to see if it is a valid issue, or find someone who knows it?;;;","17/Aug/20 04:04;jark;This is a bug. I will fix this. ;;;","17/Aug/20 04:09;zhuzh;Thanks for the updates [~jark];;;","18/Aug/20 02:46;jark;Fixed in
 - master (1.12.0): a2f01995166b1404ce51881c80660af522b005b7
 - 1.11.2: 3e6a74a177062109eff98da404476486721a68b4
 - 1.10.3: 13c4b425228c7be691173deec48024d97fce0dc1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink es connector exists 2 Spelling mistakes,FLINK-18208,13310333,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,09/Jun/20 08:38,10/Jun/20 01:14,13/Jul/23 08:12,09/Jun/20 11:42,,,,,,1.11.0,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,"flink es connector exists Spelling mistakes

1. es connector6 exists Elasticsearch7RequestFactory, it should be Elasticsearch6RequestFactory

2. es connector 6,7 Elasticsearch6DynamicSinkFactory exists not unified method (validate and validateOption)",,dwysakowicz,jackylau,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 13:56:32 UTC 2020,,,,,,,,,,"0|z0fntc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 13:56;dwysakowicz;Fixed:
* master
** bed750f0d5fd8e161f27edf979dd8961158699c1
* 1.11
** 14b3c483c72e17dd1bcad3508bcd243f8c41e327;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowGenerator in datagen factory should implement snapshotState,FLINK-18207,13310331,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Jun/20 08:25,09/Jun/20 16:41,13/Jul/23 08:12,09/Jun/20 12:21,,,,,,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18025,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 12:21:40 UTC 2020,,,,,,,,,,"0|z0fnsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 12:21;lzljs3620320;master: d1ed9bfc3eef969098a18fc173c7819b38b639a3

release-1.11: e3a0772a48fa8a416e139a84e322b2510cfd49b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 The timestamp is displayed incorrectly,FLINK-18206,13310326,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,JasonLee,JasonLee,09/Jun/20 08:08,28/Aug/21 11:16,13/Jul/23 08:12,21/Apr/21 04:39,1.10.1,,,,,1.13.0,1.14.0,,Table SQL / Client,,,,,0,pull-request-available,,,,,"I am using the latest Flink version. When I run a scrolling window SQL in SQL client, the time stamp of the printed result will not be correct

 
The results are as follows

 
+ jason 49 2 2020-06-09T07:59:40 2020-06-09T07:59:45
 + jason 50 2 2020-06-09T07:59:45 2020-06-09T07:59:50
 + jason 50 2 2020-06-09T07:59:50 2020-06-09T07:59:55
 + jason 50 2 2020-06-09T07:59:55 2020-06-09T08:00
 + jason 49 2 2020-06-09T08:00 2020-06-09T08:00:05
 + jason 50 2 2020-06-09T08:00:05 2020-06-09T08:00:10
 + jason 50 2 2020-06-09T08:00:10 2020-06-09T08:00:15
 + jason 50 2 2020-06-09T08:00:15 2020-06-09T08:00:20
 + jason 49 2 2020-06-09T08:00:20 2020-06-09T08:00:25
 + jason 50 2 2020-06-09T08:00:25 2020-06-09T08:00:30
 + jason 50 2 2020-06-09T08:00:30 2020-06-09T08:00:35
 + jason 49 2 2020-06-09T08:00:35 2020-06-09T08:00:40
 + jason 51 2 2020-06-09T08:00:40 2020-06-09T08:00:45
 + jason 50 2 2020-06-09T08:00:45 2020-06-09T08:00:50
 + jason 49 2 2020-06-09T08:00:50 2020-06-09T08:00:55
 + jason 50 2 2020-06-09T08:00:55 2020-06-09T08:01
 + jason 50 2 2020-06-09T08:01 2020-06-09T08:01:05
 + jason 51 2 2020-06-09T08:01:05 2020-06-09T08:01:10
 + jason 49 2 2020-06-09T08:01:10 2020-06-09T08:01:15
 + jason 46 2 2020-06-09T08:01:15 2020-06-09T08:01:20
 + jason 54 2 2020-06-09T08:01:20 2020-06-09T08:01:25
 + jason 50 2 2020-06-09T08:01:25 2020-06-09T08:01:30
 + jason 49 2 2020-06-09T08:01:30 2020-06-09T08:01:35
 + jason 50 2 2020-06-09T08:01:35 2020-06-09T08:01:40
 + jason 50 2 2020-06-09T08:01:40 2020-06-09T08:01:45
 + jason 50 2 2020-06-09T08:01:45 2020-06-09T08:01:50
 + jason 49 2 2020-06-09T08:01:50 2020-06-09T08:01:55
 + jason 50 2 2020-06-09T08:01:55 2020-06-09T08:02
 + jason 49 2 2020-06-09T08:02 2020-06-09T08:02:05
 + jason 51 2 2020-06-09T08:02:05 2020-06-09T08:02:10
 + jason 49 2 2020-06-09T08:02:10 2020-06-09T08:02:15
 + jason 50 2 2020-06-09T08:02:15 2020-06-09T08:02:20
 + jason 50 2 2020-06-09T08:02:20 2020-06-09T08:02:25
 + jason 50 2 2020-06-09T08:02:25 2020-06-09T08:02:30
 + jason 50 2 2020-06-09T08:02:30 2020-06-09T08:02:35
 + jason 50 2 2020-06-09T08:02:35 2020-06-09T08:02:40
 + jason 49 2 2020-06-09T08:02:40 2020-06-09T08:02:45
 + jason 50 2 2020-06-09T08:02:45 2020-06-09T08:02:50
 + jason 50 2 2020-06-09T08:02:50 2020-06-09T08:02:55
 + jason 50 2 2020-06-09T08:02:55 2020-06-09T08:03
 + jason 49 2 2020-06-09T08:03 2020-06-09T08:03:05
 + jason 51 2 2020-06-09T08:03:05 2020-06-09T08:03:10",,jark,leonard,lsy,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20162,,,,,"09/Apr/21 04:11;leonard;image-2021-04-09-12-11-29-049.png;https://issues.apache.org/jira/secure/attachment/13023590/image-2021-04-09-12-11-29-049.png","19/Apr/21 09:08;jark;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13024274/screenshot-1.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 04:39:28 UTC 2021,,,,,,,,,,"0|z0fnrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 09:38;twalthr;This is a deeper issue that we should address in the next release. We need to define a string representation for all logical types that can be used in `TableResult.print` and SQL Client.;;;","07/Sep/20 05:50;zhuzh;Hi [~twalthr], is this issue fixed?;;;","07/Sep/20 06:59;twalthr;[~zhuzh] no, this is not fixed yet.;;;","07/Sep/20 07:02;zhuzh;Thanks for the updates [~twalthr];;;","07/Sep/20 07:08;twalthr;[~zhuzh] I will remove the fixed version. We don't need to fix that in a minor release.;;;","17/Nov/20 03:06;jark;I think this related to the time zone problem which should be addressed in the next release. ;;;","09/Apr/21 04:12;leonard;we should update the TIMESTAMP_LTZ display, the right display should be 2021-04-09 11:56:30
 !image-2021-04-09-12-11-29-049.png!;;;","19/Apr/21 09:09;jark;Please also fix the display of LOCALTIME (in streaming mode + tableau result mode):


 !screenshot-1.png! ;;;","21/Apr/21 04:39;jark;Fixed in
 - master:  92a13e61dfff6db1d77fc448d05058c55c651802
 - release-1.13: 26d92d4539405db06c3aaa1c14db09f489b401f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should add more logs for hive streaming integration,FLINK-18197,13310278,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,09/Jun/20 04:00,15/Jun/20 08:17,13/Jul/23 08:12,15/Jun/20 08:17,,,,,,1.11.0,,,Connectors / FileSystem,Connectors / Hive,,,,0,pull-request-available,,,,,"When found bugs for hive streaming integration, it is very hard to debugging because we don't have useful logs.

Should add more logs for Hive table streaming source/sink and lookup join.",,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 08:17:49 UTC 2020,,,,,,,,,,"0|z0fnh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 08:17;lzljs3620320;master: 18300108397d1d3568db4cb5a180a7db9a77ef67

release-1.11: 26af028d742f6ae54d62fc40c3e240270a0aadbc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect size check for sum of heap/non-heap memory,FLINK-18189,13310160,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,chesnay,chesnay,08/Jun/20 14:56,10/Jun/20 15:31,13/Jul/23 08:12,10/Jun/20 15:31,,,,,,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Configuration:
{code}
jobmanager.memory.process.size=1 gb
jobmanager.memory.flink.size=512 mb
jobmanager.memory.heap.size=128 mb
{code}

{code}
org.apache.flink.configuration.IllegalConfigurationException: Sum of the configured JVM Heap Memory (128.000mb (134217728 bytes)) and the configured or default Off-heap Memory (128.000mb (134217728 bytes)) exceeds the configured Total Flink Memory (512.000mb (536870912 bytes)). Please, make the configuration consistent or configure only one option: either JVM Heap or Total Flink Memory.

{code}",,azagrebin,nicholasjiang,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 15:31:03 UTC 2020,,,,,,,,,,"0|z0fmqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 15:53;azagrebin;Indeed, the error message is incorrect. It must be `does not match` instead of `exceeds`. Some copy/paste error.;;;","09/Jun/20 01:09;nicholasjiang;[~chesnay],[~azagrebin], could you please assign this to me for correct?;;;","09/Jun/20 11:21;azagrebin;[~nicholasjiang] could you, please, move the issues, you work on, to 'in progress'? you can push the 'start progress' button.;;;","09/Jun/20 15:03;azagrebin;This will be already fixed as part of [https://github.com/apache/flink/pull/12557];;;","10/Jun/20 15:31;azagrebin;merged into master by 4555ad91b4bd0df8887c4b4eb3119cbae272e805
merged into 1.11 by 31a17cb523603a0b0d2daaf8ab7c1a18f9fc7999;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckPubSubEmulatorTest failed on azure,FLINK-18187,13310147,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,roman,roman,08/Jun/20 14:20,02/Jul/20 16:31,13/Jul/23 08:12,02/Jul/20 16:31,1.11.0,,,,,1.11.0,,,Connectors / Google Cloud PubSub,Tests,,,,0,pull-request-available,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

 

 
{code:java}
2020-06-08T12:45:15.9874996Z 82609 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest [] - Waiting a while to receive the m
 essage...
*2020-06-08T12:45:16.1955546Z 82816 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest [] - Timeout during shutdown
*2020-06-08T12:45:16.1956405Z java.util.concurrent.TimeoutException: Timed out waiting for InnerService [STOPPING] to reach a terminal state. Current state: ST*OPPING
...
2020-06-08T12:46:08.5914230Z 135213 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager [] -
 2020-06-08T12:46:08.6054783Z [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 54.754 s <<< FAILURE! - in org.apache.flink.streaming.con nectors.gcp.pubsub.CheckPubSubEmulatorTest
 2020-06-08T12:46:08.6062906Z [ERROR] testPull(org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest) Time elapsed: 52.123 s <<< FAILURE!
 2020-06-08T12:46:08.6063659Z java.lang.AssertionError: expected:<1> but was:<0>
{code}
 ",,dian.fu,jark,pnowojski,rmetzger,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16572,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 16:31:42 UTC 2020,,,,,,,,,,"0|z0fmo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/20 10:37;pnowojski;another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3814&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","19/Jun/20 10:44;rmetzger;If all failures happened on the 1.11 branch, then we can backport the last commit from FLINK-16572 to ""release-1.11"" to fix it in 1.11 as well. I didn't backport it right away, because it was already the 4th attempt to fix that test.;;;","19/Jun/20 11:09;pnowojski;I'm not sure. My report comes from 1.11. [~roman_khachatryan]'s are I think from before you fixed it on master?

But maybe it's better to merge it unify the code anyway while hopping for the bug to disappear.;;;","22/Jun/20 02:50;dian.fu;Another instance: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3878&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=08866332-78f7-59e4-4f7e-49a56faa3179];;;","22/Jun/20 04:54;pnowojski;Another instance on the 1.11 branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3878&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","22/Jun/20 08:09;jark;Another instance on the 1.11 branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3885&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","23/Jun/20 02:11;dian.fu;Another instance on the 1.11 branch:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=08866332-78f7-59e4-4f7e-49a56faa3179];;;","23/Jun/20 06:16;pnowojski;another instance on 1.11 branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334

[~rmetzger] can you backport the fix from master?;;;","24/Jun/20 02:10;dian.fu;Another instance on 1.11 branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3984&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3984&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179;;;","29/Jun/20 05:25;rmetzger;[~pnowojski]: Yes, I'll backport to the 1.11 branch.;;;","02/Jul/20 16:31;rmetzger;Resolved in https://github.com/apache/flink/commit/7c440f39c2afb6d89ffd72d9901d999f1e11e553;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingFileCommitter should not use fs modification time for proc committer,FLINK-18181,13310086,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Jun/20 10:32,09/Jun/20 16:14,13/Jul/23 08:12,09/Jun/20 09:14,,,,,,1.11.0,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,,"The filesystem path modification time is the last modification time of all files in directory. 

If there is new data in partition all the time, it will never end.

We should use earlier time for committer.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18025,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 09:14:05 UTC 2020,,,,,,,,,,"0|z0fmag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 09:14;lzljs3620320;master: 4e24a8b599f099a260dc53ab2ec9e5fb2bd9ae1d

release-1.11: 21ffa8f841d7e9d84080e3eaf81248f4a8997de2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-training exercises don't build with Eclipse,FLINK-18178,13310065,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,alpinegizmo,alpinegizmo,08/Jun/20 09:19,09/Jun/20 18:55,13/Jul/23 08:12,08/Jun/20 12:54,,,,,,1.11.0,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,,,The joda-time dependency can't be found.,,alpinegizmo,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 12:54:05 UTC 2020,,,,,,,,,,"0|z0fm5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 12:54;nkruber;Fixed in master via 163558d39b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add supplement for file system connector document,FLINK-18176,13310053,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,08/Jun/20 08:23,09/Jun/20 16:40,13/Jul/23 08:12,09/Jun/20 12:44,1.11.0,,,,,1.11.0,,,Connectors / FileSystem,Documentation,,,,0,pull-request-available,,,,,"Illustate difference between old file system connector and new file system connector.

Add supplement for rolling policy.",,fsk119,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 12:44:29 UTC 2020,,,,,,,,,,"0|z0fm34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 12:44;lzljs3620320;master: 3b5a1c97215f91a4630e9f700c3c70cde35cc980

release-1.11: e3a8a6d60555f43131379534019a0efa004cbcd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bundle flink-csv and flink-json jars in lib,FLINK-18173,13310026,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Jun/20 05:43,06/Jul/20 18:24,13/Jul/23 08:12,15/Jun/20 03:16,,,,,,1.11.0,,,Build System,Table SQL / API,,,,0,pull-request-available,,,,,"The biggest problem for distributions I see is the variety of problems caused by users' lack of format dependency.
 These three formats are very small and no third party dependence, and they are widely used by table users.
 Actually, we don't have any other built-in table formats now..
 * flink-csv-1.10.0.jar
 * flink-json-1.10.0.jar

 We can just bundle them in ""flink/lib/"".
 It not solve all problems and it is independent of ""fat"" and ""slim"". But also improve usability.",,aljoscha,jark,libenchao,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 03:16:34 UTC 2020,,,,,,,,,,"0|z0flx4:",9223372036854775807,"There is no need to download manually jar files for `flink-csv` and `flink-json` as they are now bundled in the `lib` folder.
",,,,,,,,,,,,,,,,,,,"09/Jun/20 11:33;lzljs3620320;[https://github.com/apache/flink/pull/12527];;;","15/Jun/20 03:16;lzljs3620320;master: 2b2b643cc27b0396b61c9996d9db15de59b01e22

release-1.11: f55b23cde9cef5489d137c80fdcfcd39f138835e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error results when use UDAF with Object Array return type,FLINK-18168,13309915,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,FrankZou,FrankZou,FrankZou,07/Jun/20 06:33,15/Jul/20 08:42,13/Jul/23 08:12,25/Jun/20 07:27,,,,,,1.10.2,1.11.0,1.12.0,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"I get error results when I use an UDAF with Object Array return type (e.g. Row[]). I find that the problem is we reuse 'reuseArray' as the return value of ObjectArrayConverter.toBinaryArray(). It leads to 'prevAggValue' and 'newAggValue' in GroupAggFunction.processElement() contains exactly the same BinaryArray, so 'equaliser.equalsWithoutHeader(prevAggValue, newAggValue)' is always true.",,FrankZou,jark,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 07:27:58 UTC 2020,,,,,,,,,,"0|z0fl8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/20 12:19;libenchao;[~FrankZou] Thanks for reporting this bug. Do you want to fix this?;;;","08/Jun/20 01:23;FrankZou;[~libenchao]  Yes, I'd like to take this.;;;","25/Jun/20 07:27;twalthr;Fixed in 1.12.0: 581fabe7df12961d20753dff8947dec07cbb2c56
Fixed in 1.11.1: f5ac8c352b7fb5aff5a78cffa348f72bd8492509
Fixed in 1.10.2: 23ad7e33117b7c02b28fda77596b12668b5117c1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should be volatile: network.api.writer.RecordWriter.flusherException ,FLINK-18163,13309762,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,roman,roman,roman,05/Jun/20 16:08,16/Jul/20 18:11,13/Jul/23 08:12,16/Jul/20 18:11,1.11.0,,,,,1.12.0,,,Runtime / Task,,,,,0,pull-request-available,,,,,,,pnowojski,roman,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 16 18:11:19 UTC 2020,,,,,,,,,,"0|z0fkag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/20 09:05;roman;[~NicoK] ,

I see you were involved in a discussion of RecordWriter.flusherException with [~pnowojski] .

What do you think about having a volatile field (in addition to non-volatile) and check it periodically (say once per 100 emits)?

I filed a PR showing this approach: [https://github.com/apache/flink/pull/12827 |https://github.com/apache/flink/pull/12827];;;","06/Jul/20 09:06;roman;(downgraded priority of the issue to minor);;;","06/Jul/20 13:46;roman;Performance isn't affected according to benchmarks: the difference is less than score error.

[http://codespeed.dak8s.net:8080/job/flink-benchmark-request/187/artifact/jmh-result.csv/*view*/]

 ;;;","16/Jul/20 18:11;pnowojski;merged commit 9fa0a99 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AddSplitEvents should serialize the splits into bytes.,FLINK-18162,13309747,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,becket_qin,becket_qin,becket_qin,05/Jun/20 14:41,16/Jun/20 06:17,13/Jul/23 08:12,15/Jun/20 17:43,,,,,,1.11.0,,,Connectors / Common,,,,,0,pull-request-available,,,,,"{{AddSplitsEvent}} is a serializable at the moment which contains a list of splits. However, the {{SourceSplit}} is not a subclass of {{Serializable}}. We need to serialize the splits in the {{AddSplitsEvent}} using SplitSerializer before sending it over the wire and deserialize the splits on the reader side.",,becket_qin,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 17:43:40 UTC 2020,,,,,,,,,,"0|z0fk7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/20 17:43;becket_qin;Merged to master:
f883f1190132f9dd6b37f1e5c8ae0e0d25f78333

Cherry-picked to release-1.11:
6cf60fda000d25baddfd7a2c3725eeecba77f886

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing parallelism is not possible in sql-client.sh,FLINK-18161,13309736,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,rmetzger,rmetzger,05/Jun/20 13:51,30/Jun/20 14:03,13/Jul/23 08:12,16/Jun/20 13:53,1.11.0,,,,,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"I tried using 
{code}
SET execution.parallelism=12
{code}

and changing the parallelism in the configuration file.

My SQL queries were always running with p=1 for all operators.",,dwysakowicz,godfreyhe,jark,libenchao,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18144,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 13:53:40 UTC 2020,,,,,,,,,,"0|z0fk4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 13:59;dwysakowicz;The problem is that the command will result in setting the parallelism directly on the underlying StreamExecutionEnvironment/BatchExecutionEnvironment. 

At the same time the {{TableConfig}} is populated with a {{Configuration}} ({{LocalExecutor#flinkConfig}}) populated from the {{GlobalConfiguration}} which sets the {{parallelism.default}}.
This setting always overwrites the parallelism of the {{StreamExecutionEnvironment}}

The flow is as follows:
1. We set value of {{execution.parallelism}} through {{StreamExecutionEnvironment#setParallelism}}
2. We set value of {{parallelism.default}} from TableConfig#configuration on {{StreamExecutionEnvironment}} effectively overwriting the previous setting.;;;","05/Jun/20 15:20;jark;Does {{parallelism.default}} works? ;;;","05/Jun/20 15:23;dwysakowicz;{{parallelism.default}} works from {{flink-conf.yaml}} There is no way to set it from the {{sql-client}}
{{execution.parallelism}} works if we remove the {{parallelism.default}} from {{flink-conf.yaml}};;;","05/Jun/20 15:24;dwysakowicz;The simplest fix would be to overwrite the {{parallelism.default}} with the value of {{execution.parallelism}} (which is only sql-client specific).;;;","05/Jun/20 15:29;jark;Could we just allow SQL CLI to set Flink configurations? This will also make it possible to set checkpointing/watermark in SQL CLI. ;;;","14/Jun/20 02:24;godfreyhe;For 1.11, I think we want to just fix the problem that the flink configurations overwrite sql-client properties. 
For 1.12, I want to remove the sql-client's specific properties (e.g. {{execution.parallelism}}, {{execution.time-characteristic}}, {{execution.restart-strategy.type}}), instead we should use the properties defined in flink runtime (e.g. {{parallelism.default}}, {{pipeline.time-characteristic}}, {{restart-strategy}}). {{Configuration}} section in sql-client file can be set options defined in table and runtime. And then, we can also set checkpointing/watermark in SQL CLI.

I investigate that if we just deprecate the sql-client's specific properties and also introduce flink runtime's properties. (that means they can both work in sql-client) the code is very complex to handle the conflicts and users will very confuse about the behavior. The possible scenarios and solutions are listed below:
1. conflicts in one sql-client config file, which one should be used?   --->  throw exception if exists, add log to tell users which keys are deprecated, and give suggested keys to replace them.
2. conflicts in default config file and session config file, which one should be used?   --->  throw exception if exists. add log to tell users which keys are deprecated, and give suggested keys to replace them.
3. conflicts in {{set}} operation, which one should be used? ---> if the given key in {{set}} operation is deprecated, check wether there is a corresponding new key exists in {{Environment}}. if exists, remove it first (to make sure the final result is correct), and then set the value to  {{Environment}} . vice versa. If users execute {{set}} command to list all properties, they may find some properties do not exist.

WDYT? [~dwysakowicz] [~jark]

;;;","14/Jun/20 05:04;jark;+1 to remove the duplicate congiuratiion in SQL CLI in the future. 

For now, I would suggest to make {{execution.parallelism}} as a synonym for {{parallelism.default}}. The set of {{execution.parallelism}} will be translated to {{parallelism.default}}. So there is no priority problem, it's just depends on the set order. And I guess this will simplify much things? ;;;","15/Jun/20 03:50;godfreyhe;[~jark], thanks for the suggestion, but I don't think that could simplify design. First, If sql-client's configuration accept {{parallelism.default}}, we also should handle the conflict. Second, when users execute {{execution.parallelism=12}} and then execute {{set}} command to list all properties, they find {{execution.parallelism}} does not exist . Last, we must handle this special case in {{ConfigurationEntry}}.;;;","15/Jun/20 05:49;dwysakowicz;Thanks for the discussion. I don't know if it is the right solution, but I'd go with:

For 1.11 I'd keep the old behaviour and make the {{execution.parallelism}} work. I'd not care too much to make the {{parallelism.default}} work well in that version, as it was not supported, it is not documented and it's more of a coincidence it works. To achieve that I'd do what [~jark] is suggesting, meaning instead of using {{env.setParallelism()}} I'd use {{set(""parallelism.default"", ...)}} for the {{execution.parallelism}}.

For 1.12 I'd be ok with removing the sql-cli specific parameters and supporting the common parameters. Probably though I'd still support the old parameters for one more version and thrown either an exception or very prominent warning that this setting will be removed in the next version. ;;;","15/Jun/20 06:30;jark;I have the similar thought, 

either (1) we fix {{execution.parallelism}} only in 1.11, and support {{parallelism.default}} in 1.12 (warning for {{execution.parallelism}}), and remove {{execution.parallelism}} in 1.13
or (2) we fix {{execution.parallelism}} and support {{parallelism.default}} in 1.11 (warning for {{execution.parallelism}}), and remove in 1.12;;;","15/Jun/20 09:11;godfreyhe;[~dwysakowicz] [~jark], I had opened a pr, it's very appreciate that you could have a look. In that pr, I do not introduce {{parallelism.default}}, instead I change the sql-client's special properties to common parameters when building {{TableConfig}} instance.

For 1.12, I also prefer to just deprecate the sql-client's special properties, and throw an exception directly when meeting a conflict.;;;","16/Jun/20 13:53;dwysakowicz;Fixed in:
* master
** 02cdfe88d5e686bedb7461e8e0b5f60bf807e095...94ea5e6ca9a1a0c6dda6b832473e40578207e78b
* 1.11
** 1b0d73a0599b6b7bceaeeecd50f3df48d835ef10...0ce4297e524397ec6bb8cf74c129db6daf86146a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobstore size check compares against offHeapMemory,FLINK-18157,13309706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,chesnay,chesnay,05/Jun/20 11:47,09/Jun/20 16:42,13/Jul/23 08:12,09/Jun/20 13:51,1.11.0,,,,,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"Setting {{jobmanager.memory.off-heap.size}} to 0 results in this confusing error:
{code}
[] - Loading configuration property: jobmanager.memory.process.size, 2000m
[] - Loading configuration property: jobmanager.memory.heap.size, 1500m
[] - Loading configuration property: jobmanager.memory.jvm-overhead.min, 100m
[] - Loading configuration property: jobmanager.memory.jvm-overhead.max, 350m
[] - Loading configuration property: jobmanager.memory.off-heap.size, 0m
[] - The configured or derived JVM heap memory size (jobstore.cache-size: 0 bytes) is less than the configured or default size of the job store cache (jobmanager.memory.heap.size: 50.000mb (52428800 bytes))
{code}

According to the documentation the jobstore uses the heap though.

{code}
private static JobManagerFlinkMemory createJobManagerFlinkMemory(
		Configuration config,
		MemorySize jvmHeap,
		MemorySize offHeapMemory) {
	verifyJvmHeapSize(jvmHeap);
	verifyJobStoreCacheSize(config, offHeapMemory);
	return new JobManagerFlinkMemory(jvmHeap, offHeapMemory);
}

private static void verifyJvmHeapSize(MemorySize jvmHeapSize) {
	if (jvmHeapSize.compareTo(JobManagerOptions.MIN_JVM_HEAP_SIZE) < 1) {
		LOG.warn(
			""The configured or derived JVM heap memory size ({}) is less than its recommended minimum value ({})"",
			jvmHeapSize.toHumanReadableString(),
			JobManagerOptions.MIN_JVM_HEAP_SIZE);
	}
}
{code}",,azagrebin,nicholasjiang,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18214,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 13:51:28 UTC 2020,,,,,,,,,,"0|z0fjy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 13:07;nicholasjiang;[~chesnay] Just the log of method verifyJobStoreCacheSize is incorrect. This log should use offheap.

{code:java}
private static JobManagerFlinkMemory createJobManagerFlinkMemory(
			Configuration config,
			MemorySize jvmHeap,
			MemorySize offHeapMemory) {
		verifyJvmHeapSize(jvmHeap);
		verifyJobStoreCacheSize(config, offHeapMemory);
		return new JobManagerFlinkMemory(jvmHeap, offHeapMemory);
}
private static void verifyJobStoreCacheSize(Configuration config, MemorySize jvmHeapSize) {
		MemorySize jobStoreCacheHeapSize =
			MemorySize.parse(config.getLong(JobManagerOptions.JOB_STORE_CACHE_SIZE) + ""b"");
		if (jvmHeapSize.compareTo(jobStoreCacheHeapSize) < 1) {
			LOG.warn(
				""The configured or derived JVM heap memory size ({}: {}) is less than the configured or default size "" +
					""of the job store cache ({}: {})"",
				JobManagerOptions.JOB_STORE_CACHE_SIZE.key(),
				jvmHeapSize.toHumanReadableString(),
				JobManagerOptions.JVM_HEAP_MEMORY.key(),
				jobStoreCacheHeapSize.toHumanReadableString());
		}
}
{code}

Could you please assign to me for fixing?;;;","05/Jun/20 14:14;azagrebin;[~nicholasjiang] good point, I believe we also miss a test for this. I am assigning you. We need to fix asap before the release. Please, move the issue to progress to work on this;;;","09/Jun/20 13:51;azagrebin;merged into master by 040ce4be4c37652bebd28a75d97d903fca64616d

merged into 1.11 by 823720de4e75faf31f181722bdaaedc781f22d58;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading error message if derived JVM overhead is too small,FLINK-18156,13309705,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,chesnay,chesnay,05/Jun/20 11:36,09/Jun/20 16:34,13/Jul/23 08:12,09/Jun/20 14:31,1.11.0,,,,,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The error message says that the derived size is lower then the configured overhead range, but now such property was configured by the user.

{code}
[] - Loading configuration property: jobmanager.memory.process.size, 2000m
[] - Loading configuration property: jobmanager.memory.heap.size, 1500m
Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: Derived JVM Overhead size (116.000mb (121634816 bytes)) is not in configured JVM Overhead range [192.000mb (201326592 bytes), 1024.000mb (107374182$        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.sanityCheckJvmOverhead(ProcessMemoryUtils.java:171)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveJvmMetaspaceAndOverheadFromTotalFlinkMemory(ProcessMemoryUtils.java:148)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveProcessSpecWithExplicitInternalMemory(ProcessMemoryUtils.java:86)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:71)
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:76)
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:71)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:92)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:66)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:54)
{code}",,azagrebin,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 14:31:59 UTC 2020,,,,,,,,,,"0|z0fjy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 14:11;azagrebin;[~chesnay] which property was configured by the user?;;;","07/Jun/20 11:28;chesnay;[~azagrebin]
{code}
[] - Loading configuration property: jobmanager.memory.process.size, 2000m
[] - Loading configuration property: jobmanager.memory.heap.size, 1500m
{code};;;","08/Jun/20 08:03;azagrebin;In this case, we always considered that the overhead is derived as it is not explicitly set. It just turned to have to be fixed as restricted by other option values. We decided to fail fast in case any inconsistencies like this one in user configuration to be explicit about the contracts. The min/max range is a hard contract, only fractions are soft.

We can change a bit the error message to add hints about adjusting min/max or other memory components to pass the check. Then user can at least do this consciously. ;;;","08/Jun/20 08:41;chesnay;The issue isn't that it fails, but that it claims something was configured when it wasn't.;;;","08/Jun/20 13:30;azagrebin;Do you mean that it says that the JVM Overhead was configured? but it says the JVM Overhead was derived. We use `derive` also in user docs.
The min/max values do not have to be configured, they have default values as well, we could add `configured or default` for min/max.;;;","08/Jun/20 13:41;chesnay;""Derived JVM Overhead size (116.000mb (121634816 bytes)) is not in >>>>>>*configured*<<<<<<< JVM Overhead range"";;;","08/Jun/20 14:06;azagrebin;Yes, we can add `configured or *default* JVM Overhead range`;;;","09/Jun/20 14:31;azagrebin;merged into master by 9d9ca95c64d9fadf72764dc4f6fbbcbffd5d11da
merged into 1.11 by 056d677eb27b56751a6a8f4ff74313324265738c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unhelpful error message if heap.size takes up signficant portion of process.size,FLINK-18154,13309703,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,chesnay,chesnay,05/Jun/20 11:33,09/Jun/20 16:28,13/Jul/23 08:12,09/Jun/20 15:09,1.11.0,,,,,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The process still starts fine though, similar to FLINK-18152.

{code}
[] - Loading configuration property: jobmanager.memory.process.size, 2000m
[] - Loading configuration property: jobmanager.memory.heap.size, 1999m
Exception in thread ""main"" java.lang.IllegalArgumentException: bytes must be >= 0
        at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139)
        at org.apache.flink.configuration.MemorySize.<init>(MemorySize.java:82)
        at org.apache.flink.configuration.MemorySize.subtract(MemorySize.java:216)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveJvmMetaspaceAndOverheadFromTotalFlinkMemory(ProcessMemoryUtils.java:147)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveProcessSpecWithExplicitInternalMemory(ProcessMemoryUtils.java:86)
        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:71)
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:76)
        at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:71)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:92)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:66)
        at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:54)
{code}",,azagrebin,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 15:09:45 UTC 2020,,,,,,,,,,"0|z0fjxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 14:30;azagrebin;Indeed, we should check `totalProcessMemorySize >= totalFlinkAndJvmMetaspaceSize` in `ProcessMemoryUtils.deriveJvmMetaspaceAndOverheadFromTotalFlinkMemory` for better error message.;;;","09/Jun/20 15:09;azagrebin;merged into master by 14c4be53b260631610c4890ed76bedb62a17330c

merged into 1.11 by 6498f4b76f6b7b4cfe99972e241a2257575155ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master starts despite IllegalConfigurationException,FLINK-18152,13309699,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,chesnay,chesnay,05/Jun/20 11:17,09/Jun/20 16:44,13/Jul/23 08:12,09/Jun/20 09:15,1.11.0,,,,,1.11.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,"When no memory configuration parameters are set for the Master process then this exception is logged by the BashJavaUtils:
{code}
Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: Either required fine-grained memory (jobmanager.memory.heap.size), or Total Flink Memory size (Key: 'jobmanager.memory.flink.size' , default: null $        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.failBecauseRequiredOptionsNotConfigured(ProcessMemoryUtils.java:111)                                                                                                       at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:81)                                                                                                                    at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:76)                                                                                                                          at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:71)                                                                                        at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:92)                                                                                                                                               at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:66)                                                                                                                                                        at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:54)
{code}

Afterwards, the Master process however continues to start just fine.

TaskManagers will not be started given the same situation, which is inconsistent behavior, and it seems dangerous to start a process despite an illegal configuration.",,azagrebin,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17973,,,,,,,,,,,,,,,,,,,,,,FLINK-18217,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 09:15:19 UTC 2020,,,,,,,,,,"0|z0fjwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 11:22;chesnay;In a similar vein, if {{jobmanager.memory.process.size}} is set to low value you also get an exception, but the process starts nonetheless:
{code}
[] - The derived from fraction jvm overhead memory (512.000kb (524288 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead                                                                         Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: Sum of configured JVM Metaspace (256.000mb (268435456 bytes)) and JVM Overhead (192.000mb (201326592 bytes)) exceed configured Total Process Memory$        at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveJvmMetaspaceAndOverheadWithTotalProcessMemory(ProcessMemoryUtils.java:133)                                                                                           at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveProcessSpecWithTotalProcessMemory(ProcessMemoryUtils.java:102)                                                                                                       at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:79)                                                                                                                    at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:76)                                                                                                                          at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:71)                                                                                        at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:92)                                                                                                                                               at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:66)                                                                                                                                                        at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:54)
{code}

Either there should also be a minimum total process memory to properly handle the JMV overhead minimum, or this should fail the startup.;;;","05/Jun/20 11:24;chesnay;Exceptions due to negative (or in any way malformatted) values also don't fail the startup:
{code}
Exception in thread ""main"" org.apache.flink.configuration.IllegalConfigurationException: Cannot read memory size from config option 'jobmanager.memory.process.size'.                                                                                at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.getMemorySizeFromConfig(ProcessMemoryUtils.java:217)                                                                                                                       at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.deriveProcessSpecWithTotalProcessMemory(ProcessMemoryUtils.java:100)                                                                                                       at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.memoryProcessSpecFromConfig(ProcessMemoryUtils.java:79)                                                                                                                    at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfig(JobManagerProcessUtils.java:76)                                                                                                                          at org.apache.flink.runtime.jobmanager.JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(JobManagerProcessUtils.java:71)                                                                                        at org.apache.flink.runtime.util.bash.BashJavaUtils.getJmResourceParams(BashJavaUtils.java:92)                                                                                                                                               at org.apache.flink.runtime.util.bash.BashJavaUtils.runCommand(BashJavaUtils.java:66)                                                                                                                                                        at org.apache.flink.runtime.util.bash.BashJavaUtils.main(BashJavaUtils.java:54)                                                                                                                                                      Caused by: java.lang.IllegalArgumentException: Could not parse value '-5m' for key 'jobmanager.memory.process.size'.                                                                                                                                 at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:753)                                                                                                                                                          at org.apache.flink.configuration.Configuration.get(Configuration.java:738)                                                                                                                                                                  at org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils.getMemorySizeFromConfig(ProcessMemoryUtils.java:215)                                                                                                                       ... 7 more                                                                                                                                                                                                                           Caused by: java.lang.NumberFormatException: text does not start with a number                                                                                                                                                                        at org.apache.flink.configuration.MemorySize.parseBytes(MemorySize.java:294)                                                                                                                                                                 at org.apache.flink.configuration.MemorySize.parse(MemorySize.java:247)                                                                                                                                                                      at org.apache.flink.configuration.Configuration.convertToMemorySize(Configuration.java:951)                                                                                                                                                  at org.apache.flink.configuration.Configuration.convertValue(Configuration.java:885)                                                                                                                                                         at org.apache.flink.configuration.Configuration.lambda$getOptional$2(Configuration.java:750)                                                                                                                                                 at java.base/java.util.Optional.map(Optional.java:265)                                                                                                                                                                                       at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:750)                                                                                                                                                          ... 9 more
{code};;;","05/Jun/20 14:26;azagrebin;I think we forgot to check called bash function exit code in the refactoring of `parseJmJvmArgsAndExportLogs`:
{code:java}
    if [[ $? -ne 0 ]]; then
        echo ""[ERROR] Could not get JVM parameters and dynamic configurations properly.""
        exit 1
    fi
{code}
 ;;;","09/Jun/20 09:15;azagrebin;merged into master by 44acd5017502688be128e7208dcd2f1fbfc7c396

merged into 1.11 by 822e8a3b67bcf48551eaaa124d987c684c5008d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve CWE22 problems in pyflink_gateway_server.py ,FLINK-18151,13309697,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,csq,hequn8128,hequn8128,05/Jun/20 11:12,08/Jun/20 02:58,13/Jul/23 08:12,08/Jun/20 02:58,1.11.0,1.12.0,,,,1.11.0,1.12.0,,API / Python,,,,,0,pull-request-available,,,,,"For example, the code `if os.path.isfile(flink_conf_path):` contains CWE22 problem that calling ""os.path.isfile"" with the tainted value in argument 1. This constructs a path or URI using the tainted value and may thus allow an attacker to access, modify, or test the existence of critical or sensitive files.

More information about CWE22 here: https://cwe.mitre.org/data/definitions/22.html",,hequn8128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 02:57:56 UTC 2020,,,,,,,,,,"0|z0fjw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 02:57;hequn8128;Resolved in
1.12.0 via c27a7a6f73ac7a082b0c9418f154ccfdcf7a0a31
1.11.0 via 941b7a4fa6cef52c384cbd80a382addef188a6ce
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Taskmanager logs could not show up in native K8s deployment,FLINK-18149,13309646,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,05/Jun/20 07:56,08/Jun/20 11:01,13/Jul/23 08:12,08/Jun/20 07:48,1.11.0,1.12.0,,,,1.11.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,,"In FLINK-17935, we use {{flinkConfig.get(DeploymentOptionsInternal.CONF_DIR)}} to replace {{CliFrontend.getConfigurationDirectoryFromEnv}}. It will cause problem in native K8s integration. The root cause we set the {{DeploymentOptionsInternal.CONF_DIR}} in flink-conf.yaml to a local path. However, it does not exist in JobManager pod.",,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18085,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17935,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 07:48:46 UTC 2020,,,,,,,,,,"0|z0fjkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 07:57;wangyang0918;cc [~kkl0u], could you assign this ticket to me? I will attach a PR to fix this issue.;;;","08/Jun/20 07:48;trohrmann;Fixed via

master: a4a99bac919d57387d54a2db80a249becf3ba680
1.11.0: 0965b47f5b55ce64c5ed5687c37d4b740aceec36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Resuming Savepoint"" e2e fails with TimeoutException in CliFrontend.stop() ",FLINK-18148,13309638,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,05/Jun/20 07:20,17/Jun/20 09:28,13/Jul/23 08:12,17/Jun/20 09:28,1.11.0,1.12.0,,,,1.11.0,,,Command Line Client,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2759&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""081bda854bc250e01055ed1ba9d43178"".
	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:495)
	at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:864)
	at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:487)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:931)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:493)
	... 6 more
Waiting for job (081bda854bc250e01055ed1ba9d43178) to reach terminal state FINISHED ...
Job (081bda854bc250e01055ed1ba9d43178) reached terminal state FINISHED
Savepoint location was empty. This may mean that the stop-with-savepoint failed.
[FAIL] Test script contains errors.
{code}",,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 09:28:17 UTC 2020,,,,,,,,,,"0|z0fjj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 07:23;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2759&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b;;;","05/Jun/20 07:26;rmetzger;{code}
2020-06-04T20:45:26.8965495Z 2020-06-04 20:43:17,927 WARN  org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel [] - Force-closing a channel whose registration task was not accepted by an event loop: [id: 0x19b77503]
2020-06-04T20:45:26.8966167Z java.util.concurrent.RejectedExecutionException: event executor terminated
2020-06-04T20:45:26.8967054Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:855) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8968164Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:340) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8969809Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:333) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8971146Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:766) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8972368Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe.register(AbstractChannel.java:472) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8980429Z 	at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:87) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8981747Z 	at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:81) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8982893Z 	at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.register(MultithreadEventLoopGroup.java:86) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8987906Z 	at org.apache.flink.shaded.netty4.io.netty.bootstrap.AbstractBootstrap.initAndRegister(AbstractBootstrap.java:322) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8989457Z 	at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.doResolveAndConnect(Bootstrap.java:159) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8990418Z 	at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:143) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8991327Z 	at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:127) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8992214Z 	at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:333) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8993075Z 	at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:272) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8994034Z 	at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:214) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8994967Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$23(RestClusterClient.java:666) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.8995554Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) [?:1.8.0_252]
2020-06-04T20:45:26.8996060Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) [?:1.8.0_252]
2020-06-04T20:45:26.9000099Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252]
2020-06-04T20:45:26.9000612Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) [?:1.8.0_252]
2020-06-04T20:45:26.9001111Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:594) [?:1.8.0_252]
2020-06-04T20:45:26.9001612Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_252]
2020-06-04T20:45:26.9002114Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_252]
2020-06-04T20:45:26.9002610Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_252]
2020-06-04T20:45:26.9003010Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
2020-06-04T20:45:26.9003944Z 2020-06-04 20:43:17,933 ERROR org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?
2020-06-04T20:45:26.9004509Z java.util.concurrent.RejectedExecutionException: event executor terminated
2020-06-04T20:45:26.9005413Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:855) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9006648Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:340) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9007807Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:333) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9008903Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:766) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9013958Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:764) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9015352Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:421) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9016498Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:149) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9017611Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9022680Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9023773Z 	at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:337) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9024636Z 	at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:272) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9025475Z 	at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:214) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9027095Z 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$23(RestClusterClient.java:666) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9027708Z 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) [?:1.8.0_252]
2020-06-04T20:45:26.9028213Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) [?:1.8.0_252]
2020-06-04T20:45:26.9028722Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252]
2020-06-04T20:45:26.9029378Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) [?:1.8.0_252]
2020-06-04T20:45:26.9033740Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:594) [?:1.8.0_252]
2020-06-04T20:45:26.9034425Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_252]
2020-06-04T20:45:26.9034968Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_252]
2020-06-04T20:45:26.9035508Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_252]
2020-06-04T20:45:26.9035945Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
2020-06-04T20:45:26.9036840Z 2020-06-04 20:43:17,934 ERROR org.apache.flink.client.cli.CliFrontend                      [] - Error while running the command.
2020-06-04T20:45:26.9037389Z org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""44312127344716c7caf340918231aeaa"".
2020-06-04T20:45:26.9038264Z 	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:495) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9043465Z 	at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:864) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9044395Z 	at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:487) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9045470Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:931) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9046472Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9047493Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9048451Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9048920Z Caused by: java.util.concurrent.TimeoutException
2020-06-04T20:45:26.9049343Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784) ~[?:1.8.0_252]
2020-06-04T20:45:26.9049846Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) ~[?:1.8.0_252]
2020-06-04T20:45:26.9054664Z 	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:493) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-06-04T20:45:26.9055247Z 	... 6 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2759&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8;;;","05/Jun/20 07:28;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2759&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=679407b1-ea2c-5965-2c8d-1467777fff88;;;","09/Jun/20 06:28;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2973&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","09/Jun/20 06:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2973&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=375367d9-d72e-5c21-3be0-b45149130f6b;;;","09/Jun/20 06:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2973&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334;;;","09/Jun/20 06:39;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2971&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","09/Jun/20 16:37;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3056&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","09/Jun/20 16:47;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3054&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 06:08;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3077&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 06:09;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3058&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 06:46;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3120&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","10/Jun/20 09:50;rmetzger;Stop with savepoint has to complete within client.timeout = 60s (default).

Time breakdown:
checkpoint creation takes ~ 52 seconds
{code}
2020-06-10 05:02:49,464 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 3 @ 1591765369463 for job 327846815d4c49ae30f9bdc7352218bc.
2020-06-10 05:03:41,100 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 3 for job 327846815d4c49ae30f9bdc7352218bc (401806 bytes in 51636 ms).
{code}

Stopping the job takes ~ 5 minutes
{code}
2020-06-10 05:07:20,002 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job General purpose test job (327846815d4c49ae30f9bdc7352218bc) switched from state RUNNING to FINISHED.
{code}

The slow operators to stop are:
{code}
2020-06-10 05:03:41,175 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: EventSource -> Timestamps/Watermarks (1/4) (396d333e1197d54dfc96352bc4aa5db4) switched from RUNNING to FINISHED.
2020-06-10 05:06:09,917 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ArtificalKeyedStateMapper_Kryo_and_Custom_Stateful (3/4) (7dfc199016041880c09f9a43a2396c7e) switched from RUNNING to FINISHED.
2020-06-10 05:06:44,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ArtificalKeyedStateMapper_Kryo_and_Custom_Stateful (2/4) (1d56a721f54ab6ae58c9782a2477d484) switched from RUNNING to FINISHED.
2020-06-10 05:07:09,917 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ArtificalKeyedStateMapper_Kryo_and_Custom_Stateful (4/4) (9fa1b1913284b6a75b8021f9513ce932) switched from RUNNING to FINISHED.
2020-06-10 05:07:18,320 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ArtificalKeyedStateMapper_Kryo_and_Custom_Stateful (1/4) (26f36930d6622198eb67711eaacae00c) switched from RUNNING to FINISHED.
{code}

From the TM logs, it that the ""Un-registration"" is slow?! 
{code}
2020-06-10 05:03:41,159 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: EventSource -> Timestamps/Watermarks (1/4) 396d333e1197d54dfc96352bc4aa5db4.
2020-06-10 05:06:09,908 INFO  org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend [] - Closed RocksDB State Backend. Cleaning up RocksDB working directory /tmp/flink-io-61f9aa77-daa2-4f83-9552-1f598583d156/job_327846815d4c49ae30f9bdc7352218bc_op_StreamMap_5271c210329e73bd743f3227edfb3b71__3_4__uuid_6e5a12d5-4906-4622-923e-70fb8eb9a23f.
2020-06-10 05:06:09,915 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - ArtificalKeyedStateMapper_Kryo_and_Custom_Stateful (3/4) (7dfc199016041880c09f9a43a2396c7e) switched from RUNNING to FINISHED.
{code}
;;;","11/Jun/20 07:52;rmetzger;This issue will probably be fixed in FLINK-17824.;;;","17/Jun/20 09:12;trohrmann;Can this ticket now be closed [~rmetzger]?;;;","17/Jun/20 09:28;rmetzger;Yeah, sorry. I should garbage collect my tickets more frequently.

Closing this ticket...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Orc document display is disordered,FLINK-18147,13309634,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jkillers,jkillers,jkillers,05/Jun/20 07:01,15/Jun/20 14:03,13/Jul/23 08:12,15/Jun/20 14:03,1.11.0,,,,,1.11.0,,,Documentation,FileSystems,,,,0,pull-request-available,,,,,"Official documents show that there is a problem.

link: [https://ci.apache.org/projects/flink/flink-docs-master/dev/connectors/streamfile_sink.html#tab_java_2]",,jkillers,lzljs3620320,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 14:03:17 UTC 2020,,,,,,,,,,"0|z0fji8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 07:17;lzljs3620320;Thanks [~jkillers] for reporting, assigned to you~;;;","15/Jun/20 14:03;trohrmann;Fixed via

master: e72ca4a9d0ba7cd034e7c46333e9b3fa3025ec65
1.11.0: 8a5d5ca6852ebf45d909e0dc2a2e7209613923e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State TTL configuration can't be set in SQL CLI,FLINK-18144,13309624,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jark,jark,05/Jun/20 06:03,30/Jun/20 14:06,13/Jul/23 08:12,30/Jun/20 14:03,,,,,,1.11.0,1.12.0,,Table SQL / API,Table SQL / Client,,,,0,,,,,,"A user reported this problem in user mailing list: http://apache-flink.147419.n8.nabble.com/flink-sql-ddl-ttl-td3602.html

",,jark,leonard,libenchao,liyu,lsy,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18161,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 14:06:46 UTC 2020,,,,,,,,,,"0|z0fjg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/20 14:06;leonard;This issue has been fixed in FLINK-18161

I close this one, feel free to reopen.

cc：[~jark];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Python meter metric not correct problem,FLINK-18143,13309597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,05/Jun/20 05:11,05/Jun/20 05:39,13/Jul/23 08:12,05/Jun/20 05:39,1.11.0,,,,,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,,"We should report the diff value instead of the total ones. Code `meter.markEvent(update);` in `FlinkMetricContainer` should be changed to `meter.markEvent(update - meter.getCount());`",,hequn8128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18100,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 05:39:18 UTC 2020,,,,,,,,,,"0|z0fja0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 05:39;hequn8128;Fixed 
in 1.12.0 via dcd764a8a0df788846886f2c3aa2b38828ae0e21
in 1.11.0 via 37f436ec96ffa27fec6650a3ef4726a2a93cada4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong state names in HiveContinuousMonitoringFunction,FLINK-18142,13309589,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Jun/20 04:22,05/Jun/20 07:52,13/Jul/23 08:12,05/Jun/20 07:52,,,,,,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,Should have different name between {{currReadTimeState}} and {{distinctPartsState}}.,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 07:52:29 UTC 2020,,,,,,,,,,"0|z0fj88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 07:52;lzljs3620320;master: d3dbf6114c8cd4651024dccb8f2bcaadd5874d2f

release-1.11: 78b3f48d5d13cb64cecccd833cc1dd18c8d520e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterTriggerSavepointITCase.testStopJobAfterSavepoint fails with AskTimeoutException,FLINK-18137,13309497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,rmetzger,rmetzger,04/Jun/20 17:28,15/Jun/20 09:09,13/Jul/23 08:12,12/Jun/20 13:48,1.11.0,1.12.0,,,,1.11.0,,,Runtime / Checkpointing,Runtime / Coordination,Runtime / Task,Tests,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2747&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

{code}
2020-06-04T16:17:20.4404189Z [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.352 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase
2020-06-04T16:17:20.4405548Z [ERROR] testStopJobAfterSavepoint(org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase)  Time elapsed: 10.058 s  <<< ERROR!
2020-06-04T16:17:20.4407342Z java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2020-06-04T16:17:20.4409562Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-06-04T16:17:20.4410333Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-06-04T16:17:20.4411259Z 	at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.cancelWithSavepoint(JobMasterTriggerSavepointITCase.java:264)
2020-06-04T16:17:20.4412292Z 	at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.testStopJobAfterSavepoint(JobMasterTriggerSavepointITCase.java:127)
2020-06-04T16:17:20.4413163Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-04T16:17:20.4413990Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-04T16:17:20.4414783Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-04T16:17:20.4415936Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-04T16:17:20.4416693Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-04T16:17:20.4417632Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-04T16:17:20.4418637Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-04T16:17:20.4419367Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-04T16:17:20.4420118Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T16:17:20.4420742Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-04T16:17:20.4421909Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T16:17:20.4422493Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-04T16:17:20.4423247Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-04T16:17:20.4424263Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-04T16:17:20.4424876Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-04T16:17:20.4426346Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-04T16:17:20.4427052Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-04T16:17:20.4427772Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-04T16:17:20.4428562Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-04T16:17:20.4429158Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T16:17:20.4429861Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T16:17:20.4430448Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T16:17:20.4431060Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-04T16:17:20.4431678Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-04T16:17:20.4432513Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-04T16:17:20.4433396Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-04T16:17:20.4434298Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-04T16:17:20.4440904Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-04T16:17:20.4443425Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-04T16:17:20.4444349Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-04T16:17:20.4445160Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-04T16:17:20.4446389Z Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2020-06-04T16:17:20.4447610Z 	at com.sun.proxy.$Proxy31.triggerSavepoint(Unknown Source)
2020-06-04T16:17:20.4448545Z 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:595)
2020-06-04T16:17:20.4449259Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-06-04T16:17:20.4449990Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2020-06-04T16:17:20.4450789Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2020-06-04T16:17:20.4451584Z 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:621)
2020-06-04T16:17:20.4452473Z 	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:595)
2020-06-04T16:17:20.4453572Z 	at org.apache.flink.client.program.MiniClusterClient.cancelWithSavepoint(MiniClusterClient.java:89)
2020-06-04T16:17:20.4454746Z 	at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.cancelWithSavepoint(JobMasterTriggerSavepointITCase.java:262)
2020-06-04T16:17:20.4455517Z 	... 32 more
2020-06-04T16:17:20.4457589Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#830345697]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-06-04T16:17:20.4459164Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-06-04T16:17:20.4460107Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-06-04T16:17:20.4460819Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-06-04T16:17:20.4461613Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-06-04T16:17:20.4462444Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-06-04T16:17:20.4463203Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-06-04T16:17:20.4464089Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-06-04T16:17:20.4464833Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-06-04T16:17:20.4465800Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-06-04T16:17:20.4466746Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-06-04T16:17:20.4467579Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-06-04T16:17:20.4468467Z 	at java.lang.Thread.run(Thread.java:748)
{code}
",,pnowojski,rmetzger,roman,SleePy,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 13:48:32 UTC 2020,,,,,,,,,,"0|z0fins:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 10:48;chesnay;Likely closely related to FLINK-18138 since they both timeout on triggering a savepoint.;;;","08/Jun/20 06:31;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2852&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc;;;","10/Jun/20 10:34;rmetzger;And FLINK-18148 is also a savepoint trigger related issue.;;;","10/Jun/20 13:11;rmetzger;(debugging the run mentioned in the comments ... )
Timeout is configured to akka.ask.timeout = 10s (default of Minicluster)

It seems that the checkpoint gets triggered 
{code}
21:35:46,509 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 @ 1591392946509 for job c671d421389fe49b8d86e40fce074a10.
{code}
But that's the last message regarding this checkpoint, job or the test vertex. There are a lot of other checkpoints being triggered from other tests though.

(The failure in the ticket description run looks exactly the same);;;","11/Jun/20 06:54;rmetzger;The error is somewhat reproducible locally, BUT not when running the test until failure.
Only when starting it from scratch.

It seems that the test is passing when the savepoint is triggered right after the vertex switches to RUNNING (my logs distinguish between SAVEPOINT and CHECKPOINT): 
{code}
3431 [testVertex (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task [] - testVertex (1/1) (40dd161a086ef633b70d9f505ba614c6) switched from DEPLOYING to RUNNING.
3432 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - testVertex (1/1) (40dd161a086ef633b70d9f505ba614c6) switched from DEPLOYING to RUNNING.
3496 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering SAVEPOINT 2 @ 1591798767033 for job f51485b0d91e3ff6be1d7a55d78c57d7.
3499 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Trigger checkpoint 2@1591798767033 for 40dd161a086ef633b70d9f505ba614c6.
3504 [jobmanager-future-thread-3] DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Received acknowledge message for checkpoint 2 from task 40dd161a086ef633b70d9f505ba614c6 of job f51485b0d91e3ff6be1d7a55d78c57d7 at 3ddadd8d-c5f3-495e-9c9c-b9278100c72a @ localhost (dataPort=-1).
3522 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.jobmaster.JobMaster [] - Savepoint stored in file:/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/junit3940216507898676469/junit3659258369742740599/savepoint-f51485-2066a260a0e0. Now cancelling f51485b0d91e3ff6be1d7a55d78c57d7.
3523 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job (unnamed job) (f51485b0d91e3ff6be1d7a55d78c57d7) switched from state RUNNING to CANCELLING.
{code}

However, if the Checkpoint Timer manages to trigger a checkpoint in between, then the test is failing.
{code}
3585 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering CHECKPOINT 1 @ 1591797060672 for job c5388d02ac93d92790e7a43b0b9f8c08.
3589 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Abort checkpoint 1@1591797060672 for 4174621aed015dd50b4ec8a204ba8724.
{code}

What I believe is happening:
- C1: Checkpoint 1 gets triggered, sets CheckpointCoordinator.isTriggering flag to ""true""
- S1: triggerSavepoint call comes in
- S1: SchedulerBase.triggerSavepoint(SchedulerBase.java:751) calls checkpointCoordinator.stopCheckpointScheduler() and cancels all pending checkpoints, in this case Checkpoint 1. 
- S1: SchedulerBase.triggerSavepoint(SchedulerBase.java:751) now is triggering a checkpoint at the Coordinator, however, the CheckpointRequestDecider decides to not execute any CheckpointTriggerRequest, because isTriggering = true.
- the system never returns anything on the SchedulerBase.triggerSavepoint call (because the case that the decider opts for ""nothing"" is not handled here: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L501) 

Solutions:
a) set isTriggering to ""false"" when aborting pending checkpoints
b) only modify isTriggering under the CheckpointCoordinator.lock (the checkpoint triggering is not happening under the lock)

I would appreciate if somebody who knows this component better could validate my analysis and suggest how to resolve the problem.
;;;","11/Jun/20 08:57;roman;From the code, the isTriggering should be set to false regardless of whether the checkpoint is aborted:
 * if startTriggeringCheckpoint() fail ""synchronously"" then failure is caught in the end and onTriggerFailure() is called
 * if Future<PendingCheckpoint> fails ""asynchronously"" then whenCompleteAsync handles it;;;","11/Jun/20 10:41;rmetzger;[~trohrmann] told me offline that he has found the issue. I'll assign him to the ticket.;;;","11/Jun/20 10:51;roman;I think there are several issues that can this problem by not completing masterStateCompletableFuture:
 # -masterHooks is empty-
 # checkpoint is discarded (aborted) - line 690 throws an exception instead of completing the future
 # checkpoint is discarded (aborted) - line 696 doesn't complete the future even if everything is acked (what  has [~trohrmann] found)
 # CompletableFuture.allOf waits for both masterStates and coordinatorCheckpoints futures while it could ""return"" as soon as one fails

 
Edit:
I've created a separate ticket for the issues above: FLINK-18257;;;","11/Jun/20 12:25;trohrmann;The problem seems to be that we call {{onTriggerFailure}} with a {{null}} value {{Throwable}} if the {{PendingCheckpoint}} has been discarded: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L564. Due to https://github.com/apache/flink/commit/1af33f1285d557f0171f4587d7f4e789df27e7cb {{onTriggerFailure}} will fail with a {{NullPointerException}}. This exception will then be swallowed by the {{CompletableFuture}} returned by the {{whenCompleteAsync}} call https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L542.;;;","11/Jun/20 15:30;SleePy;I just saw this issue. I think [~trohrmann] is right.
There is a problem of if/else in [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L547]. The {{throwable}} passed to {{onTriggerFailure}} can be null unexpectedly. Actually that's my fault, this code is written by me and I realized it some days ago. I was planning to fix it in the later PR because I checked it at that time that it can't raise NPE, so I thought it's not emergency. However FLINK-16770 breaks the plan, I reverted a lot of codes and forgot to fix this potential issue separately. Unfortunately https://github.com/apache/flink/commit/1af33f1285d557f0171f4587d7f4e789df27e7cb hits this NPE. {{onTriggerFailure}} shouldn't throw any exception by design.
The codes changed a bit from my last commit. I need to double check the comment mentioned by [~roman_khachatryan] to make sure there is no other issue.
;;;","11/Jun/20 15:58;trohrmann;Thanks for looking into the problem [~SleePy]!;;;","12/Jun/20 13:48;trohrmann;Fixed via

master: 38b2755bdf934b9e6d1aa6584b81fa210953d5be
1.11.0: 90a7dab894b12b85f21d4cfc637d0d770841cfe5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't start channel state writing for savepoints (RPC),FLINK-18136,13309485,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,04/Jun/20 16:20,10/Jun/20 05:27,13/Jul/23 08:12,09/Jun/20 09:00,1.11.0,,,,,1.11.0,1.12.0,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,,"ChannelStateWriter#start should be only called for unaligned checkpoint. While source triggering savepoint, SubtaskCheckpointCoordinator#initCheckpoint is introduced to judge the condition whether to start the internal writer or not. And this new method is also used in other places like CheckpointBarrierUnaligner.",,roman,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 09:00:56 UTC 2020,,,,,,,,,,"0|z0fil4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 09:00;zjwang;Merged in master: 6dc624d931858e0ee5da14bc6b41b834c94cbba7

Merged in release-1.11: 7f410fa4f4758e48fcc6c33d8354c21ab02e2dc6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File name conflict for different jobs in filesystem/hive sink,FLINK-18130,13309448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Jun/20 12:56,09/Jun/20 16:03,13/Jul/23 08:12,09/Jun/20 05:51,,,,,,1.11.0,,,Connectors / FileSystem,Connectors / Hive,,,,0,pull-request-available,,,,,"The sink of different jobs (or the same SQL runs multiple times) will produce the same file name, the rename will fail, and different file systems will produce different results:
 * HadoopFileSystem: ignore new data.
 * LocalFileSystem: overwrite old data.

We can add UUID to prefix, make sure there is no conflict between jobs.",,gaoyunhaii,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15066,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 05:51:36 UTC 2020,,,,,,,,,,"0|z0ficw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 05:51;lzljs3620320;master: 603cd830073f95d1b48cf8fa6817100ac17e1eb3

release-1.11: ab6cf4083c84bf8c04564c74f6d9f9783adf5e21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatedSourceITCase.testMultipleSources gets stuck,FLINK-18128,13309445,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,rmetzger,rmetzger,04/Jun/20 12:44,15/Nov/20 22:09,13/Jul/23 08:12,15/Sep/20 20:15,1.11.0,,,,,1.11.3,1.12.0,,Runtime / Checkpointing,Runtime / Task,Tests,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2705&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa

{code}
2020-06-04T11:19:39.6335702Z [INFO] 
2020-06-04T11:19:39.6337440Z [INFO] -------------------------------------------------------
2020-06-04T11:19:39.6338176Z [INFO]  T E S T S
2020-06-04T11:19:39.6339305Z [INFO] -------------------------------------------------------
2020-06-04T11:19:40.1906157Z [INFO] Running org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase
2020-06-04T11:34:51.0599860Z ==============================================================================
2020-06-04T11:34:51.0603015Z Maven produced no output for 900 seconds.
2020-06-04T11:34:51.0604174Z ==============================================================================
2020-06-04T11:34:51.0613908Z ==============================================================================
2020-06-04T11:34:51.0615097Z The following Java processes are running (JPS)
2020-06-04T11:34:51.0616043Z ==============================================================================
2020-06-04T11:34:51.0762007Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-06-04T11:34:51.2775341Z 29635 surefirebooter5307550588991461882.jar
2020-06-04T11:34:51.2931264Z 2100 Launcher
2020-06-04T11:34:51.3012583Z 32203 Jps
2020-06-04T11:34:51.3258038Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-06-04T11:34:51.5443730Z ==============================================================================
2020-06-04T11:34:51.5445134Z Printing stack trace of Java process 29635
2020-06-04T11:34:51.5445984Z ==============================================================================
2020-06-04T11:34:51.5528602Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-06-04T11:34:51.9617670Z 2020-06-04 11:34:51
2020-06-04T11:34:51.9619131Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-06-04T11:34:51.9619732Z 
2020-06-04T11:34:51.9620618Z ""Attach Listener"" #299 daemon prio=9 os_prio=0 tid=0x00007f4d60001000 nid=0x7e59 waiting on condition [0x0000000000000000]
2020-06-04T11:34:51.9621720Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:51.9622190Z 
2020-06-04T11:34:51.9623631Z ""flink-akka.actor.default-dispatcher-185"" #297 prio=5 os_prio=0 tid=0x00007f4ca0003000 nid=0x7db4 waiting on condition [0x00007f4d10136000]
2020-06-04T11:34:51.9624972Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9625716Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9627072Z 	- parking to wait for  <0x0000000080c557f0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-06-04T11:34:51.9628593Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-06-04T11:34:51.9629649Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-04T11:34:51.9630825Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-04T11:34:51.9631559Z 
2020-06-04T11:34:51.9633020Z ""flink-akka.actor.default-dispatcher-186"" #298 prio=5 os_prio=0 tid=0x00007f4d08006800 nid=0x7db3 waiting on condition [0x00007f4d12974000]
2020-06-04T11:34:51.9634074Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9634965Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9636384Z 	- parking to wait for  <0x0000000080c557f0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-06-04T11:34:51.9637683Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-06-04T11:34:51.9638795Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-04T11:34:51.9639845Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-04T11:34:51.9640475Z 
2020-06-04T11:34:51.9642008Z ""flink-akka.actor.default-dispatcher-182"" #293 prio=5 os_prio=0 tid=0x00007f4ce4007000 nid=0x7d7f waiting on condition [0x00007f4d1063b000]
2020-06-04T11:34:51.9643266Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:51.9644081Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9645784Z 	- parking to wait for  <0x0000000080c557f0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-06-04T11:34:51.9647361Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-06-04T11:34:51.9648516Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-06-04T11:34:51.9649692Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-04T11:34:51.9651016Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-04T11:34:51.9651739Z 
2020-06-04T11:34:51.9653311Z ""jobmanager-future-thread-16"" #236 daemon prio=5 os_prio=0 tid=0x00007f4c94002000 nid=0x7a99 waiting on condition [0x00007f4d12873000]
2020-06-04T11:34:51.9654578Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9655376Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9656968Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9658360Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9659770Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9661421Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9663047Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9664605Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9665917Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9667296Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9668392Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9668914Z 
2020-06-04T11:34:51.9670561Z ""Flink-DispatcherRestEndpoint-thread-4"" #119 daemon prio=5 os_prio=0 tid=0x00007f4cc0004800 nid=0x749b waiting on condition [0x00007f4d1328f000]
2020-06-04T11:34:51.9671805Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9672620Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9674216Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9675411Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9676984Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9678652Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9680193Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9681672Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9682904Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9684149Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9685116Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9685563Z 
2020-06-04T11:34:51.9687028Z ""Flink-DispatcherRestEndpoint-thread-3"" #118 daemon prio=5 os_prio=0 tid=0x00007f4cc0003800 nid=0x7490 waiting on condition [0x00007f4c825e4000]
2020-06-04T11:34:51.9688144Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9688792Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9690067Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9691152Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9692335Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9693745Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9695311Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9696509Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9697640Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9698724Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9699618Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9700045Z 
2020-06-04T11:34:51.9701396Z ""Flink-DispatcherRestEndpoint-thread-2"" #115 daemon prio=5 os_prio=0 tid=0x00007f4cc0002800 nid=0x7478 waiting on condition [0x00007f4d12f78000]
2020-06-04T11:34:51.9702417Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9703084Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9704565Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9705653Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9706804Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9708230Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9709574Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9710779Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9711890Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9713162Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9714226Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9714854Z 
2020-06-04T11:34:51.9715760Z ""SourceFetcher"" #112 prio=5 os_prio=0 tid=0x00007f4c10006800 nid=0x746b waiting on condition [0x00007f4c812d3000]
2020-06-04T11:34:51.9716852Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9717754Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9719392Z 	- parking to wait for  <0x000000008777dc80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9726297Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9727728Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9729254Z 	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
2020-06-04T11:34:51.9730516Z 	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
2020-06-04T11:34:51.9731853Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:113)
2020-06-04T11:34:51.9733084Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:94)
2020-06-04T11:34:51.9734253Z 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
2020-06-04T11:34:51.9735413Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-06-04T11:34:51.9736573Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-04T11:34:51.9737863Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-04T11:34:51.9739214Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9740349Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9740883Z 
2020-06-04T11:34:51.9743025Z ""SourceCoordinator-Source: TestingSource2"" #110 prio=5 os_prio=0 tid=0x00007f4cf406a000 nid=0x7469 waiting on condition [0x00007f4c814d5000]
2020-06-04T11:34:51.9744816Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9745675Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9747526Z 	- parking to wait for  <0x0000000087a7aff8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9749566Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9751072Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9752591Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:51.9753926Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9755414Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9756776Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9757999Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9758523Z 
2020-06-04T11:34:51.9759948Z ""SourceCoordinator-Source: TestingSource1"" #109 prio=5 os_prio=0 tid=0x00007f4cf4068800 nid=0x7468 waiting on condition [0x00007f4c815d6000]
2020-06-04T11:34:51.9760998Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9761673Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9762927Z 	- parking to wait for  <0x0000000087200178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9764012Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9765377Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9766639Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:51.9767882Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9769195Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9770567Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9771666Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9772235Z 
2020-06-04T11:34:51.9773191Z ""Sink: Unnamed (2/4)"" #105 prio=5 os_prio=0 tid=0x00007f4cf4060800 nid=0x7464 waiting on condition [0x00007f4c81bda000]
2020-06-04T11:34:51.9774776Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9775591Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9777459Z 	- parking to wait for  <0x0000000087a7b278> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9778794Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9780275Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9782114Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:146)
2020-06-04T11:34:51.9783693Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:279)
2020-06-04T11:34:51.9785319Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:190)
2020-06-04T11:34:51.9786679Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181)
2020-06-04T11:34:51.9788033Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:557)
2020-06-04T11:34:51.9789165Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:529)
2020-06-04T11:34:51.9790155Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-04T11:34:51.9791088Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-04T11:34:51.9791879Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9792357Z 
2020-06-04T11:34:51.9793428Z ""OutputFlusher for Source: TestingSource2"" #98 daemon prio=5 os_prio=0 tid=0x00007f4c10001800 nid=0x745d waiting on condition [0x00007f4c822e1000]
2020-06-04T11:34:51.9794635Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-06-04T11:34:51.9795319Z 	at java.lang.Thread.sleep(Native Method)
2020-06-04T11:34:51.9796321Z 	at org.apache.flink.runtime.io.network.api.writer.RecordWriter$OutputFlusher.run(RecordWriter.java:334)
2020-06-04T11:34:51.9797036Z 
2020-06-04T11:34:51.9797921Z ""Source: TestingSource2 (2/4)"" #94 prio=5 os_prio=0 tid=0x00007f4cf4055800 nid=0x7459 waiting on condition [0x00007f4c826e5000]
2020-06-04T11:34:51.9798908Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9799555Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9800954Z 	- parking to wait for  <0x0000000087500178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9802006Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9803187Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9804559Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:146)
2020-06-04T11:34:51.9805858Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:279)
2020-06-04T11:34:51.9807228Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:190)
2020-06-04T11:34:51.9808529Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181)
2020-06-04T11:34:51.9809766Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:557)
2020-06-04T11:34:51.9810856Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:529)
2020-06-04T11:34:51.9811862Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-04T11:34:51.9812794Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-04T11:34:51.9813580Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9814025Z 
2020-06-04T11:34:51.9814876Z ""CloseableReaperThread"" #89 daemon prio=5 os_prio=0 tid=0x00007f4c28004800 nid=0x7454 in Object.wait() [0x00007f4c82bea000]
2020-06-04T11:34:51.9815918Z    java.lang.Thread.State: WAITING (on object monitor)
2020-06-04T11:34:51.9816734Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:51.9817885Z 	- waiting on <0x00000000878034a0> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:51.9818759Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-06-04T11:34:51.9819928Z 	- locked <0x00000000878034a0> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:51.9820808Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-06-04T11:34:51.9821937Z 	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:204)
2020-06-04T11:34:51.9822706Z 
2020-06-04T11:34:51.9823940Z ""Flink-MetricRegistry-thread-1"" #87 daemon prio=5 os_prio=0 tid=0x00007f4cf403a000 nid=0x7452 waiting on condition [0x00007f4c82dec000]
2020-06-04T11:34:51.9825063Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:51.9825768Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9827061Z 	- parking to wait for  <0x0000000080c46498> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9828195Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:51.9829405Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:51.9830790Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:51.9832105Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9833408Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9834537Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9835667Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9836568Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9837003Z 
2020-06-04T11:34:51.9838290Z ""jobmanager-future-thread-15"" #86 daemon prio=5 os_prio=0 tid=0x00007f4d000e4800 nid=0x7451 waiting on condition [0x00007f4c82eed000]
2020-06-04T11:34:51.9839293Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9839970Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9841223Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9842294Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9843451Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9844893Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:51.9846266Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9847511Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9848582Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9849641Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9850531Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9850957Z 
2020-06-04T11:34:51.9852196Z ""jobmanager-future-thread-14"" #85 daemon prio=5 os_prio=0 tid=0x00007f4d000e2800 nid=0x7450 waiting on condition [0x00007f4c82fee000]
2020-06-04T11:34:51.9853177Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9853845Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9855174Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9856287Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9857618Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9858956Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9860293Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9861465Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9862544Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9863626Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9864587Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9865040Z 
2020-06-04T11:34:51.9866307Z ""jobmanager-future-thread-13"" #84 daemon prio=5 os_prio=0 tid=0x00007f4d000e1000 nid=0x744f waiting on condition [0x00007f4c830ef000]
2020-06-04T11:34:51.9867374Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9868019Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9869286Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9870338Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9871506Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9872859Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:51.9874275Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9875582Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9876638Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9877780Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9878683Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9879109Z 
2020-06-04T11:34:51.9880327Z ""jobmanager-future-thread-12"" #83 daemon prio=5 os_prio=0 tid=0x00007f4d000df800 nid=0x744e waiting on condition [0x00007f4c831f0000]
2020-06-04T11:34:51.9881409Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9882126Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9883427Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9884608Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9885796Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9887213Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:51.9888559Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9889735Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9890801Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9891859Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9892754Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9893187Z 
2020-06-04T11:34:51.9894424Z ""jobmanager-future-thread-11"" #82 daemon prio=5 os_prio=0 tid=0x00007f4d000dd800 nid=0x744d waiting on condition [0x00007f4c832f1000]
2020-06-04T11:34:51.9895512Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9896193Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9897529Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9898711Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9899878Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9901271Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9902744Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9904043Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9905324Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9906518Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9907468Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9907922Z 
2020-06-04T11:34:51.9909177Z ""jobmanager-future-thread-10"" #81 daemon prio=5 os_prio=0 tid=0x00007f4d000db800 nid=0x744c waiting on condition [0x00007f4c833f2000]
2020-06-04T11:34:51.9910178Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9910825Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9912091Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9913164Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9914314Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9915996Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9917386Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9918590Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9919657Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9920718Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9921606Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9922033Z 
2020-06-04T11:34:51.9923282Z ""jobmanager-future-thread-9"" #80 daemon prio=5 os_prio=0 tid=0x00007f4d000da000 nid=0x744b waiting on condition [0x00007f4c834f3000]
2020-06-04T11:34:51.9924257Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9925107Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9926445Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9927613Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9928764Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9930131Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9931469Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9932647Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9933721Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9934914Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9935852Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9936281Z 
2020-06-04T11:34:51.9937591Z ""jobmanager-future-thread-8"" #79 daemon prio=5 os_prio=0 tid=0x00007f4d000d8000 nid=0x744a waiting on condition [0x00007f4c835f4000]
2020-06-04T11:34:51.9938593Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9939359Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9940645Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9941698Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9942880Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9944220Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9945780Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9946985Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9948097Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9949185Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9950060Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9950506Z 
2020-06-04T11:34:51.9951732Z ""jobmanager-future-thread-7"" #78 daemon prio=5 os_prio=0 tid=0x00007f4d000d6800 nid=0x7449 waiting on condition [0x00007f4c836f5000]
2020-06-04T11:34:51.9952738Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9953383Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9954784Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9956012Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9957224Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9958589Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9959909Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9961100Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9962169Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9963229Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9964122Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9964684Z 
2020-06-04T11:34:51.9965965Z ""jobmanager-future-thread-6"" #77 daemon prio=5 os_prio=0 tid=0x00007f4d000d5000 nid=0x7448 waiting on condition [0x00007f4c837f6000]
2020-06-04T11:34:51.9966947Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9967675Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9968927Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9970017Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9971191Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9972532Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9973873Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9975208Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9976321Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9977472Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9978347Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9978776Z 
2020-06-04T11:34:51.9980010Z ""jobmanager-future-thread-5"" #76 daemon prio=5 os_prio=0 tid=0x00007f4d000d3800 nid=0x7447 waiting on condition [0x00007f4c838f7000]
2020-06-04T11:34:51.9981144Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:51.9981812Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9983091Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9984161Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:51.9985490Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:51.9986902Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:51.9988288Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9989486Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9990539Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9991623Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9992519Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9992945Z 
2020-06-04T11:34:51.9994158Z ""jobmanager-future-thread-4"" #75 daemon prio=5 os_prio=0 tid=0x00007f4d000d1000 nid=0x7446 waiting on condition [0x00007f4c839f8000]
2020-06-04T11:34:51.9995240Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9996082Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9997404Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9998474Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9999624Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0000989Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0002324Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0003501Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0004656Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0005750Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0006656Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0007083Z 
2020-06-04T11:34:52.0008361Z ""mini-cluster-io-thread-14"" #74 daemon prio=5 os_prio=0 tid=0x00007f4cf402b000 nid=0x7445 waiting on condition [0x00007f4c83af9000]
2020-06-04T11:34:52.0009334Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0010002Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0011256Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0012329Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0013501Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0014777Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0015971Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0017254Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0018345Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0019239Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0019668Z 
2020-06-04T11:34:52.0020806Z ""pool-2-thread-1"" #73 prio=5 os_prio=0 tid=0x00007f4cf4028800 nid=0x7444 waiting on condition [0x00007f4c83bfa000]
2020-06-04T11:34:52.0021869Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0022542Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0023806Z 	- parking to wait for  <0x000000008797ae48> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0024972Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0026160Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0027597Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:52.0028931Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0030107Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0031193Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0032255Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0033149Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0033574Z 
2020-06-04T11:34:52.0034882Z ""jobmanager-future-thread-3"" #72 daemon prio=5 os_prio=0 tid=0x00007f4cf4021000 nid=0x7441 waiting on condition [0x00007f4c83cfb000]
2020-06-04T11:34:52.0035891Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0036563Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0038036Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0039090Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0040263Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0041614Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0042956Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0044161Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0045427Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0046545Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0047480Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0047941Z 
2020-06-04T11:34:52.0049160Z ""mini-cluster-io-thread-13"" #71 daemon prio=5 os_prio=0 tid=0x00007f4d000c2000 nid=0x7440 waiting on condition [0x00007f4c83dfc000]
2020-06-04T11:34:52.0050151Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0050796Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0052074Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0053149Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0054301Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0055639Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0056678Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0057821Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0058882Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0059783Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0060209Z 
2020-06-04T11:34:52.0061450Z ""jobmanager-future-thread-2"" #70 daemon prio=5 os_prio=0 tid=0x00007f4d0802a800 nid=0x743f waiting on condition [0x00007f4c83efd000]
2020-06-04T11:34:52.0062567Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0063214Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0064577Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0065663Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0066842Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0068250Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0069584Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0070782Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0071836Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0072916Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0073789Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0074242Z 
2020-06-04T11:34:52.0075755Z ""FlinkCompletableFutureDelayScheduler-thread-1"" #69 daemon prio=5 os_prio=0 tid=0x00007f4d08021000 nid=0x743e waiting on condition [0x00007f4c83ffe000]
2020-06-04T11:34:52.0076828Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0077536Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0078926Z 	- parking to wait for  <0x000000008777e698> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0079998Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0081149Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0082514Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:52.0083830Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0085239Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0086384Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0087523Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0088428Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0088855Z 
2020-06-04T11:34:52.0090097Z ""mini-cluster-io-thread-12"" #68 daemon prio=5 os_prio=0 tid=0x00007f4d0800d000 nid=0x743d waiting on condition [0x00007f4d115da000]
2020-06-04T11:34:52.0091065Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0091736Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0092994Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0094069Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0095322Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0096546Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0097702Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0098758Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0099837Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0100733Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0101161Z 
2020-06-04T11:34:52.0102374Z ""mini-cluster-io-thread-11"" #67 daemon prio=5 os_prio=0 tid=0x00007f4d000b4800 nid=0x743c waiting on condition [0x00007f4d1196c000]
2020-06-04T11:34:52.0103496Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0104166Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0105557Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0106636Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0107863Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0109078Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0110111Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0111187Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0112263Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0113141Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0113590Z 
2020-06-04T11:34:52.0114888Z ""mini-cluster-io-thread-10"" #64 daemon prio=5 os_prio=0 tid=0x00007f4cb000b000 nid=0x7439 waiting on condition [0x00007f4d10338000]
2020-06-04T11:34:52.0115908Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0116557Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0117907Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0125989Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0127262Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0128486Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0129522Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0130608Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0131667Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0132564Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0132994Z 
2020-06-04T11:34:52.0134343Z ""mini-cluster-io-thread-9"" #63 daemon prio=5 os_prio=0 tid=0x00007f4cb0006800 nid=0x7438 waiting on condition [0x00007f4d10439000]
2020-06-04T11:34:52.0135607Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0136372Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0137749Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0138798Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0139975Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0141174Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0142229Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0143301Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0144361Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0145444Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0145909Z 
2020-06-04T11:34:52.0147243Z ""jobmanager-future-thread-1"" #62 daemon prio=5 os_prio=0 tid=0x00007f4cb001c000 nid=0x7437 waiting on condition [0x00007f4d1053a000]
2020-06-04T11:34:52.0148220Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0148887Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0150135Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0151379Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0152526Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0153885Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0155318Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0156536Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0157700Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0158762Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0159663Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0160089Z 
2020-06-04T11:34:52.0161328Z ""mini-cluster-io-thread-8"" #60 daemon prio=5 os_prio=0 tid=0x00007f4cfc017800 nid=0x7435 waiting on condition [0x00007f4d1093c000]
2020-06-04T11:34:52.0162317Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0162964Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0164236Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0165367Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0166572Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0168520Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0169554Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0170631Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0171697Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0172596Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0173024Z 
2020-06-04T11:34:52.0174262Z ""mini-cluster-io-thread-7"" #59 daemon prio=5 os_prio=0 tid=0x00007f4cfc016000 nid=0x7434 waiting on condition [0x00007f4d10a3d000]
2020-06-04T11:34:52.0175318Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0176020Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0177340Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0178416Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0179587Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0180777Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0181838Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0182888Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0183966Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0185038Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0185530Z 
2020-06-04T11:34:52.0186770Z ""mini-cluster-io-thread-6"" #58 daemon prio=5 os_prio=0 tid=0x00007f4cfc014800 nid=0x7433 waiting on condition [0x00007f4d10b3e000]
2020-06-04T11:34:52.0187836Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0188506Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0189757Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0190825Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0191976Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0193303Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0194355Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0195678Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0197319Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0198196Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0198648Z 
2020-06-04T11:34:52.0199892Z ""mini-cluster-io-thread-5"" #57 daemon prio=5 os_prio=0 tid=0x00007f4cb4003800 nid=0x7432 waiting on condition [0x00007f4d10c3f000]
2020-06-04T11:34:52.0200877Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0201523Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0202784Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0203856Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0205091Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0206342Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0207441Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0208512Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0209704Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0210593Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0211018Z 
2020-06-04T11:34:52.0212247Z ""mini-cluster-io-thread-4"" #56 daemon prio=5 os_prio=0 tid=0x00007f4cc616e000 nid=0x7431 waiting on condition [0x00007f4d10d40000]
2020-06-04T11:34:52.0213243Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0213887Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0215292Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0216467Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0217738Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0218929Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0219990Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0221055Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0222114Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0223005Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0223439Z 
2020-06-04T11:34:52.0224768Z ""mini-cluster-io-thread-3"" #55 daemon prio=5 os_prio=0 tid=0x00007f4cf4014800 nid=0x7430 waiting on condition [0x00007f4d10e41000]
2020-06-04T11:34:52.0225773Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0226441Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0227766Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0228837Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0230003Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0231194Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0232248Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0233294Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0234581Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0235493Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0235938Z 
2020-06-04T11:34:52.0237207Z ""mini-cluster-io-thread-2"" #54 daemon prio=5 os_prio=0 tid=0x00007f4cc6141800 nid=0x742f waiting on condition [0x00007f4d10f42000]
2020-06-04T11:34:52.0238195Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0238860Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0240116Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0241187Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0242335Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0243547Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0244662Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0245767Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0246845Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0247775Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0248219Z 
2020-06-04T11:34:52.0249480Z ""Flink-DispatcherRestEndpoint-thread-1"" #53 daemon prio=5 os_prio=0 tid=0x00007f4cc60c4800 nid=0x742e waiting on condition [0x00007f4d11043000]
2020-06-04T11:34:52.0250644Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:52.0251313Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0252604Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0253674Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:52.0254977Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:52.0256397Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:52.0257767Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0258964Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0260021Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0261102Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0261991Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0262415Z 
2020-06-04T11:34:52.0263611Z ""mini-cluster-io-thread-1"" #52 daemon prio=5 os_prio=0 tid=0x00007f4cc60c3000 nid=0x742d waiting on condition [0x00007f4d11144000]
2020-06-04T11:34:52.0264670Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0265335Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0266619Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0267750Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0268900Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0270115Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0271168Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0272219Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0273298Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0274314Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0274838Z 
2020-06-04T11:34:52.0276095Z ""flink-rest-server-netty-boss-thread-1"" #51 daemon prio=5 os_prio=0 tid=0x00007f4cc6062000 nid=0x742c runnable [0x00007f4d11245000]
2020-06-04T11:34:52.0277062Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0277813Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-06-04T11:34:52.0278653Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-06-04T11:34:52.0279581Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-06-04T11:34:52.0280504Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-06-04T11:34:52.0281888Z 	- locked <0x00000000879040b0> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-06-04T11:34:52.0283156Z 	- locked <0x00000000879040a0> (a java.util.Collections$UnmodifiableSet)
2020-06-04T11:34:52.0284265Z 	- locked <0x0000000087904058> (a sun.nio.ch.EPollSelectorImpl)
2020-06-04T11:34:52.0285156Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-06-04T11:34:52.0286383Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-06-04T11:34:52.0287813Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-06-04T11:34:52.0288990Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-06-04T11:34:52.0290320Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-06-04T11:34:52.0291783Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-06-04T11:34:52.0292788Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0293213Z 
2020-06-04T11:34:52.0294026Z ""IOManager reader thread #1"" #46 daemon prio=5 os_prio=0 tid=0x00007f4e711b9000 nid=0x7429 waiting on condition [0x00007f4d1206d000]
2020-06-04T11:34:52.0295102Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0295781Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0297076Z 	- parking to wait for  <0x000000008797ba50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0298195Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0299365Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0300565Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0301714Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-06-04T11:34:52.0302413Z 
2020-06-04T11:34:52.0303226Z ""IOManager writer thread #1"" #45 daemon prio=5 os_prio=0 tid=0x00007f4e711b7000 nid=0x7428 waiting on condition [0x00007f4d1216e000]
2020-06-04T11:34:52.0304216Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0304942Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0306264Z 	- parking to wait for  <0x000000008777ee58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0307417Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0308735Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0309996Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0311166Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-06-04T11:34:52.0311865Z 
2020-06-04T11:34:52.0312999Z ""Timer-2"" #43 daemon prio=5 os_prio=0 tid=0x00007f4e7117f000 nid=0x7427 in Object.wait() [0x00007f4d1226f000]
2020-06-04T11:34:52.0313971Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-06-04T11:34:52.0314967Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0316159Z 	- waiting on <0x000000008797bc58> (a java.util.TaskQueue)
2020-06-04T11:34:52.0316929Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-06-04T11:34:52.0318069Z 	- locked <0x000000008797bc58> (a java.util.TaskQueue)
2020-06-04T11:34:52.0318790Z 	at java.util.TimerThread.run(Timer.java:505)
2020-06-04T11:34:52.0319253Z 
2020-06-04T11:34:52.0320336Z ""Timer-1"" #41 daemon prio=5 os_prio=0 tid=0x00007f4e7117c800 nid=0x7426 in Object.wait() [0x00007f4d12370000]
2020-06-04T11:34:52.0321313Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-06-04T11:34:52.0322018Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0322990Z 	- waiting on <0x0000000087803a80> (a java.util.TaskQueue)
2020-06-04T11:34:52.0323768Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-06-04T11:34:52.0324863Z 	- locked <0x0000000087803a80> (a java.util.TaskQueue)
2020-06-04T11:34:52.0325652Z 	at java.util.TimerThread.run(Timer.java:505)
2020-06-04T11:34:52.0326096Z 
2020-06-04T11:34:52.0326861Z ""BLOB Server listener at 41173"" #37 daemon prio=5 os_prio=0 tid=0x00007f4e7117a000 nid=0x7425 runnable [0x00007f4d12471000]
2020-06-04T11:34:52.0327864Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0328567Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-06-04T11:34:52.0329437Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-06-04T11:34:52.0330392Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-06-04T11:34:52.0331352Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-06-04T11:34:52.0332267Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)
2020-06-04T11:34:52.0332818Z 
2020-06-04T11:34:52.0333941Z ""Timer-0"" #38 daemon prio=5 os_prio=0 tid=0x00007f4e71160000 nid=0x7424 in Object.wait() [0x00007f4d12772000]
2020-06-04T11:34:52.0334980Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-06-04T11:34:52.0335747Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0336733Z 	- waiting on <0x0000000087803a68> (a java.util.TaskQueue)
2020-06-04T11:34:52.0337565Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-06-04T11:34:52.0338608Z 	- locked <0x0000000087803a68> (a java.util.TaskQueue)
2020-06-04T11:34:52.0339327Z 	at java.util.TimerThread.run(Timer.java:505)
2020-06-04T11:34:52.0339785Z 
2020-06-04T11:34:52.0341535Z ""flink-metrics-scheduler-1"" #33 prio=5 os_prio=0 tid=0x00007f4e71126000 nid=0x7420 waiting on condition [0x00007f4d12b76000]
2020-06-04T11:34:52.0342549Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-06-04T11:34:52.0343227Z 	at java.lang.Thread.sleep(Native Method)
2020-06-04T11:34:52.0344121Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-06-04T11:34:52.0345384Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-06-04T11:34:52.0346626Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-06-04T11:34:52.0347630Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0348057Z 
2020-06-04T11:34:52.0349204Z ""flink-scheduler-1"" #27 prio=5 os_prio=0 tid=0x00007f4e70d14800 nid=0x7418 waiting on condition [0x00007f4d13390000]
2020-06-04T11:34:52.0350172Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-06-04T11:34:52.0350872Z 	at java.lang.Thread.sleep(Native Method)
2020-06-04T11:34:52.0351746Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-06-04T11:34:52.0352929Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-06-04T11:34:52.0354102Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-06-04T11:34:52.0355209Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0355675Z 
2020-06-04T11:34:52.0356453Z ""process reaper"" #24 daemon prio=10 os_prio=0 tid=0x00007f4d18048800 nid=0x7415 waiting on condition [0x00007f4d13dce000]
2020-06-04T11:34:52.0357656Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:52.0358349Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0359574Z 	- parking to wait for  <0x0000000080c55a08> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-06-04T11:34:52.0360584Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:52.0361681Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-06-04T11:34:52.0362832Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-06-04T11:34:52.0363905Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-06-04T11:34:52.0365103Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-06-04T11:34:52.0366197Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0367350Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0368223Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0368669Z 
2020-06-04T11:34:52.0369901Z ""surefire-forkedjvm-ping-30s"" #23 daemon prio=5 os_prio=0 tid=0x00007f4e703f6000 nid=0x7412 waiting on condition [0x00007f4d343c2000]
2020-06-04T11:34:52.0370924Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:52.0371592Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0372853Z 	- parking to wait for  <0x0000000080c79c40> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0374047Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:52.0375351Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:52.0376773Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:52.0378153Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0379351Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0380402Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0381480Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0382367Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0382797Z 
2020-06-04T11:34:52.0383991Z ""surefire-forkedjvm-command-thread"" #22 daemon prio=5 os_prio=0 tid=0x00007f4e703c4000 nid=0x7411 runnable [0x00007f4d346cd000]
2020-06-04T11:34:52.0385010Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0385725Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-06-04T11:34:52.0386500Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-06-04T11:34:52.0387475Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-06-04T11:34:52.0388394Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-06-04T11:34:52.0389548Z 	- locked <0x0000000080c9fff8> (a java.io.BufferedInputStream)
2020-06-04T11:34:52.0390386Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-06-04T11:34:52.0391401Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-06-04T11:34:52.0392588Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-06-04T11:34:52.0393518Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0393960Z 
2020-06-04T11:34:52.0394775Z ""Service Thread"" #21 daemon prio=9 os_prio=0 tid=0x00007f4e702d5000 nid=0x740f runnable [0x0000000000000000]
2020-06-04T11:34:52.0395757Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0396158Z 
2020-06-04T11:34:52.0396945Z ""C1 CompilerThread14"" #20 daemon prio=9 os_prio=0 tid=0x00007f4e702c8000 nid=0x740e waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0398088Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0398505Z 
2020-06-04T11:34:52.0399271Z ""C1 CompilerThread13"" #19 daemon prio=9 os_prio=0 tid=0x00007f4e702c5800 nid=0x740d waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0400201Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0400594Z 
2020-06-04T11:34:52.0401379Z ""C1 CompilerThread12"" #18 daemon prio=9 os_prio=0 tid=0x00007f4e702c3800 nid=0x740c waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0402290Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0402685Z 
2020-06-04T11:34:52.0403469Z ""C1 CompilerThread11"" #17 daemon prio=9 os_prio=0 tid=0x00007f4e702c1800 nid=0x740b waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0404393Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0404872Z 
2020-06-04T11:34:52.0405666Z ""C1 CompilerThread10"" #16 daemon prio=9 os_prio=0 tid=0x00007f4e702c0000 nid=0x740a waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0406601Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0406994Z 
2020-06-04T11:34:52.0407850Z ""C2 CompilerThread9"" #15 daemon prio=9 os_prio=0 tid=0x00007f4e702bd000 nid=0x7409 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0408779Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0409172Z 
2020-06-04T11:34:52.0409935Z ""C2 CompilerThread8"" #14 daemon prio=9 os_prio=0 tid=0x00007f4e702bb000 nid=0x7408 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0410862Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0411376Z 
2020-06-04T11:34:52.0412166Z ""C2 CompilerThread7"" #13 daemon prio=9 os_prio=0 tid=0x00007f4e702b9000 nid=0x7407 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0413076Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0413488Z 
2020-06-04T11:34:52.0414247Z ""C2 CompilerThread6"" #12 daemon prio=9 os_prio=0 tid=0x00007f4e702b7000 nid=0x7406 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0415279Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0415709Z 
2020-06-04T11:34:52.0416490Z ""C2 CompilerThread5"" #11 daemon prio=9 os_prio=0 tid=0x00007f4e702b5000 nid=0x7405 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0417456Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0417872Z 
2020-06-04T11:34:52.0418634Z ""C2 CompilerThread4"" #10 daemon prio=9 os_prio=0 tid=0x00007f4e702b3000 nid=0x7404 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0419554Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0419952Z 
2020-06-04T11:34:52.0420729Z ""C2 CompilerThread3"" #9 daemon prio=9 os_prio=0 tid=0x00007f4e702a9000 nid=0x7403 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0421635Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0422055Z 
2020-06-04T11:34:52.0422815Z ""C2 CompilerThread2"" #8 daemon prio=9 os_prio=0 tid=0x00007f4e702a6800 nid=0x7402 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0423747Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0424141Z 
2020-06-04T11:34:52.0424985Z ""C2 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f4e702a4800 nid=0x7401 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0425921Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0426339Z 
2020-06-04T11:34:52.0427094Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f4e702a2800 nid=0x7400 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0428062Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0428457Z 
2020-06-04T11:34:52.0429191Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f4e702a0800 nid=0x73ff runnable [0x0000000000000000]
2020-06-04T11:34:52.0430050Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0430443Z 
2020-06-04T11:34:52.0431290Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f4e7029f000 nid=0x73fe waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0432508Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0432969Z 
2020-06-04T11:34:52.0433783Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f4e7026e800 nid=0x73fd in Object.wait() [0x00007f4d7d0b2000]
2020-06-04T11:34:52.0434997Z    java.lang.Thread.State: WAITING (on object monitor)
2020-06-04T11:34:52.0435827Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0437241Z 	- waiting on <0x0000000080c56688> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:52.0438175Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-06-04T11:34:52.0439453Z 	- locked <0x0000000080c56688> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:52.0440348Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-06-04T11:34:52.0441259Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-06-04T11:34:52.0441825Z 
2020-06-04T11:34:52.0442576Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f4e7026a000 nid=0x73fc in Object.wait() [0x00007f4d7d1b3000]
2020-06-04T11:34:52.0443567Z    java.lang.Thread.State: WAITING (on object monitor)
2020-06-04T11:34:52.0444269Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0445010Z 	at java.lang.Object.wait(Object.java:502)
2020-06-04T11:34:52.0445862Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-06-04T11:34:52.0447007Z 	- locked <0x0000000080c56678> (a java.lang.ref.Reference$Lock)
2020-06-04T11:34:52.0447959Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-06-04T11:34:52.0448509Z 
2020-06-04T11:34:52.0449218Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f4e7000b800 nid=0x73c4 waiting on condition [0x00007f4e768fd000]
2020-06-04T11:34:52.0450228Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0450892Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0452052Z 	- parking to wait for  <0x000000008757aa28> (a java.util.concurrent.CompletableFuture$Signaller)
2020-06-04T11:34:52.0453050Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0454097Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-06-04T11:34:52.0455234Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-06-04T11:34:52.0456299Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-06-04T11:34:52.0457383Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-06-04T11:34:52.0458470Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:671)
2020-06-04T11:34:52.0459639Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-06-04T11:34:52.0460933Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1685)
2020-06-04T11:34:52.0462322Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1667)
2020-06-04T11:34:52.0463724Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.executeAndVerify(CoordinatedSourceITCase.java:84)
2020-06-04T11:34:52.0465237Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.testMultipleSources(CoordinatedSourceITCase.java:68)
2020-06-04T11:34:52.0466388Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-04T11:34:52.0467359Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-04T11:34:52.0468461Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-04T11:34:52.0469422Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-04T11:34:52.0470390Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-04T11:34:52.0471490Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-04T11:34:52.0472601Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-04T11:34:52.0473807Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-04T11:34:52.0474938Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-04T11:34:52.0475922Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T11:34:52.0476784Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-04T11:34:52.0477853Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-04T11:34:52.0478931Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-04T11:34:52.0479942Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-04T11:34:52.0480862Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-04T11:34:52.0481788Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-04T11:34:52.0482735Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-04T11:34:52.0483662Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-04T11:34:52.0484764Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T11:34:52.0485851Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T11:34:52.0486750Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T11:34:52.0487677Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-04T11:34:52.0488648Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-04T11:34:52.0489900Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-04T11:34:52.0491041Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-04T11:34:52.0492159Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-04T11:34:52.0493333Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-04T11:34:52.0494619Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-04T11:34:52.0495855Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-04T11:34:52.0496879Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-04T11:34:52.0497557Z 
2020-06-04T11:34:52.0498119Z ""VM Thread"" os_prio=0 tid=0x00007f4e70260800 nid=0x73fb runnable 
{code}",,dian.fu,rmetzger,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18071,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 15 22:09:11 UTC 2020,,,,,,,,,,"0|z0fic8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 06:32;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2852&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9;;;","26/Aug/20 16:47;rmetzger;(This is a custom branch, which might cause the problem, but I consider it unlikely): https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8324&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=78fc09d9-49a8-5d28-5888-b5228377203f;;;","14/Sep/20 21:15;sewen;The problem is the following:

  - When the fetching thread gets the fetch that signals ""end of split"" it enqueues that into the handover queue
  - Then there may be a context switch to the reader thread that checks this fetch and finishes its own split handling and checks whether the split fetcher is idle. It it not at that point.
  - The result is that the reader eventually goes to MORE_AVAILABLE_LATER

  - The split fetcher thread continues and marks the split fetcher as idle. However, it will never be checked again, because the reader thread is never notified again of any availability so it never checks.


*Solution part 1:*

The split fetcher needs to notify the availability future when it turns idle. I think there is no way to guarantee that the fetcher gets checked for being idle otherwise, save for introducing locking around the enqueuing and the idleness check. That is more expensive, the notification is rare and cheap.

*Solution part 2:*

The current model how the availability future is implemented makes this still dead-lock dangerous: The reader thread (mailbox thread) creates the availability future when obtaining it. It may be that the notification comes before the thread obtains the future and the notification is lost.

Consider this scenario:

  - Fetcher Thread gets ""end of split"" it enqueues that into the handover queue
  - Reader Thread obtains this fetch and removes the split locally. Split Reader is not idle, yet, Reader Thread concludes MORE_AVAILABLE_LATER
  - Fetcher Thread notifies the FutureNotifier - no future has been requested so far, the notification does nothing.
  - Reader Thread gets the future from the FutureNotifier, which will now never be complete.

We could fix this by adding a dummy element into the {{FutureCompletingBlockingQueue}}, but I think that  FLINK-19223 is a better solution when looking at the bigger picture.;;;","15/Sep/20 20:15;sewen;Fixed in 1.12.0 (master) via 511857049ba30c8ff0ee56da551fa4a479dc583e;;;","13/Oct/20 02:13;dian.fu;Instance on 1.11: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7471&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa

[~sewen] Is it possible to also backport the fix to 1.11 as this issue also exists in 1.11.;;;","13/Oct/20 09:23;sewen;I think we should backport all the changes from the connector framework to 1.11. There are many critical fixes in there.;;;","15/Nov/20 22:09;sewen;Fixed in 1.11.3 (release-1.11) via e72e48533902fe6a7271310736584e77b64d05b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the exception handling of the Python CompletableFuture,FLINK-18126,13309436,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,04/Jun/20 12:02,09/Jun/20 16:45,13/Jul/23 08:12,09/Jun/20 12:13,1.11.0,,,,,1.11.0,,,API / Python,,,,,0,pull-request-available,,,,,The implementation of method `exception` and `set_exception` are not correct. The purpose of the Python CompletableFuture is to hole the results from the Java CompletableFuture. We should expose the exception inside the Java CompletableFuture to users instead of allow Python users set the exception.,,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 12:13:03 UTC 2020,,,,,,,,,,"0|z0fia8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 12:13;dian.fu;master: 8a7093baa2a443b0ad88968bc7f86380f828da02
release-1.11: e042c8136765c567dbf5c55c4c8ca2557d9a48dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesClusterDescriptorTest.testDeployApplicationCluster:135->checkUpdatedConfigAndResourceSetting:194 expected:<my-flink-cluster1.test> but was:<null>,FLINK-18123,13309426,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,rmetzger,rmetzger,04/Jun/20 11:33,05/Jun/20 11:15,13/Jul/23 08:12,05/Jun/20 11:15,1.12.0,,,,,,,,Deployment / Kubernetes,,,,,0,test-stability,,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2696&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code}
2020-06-04T10:17:00.2971211Z [ERROR] Failures: 
2020-06-04T10:17:00.2973074Z [ERROR]   KubernetesClusterDescriptorTest.testDeployApplicationCluster:135->checkUpdatedConfigAndResourceSetting:194 expected:<my-flink-cluster1.test> but was:<null>
2020-06-04T10:17:00.2973889Z [ERROR] Errors: 
2020-06-04T10:17:00.2974940Z [ERROR]   KubernetesClusterDescriptorTest.testDeployHighAvailabilitySessionCluster:86->deploySessionCluster:181 Â» ClusterDeployment
2020-06-04T10:17:00.2976255Z [ERROR]   KubernetesClusterDescriptorTest.testDeploySessionCluster:77->deploySessionCluster:181 Â» ClusterDeployment
2020-06-04T10:17:00.2978026Z [ERROR]   KubernetesClusterDescriptorTest.testKillCluster:112->deploySessionCluster:181 Â» ClusterDeployment
2020-06-04T10:17:00.2980261Z [ERROR]   KubernetesResourceManagerTest.testCreateTaskManagerPodFailedAndRetry:306 Â» Execution
2020-06-04T10:17:00.2981587Z [ERROR]   KubernetesResourceManagerTest.testDuplicatedPodAdded:400 Â» Execution java.lang...
2020-06-04T10:17:00.2982675Z [ERROR]   KubernetesResourceManagerTest.testGetWorkerNodesFromPreviousAttempts:277 Â» Execution
2020-06-04T10:17:00.2983856Z [ERROR]   KubernetesResourceManagerTest.testPodAddedBeforeCreateTaskManagerPodFutureComplete:485 Â» Execution
2020-06-04T10:17:00.2984993Z [ERROR]   KubernetesResourceManagerTest.testPodTerminatedBeforeAdded:425 Â» Execution jav...
2020-06-04T10:17:00.2986083Z [ERROR]   KubernetesResourceManagerTest.testPreviousAttemptPodAdded:368 Â» Execution java...
2020-06-04T10:17:00.2987522Z [ERROR]   KubernetesResourceManagerTest.testStartAndRecoverVariousResourceSpec:328 Â» NullPointer
2020-06-04T10:17:00.2988812Z [ERROR]   KubernetesResourceManagerTest.testStartAndStopWorker:188 Â» Execution java.lang...
2020-06-04T10:17:00.2989909Z [ERROR]   KubernetesResourceManagerTest.testTaskManagerPodTerminated:236 Â» Execution jav...
2020-06-04T10:17:00.2990987Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2992098Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2993175Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2994271Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2995381Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2996474Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2997869Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.2999050Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.3000190Z [ERROR]   Fabric8FlinkKubeClientTest>KubernetesTestBase.setup:101->onSetup:102 Â» NullPointer
2020-06-04T10:17:00.3001184Z [ERROR]   FlinkConfMountDecoratorTest.testConfigMap:84 Â» NullPointer
2020-06-04T10:17:00.3002153Z [ERROR]   FlinkConfMountDecoratorTest.testDecoratedFlinkContainer:214 Â» NullPointer
2020-06-04T10:17:00.3003179Z [ERROR]   FlinkConfMountDecoratorTest.testDecoratedFlinkPodWithLog4j:133 Â» NullPointer
2020-06-04T10:17:00.3004271Z [ERROR]   FlinkConfMountDecoratorTest.testDecoratedFlinkPodWithLog4jAndLogback:186 Â» NullPointer
2020-06-04T10:17:00.3005594Z [ERROR]   FlinkConfMountDecoratorTest.testDecoratedFlinkPodWithLogback:159 Â» NullPointer
2020-06-04T10:17:00.3006692Z [ERROR]   FlinkConfMountDecoratorTest.testDecoratedFlinkPodWithoutLog4jAndLogback:104 Â» NullPointer
2020-06-04T10:17:00.3007991Z [ERROR]   FlinkConfMountDecoratorTest.testWhetherPodOrContainerIsDecorated:74 Â» NullPointer
2020-06-04T10:17:00.3009236Z [ERROR]   JavaCmdJobManagerDecoratorTest.testContainerStartCommandTemplate1:200 Â» NullPointer
2020-06-04T10:17:00.3010379Z [ERROR]   JavaCmdJobManagerDecoratorTest.testContainerStartCommandTemplate2:228 Â» NullPointer
2020-06-04T10:17:00.3011431Z [ERROR]   JavaCmdJobManagerDecoratorTest.testStartCommandWithLog4j:114 Â» NullPointer
2020-06-04T10:17:00.3012454Z [ERROR]   JavaCmdJobManagerDecoratorTest.testStartCommandWithLog4jAndLogback:143 Â» NullPointer
2020-06-04T10:17:00.3013534Z [ERROR]   JavaCmdJobManagerDecoratorTest.testStartCommandWithLogAndJMOpts:177 Â» NullPointer
2020-06-04T10:17:00.3014593Z [ERROR]   JavaCmdJobManagerDecoratorTest.testStartCommandWithLogAndJVMOpts:160 Â» NullPointer
2020-06-04T10:17:00.3015597Z [ERROR]   JavaCmdJobManagerDecoratorTest.testStartCommandWithLogback:128 Â» NullPointer
2020-06-04T10:17:00.3016694Z [ERROR]   JavaCmdJobManagerDecoratorTest.testStartCommandWithoutLog4jAndLogback:99 Â» NullPointer
2020-06-04T10:17:00.3018046Z [ERROR]   JavaCmdJobManagerDecoratorTest.testWhetherContainerOrPodIsReplaced:91 Â» NullPointer
2020-06-04T10:17:00.3019718Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testContainerStartCommandTemplate1:201 Â» NullPointer
2020-06-04T10:17:00.3020797Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testContainerStartCommandTemplate2:227 Â» NullPointer
2020-06-04T10:17:00.3021837Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testStartCommandWithLog4j:117 Â» NullPointer
2020-06-04T10:17:00.3022914Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testStartCommandWithLog4jAndLogback:146 Â» NullPointer
2020-06-04T10:17:00.3023940Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testStartCommandWithLogAndJMOpts:178 Â» NullPointer
2020-06-04T10:17:00.3025026Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testStartCommandWithLogAndJVMOpts:161 Â» NullPointer
2020-06-04T10:17:00.3026066Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testStartCommandWithLogback:131 Â» NullPointer
2020-06-04T10:17:00.3027402Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testStartCommandWithoutLog4jAndLogback:104 Â» NullPointer
2020-06-04T10:17:00.3028665Z [ERROR]   JavaCmdTaskManagerDecoratorTest.testWhetherContainerOrPodIsUpdated:96 Â» NullPointer
2020-06-04T10:17:00.3029759Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3030801Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3031858Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3032958Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3034062Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3035158Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3036285Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3037694Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3038899Z [ERROR]   KubernetesJobManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:86 Â» NullPointer
2020-06-04T10:17:00.3040021Z [ERROR]   KubernetesTaskManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:52 Â» NullPointer
2020-06-04T10:17:00.3041102Z [ERROR]   KubernetesTaskManagerFactoryTest>KubernetesTestBase.setup:101->onSetup:52 Â» NullPointer
2020-06-04T10:17:00.3041656Z [INFO] 
2020-06-04T10:17:00.3042081Z [ERROR] Tests run: 154, Failures: 1, Errors: 57, Skipped: 0

...

2020-06-04T10:16:49.0586719Z [INFO] Running org.apache.flink.kubernetes.KubernetesResourceManagerTest
2020-06-04T10:16:49.4577251Z Jun 04, 2020 10:16:49 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:49.4578124Z INFO: MockWebServer[57263] starting to accept connections
2020-06-04T10:16:49.7968818Z Jun 04, 2020 10:16:49 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:49.7988181Z INFO: MockWebServer[47703] starting to accept connections
2020-06-04T10:16:50.3020120Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:50.3022264Z INFO: MockWebServer[57263] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:50.3498453Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:50.3499415Z INFO: MockWebServer[57263] done accepting connections: Socket closed
2020-06-04T10:16:50.3840844Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:50.3842646Z INFO: MockWebServer[46371] starting to accept connections
2020-06-04T10:16:50.7378291Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:50.7380022Z INFO: MockWebServer[46371] received request: DELETE /apis/apps/v1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:50.7998891Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:50.8001161Z INFO: MockWebServer[46371] received request: DELETE /apis/extensions/v1beta1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:50.8038639Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:50.8039587Z INFO: MockWebServer[46371] done accepting connections: Socket closed
2020-06-04T10:16:50.8200105Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:50.8201008Z INFO: MockWebServer[40710] starting to accept connections
2020-06-04T10:16:50.9488756Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:50.9509278Z INFO: MockWebServer[40710] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:50.9538279Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:50.9539021Z INFO: MockWebServer[40710] done accepting connections: Socket closed
2020-06-04T10:16:50.9718263Z Jun 04, 2020 10:16:50 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:50.9719929Z INFO: MockWebServer[34104] starting to accept connections
2020-06-04T10:16:51.0418740Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.0420402Z INFO: MockWebServer[47703] received request: POST /apis/apps/v1/namespaces/test/deployments HTTP/1.1 and responded: HTTP/1.1 202 OK
2020-06-04T10:16:51.0778526Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.0781289Z INFO: MockWebServer[34104] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.0832353Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:51.0833071Z INFO: MockWebServer[34104] done accepting connections: Socket closed
2020-06-04T10:16:51.0833696Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:51.0834290Z INFO: MockWebServer[51103] starting to accept connections
2020-06-04T10:16:51.2059325Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.2077587Z INFO: MockWebServer[51103] received request: DELETE /apis/apps/v1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.2079262Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.2081091Z INFO: MockWebServer[51103] received request: DELETE /apis/extensions/v1beta1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.2090295Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:51.2091317Z INFO: MockWebServer[51103] done accepting connections: Socket closed
2020-06-04T10:16:51.2379623Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:51.2381411Z INFO: MockWebServer[47178] starting to accept connections
2020-06-04T10:16:51.3833748Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.3836250Z INFO: MockWebServer[47178] received request: DELETE /apis/apps/v1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.3848801Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.3851345Z INFO: MockWebServer[47178] received request: DELETE /apis/extensions/v1beta1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.3852736Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:51.3853736Z INFO: MockWebServer[47178] done accepting connections: Socket closed
2020-06-04T10:16:51.4278442Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:51.4288070Z INFO: MockWebServer[37219] starting to accept connections
2020-06-04T10:16:51.5368513Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.5379253Z INFO: MockWebServer[37219] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.5918679Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.5921895Z INFO: MockWebServer[37219] received request: DELETE /apis/apps/v1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.5924083Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.5925854Z INFO: MockWebServer[37219] received request: DELETE /apis/extensions/v1beta1/namespaces/test/deployments/my-flink-cluster1 HTTP/1.1 and responded: HTTP/1.1 404 Client Error
2020-06-04T10:16:51.6121342Z Jun 04, 2020 10:16:51 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:51.6124205Z INFO: MockWebServer[37219] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 200 OK
2020-06-04T10:16:53.0608438Z Jun 04, 2020 10:16:53 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:53.0619366Z INFO: MockWebServer[37219] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 200 OK
2020-06-04T10:16:53.0839283Z Jun 04, 2020 10:16:53 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:53.0840974Z INFO: MockWebServer[37219] done accepting connections: Socket closed
2020-06-04T10:16:53.0932052Z Jun 04, 2020 10:16:53 AM okhttp3.mockwebserver.MockWebServer$2 execute
2020-06-04T10:16:53.0932904Z INFO: MockWebServer[42359] starting to accept connections
2020-06-04T10:16:53.2448447Z Jun 04, 2020 10:16:53 AM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
2020-06-04T10:16:53.2469374Z INFO: MockWebServer[42359] received request: GET /api/v1/namespaces/test/services/my-flink-cluster1-rest HTTP/1.1 and responded: HTTP/1.1 200 OK
2020-06-04T10:16:53.2773835Z Jun 04, 2020 10:16:53 AM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
2020-06-04T10:16:53.2775683Z INFO: MockWebServer[42359] done accepting connections: Socket closed
2020-06-04T10:16:53.3002721Z [ERROR] Tests run: 8, Failures: 1, Errors: 3, Skipped: 0, Time elapsed: 5.589 s <<< FAILURE! - in org.apache.flink.kubernetes.KubernetesClusterDescriptorTest
2020-06-04T10:16:53.3003910Z [ERROR] testDeployHighAvailabilitySessionCluster(org.apache.flink.kubernetes.KubernetesClusterDescriptorTest)  Time elapsed: 0.444 s  <<< ERROR!
2020-06-04T10:16:53.3005319Z org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster ""my-flink-cluster1"".
2020-06-04T10:16:53.3006129Z 	at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployClusterInternal(KubernetesClusterDescriptor.java:238)
2020-06-04T10:16:53.3007192Z 	at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deploySessionCluster(KubernetesClusterDescriptor.java:137)
2020-06-04T10:16:53.3008015Z 	at org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.deploySessionCluster(KubernetesClusterDescriptorTest.java:181)
2020-06-04T10:16:53.3008676Z 	at org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.testDeployHighAvailabilitySessionCluster(KubernetesClusterDescriptorTest.java:86)
2020-06-04T10:16:53.3009208Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-04T10:16:53.3009587Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-04T10:16:53.3010059Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-04T10:16:53.3010454Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-04T10:16:53.3010868Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-04T10:16:53.3011325Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-04T10:16:53.3011798Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-04T10:16:53.3012259Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-04T10:16:53.3012698Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-06-04T10:16:53.3013146Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-06-04T10:16:53.3013567Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T10:16:53.3013988Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T10:16:53.3014387Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-04T10:16:53.3014741Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T10:16:53.3015110Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-04T10:16:53.3015518Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-04T10:16:53.3015980Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-04T10:16:53.3016380Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-04T10:16:53.3016768Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-04T10:16:53.3017651Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-04T10:16:53.3018182Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-04T10:16:53.3018577Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-04T10:16:53.3018948Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-04T10:16:53.3019366Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-04T10:16:53.3019849Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-04T10:16:53.3020328Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-04T10:16:53.3020803Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-04T10:16:53.3021408Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-04T10:16:53.3022064Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-04T10:16:53.3022528Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-04T10:16:53.3022974Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-04T10:16:53.3023321Z Caused by: java.lang.NullPointerException
2020-06-04T10:16:53.3023655Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58)
2020-06-04T10:16:53.3024204Z 	at org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.getConfigDirectory(AbstractKubernetesParameters.java:60)
2020-06-04T10:16:53.3024851Z 	at org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.hasLogback(AbstractKubernetesParameters.java:140)
2020-06-04T10:16:53.3025492Z 	at org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecorator.decorateFlinkPod(JavaCmdJobManagerDecorator.java:59)
2020-06-04T10:16:53.3026179Z 	at org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.buildKubernetesJobManagerSpecification(KubernetesJobManagerFactory.java:66)
2020-06-04T10:16:53.3026949Z 	at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployClusterInternal(KubernetesClusterDescriptor.java:226)
2020-06-04T10:16:53.3027445Z 	... 34 more
2020-06-04T10:16:53.3027559Z 
{code}",,rmetzger,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 11:15:32 UTC 2020,,,,,,,,,,"0|z0fi80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/20 12:26;wangyang0918;FYI: This commit[1] has been reverted.

 

[1]. [https://github.com/apache/flink/commit/2eb7377163490878f12e4687db5fb4db0e6f47c2] ;;;","05/Jun/20 11:15;rmetzger;Thanks, I'm closing this ticket then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kubernetes test fails with ""error: timed out waiting for the condition on jobs/flink-job-cluster""",FLINK-18122,13309424,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,04/Jun/20 11:27,30/Oct/20 06:39,13/Jul/23 08:12,30/Oct/20 06:39,1.11.0,,,,,1.12.0,,,Deployment / Kubernetes,Tests,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2697&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-06-04T09:25:40.7205843Z service/flink-job-cluster created
2020-06-04T09:25:40.9661515Z job.batch/flink-job-cluster created
2020-06-04T09:25:41.2189123Z deployment.apps/flink-task-manager created
2020-06-04T10:32:32.6402983Z error: timed out waiting for the condition on jobs/flink-job-cluster
2020-06-04T10:32:33.8057757Z error: unable to upgrade connection: container not found (""flink-task-manager"")
2020-06-04T10:32:33.8111302Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-56335570120/out/kubernetes_wc_out*': No such file or directory
2020-06-04T10:32:33.8124455Z FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected e682ec6622b5e83f2eb614617d5ab2cf.
2020-06-04T10:32:33.8125379Z head hexdump of actual:
2020-06-04T10:32:33.8136133Z head: cannot open '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-56335570120/out/kubernetes_wc_out*' for reading: No such file or directory
2020-06-04T10:32:33.8344715Z Debugging failed Kubernetes test:
2020-06-04T10:32:33.8345469Z Currently existing Kubernetes resources
2020-06-04T10:32:36.4977853Z I0604 10:32:36.497383   13191 request.go:621] Throttling request took 1.198606989s, request: GET:https://10.1.0.4:8443/apis/rbac.authorization.k8s.io/v1?timeout=32s
2020-06-04T10:32:46.6975735Z I0604 10:32:46.697234   13191 request.go:621] Throttling request took 4.398107353s, request: GET:https://10.1.0.4:8443/apis/authorization.k8s.io/v1?timeout=32s
2020-06-04T10:32:57.4978637Z I0604 10:32:57.497209   13191 request.go:621] Throttling request took 1.198449167s, request: GET:https://10.1.0.4:8443/apis/apps/v1?timeout=32s
2020-06-04T10:33:07.4980104Z I0604 10:33:07.497320   13191 request.go:621] Throttling request took 4.198274438s, request: GET:https://10.1.0.4:8443/apis/apiextensions.k8s.io/v1?timeout=32s
2020-06-04T10:33:18.4976060Z I0604 10:33:18.497258   13191 request.go:621] Throttling request took 1.19871495s, request: GET:https://10.1.0.4:8443/apis/apps/v1?timeout=32s
2020-06-04T10:33:28.4979129Z I0604 10:33:28.497276   13191 request.go:621] Throttling request took 4.198369672s, request: GET:https://10.1.0.4:8443/apis/rbac.authorization.k8s.io/v1?timeout=32s
2020-06-04T10:33:30.9182069Z NAME                                     READY   STATUS              RESTARTS   AGE
2020-06-04T10:33:30.9184099Z pod/flink-job-cluster-dtb67              0/1     ErrImageNeverPull   0          67m
2020-06-04T10:33:30.9184869Z pod/flink-task-manager-74ccc9bd9-psqwm   0/1     ErrImageNeverPull   0          67m
2020-06-04T10:33:30.9185226Z 
2020-06-04T10:33:30.9185926Z NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                       AGE
2020-06-04T10:33:30.9186832Z service/flink-job-cluster   NodePort    10.111.92.199   <none>        6123:32501/TCP,6124:31360/TCP,6125:30025/TCP,8081:30081/TCP   67m
2020-06-04T10:33:30.9187545Z service/kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP                                                       68m
2020-06-04T10:33:30.9187976Z 
2020-06-04T10:33:30.9188472Z NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
2020-06-04T10:33:30.9189179Z deployment.apps/flink-task-manager   0/1     1            0           67m
2020-06-04T10:33:30.9189508Z 
2020-06-04T10:33:30.9189815Z NAME                                           DESIRED   CURRENT   READY   AGE
2020-06-04T10:33:30.9190418Z replicaset.apps/flink-task-manager-74ccc9bd9   1         1         0       67m
2020-06-04T10:33:30.9190662Z 
2020-06-04T10:33:30.9190891Z NAME                          COMPLETIONS   DURATION   AGE
2020-06-04T10:33:30.9191423Z job.batch/flink-job-cluster   0/1           67m        67m
2020-06-04T10:33:33.7840921Z I0604 10:33:33.783675   13482 request.go:621] Throttling request took 1.198522435s, request: GET:https://10.1.0.4:8443/apis/batch/v1beta1?timeout=32s
2020-06-04T10:33:43.7842354Z I0604 10:33:43.783896   13482 request.go:621] Throttling request took 4.198512626s, request: GET:https://10.1.0.4:8443/apis/rbac.authorization.k8s.io/v1?timeout=32s
2020-06-04T10:33:54.7840871Z I0604 10:33:54.783716   13482 request.go:621] Throttling request took 1.198557318s, request: GET:https://10.1.0.4:8443/apis/batch/v1?timeout=32s
2020-06-04T10:34:04.7844128Z I0604 10:34:04.783887   13482 request.go:621] Throttling request took 4.199015469s, request: GET:https://10.1.0.4:8443/apis/authorization.k8s.io/v1?timeout=32s
2020-06-04T10:34:15.7841963Z I0604 10:34:15.783767   13482 request.go:621] Throttling request took 1.198438099s, request: GET:https://10.1.0.4:8443/apis/apiregistration.k8s.io/v1?timeout=32s
2020-06-04T10:34:25.9842118Z I0604 10:34:25.983800   13482 request.go:621] Throttling request took 4.39847004s, request: GET:https://10.1.0.4:8443/apis/apiextensions.k8s.io/v1beta1?timeout=32s
2020-06-04T10:34:34.1008762Z Name:         flink-job-cluster-dtb67
2020-06-04T10:34:34.1009742Z Namespace:    default
2020-06-04T10:34:34.1010171Z Priority:     0
2020-06-04T10:34:34.1010837Z Node:         fv-az555/10.1.0.4
2020-06-04T10:34:34.1011354Z Start Time:   Thu, 04 Jun 2020 09:25:40 +0000
2020-06-04T10:34:34.1011820Z Labels:       app=flink
2020-06-04T10:34:34.1012442Z               component=job-cluster
2020-06-04T10:34:34.1013225Z               controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1013955Z               job-name=flink-job-cluster
2020-06-04T10:34:34.1014406Z Annotations:  <none>
2020-06-04T10:34:34.1014802Z Status:       Pending
2020-06-04T10:34:34.1015188Z IP:           172.17.0.4
2020-06-04T10:34:34.1015553Z IPs:
2020-06-04T10:34:34.1015933Z   IP:           172.17.0.4
2020-06-04T10:34:34.1016936Z Controlled By:  Job/flink-job-cluster
2020-06-04T10:34:34.1017389Z Containers:
2020-06-04T10:34:34.1017951Z   flink-job-cluster:
2020-06-04T10:34:34.1018377Z     Container ID:  
2020-06-04T10:34:34.1018814Z     Image:         test_kubernetes_embedded_job
2020-06-04T10:34:34.1019257Z     Image ID:      
2020-06-04T10:34:34.1019726Z     Ports:         6123/TCP, 6124/TCP, 6125/TCP, 8081/TCP
2020-06-04T10:34:34.1020231Z     Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP
2020-06-04T10:34:34.1020652Z     Args:
2020-06-04T10:34:34.1021192Z       standalone-job
2020-06-04T10:34:34.1021782Z       --job-classname
2020-06-04T10:34:34.1022241Z       org.apache.flink.examples.java.wordcount.WordCount
2020-06-04T10:34:34.1022995Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1023666Z       -Dparallelism.default=1
2020-06-04T10:34:34.1024289Z       -Dblob.server.port=6124
2020-06-04T10:34:34.1025391Z       -Dqueryable-state.server.ports=6125
2020-06-04T10:34:34.1027276Z       --output
2020-06-04T10:34:34.1027537Z       /cache/kubernetes_wc_out
2020-06-04T10:34:34.1027841Z     State:          Waiting
2020-06-04T10:34:34.1028137Z       Reason:       ErrImageNeverPull
2020-06-04T10:34:34.1028442Z     Ready:          False
2020-06-04T10:34:34.1028702Z     Restart Count:  0
2020-06-04T10:34:34.1028985Z     Environment:    <none>
2020-06-04T10:34:34.1029230Z     Mounts:
2020-06-04T10:34:34.1029760Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1030426Z       /var/run/secrets/kubernetes.io/serviceaccount from default-token-lgfgh (ro)
2020-06-04T10:34:34.1030791Z Conditions:
2020-06-04T10:34:34.1031038Z   Type              Status
2020-06-04T10:34:34.1031293Z   Initialized       True 
2020-06-04T10:34:34.1031561Z   Ready             False 
2020-06-04T10:34:34.1031814Z   ContainersReady   False 
2020-06-04T10:34:34.1032554Z   PodScheduled      True 
2020-06-04T10:34:34.1032784Z Volumes:
2020-06-04T10:34:34.1033242Z   job-artifacts-volume:
2020-06-04T10:34:34.1033582Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1034672Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1035133Z     HostPathType:  
2020-06-04T10:34:34.1035585Z   default-token-lgfgh:
2020-06-04T10:34:34.1035928Z     Type:        Secret (a volume populated by a Secret)
2020-06-04T10:34:34.1036470Z     SecretName:  default-token-lgfgh
2020-06-04T10:34:34.1036778Z     Optional:    false
2020-06-04T10:34:34.1037042Z QoS Class:       BestEffort
2020-06-04T10:34:34.1037504Z Node-Selectors:  <none>
2020-06-04T10:34:34.1038098Z Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
2020-06-04T10:34:34.1038529Z                  node.kubernetes.io/unreachable:NoExecute for 300s
2020-06-04T10:34:34.1038850Z Events:
2020-06-04T10:34:34.1039173Z   Type     Reason             Age                    From               Message
2020-06-04T10:34:34.1040006Z   ----     ------             ----                   ----               -------
2020-06-04T10:34:34.1041609Z   Warning  ErrImageNeverPull  8m48s (x278 over 68m)  kubelet, fv-az555  Container image ""test_kubernetes_embedded_job"" is not present with pull policy of Never
2020-06-04T10:34:34.1042600Z   Warning  Failed             3m50s (x301 over 68m)  kubelet, fv-az555  Error: ErrImageNeverPull
2020-06-04T10:34:34.1071792Z 
2020-06-04T10:34:34.1072076Z 
2020-06-04T10:34:34.1073266Z Name:         flink-task-manager-74ccc9bd9-psqwm
2020-06-04T10:34:34.1073595Z Namespace:    default
2020-06-04T10:34:34.1073998Z Priority:     0
2020-06-04T10:34:34.1074519Z Node:         fv-az555/10.1.0.4
2020-06-04T10:34:34.1075030Z Start Time:   Thu, 04 Jun 2020 09:25:41 +0000
2020-06-04T10:34:34.1075349Z Labels:       app=flink
2020-06-04T10:34:34.1076024Z               component=task-manager
2020-06-04T10:34:34.1076729Z               pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1077193Z Annotations:  <none>
2020-06-04T10:34:34.1077470Z Status:       Pending
2020-06-04T10:34:34.1077878Z IP:           172.17.0.5
2020-06-04T10:34:34.1078133Z IPs:
2020-06-04T10:34:34.1078354Z   IP:           172.17.0.5
2020-06-04T10:34:34.1079106Z Controlled By:  ReplicaSet/flink-task-manager-74ccc9bd9
2020-06-04T10:34:34.1079575Z Containers:
2020-06-04T10:34:34.1080041Z   flink-task-manager:
2020-06-04T10:34:34.1080461Z     Container ID:  
2020-06-04T10:34:34.1080783Z     Image:         test_kubernetes_embedded_job
2020-06-04T10:34:34.1081216Z     Image ID:      
2020-06-04T10:34:34.1081488Z     Port:          <none>
2020-06-04T10:34:34.1081902Z     Host Port:     <none>
2020-06-04T10:34:34.1082159Z     Args:
2020-06-04T10:34:34.1082382Z       taskmanager
2020-06-04T10:34:34.1083054Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1083554Z     State:          Waiting
2020-06-04T10:34:34.1083851Z       Reason:       ErrImageNeverPull
2020-06-04T10:34:34.1084469Z     Ready:          False
2020-06-04T10:34:34.1084736Z     Restart Count:  0
2020-06-04T10:34:34.1085027Z     Environment:    <none>
2020-06-04T10:34:34.1085431Z     Mounts:
2020-06-04T10:34:34.1086215Z       /cache from cache-volume (rw)
2020-06-04T10:34:34.1086969Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1087858Z       /var/run/secrets/kubernetes.io/serviceaccount from default-token-lgfgh (ro)
2020-06-04T10:34:34.1088236Z Conditions:
2020-06-04T10:34:34.1088620Z   Type              Status
2020-06-04T10:34:34.1088896Z   Initialized       True 
2020-06-04T10:34:34.1089459Z   Ready             False 
2020-06-04T10:34:34.1089732Z   ContainersReady   False 
2020-06-04T10:34:34.1090529Z   PodScheduled      True 
2020-06-04T10:34:34.1090783Z Volumes:
2020-06-04T10:34:34.1091497Z   cache-volume:
2020-06-04T10:34:34.1092464Z     Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
2020-06-04T10:34:34.1092940Z     Medium:     
2020-06-04T10:34:34.1093155Z     SizeLimit:  <unset>
2020-06-04T10:34:34.1097800Z   job-artifacts-volume:
2020-06-04T10:34:34.1098334Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1099487Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1099959Z     HostPathType:  
2020-06-04T10:34:34.1100476Z   default-token-lgfgh:
2020-06-04T10:34:34.1100744Z     Type:        Secret (a volume populated by a Secret)
2020-06-04T10:34:34.1101341Z     SecretName:  default-token-lgfgh
2020-06-04T10:34:34.1101574Z     Optional:    false
2020-06-04T10:34:34.1101995Z QoS Class:       BestEffort
2020-06-04T10:34:34.1102342Z Node-Selectors:  <none>
2020-06-04T10:34:34.1102849Z Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
2020-06-04T10:34:34.1103161Z                  node.kubernetes.io/unreachable:NoExecute for 300s
2020-06-04T10:34:34.1103370Z Events:
2020-06-04T10:34:34.1103707Z   Type     Reason             Age                    From               Message
2020-06-04T10:34:34.1104581Z   ----     ------             ----                   ----               -------
2020-06-04T10:34:34.1106176Z   Warning  ErrImageNeverPull  18m (x233 over 68m)    kubelet, fv-az555  Container image ""test_kubernetes_embedded_job"" is not present with pull policy of Never
2020-06-04T10:34:34.1107412Z   Warning  Failed             3m44s (x301 over 68m)  kubelet, fv-az555  Error: ErrImageNeverPull
2020-06-04T10:34:34.1171266Z 
2020-06-04T10:34:34.1171483Z 
2020-06-04T10:34:34.1172145Z Name:                     flink-job-cluster
2020-06-04T10:34:34.1172399Z Namespace:                default
2020-06-04T10:34:34.1172649Z Labels:                   app=flink
2020-06-04T10:34:34.1173085Z                           component=job-cluster
2020-06-04T10:34:34.1173333Z Annotations:              <none>
2020-06-04T10:34:34.1173795Z Selector:                 app=flink,component=job-cluster
2020-06-04T10:34:34.1174063Z Type:                     NodePort
2020-06-04T10:34:34.1174311Z IP:                       10.111.92.199
2020-06-04T10:34:34.1174567Z Port:                     rpc  6123/TCP
2020-06-04T10:34:34.1174817Z TargetPort:               6123/TCP
2020-06-04T10:34:34.1175062Z NodePort:                 rpc  32501/TCP
2020-06-04T10:34:34.1175304Z Endpoints:                
2020-06-04T10:34:34.1175542Z Port:                     blob  6124/TCP
2020-06-04T10:34:34.1175781Z TargetPort:               6124/TCP
2020-06-04T10:34:34.1176034Z NodePort:                 blob  31360/TCP
2020-06-04T10:34:34.1176460Z Endpoints:                
2020-06-04T10:34:34.1176706Z Port:                     query  6125/TCP
2020-06-04T10:34:34.1176941Z TargetPort:               6125/TCP
2020-06-04T10:34:34.1177196Z NodePort:                 query  30025/TCP
2020-06-04T10:34:34.1177513Z Endpoints:                
2020-06-04T10:34:34.1177722Z Port:                     ui  8081/TCP
2020-06-04T10:34:34.1177939Z TargetPort:               8081/TCP
2020-06-04T10:34:34.1178144Z NodePort:                 ui  30081/TCP
2020-06-04T10:34:34.1178351Z Endpoints:                
2020-06-04T10:34:34.1178544Z Session Affinity:         None
2020-06-04T10:34:34.1178764Z External Traffic Policy:  Cluster
2020-06-04T10:34:34.1178965Z Events:                   <none>
2020-06-04T10:34:34.1226465Z 
2020-06-04T10:34:34.1226805Z 
2020-06-04T10:34:34.1227023Z Name:              kubernetes
2020-06-04T10:34:34.1227223Z Namespace:         default
2020-06-04T10:34:34.1227453Z Labels:            component=apiserver
2020-06-04T10:34:34.1227811Z                    provider=kubernetes
2020-06-04T10:34:34.1228028Z Annotations:       <none>
2020-06-04T10:34:34.1228306Z Selector:          <none>
2020-06-04T10:34:34.1228506Z Type:              ClusterIP
2020-06-04T10:34:34.1228703Z IP:                10.96.0.1
2020-06-04T10:34:34.1228892Z Port:              https  443/TCP
2020-06-04T10:34:34.1229100Z TargetPort:        8443/TCP
2020-06-04T10:34:34.1229301Z Endpoints:         10.1.0.4:8443
2020-06-04T10:34:34.1229513Z Session Affinity:  None
2020-06-04T10:34:34.1229693Z Events:            <none>
2020-06-04T10:34:34.1335732Z 
2020-06-04T10:34:34.1336572Z 
2020-06-04T10:34:34.1337318Z Name:                   flink-task-manager
2020-06-04T10:34:34.1337768Z Namespace:              default
2020-06-04T10:34:34.1338052Z CreationTimestamp:      Thu, 04 Jun 2020 09:25:41 +0000
2020-06-04T10:34:34.1338347Z Labels:                 <none>
2020-06-04T10:34:34.1338634Z Annotations:            deployment.kubernetes.io/revision: 1
2020-06-04T10:34:34.1339150Z Selector:               app=flink,component=task-manager
2020-06-04T10:34:34.1339510Z Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
2020-06-04T10:34:34.1339825Z StrategyType:           RollingUpdate
2020-06-04T10:34:34.1340066Z MinReadySeconds:        0
2020-06-04T10:34:34.1340333Z RollingUpdateStrategy:  25% max unavailable, 25% max surge
2020-06-04T10:34:34.1340588Z Pod Template:
2020-06-04T10:34:34.1340772Z   Labels:  app=flink
2020-06-04T10:34:34.1341151Z            component=task-manager
2020-06-04T10:34:34.1341367Z   Containers:
2020-06-04T10:34:34.1341778Z    flink-task-manager:
2020-06-04T10:34:34.1342030Z     Image:      test_kubernetes_embedded_job
2020-06-04T10:34:34.1342267Z     Port:       <none>
2020-06-04T10:34:34.1342478Z     Host Port:  <none>
2020-06-04T10:34:34.1342657Z     Args:
2020-06-04T10:34:34.1342835Z       taskmanager
2020-06-04T10:34:34.1343217Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1343470Z     Environment:  <none>
2020-06-04T10:34:34.1343659Z     Mounts:
2020-06-04T10:34:34.1344016Z       /cache from cache-volume (rw)
2020-06-04T10:34:34.1344451Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1344679Z   Volumes:
2020-06-04T10:34:34.1344997Z    cache-volume:
2020-06-04T10:34:34.1345459Z     Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
2020-06-04T10:34:34.1345752Z     Medium:     
2020-06-04T10:34:34.1345945Z     SizeLimit:  <unset>
2020-06-04T10:34:34.1346310Z    job-artifacts-volume:
2020-06-04T10:34:34.1346571Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1347294Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1347659Z     HostPathType:  
2020-06-04T10:34:34.1347841Z Conditions:
2020-06-04T10:34:34.1348044Z   Type           Status  Reason
2020-06-04T10:34:34.1348412Z   ----           ------  ------
2020-06-04T10:34:34.1348675Z   Available      False   MinimumReplicasUnavailable
2020-06-04T10:34:34.1348948Z   Progressing    False   ProgressDeadlineExceeded
2020-06-04T10:34:34.1349301Z OldReplicaSets:  <none>
2020-06-04T10:34:34.1349693Z NewReplicaSet:   flink-task-manager-74ccc9bd9 (1/1 replicas created)
2020-06-04T10:34:34.1349944Z Events:          <none>
2020-06-04T10:34:34.1493429Z 
2020-06-04T10:34:34.1493887Z 
2020-06-04T10:34:34.1494891Z Name:           flink-task-manager-74ccc9bd9
2020-06-04T10:34:34.1495330Z Namespace:      default
2020-06-04T10:34:34.1496049Z Selector:       app=flink,component=task-manager,pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1496748Z Labels:         app=flink
2020-06-04T10:34:34.1497158Z                 component=task-manager
2020-06-04T10:34:34.1497717Z                 pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1498300Z Annotations:    deployment.kubernetes.io/desired-replicas: 1
2020-06-04T10:34:34.1498904Z                 deployment.kubernetes.io/max-replicas: 2
2020-06-04T10:34:34.1499173Z                 deployment.kubernetes.io/revision: 1
2020-06-04T10:34:34.1499742Z Controlled By:  Deployment/flink-task-manager
2020-06-04T10:34:34.1499971Z Replicas:       1 current / 1 desired
2020-06-04T10:34:34.1500580Z Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
2020-06-04T10:34:34.1500850Z Pod Template:
2020-06-04T10:34:34.1501033Z   Labels:  app=flink
2020-06-04T10:34:34.1501607Z            component=task-manager
2020-06-04T10:34:34.1502188Z            pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1502416Z   Containers:
2020-06-04T10:34:34.1502957Z    flink-task-manager:
2020-06-04T10:34:34.1503226Z     Image:      test_kubernetes_embedded_job
2020-06-04T10:34:34.1503792Z     Port:       <none>
2020-06-04T10:34:34.1504011Z     Host Port:  <none>
2020-06-04T10:34:34.1504190Z     Args:
2020-06-04T10:34:34.1504536Z       taskmanager
2020-06-04T10:34:34.1505344Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1505611Z     Environment:  <none>
2020-06-04T10:34:34.1505831Z     Mounts:
2020-06-04T10:34:34.1506423Z       /cache from cache-volume (rw)
2020-06-04T10:34:34.1507112Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1507363Z   Volumes:
2020-06-04T10:34:34.1507951Z    cache-volume:
2020-06-04T10:34:34.1508616Z     Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
2020-06-04T10:34:34.1508939Z     Medium:     
2020-06-04T10:34:34.1509279Z     SizeLimit:  <unset>
2020-06-04T10:34:34.1509710Z    job-artifacts-volume:
2020-06-04T10:34:34.1510145Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1511303Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1511852Z     HostPathType:  
2020-06-04T10:34:34.1512076Z Events:            <none>
2020-06-04T10:34:34.1566048Z 
2020-06-04T10:34:34.1566248Z 
2020-06-04T10:34:34.1566883Z Name:           flink-job-cluster
2020-06-04T10:34:34.1568025Z Namespace:      default
2020-06-04T10:34:34.1568988Z Selector:       controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1569250Z Labels:         app=flink
2020-06-04T10:34:34.1569760Z                 component=job-cluster
2020-06-04T10:34:34.1570182Z                 controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1570589Z                 job-name=flink-job-cluster
2020-06-04T10:34:34.1570788Z Annotations:    <none>
2020-06-04T10:34:34.1570972Z Parallelism:    1
2020-06-04T10:34:34.1571149Z Completions:    1
2020-06-04T10:34:34.1571397Z Start Time:     Thu, 04 Jun 2020 09:25:40 +0000
2020-06-04T10:34:34.1571680Z Pods Statuses:  1 Running / 0 Succeeded / 0 Failed
2020-06-04T10:34:34.1571883Z Pod Template:
2020-06-04T10:34:34.1572056Z   Labels:  app=flink
2020-06-04T10:34:34.1572369Z            component=job-cluster
2020-06-04T10:34:34.1572776Z            controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1573155Z            job-name=flink-job-cluster
2020-06-04T10:34:34.1573337Z   Containers:
2020-06-04T10:34:34.1573634Z    flink-job-cluster:
2020-06-04T10:34:34.1573843Z     Image:       test_kubernetes_embedded_job
2020-06-04T10:34:34.1574101Z     Ports:       6123/TCP, 6124/TCP, 6125/TCP, 8081/TCP
2020-06-04T10:34:34.1574345Z     Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
2020-06-04T10:34:34.1574542Z     Args:
2020-06-04T10:34:34.1574806Z       standalone-job
2020-06-04T10:34:34.1575098Z       --job-classname
2020-06-04T10:34:34.1575306Z       org.apache.flink.examples.java.wordcount.WordCount
2020-06-04T10:34:34.1575704Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1576055Z       -Dparallelism.default=1
2020-06-04T10:34:34.1576805Z       -Dblob.server.port=6124
2020-06-04T10:34:34.1577160Z       -Dqueryable-state.server.ports=6125
2020-06-04T10:34:34.1577456Z       --output
2020-06-04T10:34:34.1577637Z       /cache/kubernetes_wc_out
2020-06-04T10:34:34.1577820Z     Environment:  <none>
2020-06-04T10:34:34.1578002Z     Mounts:
2020-06-04T10:34:34.1578343Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1578555Z   Volumes:
2020-06-04T10:34:34.1578840Z    job-artifacts-volume:
2020-06-04T10:34:34.1579081Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1579623Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1579918Z     HostPathType:  
2020-06-04T10:34:34.1580108Z Events:            <none>
2020-06-04T10:34:34.1598723Z Flink logs:
2020-06-04T10:34:34.3990017Z Error from server (BadRequest): container ""flink-job-cluster"" in pod ""flink-job-cluster-dtb67"" is waiting to start: ErrImageNeverPull
2020-06-04T10:34:34.4881295Z Error from server (BadRequest): container ""flink-task-manager"" in pod ""flink-task-manager-74ccc9bd9-psqwm"" is waiting to start: ErrImageNeverPull
2020-06-04T10:34:34.6105005Z job.batch ""flink-job-cluster"" deleted
2020-06-04T10:34:34.7547554Z service ""flink-job-cluster"" deleted
2020-06-04T10:34:34.9134745Z deployment.apps ""flink-task-manager"" deleted
2020-06-04T10:34:37.7996959Z pod/flink-job-cluster-dtb67 condition met
2020-06-04T10:34:37.9451396Z pod/flink-task-manager-74ccc9bd9-psqwm condition met
2020-06-04T10:34:37.9468470Z Stopping minikube ...
2020-06-04T10:34:38.0178327Z * Stopping ""minikube"" in none ...
2020-06-04T10:34:48.4722227Z * Node """" stopped.
2020-06-04T10:34:48.4767883Z [FAIL] Test script contains errors.
2020-06-04T10:34:48.4775183Z Checking for errors...
2020-06-04T10:34:48.4963953Z No errors in log files.
2020-06-04T10:34:48.4964817Z Checking for exceptions...
2020-06-04T10:34:48.5139288Z No exceptions in log files.
2020-06-04T10:34:48.5150249Z Checking for non-empty .out files...
2020-06-04T10:34:48.5157683Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-06-04T10:34:48.5158245Z No non-empty .out files.
2020-06-04T10:34:48.5158815Z 
2020-06-04T10:34:48.5159286Z [FAIL] 'Run Kubernetes test' failed after 70 minutes and 52 seconds! Test exited with exit code 1
{code}",,dian.fu,jark,rmetzger,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 30 06:39:34 UTC 2020,,,,,,,,,,"0|z0fi7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/20 11:35;chesnay;The docker image build failed, and it likely re-used the image from a previous run:
{code}
Step 4/14 : RUN set -ex;   wget -nv -O /usr/local/bin/gosu ""https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)"";   wget -nv -O /usr/local/bin/gosu.asc ""
 ---> Running in eba9fe332456
+ dpkg --print-architecture
+ wget -nv -O /usr/local/bin/gosu https://github.com/tianon/gosu/releases/download/1.11/gosu-amd64
2020-06-04 09:25:20 URL:https://github-production-release-asset-2e65be.s3.amazonaws.com/19708981/82e9dd00-d091-11e8-8734-a1caffcee352?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200604T092519Z&X-Amz-Expires=300&X-Amz-Signature=dd2292d73c781cad49cafc4064206ce385dca050762004be10d4ae5e1f5eaa9e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=19708981&response-content-disposition=attachment%3B%20filename%3Dgosu-amd64&response-content-type=application%2Foctet-stream [2294944/2294944] -> ""/usr/local/bin/gosu"" [1]
+ dpkg --print-architecture
+ wget -nv -O /usr/local/bin/gosu.asc https://github.com/tianon/gosu/releases/download/1.11/gosu-amd64.asc
GnuTLS: Error in the pull function.
Unable to establish SSL connection.
The command '/bin/sh -c set -ex;   wget -nv -O /usr/local/bin/gosu ""https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)"";   wget -nv -O /usr/local/bin/gosu.asc ""
~/work/1/s

{code};;;","04/Jun/20 11:38;rmetzger;Ah, good catch. 
The test should have a better error handling to fail if the docker image build failed?;;;","04/Jun/20 11:50;chesnay;Yes; it should be enough to check for errors in {{flink-end-to-end-tests/test-scripts/common_docker.sh:build_image}};;;","05/Jun/20 03:17;wangyang0918;I agree that we should {{exit}} when building the image failed.;;;","11/Jun/20 06:58;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3235&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5


{code}
Could not stop minikube. Aborting...
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.

[FAIL] 'Run Kubernetes test' failed after 0 minutes and 34 seconds! Test exited with exit code 1

cp: cannot stat '/home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/log/*': No such file or directory
Published e2e logs into debug logs artifact:
COMPRESSING build artifacts.
{code}
;;;","11/Jun/20 07:23;wangyang0918;[~jark] Could you rebase the latest master and try a again? Your failed pipeline is not caused by this issue. It is due to FLINK-18239.;;;","11/Jun/20 07:39;jark;Thanks for the information [~fly_in_gis];;;","25/Jul/20 04:11;dian.fu;Another instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4901&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994;;;","27/Jul/20 03:04;wangyang0918;[~dian.fu] I think your instance is caused by the port conflict. The specified node port ""30025"" is used by other processes. So it failed to create ""flink-job-cluster"" service. 

 
{code:java}
The Service ""flink-job-cluster"" is invalid: spec.ports[2].nodePort: Invalid value: 30025: provided port is already allocated

{code};;;","27/Jul/20 05:03;dian.fu;Thanks [~fly_in_gis], you are right. Have created a separate JIRA: https://issues.apache.org/jira/browse/FLINK-18725;;;","23/Oct/20 13:42;rmetzger;As proposed earlier, I'll introduce a retry and a proper exit for the image building.;;;","28/Oct/20 08:13;rmetzger;Improved test stability in https://github.com/apache/flink/commit/d47433a9384b109cba4d83ab4cf4b9a3457de885;;;","28/Oct/20 10:37;rmetzger;Reverted change in 9dd04a25bd300a725486ff08560920f548f3b1d9 due to test failure;;;","30/Oct/20 06:39;rmetzger;Merged again https://github.com/apache/flink/commit/662cec6391bf7c8f1087b6bb48365209afc560af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unlimitedly growing state for time range bounded over aggregate,FLINK-18119,13309391,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hyeonseop,hyeonseop,hyeonseop,04/Jun/20 08:31,20/Jun/20 01:55,13/Jul/23 08:12,20/Jun/20 01:55,1.10.1,,,,,1.11.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"For time range bounded over aggregation in streaming query, like below,
{code:java}
table
  .window(Over.partitionBy 'a orderBy 'rowtime preceding 1.hour as 'w)
  .groupBy('w)
  .select('a, aggregateFunction('b))
{code}
the operator must hold incoming records over the preceding time range in the state, but older records are no longer required and can be cleaned up.

Current implementation retracts the old records only when newer records come in and so the operator knows that enough time has passed. However, the retraction never happens unless a new record with the same key comes in and this causes a state that perhaps will never be released, which leads to an unlimitedly growing state especially when the keyspace mutates over time.

Since aggregate over bounded preceding time interval doesn't require old records by its nature, we can improve this by adding a timer that notifies the operator to retract old records, resulting in no changes in query result or severe performance degrade.

This is a distinct feature from state retention: state retention is to forget some states that are expected to be less important to reduce state memory, so it possibly changes query results. Enabling and disabling state retention both make sense with this change.

This issue applies to both row time range bound and proc time range bound. That is, we are going to have changes in both RowTimeRangeBoundedPrecedingFunction and ProcTimeRangeBoundedPrecedingFunction in flink-table-runtime-blink. I already have a running-in-production version with this change and would be glad to contribute.",,dian.fu,hyeonseop,jark,klion26,kyledong,libenchao,liyu,Paul Lin,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 20 01:55:22 UTC 2020,,,,,,,,,,"0|z0fi08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 06:25;libenchao;[~hyeonseop] Which planner are you using? I've checked {{RowTimeRangeUnboundedPrecedingFunction}} in blink planner in 1.11, it's implemented using processing timers. Hence it does not have the problems you described.;;;","16/Jun/20 06:48;hyeonseop;[~libenchao] {{RowTimeRange*Unbounded*PrecedingFunction}} is not the case. {{RowTimeRange*Bounded*PrecedingFunction}} in blink runtime 1.11 still has the issue.

It performs cleanup using processing timer when proper {{minRetentionTime}} and {{maxRetentionTime}} are configured, but what I want to improve is to retract records that is no longer required even when the state retention is not set (indefinite).

In my case, I first tried to set non-zero {{minRetentionTime}} to enable cleanup by retention, but that was applied to whole query and ended up with the retract stream instead of append stream. I understand setting state retention can be a walkaround to prevent OOM but I think functions must keep state as efficiently as possible.;;;","16/Jun/20 07:23;libenchao;[~hyeonseop] I got your point, nice catch, IMO this is a bug. Assigned to you.;;;","16/Jun/20 07:28;hyeonseop;Thank you! Will submit PR.;;;","19/Jun/20 07:23;jark;As discussed in the pull request, we changed the State ttl behavior for {{ProcTimeRangeBoundedPrecedingFunction}} and {{RowTimeRowsBoundedPrecedingFunction}}. They expire state automantically when no more data coming in in the bounded window (event/processing) time range, instead of expire depends on the {{TableConfig.setIdleStateRetentionTime}}. ;;;","20/Jun/20 01:55;jark;- master (1.12.0): 22abfe2461cf9814268f1e37e62349bc223c4a0b
- 1.11.0: 2cfaff87427e34e93160c56bb85b1fd8e68fb964;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some SQL Jobs with two input operators are loosing data with unaligned checkpoints,FLINK-18118,13309375,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,jark,jark,04/Jun/20 07:34,22/Jun/21 14:05,13/Jul/23 08:12,08/Jun/20 10:21,1.11.0,,,,,1.11.0,,,Runtime / Checkpointing,Table SQL / Runtime,,,,0,,,,,,"After trying to enable unaligned checkpoints by default, a lot of Blink streaming SQL/Table API tests containing joins or set operations are throwing errors that are indicating we are loosing some data (full records, without deserialisation errors). Example errors:

{noformat}
[ERROR] Failures: 
[ERROR]   JoinITCase.testFullJoinWithEqualPk:775 expected:<List(1,1, 2,2, 3,3, null,4, null,5)> but was:<List(2,2, 3,3, null,1, null,4, null,5)>
[ERROR]   JoinITCase.testStreamJoinWithSameRecord:391 expected:<List(1,1,1,1, 1,1,1,1, 2,2,2,2, 2,2,2,2, 3,3,3,3, 3,3,3,3, 4,4,4,4, 4,4,4,4, 5,5,5,5, 5,5,5,5)> but was:<List()>
[ERROR]   SemiAntiJoinStreamITCase.testAntiJoin:352 expected:<0> but was:<1>
[ERROR]   SetOperatorsITCase.testIntersect:55 expected:<MutableList(1,1,Hi, 2,2,Hello, 3,2,Hello world)> but was:<List()>
[ERROR]   JoinITCase.testJoinPushThroughJoin:1272 expected:<List(1,0,Hi, 2,1,Hello, 2,1,Hello world)> but was:<List(2,1,Hello, 2,1,Hello world)>
{noformat}
 ",,jark,kevin.cyj,klion26,libenchao,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17918,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 10:21:30 UTC 2020,,,,,,,,,,"0|z0fhwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 10:21;zjwang;After syncing with [~AHeise], all the blocker issues for two inputs in UC were already resolved in other separate tickets, so I will close this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Kerberized YARN per-job on Docker test"" fails with ""Could not start hadoop cluster.""",FLINK-18117,13309362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,04/Jun/20 06:48,09/Nov/20 15:49,13/Jul/23 08:12,05/Nov/20 08:12,1.11.0,1.12.0,,,,1.12.0,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2683&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-06-04T06:03:53.2844296Z Creating slave1 ... [32mdone[0m
2020-06-04T06:03:53.4981251Z [1BWaiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...
2020-06-04T06:03:58.5980181Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...
2020-06-04T06:04:03.6997087Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...
2020-06-04T06:04:08.7910791Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...
2020-06-04T06:04:13.8921621Z Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...
2020-06-04T06:04:18.9648844Z Waiting for hadoop cluster to come up. We have been trying for 25 seconds, retrying ...
2020-06-04T06:04:24.0381851Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...
2020-06-04T06:04:29.1220264Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...
2020-06-04T06:04:34.1882187Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...
2020-06-04T06:04:39.2784948Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...
2020-06-04T06:04:44.3843337Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...
2020-06-04T06:04:49.4703561Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...
2020-06-04T06:04:54.5463207Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...
2020-06-04T06:04:59.6650405Z Waiting for hadoop cluster to come up. We have been trying for 66 seconds, retrying ...
2020-06-04T06:05:04.7500168Z Waiting for hadoop cluster to come up. We have been trying for 71 seconds, retrying ...
2020-06-04T06:05:09.8177904Z Waiting for hadoop cluster to come up. We have been trying for 76 seconds, retrying ...
2020-06-04T06:05:14.9751297Z Waiting for hadoop cluster to come up. We have been trying for 81 seconds, retrying ...
2020-06-04T06:05:20.0336417Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...
2020-06-04T06:05:25.1627704Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...
2020-06-04T06:05:30.2583315Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...
2020-06-04T06:05:35.3283678Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...
2020-06-04T06:05:40.4184029Z Waiting for hadoop cluster to come up. We have been trying for 107 seconds, retrying ...
2020-06-04T06:05:45.5388372Z Waiting for hadoop cluster to come up. We have been trying for 112 seconds, retrying ...
2020-06-04T06:05:50.6155334Z Waiting for hadoop cluster to come up. We have been trying for 117 seconds, retrying ...
2020-06-04T06:05:55.7225186Z Command: start_hadoop_cluster failed. Retrying...
2020-06-04T06:05:55.7237999Z Starting Hadoop cluster
2020-06-04T06:05:56.5188293Z kdc is up-to-date
2020-06-04T06:05:56.5292716Z master is up-to-date
2020-06-04T06:05:56.5301735Z slave2 is up-to-date
2020-06-04T06:05:56.5306179Z slave1 is up-to-date
2020-06-04T06:05:56.6800566Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...
2020-06-04T06:06:01.7668291Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...
2020-06-04T06:06:06.8620265Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...
2020-06-04T06:06:11.9753596Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...
2020-06-04T06:06:17.0402846Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...
2020-06-04T06:06:22.1650005Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...
2020-06-04T06:06:27.2500179Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...
2020-06-04T06:06:32.3133809Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...
2020-06-04T06:06:37.4432923Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...
2020-06-04T06:06:42.5658250Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...
2020-06-04T06:06:47.6682536Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...
2020-06-04T06:06:52.7810371Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...
2020-06-04T06:06:57.8860269Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...
2020-06-04T06:07:03.0337979Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...
2020-06-04T06:07:08.1080310Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...
2020-06-04T06:07:13.2297578Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...
2020-06-04T06:07:18.3779034Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...
2020-06-04T06:07:23.4789495Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...
2020-06-04T06:07:28.6063062Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...
2020-06-04T06:07:33.8220409Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...
2020-06-04T06:07:38.9439231Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...
2020-06-04T06:07:44.0193849Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...
2020-06-04T06:07:49.1241642Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...
2020-06-04T06:07:54.2425087Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...
2020-06-04T06:07:59.3835321Z Command: start_hadoop_cluster failed. Retrying...
2020-06-04T06:07:59.3847275Z Starting Hadoop cluster
2020-06-04T06:08:00.1959109Z kdc is up-to-date
2020-06-04T06:08:00.1968717Z master is up-to-date
2020-06-04T06:08:00.1982811Z slave1 is up-to-date
2020-06-04T06:08:00.1988143Z slave2 is up-to-date
2020-06-04T06:08:00.4014781Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...
2020-06-04T06:08:05.5168483Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...
2020-06-04T06:08:10.6759355Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...
2020-06-04T06:08:15.8307550Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...
2020-06-04T06:08:21.0143341Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...
2020-06-04T06:08:26.0932297Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...
2020-06-04T06:08:31.2526775Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...
2020-06-04T06:08:36.4356124Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...
2020-06-04T06:08:41.5607530Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...
2020-06-04T06:08:46.6407963Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...
2020-06-04T06:08:51.8464789Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...
2020-06-04T06:08:56.9735817Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...
2020-06-04T06:09:02.1023842Z Waiting for hadoop cluster to come up. We have been trying for 62 seconds, retrying ...
2020-06-04T06:09:07.2390427Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...
2020-06-04T06:09:12.4433329Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...
2020-06-04T06:09:17.5390800Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...
2020-06-04T06:09:22.7020537Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...
2020-06-04T06:09:27.8754909Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...
2020-06-04T06:09:33.0447274Z Waiting for hadoop cluster to come up. We have been trying for 93 seconds, retrying ...
2020-06-04T06:09:38.1804596Z Waiting for hadoop cluster to come up. We have been trying for 98 seconds, retrying ...
2020-06-04T06:09:43.3636590Z Waiting for hadoop cluster to come up. We have been trying for 103 seconds, retrying ...
2020-06-04T06:09:48.4975410Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...
2020-06-04T06:09:53.6117328Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...
2020-06-04T06:09:58.7785946Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...
2020-06-04T06:10:03.9748663Z Command: start_hadoop_cluster failed. Retrying...
2020-06-04T06:10:03.9808244Z Command: start_hadoop_cluster failed 3 times.
2020-06-04T06:10:03.9823071Z ERROR: Could not start hadoop cluster. Aborting...
{code}

Frequent, suspicious logs
{code}
2020-06-04T06:10:04.5032658Z 20/06/04 06:05:42 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.19.0.3:9000: try once and fail.
2020-06-04T06:10:04.5033211Z java.net.ConnectException: Connection refused

...

2020-06-04T06:10:04.6867876Z 20/06/04 06:04:11 ERROR namenode.NameNode: Failed to start namenode.
2020-06-04T06:10:04.6868640Z java.net.BindException: Port in use: 0.0.0.0:50470
2020-06-04T06:10:04.6869062Z 	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:998)
2020-06-04T06:10:04.6869702Z 	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:935)
2020-06-04T06:10:04.6870199Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:171)
2020-06-04T06:10:04.6870740Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:842)
2020-06-04T06:10:04.6871235Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:693)
2020-06-04T06:10:04.6871728Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:906)
2020-06-04T06:10:04.6872202Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:885)
2020-06-04T06:10:04.6872699Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)
2020-06-04T06:10:04.6873701Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)
2020-06-04T06:10:04.6874100Z Caused by: java.net.BindException: Address already in use
2020-06-04T06:10:04.6901805Z 	at sun.nio.ch.Net.bind0(Native Method)
2020-06-04T06:10:04.6902168Z 	at sun.nio.ch.Net.bind(Net.java:433)
2020-06-04T06:10:04.6902478Z 	at sun.nio.ch.Net.bind(Net.java:425)
2020-06-04T06:10:04.6902847Z 	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
2020-06-04T06:10:04.6903296Z 	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
2020-06-04T06:10:04.6903744Z 	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
2020-06-04T06:10:04.6904395Z 	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:993)
2020-06-04T06:10:04.6904727Z 	... 8 more
2020-06-04T06:10:04.6905005Z 20/06/04 06:04:11 INFO util.ExitUtil: Exiting with status 1
2020-06-04T06:10:04.6905401Z 20/06/04 06:04:11 INFO namenode.NameNode: SHUTDOWN_MSG: 

{code}",,dian.fu,rmetzger,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19598,,,,,,,FLINK-17910,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 08:12:57 UTC 2020,,,,,,,,,,"0|z0fhts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/20 06:53;trohrmann;[~rmetzger] do we know what process was occupying {{0.0.0.0:50470}}?;;;","04/Jun/20 07:05;rmetzger;No, and the NameNode is running in a Docker container, there should not be any process around allocating that port;;;","09/Jun/20 16:36;rmetzger;Another case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3055&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","30/Jul/20 11:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5037&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","23/Aug/20 01:38;dian.fu;Instance on master: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5788&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6]

{code}
2020-08-22T21:08:33.7340285Z java.net.BindException: Port in use: 0.0.0.0:50470
2020-08-22T21:08:33.7340598Z 	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:998)
2020-08-22T21:08:33.7340920Z 	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:935)
2020-08-22T21:08:33.7341285Z 	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:171)
2020-08-22T21:08:33.7341682Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:842)
2020-08-22T21:08:33.7342045Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:693)
2020-08-22T21:08:33.7342409Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:906)
2020-08-22T21:08:33.7342761Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:885)
2020-08-22T21:08:33.7343182Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)
2020-08-22T21:08:33.7343549Z 	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)
2020-08-22T21:08:33.7343871Z Caused by: java.net.BindException: Address already in use
2020-08-22T21:08:33.7344106Z 	at sun.nio.ch.Net.bind0(Native Method)
2020-08-22T21:08:33.7344330Z 	at sun.nio.ch.Net.bind(Net.java:433)
2020-08-22T21:08:33.7344564Z 	at sun.nio.ch.Net.bind(Net.java:425)
2020-08-22T21:08:33.7344842Z 	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:220)
2020-08-22T21:08:33.7345186Z 	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)
2020-08-22T21:08:33.7345543Z 	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
2020-08-22T21:08:33.7345897Z 	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:993)
{code};;;","26/Aug/20 16:49;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5886&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","07/Sep/20 03:01;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6256&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6;;;","10/Sep/20 01:17;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6395&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","07/Oct/20 08:18;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7256&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","08/Oct/20 06:55;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8451&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d;;;","11/Oct/20 04:26;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7385&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=d47e27f5-9721-5d5f-1cf3-62adbf3d115d;;;","22/Oct/20 01:30;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8046&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873;;;","23/Oct/20 13:19;rmetzger;Added debugging code: https://github.com/apache/flink/commit/067f60ff01d76eece394a7779f5180251af2f17d;;;","26/Oct/20 18:15;rmetzger;Closing ticket until next failure.;;;","27/Oct/20 15:26;rmetzger;There's another case of this on the release-1.11 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8397&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=2b7514ee-e706-5046-657b-3430666e7bd9 ..it doesn't have the debug code :(
Unless this is going to be a frequent instability on release-1.11, I would like to refrain from adding temporary debugging code to the release-1.11 branch.;;;","02/Nov/20 15:32;rmetzger;Now we have a failure on master, from the {{PyFlink end-to-end test}} https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8780&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;","05/Nov/20 08:12;rmetzger;Resolved in https://github.com/apache/flink/commit/358713ba5b1f29fb9befa72c47864d1e49d5e30e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bucket Listener in StreamingFileSink should notify for buckets detected to be inactive at recovery,FLINK-18110,13309327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,04/Jun/20 03:01,08/Jun/20 11:01,13/Jul/23 08:12,08/Jun/20 02:21,1.11.0,,,,,1.11.0,,,Connectors / FileSystem,Connectors / Hive,,,,0,pull-request-available,,,,,"Current streaming file sink using bucket lifecycle listener to support the global commit of  Hive sink, and the listener sends notification when buckets are created and get inactive. However, after failover,  a bucket may be directly detected to be inactive and deserted directly, in this case the listener should also send the inactive notification.",,gaoyunhaii,lzljs3620320,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 02:21:58 UTC 2020,,,,,,,,,,"0|z0fhm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 02:21;lzljs3620320;master: 0a1b0c615c4968afd29f09a3494bbf137a223609

release-1.11: 26d7a124a52758305478dc71af5f8746c36a34cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server doesn't clean all job json files,FLINK-18097,13309264,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,DraCzech,DraCzech,03/Jun/20 20:07,26/Jun/21 20:23,13/Jul/23 08:12,08/Jul/20 19:11,1.10.1,,,,,1.10.2,1.11.1,1.12.0,Runtime / REST,,,,,0,pull-request-available,,,,,"Improvement introduced in https://issues.apache.org/jira/browse/FLINK-14169 does not delete all files in the history server folders completely.

There is a [json file created for each job|https://github.com/apache/flink/blob/master/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/history/HistoryServerArchiveFetcher.java#L237-L238] in history server's {{webDir/jobs/}} directory. Such file is not deleted by {{cleanupExpiredJobs}}.

And while the cleaned up job is no longer visible in History server's {{Completed Jobs List}} in web UI, it can be still accessed on {{<HistoryServerURL>/#/job/<jobId>/overview}}.

While this bug probably won't lead to any serious issues, files in history server's folders should be cleaned up thoroughly.",,DraCzech,HOBITOTO88 Togel Online Terbaik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 19:11:37 UTC 2020,,,,,,,,,,"0|z0fh80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/20 19:11;chesnay;master: 78d6ee1cef6d9c0d5d28e4e228f7050de1c6c7a2
1.11: d64a5a0c2566416158c67719dc08d7623e14fad8
1.10: 0e7278929bcf08e4e51f791a34c0220709c5e6ad ;;;","26/Jun/21 20:23;HOBITOTO88 Togel Online Terbaik;yes;;;jira-users",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index offset not consistent in CheckpointedInputGate for unaligned checkpoints.,FLINK-18094,13309244,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,arvid,AHeise,03/Jun/20 17:52,22/Jun/21 14:07,13/Jul/23 08:12,18/Jun/20 09:25,1.11.0,1.12.0,,,,1.11.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,{{org.apache.flink.streaming.runtime.io.InputProcessorUtil#createCheckpointedMultipleInputGate}} uses a different way to provide the flattened indices for {{CheckpointedInputGate}} and {{CheckpointBarrierUnaligner}}.,,AHeise,klion26,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 09:25:57 UTC 2020,,,,,,,,,,"0|z0fh3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 13:42;pnowojski;Merged to master as 03beab8ac7..1bacaeeef1. TBD decided whether it will be part of 1.11.0 or not.;;;","18/Jun/20 09:25;pnowojski;Merged to release-1.11 as 75c0b5b1f4..1d211b93c4

(fix version might need to be changed to 1.11.1 if RC2 is accepted);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uploading user artifact for Yarn job cluster could not work,FLINK-18087,13309162,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,03/Jun/20 10:41,04/Jun/20 09:41,13/Jul/23 08:12,04/Jun/20 09:41,1.11.0,1.12.0,,,,1.11.0,1.12.0,,Deployment / YARN,,,,,0,pull-request-available,,,,,"In FLINK-17632, we add the support remote user jar. However, uploading user artifact for Yarn job cluster is broken exceptionally. In the following code, we should only upload local files. Now it has the contrary behavior.
{code:java}
// only upload local files
if (Utils.isRemotePath(entry.getValue().filePath)) {
   Path localPath = new Path(entry.getValue().filePath);
   Tuple2<Path, Long> remoteFileInfo =
         fileUploader.uploadLocalFileToRemote(localPath, entry.getKey());
   jobGraph.setUserArtifactRemotePath(entry.getKey(), remoteFileInfo.f0.toString());
}
{code}
 

Another problem is the related tests {{testPerJobModeWithDistributedCache}} does not fail because we do not fetch the artifact from remote filesystem(i.e. HDFS). We directly get it from local file. It also needs to be enhanced.",,kkl0u,klion26,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17632,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 09:41:33 UTC 2020,,,,,,,,,,"0|z0fglc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/20 09:41;kkl0u;Fixed on master with d56454a3a0abe9ba34d1fec27864863bbd8a7c5f
and on release-1.11 with f394995281f888813c888303025735adb56cc952;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve exception message of TIMESTAMP/TIME  out of the HBase connector supported precision,FLINK-18083,13309118,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,03/Jun/20 07:01,16/Jun/20 09:21,13/Jul/23 08:12,16/Jun/20 09:21,1.11.0,,,,,1.11.0,,,Connectors / HBase,,,,,0,pull-request-available,,,,,,,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 09:21:27 UTC 2020,,,,,,,,,,"0|z0fgbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/20 09:21;jark;- master (1.12.0): c2a6b818dab4ae272f1ffb29fc958886ef5ac188
- 1.11.0: 2ab524b6531bdf15249e3815099ef68d357d3685;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix broken links in ""Kerberos Authentication Setup and Configuration"" doc",FLINK-18081,13309105,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,03/Jun/20 05:48,30/Nov/21 20:38,13/Jul/23 08:12,17/Aug/20 06:02,1.10.1,1.11.0,1.12.0,,,1.10.3,1.11.2,1.12.0,Documentation,Runtime / Configuration,,,,0,pull-request-available,,,,,The {{config.html#kerberos-based-security}} is not valid now.,,guoyangze,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 06:02:47 UTC 2020,,,,,,,,,,"0|z0fg8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 05:48;guoyangze;Could someone assign this to me?;;;","16/Aug/20 14:11;zhuzh;[~guoyangze] do you still want to work on this?;;;","17/Aug/20 01:46;guoyangze;Yes, please assign this to me. Thx.;;;","17/Aug/20 02:48;zhuzh;I have assigned the ticket to you [~guoyangze];;;","17/Aug/20 06:02;zhuzh;Fixed via
master: e56e7e40f15a86909ecfbbb1fd4a720f94621107
release-1.11: de95e4fa3dcf116d26d9275100d2003a183b4437
release-1.10: a568d93840db375a437b53ee1456badfd251d799;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql client uses wrong class loader when parsing queries,FLINK-18076,13309048,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,dwysakowicz,dwysakowicz,02/Jun/20 21:09,05/Jun/20 12:18,13/Jul/23 08:12,05/Jun/20 12:18,1.11.0,1.12.0,,,,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"Sql-client when parsing queries does not use the user class loader from {{ExecutionContext}}. This makes it impossible to query any sources if the dependencies are added with {{-j}} flag.

In order to reproduce it try querying e.g. KafkaDynamicSource with
{code}
CREATE TABLE MyUserTable (
   f0 BIGINT
) WITH (
  'connector' = 'kafka',       

  'topic' = 'topic_name', -- required: topic name from which the table is read

 -- required: specify the Kafka server connection string
  'properties.bootstrap.servers' = 'localhost:9092',
  -- required for Kafka source, optional for Kafka sink, specify consumer group
  'properties.group.id' = 'testGroup',
  -- optional: valid modes are ""earliest-offset"", ""latest-offset"", ""group-offsets"", ""specific-offsets"" or ""timestamp""
'scan.startup.mode' = 'earliest-offset',

   'format' = 'avro'
);


SELECT * FROM MyUserTable;
{code}

It give exception:

{code}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement.
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:95)
	at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:79)
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:256)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:212)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)
Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.MyUserTable'.

Table options are:

'connector'='kafka'
'format'='avro'
'properties.bootstrap.servers'='localhost:9092'
'properties.group.id'='testGroup'
'scan.startup.mode'='earliest-offset'
'topic'='topic_name'
	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:90)
	... 6 more
Caused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option ''connector'='kafka''.
	at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)
	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118)
	... 23 more
Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the classpath.

Available factory identifiers are:

datagen
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)
	at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)
	... 24 more

Shutting down the session...
done.
{code}

Because the factories are present only in the user classloader.",,apche-Px,dwysakowicz,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 12:18:10 UTC 2020,,,,,,,,,,"0|z0ffw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 02:55;jark;From the recently bug issues, it seems that it's urgent to add end-to-end tests for the new connectors and new DDLs. ;;;","04/Jun/20 07:53;dwysakowicz;I will review the issue. [~Leonard Xu] could you move the issue to in progress?;;;","04/Jun/20 08:07;leonard;[~dwysakowicz] sure, thanks.;;;","05/Jun/20 01:06;apche-Px;Yes  ! Urgent requirements

 ;;;","05/Jun/20 12:18;dwysakowicz;Fixed in:
master: 61e6f70dba3e724c479b8cd7753b314ebb1d5517
1.11: e7902bb4d1329833870ee53c782c6431cfc8cb80;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector does not call open method of (de)serialization schema,FLINK-18075,13309025,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,sjwiesman,sjwiesman,02/Jun/20 19:39,09/Jun/20 16:07,13/Jul/23 08:12,08/Jun/20 14:04,1.11.0,1.12.0,,,,1.11.0,,,Connectors / Kafka,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,,"The Kafka consumer and producer do not  call the open methods of plain (De)SerializationSchema interfaces. Only the Keyed and Kafka specific interfaces. The updated SQL implementations such as AvroRowDataSeriailzationSchema use these methods and so SQL queries using avro and kafka will fail in a null pointer exception. 

cc [~aljoscha]",,dwysakowicz,jark,leonard,libenchao,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17940,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 14:04:48 UTC 2020,,,,,,,,,,"0|z0ffqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 13:37;jark;- master (1.12.0): bc16d526b36a37915d01005f849e7c7faf805ee1
- 1.11.0: 2bdc6ec563705f220e14ca13d1f7e2a1bb47b444;;;","03/Jun/20 13:38;dwysakowicz;It's not finished;;;","08/Jun/20 14:04;dwysakowicz;* master:  bebe50346d9485fa2d962c5ed1d00da2778c8feb 74fd2912d146cc2db103825f0aee2282a99b5774
* 1.11: 8d9e3a034963f00f657a0f6dc33b660c663f391f 663e9f3f0e903c9e20a9e786dd56364f6482b8f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Confirm checkpoint completed on task side would not fail the task if exception thrown out,FLINK-18074,13308996,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,yunta,yunta,02/Jun/20 16:59,05/Jun/20 05:09,13/Jul/23 08:12,05/Jun/20 05:07,1.11.0,,,,,1.11.0,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,,"FLINK-17350 let the task fail immediately once sync phase of checkpoint failed. However, the included commit ['Simplify checkpoint exception handling'|https://github.com/apache/flink/pull/12101/commits/a2cd3daceca16ae841119d94a24328b4af37dcd8] actually would not fail the task if the runnable of {{() -> notifyCheckpointComplete}} throwing exception out.
In a nutshell, this actually changes previous checkpoint exception handling.

Moreover, that part of code also affect the implemented code of {{notifyCheckpointAbortAsync}} when I introduce {{notifyCheckpointAborted}} on task side. ",,klion26,liyu,pnowojski,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 05:07:41 UTC 2020,,,,,,,,,,"0|z0ffkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 17:03;yunta;[~pnowojski] What do you think of this problem?
I could take this ticket as I also need to change the exception handling part of code for {{notifyCheckpointAborted}} on task side.;;;","03/Jun/20 08:36;pnowojski;Yes, I think you are right. I've missed that {{notifyCheckpointCompleteAsync}} is also using {{submit}} (clearly we are missing a test coverage for that).

I've assigned the ticket to you.;;;","05/Jun/20 05:07;pnowojski;merged commit e27c517 into apache:master and 29f9918046 to release-1.11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroRowDataSerializationSchema is not always serializable,FLINK-18073,13308994,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,sjwiesman,sjwiesman,02/Jun/20 16:58,08/Jun/20 02:14,13/Jul/23 08:12,08/Jun/20 02:14,,,,,,1.11.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,pull-request-available,,,,,"AvroRowDeserializationSchema member runtimeConverter may hold a reference to non serializable avro schema fields. This value should be transient and generated inside of open. 


{code:java}
Caused by: java.io.NotSerializableException: org.apache.avro.Schema$RecordSchema
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) ~[?:1.8.0_252]
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) ~[?:1.8.0_252]
	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:586) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:133) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:126) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:71) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.<init>(FlinkKafkaProducer.java:632) ~[flink-sql-connector-kafka_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.<init>(FlinkKafkaProducer.java:522) ~[flink-sql-connector-kafka_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.<init>(FlinkKafkaProducer.java:483) ~[flink-sql-connector-kafka_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.createKafkaProducer(KafkaDynamicSink.java:60) ~[flink-sql-connector-kafka_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSinkBase.getSinkRuntimeProvider(KafkaDynamicSinkBase.java:84) ~[flink-sql-connector-kafka_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalSink.createSinkTransformation(CommonPhysicalSink.scala:69) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:108) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:43) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:43) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:67) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:162) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1217) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:663) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:750) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:653) ~[flink-table-blink_2.12-1.11-stream.jar:1.11-stream]
	at com.ververica.StreamingJob.main(StreamingJob.java:50) ~[?:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230) ~[flink-dist_2.12-1.11-stream.jar:1.11-stream]


{code}
",,jark,libenchao,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 02:14:58 UTC 2020,,,,,,,,,,"0|z0ffk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 16:58;sjwiesman;cc [~twalthr] [~jark];;;","02/Jun/20 17:18;sjwiesman;I did a cursory search of the serialization schema and I believe the same change will need to be made there. ;;;","02/Jun/20 19:39;sjwiesman;Update, I can confirm the issue also exists in the serialization schema. ;;;","03/Jun/20 16:52;jark;Hi [~sjwiesman], what do you mean ""the issue also exists in the serialization schema""? I guess the title and description already mentioned this issue is about avro serialization schema. Do you mean it also exists in deserialization schema or {{AvroRowSerializationSchema}}?;;;","03/Jun/20 17:14;sjwiesman;[~jark] Yes, I meant AvroRowDeserializationSchema. Thank you for catching that. ;;;","03/Jun/20 17:47;jark;[~sjwiesman], do you mean {{AvroRowDataDeserializationSchema}}? The {{AvroRowDeserializationSchema}} is the legacy format implementation. 

Btw, AvroRowDataDeserializationSchema works well in my added IT case in the PR. ;;;","03/Jun/20 17:52;sjwiesman;[~jark] You are correct, I stumbled on this and FLINK-18075 at the same time. I confused them in my head. Aplogies for the misunderstanding. ;;;","08/Jun/20 02:14;jark;- master (1.12.0): 8f858f398de3c47fe5045fbe0f1497022bfb1c15
- 1.11.0: 2dc5e0808556c35b0818a1626e8d57f4dde12f45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseLookupFunction can not work with new internal data structure RowData,FLINK-18072,13308981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,02/Jun/20 15:34,17/Jun/20 13:14,13/Jul/23 08:12,17/Jun/20 13:14,1.11.0,,,,,1.11.0,,,Connectors / HBase,,,,,0,pull-request-available,,,,," 
{code:java}
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.IllegalArgumentException: unsupported type index:-1
        at org.apache.flink.connector.hbase.util.HBaseTypeUtils.deserializeToObject(HBaseTypeUtils.java:80)
        at org.apache.flink.connector.hbase.util.HBaseReadWriteHelper.parseToRow(HBaseReadWriteHelper.java:161)
        at org.apache.flink.connector.hbase.source.HBaseLookupFunction.eval(HBaseLookupFunction.java:80)
        at LookupFunction$315.flatMap(Unknown Source)
        at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:82)
        at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:36)
        at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
{code}",,jark,leonard,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17025,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 13:14:50 UTC 2020,,,,,,,,,,"0|z0ffh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/20 06:02;jark;- master (1.12.0): 03a8ef63b47d2725f10f36ea0e6d6f13197cd8cc
- 1.11.0: b2711c5d7d197735e2cbedf4e675273ef7e9a3bb;;;","17/Jun/20 06:57;rmetzger;Reverted change in https://github.com/apache/flink/commit/8192e1bcc2f4fb5917c041a454bf9e20eda5b83e (only for release-1.11 branch);;;","17/Jun/20 07:06;jark;Sorry. Don't know that the HBase connector code is different between 1.11 and master.;;;","17/Jun/20 07:12;leonard;Thanks [~rmetzger] and [~jark],

I'll open a new PR for 1.11 branch;;;","17/Jun/20 13:14;jark;Fixed in 1.11.0: 9e20929dbda43959950509e2f4534c285bf48f3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatorEventsExactlyOnceITCase.checkListContainsSequence fails on CI,FLINK-18071,13308969,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,rmetzger,rmetzger,02/Jun/20 14:45,28/May/21 09:08,13/Jul/23 08:12,16/Apr/21 15:05,1.11.0,,,,,1.12.3,1.13.0,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,,"CI: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=330&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374

{code}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.795 s <<< FAILURE! - in org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase
[ERROR] test(org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase)  Time elapsed: 4.647 s  <<< FAILURE!
java.lang.AssertionError: List did not contain expected sequence of 200 elements, but was: [152, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.failList(CoordinatorEventsExactlyOnceITCase.java:160)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.checkListContainsSequence(CoordinatorEventsExactlyOnceITCase.java:148)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:143)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)

{code}",,becket_qin,dian.fu,dwysakowicz,gaoyunhaii,jark,kezhuw,rmetzger,sewen,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21996,,,,,,,,,,,,,,,,FLINK-22259,,FLINK-18128,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 12:09:43 UTC 2021,,,,,,,,,,"0|z0ffeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 05:38;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2587&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=93018cfc-6498-565b-e034-4b68eb90fc80;;;","04/Jun/20 11:22;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2697&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","08/Jun/20 08:21;sewen;I can take a look at this later in the week. I think I know already where this comes from.;;;","08/Jun/20 14:21;gaoyunhaii;Hi [~sewen], I also debugged the case today, it seems come from the concurrency between the timer thread to send event and the thread to reset the buffered events. The timer thread may scheduling the event sending, but before the sending finished, failover happens and another thread called resetForTask and cleared the buffered events. However, the first thread will be awaked later and continue to send the outdated event, which will be inserted to the valve buffers and cause repeat.;;;","09/Jun/20 02:51;yunta;Another instance: https://myasuka.visualstudio.com/flink/_build/results?buildId=93&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=e3cf9aa2-7cef-56d5-4b97-323ea6f63062

 ;;;","17/Jun/20 10:16;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8175&view=logs&j=d32c604a-b7f9-5ba5-2c7b-abfbb079eb71&t=33e2a6e6-1879-5933-d255-52ddc890fe5b;;;","18/Jun/20 11:53;sewen;What do you think about ignoring this test temporarily?

The reason is that this is most likely not a test instability, but there are some corner-case race conditions in the ""exactly-once"" communication between Operator Coordinator and Operator.

[~jqin] and me have this on our radar and have a design for improvement, but the change is quite complex; it involves checking against versioning/attempt information of executions during message sending.
This change is a few weeks away and will only make it into 1.11.1 I believe.;;;","18/Jun/20 11:57;sewen;[~gaoyunhaii] Yes, you diagnosis is correct. The events that linger in the sending thread cause the problem. A discussion between [~jqin] and me concluded that the {{contect.sendEvent()}} (which is the synchronous part) needs to attach version information (similar to execution attempt number) so that we can filter out the event later.

We are not 100% clear, though, on what exactly the contract is that we want to offer in the Operator Coordinator API. We would like to avoid that implementors need to check delivery status of events.;;;","19/Jun/20 12:03;gaoyunhaii;+1 for that fixing this issue should provide a more easy-to-use interface for operator coordinator users, and attaching version information should be the right direction to fix this issue. ;;;","10/Jul/20 04:03;jark;Hi [~sewen] [~gaoyunhaii], what's the status of this issue? Is this really a blocker of 1.11.1 release?;;;","14/Jul/20 02:35;dian.fu;Also cc [~jqin], could you help to confirm if this should be a blocker of 1.11.1? [~sewen] [~jqin] [~gaoyunhaii];;;","14/Jul/20 03:21;becket_qin;It is a known caveat and may involve some major change in JM to fix. Impact wise, it is not that critical. Therefore let's not block on this issue. I'll move it to 1.11.2.;;;","14/Jul/20 03:23;dian.fu;Thanks [~becket_qin].;;;","27/Oct/20 08:26;xtsong;cc [~sewen] [~gaoyunhaii] [~becket_qin]

Hi folks,
Any updates on this issue? Should this be fixed in 1.11.3?;;;","27/Oct/20 10:51;sewen;I'll try to get to this one towards end of this week.

It would be great to fix it in 1.11.3, but not necessarily a blocker. I think this is a pretty rare race condition that needed quite some effort to provoke frequently in the tests.;;;","29/Oct/20 06:22;xtsong;[~sewen] Thanks for the updates. Sounds good to me.;;;","18/Nov/20 02:59;xtsong;I'm deferring the fix version to 1.11.4 for now. Will change it back if it makes the 1.11.3 release in the end.;;;","26/Mar/21 13:54;trohrmann;Are there any plans to fix this issue [~sewen] for the upcoming release? If you don't intend to work on it, then please unassign yourself from the ticket.;;;","29/Mar/21 19:28;sewen;Let me unassign myself for now.

If anyone else would approach this issue, please share your thoughts here how to fix this before approaching this issue.
There may be a good solution for this as well by looking into using restoreTask notifications rather than taskFailure notifications, which have more well defined ordering with checkpoints. ;;;","02/Apr/21 20:37;kezhuw;Hi all, I dug and thought some time about this. I want to share what I got. I might be wrong though, forgive me then please.

h4. Symptom cause

As [~gaoyunhaii] and [~sewen] pointed out before, there are lingering sendings after {{resetToCheckpoint}}. Adds following snippets to {{sendNextEvent}} before event sending makes this test fails often.
{code:java}
// This will fails 749620d007e93a6fba6a7d9cb759ec68c7670b00 quite often.
Thread.sleep(50);

// Following make it more often in 1.13 (ps. it is not that often without this comparing to above commit).
if (periodicTask == null) {
    Thread.sleep(5000);
}
{code}

{{CoordinatorEventsExactlyOnceITCase}} tests only global failover, but not region failover. So, a simple wrapper with {{RecreateOnResetOperatorCoordinator.Provider}} (plus some minor changes) will pass this test.

h4. Root cause

Initially, I thought we might be able do something in runtime to guard this. But after tackling the code bit, I realized that it will be hard to guard region failover to achieve exactly once in current api:

# Currently, events are sending through {{Context.sendEvent(OperatorEvent evt, int targetSubtask)}}.
# Without strict promise from implementation of {{OperatorCoordinator}}, possibility to sending event from old incarnation after failed/reset will not be zero.

To achieve exactly once guarantee, we either have to enforce strict promise from implementation of {{OperatorCoordinator}} or we need to change the api a bit to my knowledge. I don't think strict promise enforcement to implementation is a good choice, it is just too fragile when there are 100 different implementations and hard to figure out where bad things happen.

For api changes, I draft followings:
* Drop {{Context.sendEvent(OperatorEvent evt, int targetSubtask)}}.
* Add {{void OperatorCoordinator#subtaskReady(int subtask, SubtaskContext context)}}. This will be called just before first event from operator.
* Main method of {{SubtaskContext}} is {{CompletableFuture<Acknowledge> sendEvent(OperatorEvent evt) throws TaskNotRunningException}}. This method will bind to single execution attempt. This guarantee that {{sendEvent}} will not mess up multiple runs of task. {{SubtaskContext}} could also extend from {{Context}}.
* Explicit restriction: operator coordinator will not be able to send event to operator instance before ready. I don't see any reason to send event first from coordinator.
* Optimization: quiesce {{SubtaskContext}} on both global/region failure and fail sending after quiesced.

Squash all to one: add subtask readiness to operator coordinator and bind sending with single execution attempt after ready.

h4. Other thoughts
Currently, to create operator coordinator on {{resetToCheckpoint}}, one has to extend {{RecreateOnResetOperatorCoordinator.Provider}}. It is a bit verbose and less explicit in api. I suggest to add tag interfaces to let runtime wrapping providers from client side automatically. This also mean these tag interfaces will be part of coordinator api. Personally, I think recreating coordinator on reset is a bit simple to use especially for complex coordinator. I guess it might be worth to propagate that in api.

[~sewen] [~gaoyunhaii] [~becket_qin] What do you think ? There are might be other approaches to solve this. Glad to hear.;;;","05/Apr/21 17:24;kezhuw;Hi all, I pushed a  [poc branch|https://github.com/kezhuw/flink/commits/FLINK-18071-operator-coordinator-subtask-context-poc] for evaluation.

I do mainly two things in that branch:
 # Add proposed readiness api. I changed to {{void OperatorCoordinator#subtaskReady(int subtask, SubtaskContext context, OperatorEvent event)}}. I guess it could help for certain use cases.
 # Adjust and divide {{CoordinatorEventsExactlyOnceITCase}} to {{CoordinatorEventsExactlyOnceUsingCoordinatorSendingITCase}} and {{CoordinatorEventsExactlyOnceUsingSubtaskSendingITCase}}. The only difference between the two is which method is used in sending operation. {{Context.sendEvent}} fails constantly after elaborated sleep operation injected.

I am no sure whether it is fair for {{Context.sendEvent}} in this poc test comparison. But it should demonstrate a bit that {{Context.sendEvent}} is more error prone to use and will cost more attention from coordinator implementations.

The key part in using {{SubtaskContext}} is ""don't mess up old and new context"". This might be relative easy to obey comparing to ensure no old sending incarnations given that blocking operations are restricted in fail/reset notification methods.

I think {{SubtaskContext}} might eliminate necessity of {{RecreateOnResetOperatorCoordinator.Provider}} in some cases. Recreating is still more simple though.;;;","07/Apr/21 16:02;dwysakowicz;Is there a committer that will have time in 1.13 to help get this ticket in? If not, can we move the fix version to the next release? [~sewen] [~becket_qin] ?;;;","08/Apr/21 22:42;sewen;I am happy to shepherd this issue. I think [~kezhuw] has a good idea for the fix. Regarding the release, I'd say let's not block the release on this. It is a rare race that requires a join of multiple FLIP-27 sources to happen. Let fix it asap, but not block the release.

I was thinking to fix it a bit different, but Kezhu's approach also looks very promising. I'll try to dig through this a bit and comment back tomorrow.

;;;","12/Apr/21 00:51;sewen;[~kezhuw] I adopted your ideas into my work on FLINK-21996.
The fix for that issue also needs exact matching of events to execution attempts, which the SubtaskGateway gives us.
I implemented it a bit different. The WIP branch is here, if you want to take a look: https://github.com/StephanEwen/flink/commits/split_fix_2;;;","12/Apr/21 01:44;kezhuw;Sure, I will take a look at. Thanks for delivery. [~sewen];;;","12/Apr/21 08:52;sewen;Thanks, [~kezhuw] for your comments on the code branch. There is an PR for the first set of commits, if you are interested. ;;;","13/Apr/21 23:58;sewen;I have a PR with a nice solution, and a strengthened test: https://github.com/apache/flink/pull/15601

The solution is partly based on [~kezhuw]'s proposal, with some adjustment, like decoupling the ready notifications from events sent by readers. I think the solution is pretty nice in the end (I am quite happy with the outcome now), I think it is a pretty simple model that gives you an intuitive model for consistency between the Coordinator's view of the world (running tasks) and the interaction with the world.

That change is also a precondition for FLINK-21996.
;;;","16/Apr/21 15:05;sewen;Fixed in 1.13.0 via
  - 4595ec755308ba0608910e67561ba9f7e6e00c43
  - 07fc4477909b6ec2374cc38f8aa445ebfe6b3b4d
  - 605d158f013ba0fd43fa0ae68b8936b9288ac715
  - 58c5e8c3cc43ddaee0fa63b8fb209eb2a4006eb7
  - 4b51987321ac75a936e0e6e3389c5288b2c2c205;;;","20/Apr/21 12:09;sewen;Fixed in 1.12.3 via
  - 33bb107fa5e3a6aaa8800a41b97eaac677399109
  - bafdebd6d73f0268f0657a3265d142bd9de27c93
  - 52e52a67c8a8b13b0fce4b6cc9954632b41a93e4
  - 207e25516fa66cc56457dbecaffd15ed0d515e34
  - d2f5df3e7383733a1f291f6f7fd3ba23ccea79e1;;;",,,,,,,,,,,,,,,,,,,,,,,,
Time attribute been materialized after sub graph optimize,FLINK-18070,13308955,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuyufei,liuyufei,liuyufei,02/Jun/20 13:21,10/Nov/20 07:23,13/Jul/23 08:12,10/Nov/20 07:23,1.10.0,,,,,1.12.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Hi, I want to use window aggregate after create temporary, and has multiple sinks. But throw exception:

{code:java}
java.lang.AssertionError: type mismatch:
ref:
TIME ATTRIBUTE(PROCTIME) NOT NULL
input:
TIMESTAMP(3) NOT NULL
{code}

I look into the optimizer logic, there is comment at {{CommonSubGraphBasedOptimizer}}:
""1. In general, for multi-sinks users tend to use VIEW which is a natural common sub-graph.""

After sub graph optimize, time attribute from source have been convert to basic TIMESTAMP type according to {{FlinkRelTimeIndicatorProgram}}. But my create view sql is simple query, I think didn't need to materialized time attribute in theory.

Here is my code:

{code:java}
// connector.type COLLECTION is for debug use
tableEnv.sqlUpdate(""CREATE TABLE source (\n"" +
	""    `ts` AS PROCTIME(),\n"" +
	""    `order_type` INT\n"" +
	"") WITH (\n"" +
	""    'connector.type' = 'COLLECTION',\n"" +
	""    'format.type' = 'json'\n"" +
	"")\n"");
tableEnv.createTemporaryView(""source_view"", tableEnv.sqlQuery(""SELECT * FROM source""));
tableEnv.sqlUpdate(""CREATE TABLE sink (\n"" +
	""    `result` BIGINT\n"" +
	"") WITH (\n"" +
	""    'connector.type' = 'COLLECTION',\n"" +
	""    'format.type' = 'json'\n"" +
	"")\n"");
tableEnv.sqlUpdate(""INSERT INTO sink \n"" +
	""SELECT\n"" +
	""    COUNT(1)\n"" +
	""FROM\n"" +
	""    `source_view`\n"" +
	""WHERE\n"" +
	""     `order_type` = 33\n"" +
	""GROUP BY\n"" +
	""    TUMBLE(`ts`, INTERVAL '5' SECOND)\n"");
tableEnv.sqlUpdate(""INSERT INTO sink \n"" +
	""SELECT\n"" +
	""    COUNT(1)\n"" +
	""FROM\n"" +
	""    `source_view`\n"" +
	""WHERE\n"" +
	""     `order_type` = 34\n"" +
	""GROUP BY\n"" +
	""    TUMBLE(`ts`, INTERVAL '5' SECOND)\n"");
{code}


",,godfreyhe,jark,libenchao,liuyufei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 07:23:35 UTC 2020,,,,,,,,,,"0|z0ffbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 07:39;libenchao;[~liuyufei] Thanks for reporting this issue, I also verified this bug in master branch.

And I've took a deeper look into it, it's because we materialize the `proctime` field in middle block. CC [~jark]

Actually we suffers from this bug in our internal 1.9 branch.

 ;;;","03/Jun/20 08:09;jark;Yes, I think this is a bug in CommonSubGraphBasedOptimizer.;;;","03/Jun/20 08:39;libenchao;[~liuyufei] Do you want to contribute to this issue？ If you don't have time, I can give a hand here.;;;","03/Jun/20 12:05;liuyufei;[~libenchao] Sure, I'd love to contribute. Please assign this issue to me.;;;","04/Jun/20 06:40;liuyufei;[~libenchao] I feel uncertain about why we need ""_materialize remaining proctime indicators_"" in {{RelTimeIndicatorConverter}}, I only change {{needFinalTimeIndicatorConversion}} to false skip follow materialize convert, seems the problem sloved and didn't see other affects.;;;","04/Jun/20 12:18;libenchao;[~liuyufei] we need to materialize the proctime time if we need it in sink, that's why it exists. ;;;","04/Jun/20 14:16;liuyufei;[~libenchao] 
process time have already been materialized at begining of convert.
{code:java}
val converter = new RelTimeIndicatorConverter(rexBuilder)
val convertedRoot = rootRel.accept(converter)
{code}
;;;","05/Jun/20 02:53;libenchao;As you can see, we only convert the time attributes from the input of sink. Actually, the sink node can have it's time attributes too, that's why this line exists:

```java

// the LogicalSink is converted in RelTimeIndicatorConverter before
if (rootRel.isInstanceOf[LogicalLegacySink] || !needFinalTimeIndicatorConversion) {
 return convertedRoot
}

```

I think the right way to fix this is to leverage the `needFinalTimeIndicatorConversion` variable in `RelTimeIndicatorConverter.convert`, it's always true for now. We need to differentiate the case whether current optimization block is a final block or an intermediate logical block.

 ;;;","05/Jun/20 09:50;liuyufei;I still didn't get it, could you give me a example about this situation?

Is a final block always a LogicSink? So if we don't need materialize processtime in intermediate block and LogicSink, it will always return convertedRoot directly. ;;;","17/Jun/20 04:05;jark;cc [~godfreyhe];;;","17/Jun/20 05:28;godfreyhe;[~liuyufei] Thanks for reporting this. the simple approach to fix this is {{needFinalTimeIndicatorConversion}} method should return the value of {{isSinkBlock}} instead of {{true}} in {{StreamCommonSubGraphBasedOptimizer}}. ;;;","17/Jun/20 05:31;godfreyhe;we need to materialize time attribute only in sink-block.;;;","10/Nov/20 07:23;godfreyhe;master: https://github.com/apache/flink/pull/13280;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scaladocs not building since inner Java interfaces cannot be recognized,FLINK-18069,13308946,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tzulitai,tzulitai,tzulitai,02/Jun/20 12:35,05/Jun/20 08:59,13/Jul/23 08:12,05/Jun/20 08:08,1.11.0,,,,,,,,API / Scala,Documentation,,,,0,pull-request-available,,,,,"Error:

{code}
/home/buildslave/slave/flink-docs-master/build/flink-scala/src/main/java/org/apache/flink/api/scala/typeutils/Tuple2CaseClassSerializerSnapshot.java:98: error: not found: type OuterSchemaCompatibility
	protected OuterSchemaCompatibility resolveOuterSchemaCompatibility(ScalaCaseClassSerializer<Tuple2<T1, T2>> newSerializer) {
                  ^
/home/buildslave/slave/flink-docs-master/build/flink-scala/src/main/java/org/apache/flink/api/scala/typeutils/TraversableSerializerSnapshot.java:101: error: not found: type OuterSchemaCompatibility
	protected OuterSchemaCompatibility resolveOuterSchemaCompatibility(TraversableSerializer<T, E> newSerializer) {
                  ^
/home/buildslave/slave/flink-docs-master/build/flink-scala/src/main/java/org/apache/flink/api/scala/typeutils/ScalaCaseClassSerializerSnapshot.java:106: error: not found: type OuterSchemaCompatibility
	protected OuterSchemaCompatibility resolveOuterSchemaCompatibility(ScalaCaseClassSerializer<T> newSerializer) {
                  ^
{code}

This is a similar issue as reported here: https://github.com/scala/bug/issues/10509.

This seems to be a problem with Scala 2.12.x. The only workaround is to
redundantly add the full-length qualifiers for such interfaces.",,rmetzger,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 08:08:12 UTC 2020,,,,,,,,,,"0|z0ff9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 18:26;rmetzger;As part of this ticket, I will open a small PR to check the scaladocs as part of the CI compile stage.;;;","04/Jun/20 04:59;tzulitai;Merged Scaladoc fixes:

master: c53739563fa15d666d2be7329025ba2969856bfc
release-1.11: b9493846a39f2dd0edb274f6c63b9029d4fef4c0;;;","05/Jun/20 08:08;rmetzger;Merged new CI check to master in https://github.com/apache/flink/commit/46d42e5b718ea2cd914dc7feaf4425e04c3e4645.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the race condition for aborting checkpoint in CheckpointBarrierUnaligner,FLINK-18063,13308910,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,zjwang,zjwang,02/Jun/20 09:07,09/Jun/20 16:45,13/Jul/23 08:12,09/Jun/20 09:49,1.11.0,,,,,1.11.0,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"There are three aborting scenarios which might encounter race condition:

    1. CheckpointBarrierUnaligner#processCancellationBarrier

    2. CheckpointBarrierUnaligner#processEndOfPartition

    3. AlternatingCheckpointBarrierHandler#processBarrier

We only consider the pending checkpoint triggered by #processBarrier from task thread to abort it. Actually the checkpoint might also be triggered by #notifyBarrierReceived from netty thread in race condition, so we should also handle properly to abort it.",,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 09:49:09 UTC 2020,,,,,,,,,,"0|z0ff1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 09:49;zjwang;Merged in master: f672ae53596aa26f1e3ed4b3d24aff2197be749d

Merged in release-1.11: 2fe888c9aff75a3ace18c30764b26cc9f53e2451;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableResult#collect should return closeable iterator to avoid resource leak,FLINK-18061,13308901,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,godfreyhe,godfreyhe,02/Jun/20 08:18,09/Jun/20 16:26,13/Jul/23 08:12,09/Jun/20 13:54,,,,,,1.11.0,,,Table SQL / API,,,,,0,pull-request-available,,,,,"as discussed in ML: http://mail-archives.apache.org/mod_mbox/flink-dev/202005.mbox/%3cd4ee47e1-0214-aa2f-f5ac-c9daf708e98f@apache.org%3e, we should return a closeable iterator for TableResult#collect method *to avoid resource leak*. The suggested change is:

{code:java}

public interface TableResult {

  CloseableIterator<Row> collect();

}
{code}

we use existing {{org.apache.flink.util.CloseableIterator}} instead of additional api class, and {{CollectResultIterator}} can also inherit from {{CloseableIterator}}.

This change does not break current api.",,dwysakowicz,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 13:54:21 UTC 2020,,,,,,,,,,"0|z0fezc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 13:54;dwysakowicz;Fixed:
* master
** d27c88820dd8ed66dcd39bc488a26c4ccff1a41e
* 1.11
** 05cecd232e158703749ac4a9dac9bc63641bbf35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not execute create/drop catalog statement in sql client,FLINK-18059,13308876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,godfreyhe,godfreyhe,02/Jun/20 06:34,05/Jun/20 07:14,13/Jul/23 08:12,05/Jun/20 07:14,1.11.0,,,,,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"when executing create catalog statement (e.g. {{create CATALOG c1 with('type'='generic_in_memory'}}) in sql client, the following exception will occur:

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unsupported command: CREATE CATALOG
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:355)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:213)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)

Similar case for {{drop catalog}}.

The reason is CliClient class does not handle CREATE_CATALOG command and DROP_CATALOG command.",,godfreyhe,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 07:14:36 UTC 2020,,,,,,,,,,"0|z0fets:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/20 07:14;jark;- master (1.12.0): b28cca265c5dc37fc1ac0aa01d67f5276d8670fa
- 1.11.0: 89be0b1c12da7a0c72f0db304479767db8f72488;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MesosResourceManagerTest.testWorkerStarted:656 » NullPointer,FLINK-18058,13308875,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,rmetzger,rmetzger,02/Jun/20 06:28,11/Jun/20 13:05,13/Jul/23 08:12,11/Jun/20 13:05,1.11.0,,,,,1.11.0,,,Deployment / Mesos,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8113&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=8d823410-c7c7-5a4d-68bb-fa7b08da17b9

{code}

[ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.905 s <<< FAILURE! - in org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest
[ERROR] testWorkerStarted(org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest)  Time elapsed: 0.219 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.runtime.resourcemanager.ResourceManager.sendSlotReport(ResourceManager.java:406)
	at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest$7.<init>(MesosResourceManagerTest.java:680)
	at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.testWorkerStarted(MesosResourceManagerTest.java:656)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)

{code}",,guoyangze,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 13:05:14 UTC 2020,,,,,,,,,,"0|z0fetk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/20 13:05;chesnay;master: f889a31d314e7435a4fbbb3b56c1e5b69906039e
1.11: 13c17061e1ab3f33f9bb36ed22a1c9341dc63996;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingleInputGateTest.testConcurrentReadStateAndProcessAndClose: expected:<3> but was:<2>,FLINK-18057,13308871,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,rmetzger,rmetzger,02/Jun/20 06:20,09/Jun/20 16:48,13/Jul/23 08:12,09/Jun/20 16:02,1.11.0,,,,,1.11.0,1.12.0,,Runtime / Network,Tests,,,,0,pull-request-available,test-stability,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2524&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
java.lang.AssertionError: expected:<3> but was:<2>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.testConcurrentReadStateAndProcessAndClose(SingleInputGateTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}",,kevin.cyj,rmetzger,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 16:02:58 UTC 2020,,,,,,,,,,"0|z0feso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/20 04:17;kevin.cyj;This is purely a test case issue, I will submit a fix patch soon.;;;","09/Jun/20 16:02;zjwang;Merged in release-1.11 : fe229a319fff73512953ea1b55391b29e48db129

Merged in master: 9a7dbfcb8cbc6d72083c03ad83d9df97660f15d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive file sink throws exception when the target in-progress file exists.,FLINK-18056,13308867,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,02/Jun/20 05:54,09/Jun/20 16:24,13/Jul/23 08:12,09/Jun/20 05:50,,,,,,1.11.0,,,Connectors / FileSystem,Connectors / Hive,,,,0,pull-request-available,,,,,"Currently after failover or restart, the Hive file sink will try to overwrite the data since the last checkpoint, however, currently neither the in-progress file is deleted nor hive uses the overwritten mode, thus an exception occurs after restarting:


{code:java}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to create file /user/hive/warehouse/datalake/dt=2020-06-01/hr=22/.part-0-10.inprogress for DFSClient_NONMAPREDUCE_-1017064593_62 for client 100.96.206.42 because current leaseholder is trying to recreate file.
{code}


The full stack of the exception is

{code:java}
org.apache.flink.connectors.hive.FlinkHiveException: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create Hive RecordWriter
    at org.apache.flink.connectors.hive.write.HiveWriterFactory.createRecordWriter(HiveWriterFactory.java:159)
    at org.apache.flink.connectors.hive.write.HiveBulkWriterFactory.create(HiveBulkWriterFactory.java:47)
    at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriter$HadoopPathBasedBucketWriter.openNewInProgressFile(HadoopPathBasedPartFileWriter.java:234)
    at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriter$HadoopPathBasedBucketWriter.openNewInProgressFile(HadoopPathBasedPartFileWriter.java:207)
    at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.rollPartFile(Bucket.java:209)
    at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.write(Bucket.java:200)
    at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.onElement(Buckets.java:284)
    at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSinkHelper.onElement(StreamingFileSinkHelper.java:104)
    at org.apache.flink.table.filesystem.stream.StreamingFileWriter.processElement(StreamingFileWriter.java:118)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
    at StreamExecCalc$16.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
    at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:123)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
    at StreamExecCalc$2.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
    at org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource.run(DataGeneratorSource.java:82)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create Hive RecordWriter
    at org.apache.flink.table.catalog.hive.client.HiveShimV110.getHiveRecordWriter(HiveShimV110.java:58)
    at org.apache.flink.connectors.hive.write.HiveWriterFactory.createRecordWriter(HiveWriterFactory.java:151)
    ... 36 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.table.catalog.hive.client.HiveShimV110.getHiveRecordWriter(HiveShimV110.java:55)
    ... 37 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to create file /user/hive/warehouse/datalake/dt=2020-06-01/hr=22/.part-0-10.inprogress for DFSClient_NONMAPREDUCE_-1017064593_62 for client 100.96.206.42 because current leaseholder is trying to recreate file.
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3075)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2783)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2676)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2561)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:593)
    at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:111)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:393)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
    at org.apache.hadoop.ipc.Client.call(Client.java:1435)
    at org.apache.hadoop.ipc.Client.call(Client.java:1345)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
    at com.sun.proxy.$Proxy35.create(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
    at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
    at com.sun.proxy.$Proxy36.create(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:265)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1274)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1216)
    at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:473)
    at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:470)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:470)
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:411)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:929)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:910)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807)
    at parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:176)
    at parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:160)
    at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:289)
    at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:267)
    at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.<init>(ParquetRecordWriterWrapper.java:66)
    at org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getParquerRecordWriterWrapper(MapredParquetOutputFormat.java:125)
    at org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getHiveRecordWriter(MapredParquetOutputFormat.java:114)
    at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:261)
    ... 41 more
{code}




",,aljoscha,gaoyunhaii,knaufk,lzljs3620320,maguowei,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 05:50:52 UTC 2020,,,,,,,,,,"0|z0fers:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 19:41;sjwiesman;[~gaoyunhaii] [~jinsong]

I also observed this issue in a running job during that was not started from previous state, writing into an empty Hive table during checkpoint.  ;;;","03/Jun/20 01:20;gaoyunhaii;Hi [~sjwiesman]is this error has the same stack with the appending one ?;;;","03/Jun/20 02:28;sjwiesman;Yes, it occured during a checkpoint while writing parquet to hdfs. ;;;","03/Jun/20 02:29;sjwiesman;The job was running at parallelism > 1 if that is relevant. ;;;","03/Jun/20 02:47;gaoyunhaii;Hi [~sjwiesman] I'm a little confusing on the scenario since when rollover the part-file on checkpoint, it should always write to a new file. Without failover or restart the new part-file should not exist before and thus the corresponding in-progress file should not be created. A single sink with parallelism > 1 should also not cause the conflict since the part-file name contains the subtask index. Does the job also have multiple sink writing to the same Hive table ? Or is it possible that the Hive table has been written by some jobs previously but may left some hidden files not cleared when cleaning the previous results ? ;;;","09/Jun/20 05:50;lzljs3620320;master: 45f42f8e6bf2c2d360a6d4fff4f23d53b4dd4f8a

release-1.11: 0d9ed9012286ef2495b1b75277601743e8c9ff6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalog does not exist in SQL Client,FLINK-18055,13308861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,godfreyhe,godfreyhe,02/Jun/20 04:29,04/Jun/20 09:45,13/Jul/23 08:12,04/Jun/20 09:42,1.11.0,,,,,1.11.0,,,Table SQL / Client,,,,,0,pull-request-available,,,,,"Flink SQL> show catalogs;
default_catalog
hive

Flink SQL> use  catalog hive;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.catalog.exceptions.CatalogException: A catalog with name [`hive`] does not exist.


The reason is {{SqlCommandParser}} adds {{``}} for catalog name, which is unnecessary. ",,dwysakowicz,godfreyhe,jark,jkillers,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17941,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 09:43:44 UTC 2020,,,,,,,,,,"0|z0feqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 08:49;jkillers;I think regular matching can be used.;;;","04/Jun/20 07:31;dwysakowicz;Isn't it a duplicate of FLINK-17941?;;;","04/Jun/20 09:42;jark;- master (1.12.0): aa40ffd94a19ed9b57988d42b8c9262ef60e172e
- 1.11.0: f391b0d433ed16f33018837f42c383992953a7bc;;;","04/Jun/20 09:43;jark;[~dwysakowicz], I discussed with [~lirui] and yes, FLINK-17941 is a duplicate. We both agree to use the pull request of this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch7DynamicSinkITCase.testWritingDocumentsNoPrimaryKey fails with NPE,FLINK-18052,13308777,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,rmetzger,rmetzger,01/Jun/20 18:48,03/Jun/20 07:21,13/Jul/23 08:12,03/Jun/20 07:21,1.11.0,,,,,1.11.0,,,Connectors / ElasticSearch,,,,,0,pull-request-available,test-stability,,,,"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2481&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8

{code}
2020-05-30T22:32:52.3703325Z [INFO] Running org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase
2020-05-30T22:33:08.3791670Z [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 16.006 s <<< FAILURE! - in org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase
2020-05-30T22:33:08.3793500Z [ERROR] testWritingDocumentsNoPrimaryKey(org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase)  Time elapsed: 7.671 s  <<< ERROR!
2020-05-30T22:33:08.3794313Z java.lang.ArrayIndexOutOfBoundsException: 0
2020-05-30T22:33:08.3794992Z 	at org.elasticsearch.search.SearchHits.getAt(SearchHits.java:157)
2020-05-30T22:33:08.3795921Z 	at org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.testWritingDocumentsNoPrimaryKey(Elasticsearch7DynamicSinkITCase.java:226)
2020-05-30T22:33:08.3796778Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-30T22:33:08.3797675Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-30T22:33:08.3798442Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-30T22:33:08.3799083Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-30T22:33:08.3799740Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-30T22:33:08.3800506Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-30T22:33:08.3801236Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-30T22:33:08.3801938Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-30T22:33:08.3802620Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
{code}",,dwysakowicz,jark,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 07:21:36 UTC 2020,,,,,,,,,,"0|z0fe80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 06:29;jark;Maybe we need to increase the timeout [~dwysakowicz]? Currently the indexing timeout is 1s. ;;;","02/Jun/20 06:48;dwysakowicz;I think that's the only improvement I can think of.  Just as an explanation for the future. One can query the Elastichsearch with Search API only for the results that were indexed internally. That's why we need to poll the ES cluster. It's different in the GET API as it returns also documents that were not yet indexed. However for the GET API we need to know the id of the document. This failling test checks the case when there is no id and thus it does not use the GET API.

I will open a PR to increase the timeout. If the test keep failling I'd suggest dropping it and depending only on the unit tests.;;;","02/Jun/20 08:24;jark;I would suggest to throw a timeout exception when timeout instead of to query es and get NPE. ;;;","03/Jun/20 07:21;dwysakowicz;Fixed:
master: 773ad7d0aa6b507c6d003bb554ee45566741cede
1.11: 00e348f699bb79f94a64c266c58163141af5274b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail Maven setup on AZP if download fails,FLINK-18051,13308774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,01/Jun/20 17:55,03/Jun/20 15:00,13/Jul/23 08:12,03/Jun/20 14:59,1.12.0,,,,,1.12.0,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,,Setup maven task is green even though the install was not a success: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2481&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=7f98ac96-cfb0-5c1a-969b-c2a0e48a2291,,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 14:59:19 UTC 2020,,,,,,,,,,"0|z0fe7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/20 14:59;rmetzger;Merged to master in https://github.com/apache/flink/commit/07133e64ab79f0870692985e36d47d4785caf783;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of recycling buffer twice once exception in ChannelStateWriteRequestDispatcher#dispatch,FLINK-18050,13308737,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,zjwang,zjwang,01/Jun/20 14:43,09/Jun/20 16:35,13/Jul/23 08:12,09/Jun/20 07:35,1.11.0,,,,,1.11.0,1.12.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"When task finishes, the `CheckpointBarrierUnaligner` will decline the current checkpoint, which would write abort request into `ChannelStateWriter`.

The abort request will be executed before other write output request in the queue, and close the underlying `CheckpointStateOutputStream`. Then when the dispatcher executes the next write output request to access the stream, it will throw ClosedByInterruptException to make dispatcher thread exit.

In this process, the underlying buffers for current write output request will be recycled twice. 
 * ChannelStateCheckpointWriter#write will recycle all the buffers in finally part, which can cover both exception and normal cases.
 * ChannelStateWriteRequestDispatcherImpl#dispatch will call `request.cancel(e)`  to recycle the underlying buffers again in the case of exception.

The effect of this bug can cause further exception in the network shuffle process, which references the same buffer as above, then this exception will send to the downstream side to make it failure.

 

This bug can be reproduced easily via running UnalignedCheckpointITCase#shouldPerformUnalignedCheckpointOnParallelRemoteChannel.",,wind_ljy,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 07:35:45 UTC 2020,,,,,,,,,,"0|z0fdz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/20 07:35;zjwang;Merged in master: 

ed7b0b1bea84a10ee45d10343f239cd183659a74, 

f2dd4b8500a82532dae17087c227ce34e1aeac9b

Merged in release-1.11: 

a233c0ff82273ca59bb1decdb1ffb6020d27ccfd,  

822e01b613b0b6821383f3cd5b0357054242b6a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""--host"" option could not take effect for standalone application cluster",FLINK-18048,13308679,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,wangyang0918,wangyang0918,01/Jun/20 09:26,05/Jun/20 12:25,13/Jul/23 08:12,05/Jun/20 12:25,1.10.2,1.11.0,,,,1.10.2,1.11.0,1.12.0,Deployment / Scripts,,,,,0,pull-request-available,,,,,"When we use the following command to start a Flink application cluster, the specified hostname could not take effect. The root cause is {{HOST_OPTION}} is not added to options in {{StandaloneApplicationClusterConfigurationParserFactory}}. It will be a critical issue when we deploy Flink on container environment. Because users usually want to specify a given hostname.

 
{code:java}
./bin/standalone-job.sh start --host external-hostname --job-classname org.apache.flink.streaming.examples.join.WindowJoin
{code}
 

For the old {{StandaloneJobClusterConfigurationParserFactory}}, it has the same issue.",,kkl0u,wangyang0918,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 12:25:13 UTC 2020,,,,,,,,,,"0|z0fdm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/20 09:28;wangyang0918;[~zhuzh] Could you assign this ticket to me? I will attach a PR to fix this.;;;","01/Jun/20 09:52;zhuzh;Yes you are right that host option should be added to `getOptions()` but was not.
I have assigned the ticket to you.;;;","05/Jun/20 12:25;kkl0u;Merged on master with 58343736a479080987dad4cc2636d425197ed29d ,
on release-1.11 with 39e3d6ad6ce25bc94a9515362a96c229f3d0c590
and on release-1.10 with 855f799d9919348619f17cfcb4d6544dc02afdfb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decimal column stats not supported for Hive table,FLINK-18046,13308650,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,01/Jun/20 07:22,08/Jun/20 10:47,13/Jul/23 08:12,08/Jun/20 10:47,,,,,,1.11.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,,"For now, we can just return {{CatalogColumnStatisticsDataDouble}} for decimal columns.",,danny0405,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 10:47:49 UTC 2020,,,,,,,,,,"0|z0fdfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 08:12;danny0405;cc [~lzljs3620320];;;","08/Jun/20 10:47;lzljs3620320;master: 88e416f988bc286b7cc0df7bcf0679184e7bb805

release-1.11: 406b5d24404c3f72ce84d8308f47aa76246000d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Kerberos credentials checking to unblock Flink on secured MapR,FLINK-18045,13308646,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,krasinski,krasinski,krasinski,01/Jun/20 07:04,05/Aug/20 12:45,13/Jul/23 08:12,09/Jun/20 12:12,1.10.1,1.11.0,,,,1.10.2,1.11.0,,Deployment / YARN,,,,,0,pull-request-available,,,,,"I was not able to run Flink 1.10.1 on YARN on a a secured MapR cluster, but the previous version (1.10.0) works fine.

After some investigation it looks like during some refactoring, checking if the enabled security method is kerberos was removed, effectively reintroducing https://issues.apache.org/jira/browse/FLINK-5949

 

Refactoring commit: [https://github.com/apache/flink/commit/8751e69037d8a9b1756b75eed62a368c3ef29137]

 

My proposal would be to bring back the kerberos check:
{code:java}
loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS
{code}
and add an unit test for that case to prevent it from happening again

I'm happy to prepare a PR after reaching consensus",,guoyangze,krasinski,rongr,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15561,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 12:12:00 UTC 2020,,,,,,,,,,"0|z0fdf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/20 10:15;trohrmann;Thanks for reporting this issue [~krasinski]. Your solution proposal sounds good to me. I just wanted to double check with [~rongr] whether there was a specific reason for removing {{loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS}} from the if statement.;;;","02/Jun/20 14:47;rongr;Yes. I think specifically checking auth method as Kerberos is the right way. I think the proposal is to replace this line: https://github.com/apache/flink/commit/8751e69037d8a9b1756b75eed62a368c3ef29137#diff-3648957eaf615f89c12aab6ea0611b99R116 correct? if so I think that works ;;;","02/Jun/20 19:22;krasinski;Honestly at first I was thinking about simply adding that back to the if statement which contains _useTicketCache && !loginUser.hasKerberosCredentials_

 

 

Your proposal [~rongr] (+if I understand correctly+ entirely replacing _UserGroupInformation.isSecurityEnabled()_ statement with user auth method check) looks nice, but on the other hand Hadoop code inside UserGroupInformation class code often calls it like that:

 
{code:java}
if (isSecurityEnabled() && this.user.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS /* then isKeytab or isKrbTkt */ )
{code}
So it looks like it checks if any auth method was configured using the hadoop config, and then the auth method for the user.
And that might be another way to solve that ticket, which also looks pretty clean in my opinion.

 

To summarize:
 # Original proposal was to add the _{{loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS}}_ back to the if statement together with _useTicketCache && !loginUser.hasKerberosCredentials_
 # The second way to go inspired by [~rongr] proposal & Hadoop code is to add _{{loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS}}_ to the statement containing _UserGroupInformation.isSecurityEnabled()_

 

What do you think [~trohrmann], [~rongr]?

 

 ;;;","02/Jun/20 20:26;rongr;yeah. that makes sense. +1 on the proposed solution #2;;;","03/Jun/20 11:57;krasinski;I've submitted the PR;;;","09/Jun/20 12:12;trohrmann;Fixed via

master: 415b78af67e4acab9a0f23af2dc74ac093a54057
1.11.0: c8205341418d9139dc59d2fcd7b5dbae6b4c5f98
1.10.2: e71a42c7271922a4feb4f7c967213644c60841b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
