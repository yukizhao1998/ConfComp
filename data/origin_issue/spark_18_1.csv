Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Outward issue link (Completes),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Incorporates),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Required),Outward issue link (Supercedes),Inward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Watermark / Append mode should work with Trigger.Once,SPARK-24699,13169278,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,chorn,chorn,29/Jun/18 22:36,31/Aug/19 19:46,13/Jul/23 08:44,23/Jul/18 20:04,2.3.1,,,,,,,,,2.4.0,,,,,Structured Streaming,,,,1,,,,"I have a use case where I would like to trigger a structured streaming job from an external scheduler (once every 15 minutes or so) and have it write window aggregates to Kafka.

I am able to get my code to work when running with `Trigger.ProcessingTime` but when I switch to `Trigger.Once` the watermarking feature of structured streams does not persist to (or is not recollected from) the checkpoint state.

This causes the stream to never generate output because the watermark is perpetually stuck at `1970-01-01T00:00:00Z`.

I have created a failing test case in the `EventTimeWatermarkSuite`, I will create a [WIP] pull request on github and link it here.

 

It seems that even if it generated the watermark, and given the current streaming behavior, I would have to trigger the job twice to generate any output.

 

The microbatcher only calculates the watermark off of the previous batch's input and emits new aggs based off of that timestamp.

This state is not available to a newly started `MicroBatchExecution` stream.

Would it be an appropriate strategy to create a new checkpoint file with the most up to watermark or watermark and query stats?",,apachespark,chorn,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 23:06;chorn;watermark-once.scala;https://issues.apache.org/jira/secure/attachment/12929787/watermark-once.scala","29/Jun/18 23:06;chorn;watermark-stream.scala;https://issues.apache.org/jira/secure/attachment/12929788/watermark-stream.scala",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 23 20:04:45 UTC 2018,,,,,,,,,,"0|i3venb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 23:06;apachespark;User 'c-horn' has created a pull request for this issue:
https://github.com/apache/spark/pull/21676;;;","29/Jun/18 23:07;chorn;I have attached two scala repl scripts for reproducing this behavior. The ""once"" variant fails to produce output or updated watermarks, the ""stream"" variant behaves mostly as expected.;;;","11/Jul/18 09:29;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/21746;;;","23/Jul/18 20:04;tdas;Issue resolved by pull request 21746
[https://github.com/apache/spark/pull/21746];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In Pyspark's ML, an Identifiable's UID has 20 random characters rather than the 12 mentioned in the documentation.",SPARK-24698,13169269,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,thomasd9,thomasd9,thomasd9,29/Jun/18 21:57,12/Dec/22 18:10,13/Jul/23 08:45,05/Jul/18 02:07,2.3.1,,,,,,,,,2.4.0,,,,,ML,,,,0,easyfix,,,"Hi.

In pyspark, an Identifiable object has a random ID assigned to help distinguish instances from each other. This ID is made by concatenating the name of the class with part of a Python's built-in UUID.

The docstring of the method (__randomUID()_) that generates this ID says that 12 random characters are used from the Python UUID, but the code actually skips the first 12 characters. The hex representation of the UUID is 32 characters, so the last 20 characters are used.

Code can be found [here|https://github.com/apache/spark/blob/master/python/pyspark/ml/util.py#L66], and also copied here for your viewing pleasure:
{code}
@classmethod
def _randomUID(cls):
    """"""
    Generate a unique unicode id for the object. The default implementation
    concatenates the class name, ""_"", and 12 random hex chars.
    """"""
    return unicode(cls.__name__ + ""_"" + uuid.uuid4().hex[12:])
{code}",,apachespark,thomasd9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 05 02:07:12 UTC 2018,,,,,,,,,,"0|i3velb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 22:14;apachespark;User 'mcteo' has created a pull request for this issue:
https://github.com/apache/spark/pull/21675;;;","05/Jul/18 02:07;gurwls223;Issue resolved by pull request 21675
[https://github.com/apache/spark/pull/21675];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests pass only one app argument,SPARK-24694,13169187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,skonto,skonto,skonto,29/Jun/18 15:38,17/May/20 18:24,13/Jul/23 08:45,05/Jul/18 21:35,2.3.1,,,,,,,,,2.4.0,,,,,Kubernetes,Spark Core,,,0,,,,"I tried to add another test in the current suite which uses more than one argument and it fails:

+ CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"")

+ exec /sbin/tini -s – /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=9.0.10.29 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.DFSReadWriteTest spark-internal '/etc/resolv.conf hdfs:///test-SGzsB'
 2018-06-29 15:31:51 WARN Utils:66 - Kubernetes master URL uses HTTP instead of HTTPS.
 2018-06-29 15:31:52 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 args size: 1
 Arg: /etc/resolv.conf hdfs:///test-SGzsB
 DFS Read-Write Test
 Usage: localFile dfsDir
 localFile - (string) local file to use in test

Reason is this line here: [https://github.com/apache/spark/blob/master/resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesTestComponents.scala#L109] which adds all args to one element in the final array. But Processbuilder will not split args later on:on: [https://github.com/apache/spark/blob/f6e6899a8b8af99cd06e84cae7c69e0fc35bc60a/resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/ProcessUtils.scala#L32]

 ",,apachespark,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 05 21:35:28 UTC 2018,,,,,,,,,,"0|i3ve33:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"29/Jun/18 16:40;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/21672;;;","05/Jul/18 21:35;srowen;Issue resolved by pull request 21672
[https://github.com/apache/spark/pull/21672];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a new config to control plan stats computation in LogicalRelation,SPARK-24690,13169081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,29/Jun/18 06:51,24/Nov/19 16:30,13/Jul/23 08:45,24/Nov/19 16:30,2.3.1,,,,,,,,,3.0.0,,,,,SQL,,,,0,,,,"We cannot currently enable StarSchemaDetection.reorderStarJoins because we need to turn off CBO to enable it but StarSchemaDetection internally references LogicalPlan.stats.rowCount.
This rowCount is used in LogicalRelation if CBO disabled. 

This ticket is to propose a new separate config so that LogicalRelation can use rowCount to compute data statistics.",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 24 16:30:50 UTC 2019,,,,,,,,,,"0|i3vdfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 06:59;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21668;;;","24/Nov/19 16:30;dongjoon;Issue resolved by pull request 21668
[https://github.com/apache/spark/pull/21668];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When NoClassDefError thrown during task serialization will cause job hang,SPARK-24687,13169038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cane,cane,cane,29/Jun/18 02:29,20/Dec/18 14:30,13/Jul/23 08:45,20/Dec/18 14:28,2.1.0,2.1.1,,,,,,,,2.3.3,2.4.1,3.0.0,,,Spark Core,,,,1,,,,"When below exception thrown:

{code:java}
Exception in thread ""dag-scheduler-event-loop"" java.lang.NoClassDefFoundError: Lcom/xxx/data/recommend/aggregator/queue/QueueName;
	at java.lang.Class.getDeclaredFields0(Native Method)
	at java.lang.Class.privateGetDeclaredFields(Class.java:2436)
	at java.lang.Class.getDeclaredField(Class.java:1946)
	at java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1659)
	at java.io.ObjectStreamClass.access$700(ObjectStreamClass.java:72)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:480)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:468)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:468)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:365)
	at java.io.ObjectOutputStream.writeClass(ObjectOutputStream.java:1212)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1119)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
{code}
Stage will always hang.Not abort.
 !hanging-960.png! 
It is because NoClassDefError will no be catch up below.
{code}
var taskBinary: Broadcast[Array[Byte]] = null
    try {
      // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).
      // For ResultTask, serialize and broadcast (rdd, func).
      val taskBinaryBytes: Array[Byte] = stage match {
        case stage: ShuffleMapStage =>
          JavaUtils.bufferToArray(
            closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
        case stage: ResultStage =>
          JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
      }

      taskBinary = sc.broadcast(taskBinaryBytes)
    } catch {
      // In the case of a failure during serialization, abort the stage.
      case e: NotSerializableException =>
        abortStage(stage, ""Task not serializable: "" + e.toString, Some(e))
        runningStages -= stage

        // Abort execution
        return
      case NonFatal(e) =>
        abortStage(stage, s""Task serialization failed: $e\n${Utils.exceptionString(e)}"", Some(e))
        runningStages -= stage
        return
    }
{code}
",,apachespark,cane,githubbot,mgaido,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 02:29;cane;hanging-960.png;https://issues.apache.org/jira/secure/attachment/12929657/hanging-960.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 20 14:30:29 UTC 2018,,,,,,,,,,"0|i3vd5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/18 13:18;apachespark;User 'caneGuy' has created a pull request for this issue:
https://github.com/apache/spark/pull/21664;;;","20/Dec/18 14:28;srowen;Issue resolved by pull request 21664
[https://github.com/apache/spark/pull/21664];;;","20/Dec/18 14:30;githubbot;srowen closed pull request #21664: [SPARK-24687][CORE] Avoid job hanging when generate task binary causes fatal error
URL: https://github.com/apache/spark/pull/21664
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala b/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
index f74425d73b392..6ee15fde5c71f 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
@@ -1045,9 +1045,11 @@ class DAGScheduler(
 
         // Abort execution
         return
-      case NonFatal(e) =>
+      case e: Throwable =>
         abortStage(stage, s""Task serialization failed: $e\n${Utils.exceptionString(e)}"", Some(e))
         runningStages -= stage
+
+        // Abort execution
         return
     }
 


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create a view from a table when a nested column name contains ':',SPARK-24681,13168913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,a.ionescu,a.ionescu,28/Jun/18 16:41,17/Jul/18 21:15,13/Jul/23 08:45,17/Jul/18 21:15,2.2.0,2.3.0,2.4.0,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Here's a patch that reproduces the issue: 
{code:java}
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala 
index 09c1547..29bb3db 100644 
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala 
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala 
@@ -19,6 +19,7 @@ package org.apache.spark.sql.hive 
 
import org.apache.spark.sql.{QueryTest, Row} 
import org.apache.spark.sql.execution.datasources.parquet.ParquetTest 
+import org.apache.spark.sql.functions.{lit, struct} 
import org.apache.spark.sql.hive.test.TestHiveSingleton 
 
case class Cases(lower: String, UPPER: String) 
@@ -76,4 +77,21 @@ class HiveParquetSuite extends QueryTest with ParquetTest with TestHiveSingleton 
      } 
    } 
  } 
+ 
+  test(""column names including ':' characters"") { 
+    withTempPath { path => 
+      withTable(""test_table"") { 
+        spark.range(0) 
+          .select(struct(lit(0).as(""nested:column"")).as(""toplevel:column"")) 
+          .write.format(""parquet"") 
+          .option(""path"", path.getCanonicalPath) 
+          .saveAsTable(""test_table"") 
+ 
+        sql(""CREATE VIEW test_view_1 AS SELECT `toplevel:column`.* FROM test_table"") 
+        sql(""CREATE VIEW test_view_2 AS SELECT * FROM test_table"") 
+ 
+      } 
+    } 
+  } 
}{code}
The first ""CREATE VIEW"" statement succeeds, but the second one fails with:
{code:java}
org.apache.spark.SparkException: Cannot recognize hive type string: struct<nested:column:int>
{code}",,a.ionescu,achuth17,apachespark,maropu,mengxr,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 04 04:39:05 UTC 2018,,,,,,,,,,"0|i3vce7:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"02/Jul/18 17:20;smilegator;cc [~maropu] [~dongjoon] Are you interested in this bug fix?;;;","02/Jul/18 23:16;maropu;ok, I'll look into this.;;;","03/Jul/18 08:36;maropu;I've looked over related code andI think we cannot use `:` in Hive metastore column names: [https://github.com/apache/hive/blob/release-1.2.1/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java#L239]

The current master checks if column names don't include comma in column names only (you fixed this a year ago): [https://github.com/apache/spark/blob/a7c8f0c8cb144a026ea21e8780107e363ceacb8d/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala#L141]
IMHO we need to check ':' and ';' here, too. WDYT?

Or, we need to accept ':' in column names?

 ;;;","04/Jul/18 04:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21711;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.executorEnv.JAVA_HOME does not take effect in Standalone mode,SPARK-24680,13168872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stanzhai,stanzhai,stanzhai,28/Jun/18 13:42,18/Dec/18 13:05,13/Jul/23 08:45,18/Dec/18 13:02,2.1.1,2.2.1,2.3.1,,,,,,,3.0.0,,,,,Deploy,,,,0,,,,spark.executorEnv.JAVA_HOME does not take effect when a Worker starting an Executor process in Standalone mode.,,apachespark,githubbot,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 18 13:05:27 UTC 2018,,,,,,,,,,"0|i3vc53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 02:13;apachespark;User 'stanzhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/21663;;;","18/Dec/18 13:02;srowen;Issue resolved by pull request 21663
[https://github.com/apache/spark/pull/21663];;;","18/Dec/18 13:05;githubbot;srowen closed pull request #21663: [SPARK-24680][Deploy]Support spark.executorEnv.JAVA_HOME in Standalone mode
URL: https://github.com/apache/spark/pull/21663
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java b/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java
index ce24400f557cd..56edceb17bfb8 100644
--- a/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java
+++ b/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java
@@ -91,14 +91,18 @@
    */
   List<String> buildJavaCommand(String extraClassPath) throws IOException {
     List<String> cmd = new ArrayList<>();
-    String envJavaHome;
 
-    if (javaHome != null) {
-      cmd.add(join(File.separator, javaHome, ""bin"", ""java""));
-    } else if ((envJavaHome = System.getenv(""JAVA_HOME"")) != null) {
-        cmd.add(join(File.separator, envJavaHome, ""bin"", ""java""));
-    } else {
-        cmd.add(join(File.separator, System.getProperty(""java.home""), ""bin"", ""java""));
+    String[] candidateJavaHomes = new String[] {
+      javaHome,
+      childEnv.get(""JAVA_HOME""),
+      System.getenv(""JAVA_HOME""),
+      System.getProperty(""java.home"")
+    };
+    for (String javaHome : candidateJavaHomes) {
+      if (javaHome != null) {
+        cmd.add(join(File.separator, javaHome, ""bin"", ""java""));
+        break;
+      }
     }
 
     // Load extra JAVA_OPTS from conf/java-opts, if it exists.


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download page should not link to unreleased code,SPARK-24679,13168865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lresende,lresende,lresende,28/Jun/18 13:14,28/Jun/18 21:42,13/Jul/23 08:45,28/Jun/18 21:42,2.3.1,,,,,,,,,,,,,,Documentation,,,,0,,,,"The download pages currently link to the git code repository.

Whilst the instructions show how to check out master or a particular release branch, this also gives access to the rest of the repo, i.e. to non-released code.

Links to code repos should only be published on pages intended for developers.",,lresende,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-06-28 13:14:38.0,,,,,,,,,,"0|i3vc3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskSetManager not updating successfulTaskDurations for old stage attempts,SPARK-24677,13168791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dzcxzl,dzcxzl,dzcxzl,28/Jun/18 07:46,20/Sep/18 17:19,13/Jul/23 08:45,18/Jul/18 18:29,2.3.1,,,,,,,,,2.2.3,2.3.3,2.4.0,,,Spark Core,,,,0,,,,"When introducing SPARK-23433 , maybe cause stop sparkcontext.
{code:java}
ERROR Utils: uncaught error in thread task-scheduler-speculation, stopping SparkContext
java.util.NoSuchElementException: MedianHeap is empty.
at org.apache.spark.util.collection.MedianHeap.median(MedianHeap.scala:83)
at org.apache.spark.scheduler.TaskSetManager.checkSpeculatableTasks(TaskSetManager.scala:968)
at org.apache.spark.scheduler.Pool$$anonfun$checkSpeculatableTasks$1.apply(Pool.scala:94)
at org.apache.spark.scheduler.Pool$$anonfun$checkSpeculatableTasks$1.apply(Pool.scala:93)
at scala.collection.Iterator$class.foreach(Iterator.scala:742)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at org.apache.spark.scheduler.Pool.checkSpeculatableTasks(Pool.scala:93)
at org.apache.spark.scheduler.Pool$$anonfun$checkSpeculatableTasks$1.apply(Pool.scala:94)
at org.apache.spark.scheduler.Pool$$anonfun$checkSpeculatableTasks$1.apply(Pool.scala:93)
{code}",,apachespark,dzcxzl,maropu,riza,roczei,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 18 18:21:56 UTC 2018,,,,,,,,,,"0|i3vbn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/18 07:48;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/21656;;;","18/Jul/18 18:21;tgraves;This is really that it isn't updating successfulTaskDurations. In this case one of the older stage attempts (that is a zombie) marked the task as successful but then the newest stage attempt checked to see if it needed to speculate;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project required data from parsed data when csvColumnPruning disabled,SPARK-24676,13168790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,28/Jun/18 07:38,16/Jul/18 03:22,13/Jul/23 08:45,16/Jul/18 03:22,2.3.1,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"I hit a bug below when parsing csv data;

{code}
./bin/spark-shell --conf spark.sql.csv.parser.columnPruning.enabled=false
scala> val dir = ""/tmp/spark-csv/csv""
scala> spark.range(10).selectExpr(""id % 2 AS p"", ""id"").write.mode(""overwrite"").partitionBy(""p"").csv(dir)
scala> spark.read.csv(dir).selectExpr(""sum(p)"").collect()
18/06/25 13:48:46 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 7)
java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to java.lang.Integer
        at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getInt(rows.scala:41)
        ...
{code}

",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 28 07:51:06 UTC 2018,,,,,,,,,,"0|i3vbmv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/18 07:51;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21657;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed table was not cleared of path after drop database cascade,SPARK-24669,13168711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Udbhav Agrawal,djiangxu,djiangxu,27/Jun/18 19:55,23/Mar/19 17:43,13/Jul/23 08:45,06/Mar/19 17:24,2.3.0,2.3.1,,,,,,,,2.3.4,2.4.1,3.0.0,,,SQL,,,,0,,,,"I can do the following in sequence
# Create a managed table using path options
# Drop the table via dropping the parent database cascade
# Re-create the database and table with a different path
# The new table shows data from the old path, not the new path

{code}
echo ""first"" > /tmp/first.csv
echo ""second"" > /tmp/second.csv
spark-shell
spark.version
res0: String = 2.3.0
spark.sql(""create database foo"")
spark.sql(""create table foo.first (id string) using csv options (path='/tmp/first.csv')"")
spark.table(""foo.first"").show()
+-----+
|   id|
+-----+
|first|
+-----+
spark.sql(""drop database foo cascade"")
spark.sql(""create database foo"")
spark.sql(""create table foo.first (id string) using csv options (path='/tmp/second.csv')"")
""note, the path is different now, pointing to second.csv, but still showing data from first file""
spark.table(""foo.first"").show()
+-----+
|   id|
+-----+
|first|
+-----+
""now, if I drop the table explicitly, instead of via dropping database cascade, then it will be the correct result""
spark.sql(""drop table foo.first"")
spark.sql(""create table foo.first (id string) using csv options (path='/tmp/second.csv')"")
spark.table(""foo.first"").show()
+------+
|    id|
+------+
|second|
+------+
{code}

Same sequence failed in 2.3.1 as well.",,codeatri,djiangxu,dongjoon,mgaido,Udbhav Agrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 06 17:34:25 UTC 2019,,,,,,,,,,"0|i3vb5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/19 11:44;Udbhav Agrawal;I will work on this;;;","06/Mar/19 17:24;dongjoon;This is resolved via https://github.com/apache/spark/pull/23905 . Thank you, [~Udbhav Agrawal]. I added you to Apache Spark Contributor group.;;;","06/Mar/19 17:34;Udbhav Agrawal;Thankyou [~dongjoon];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Word2Vec generate infinity vectors when numIterations are large,SPARK-24666,13168540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,zhongyu09,zhongyu09,27/Jun/18 07:20,20/Jan/20 01:24,13/Jul/23 08:45,06/Dec/19 00:33,2.3.1,2.4.4,,,,,,,,2.4.5,3.1.0,,,,ML,MLlib,,,0,,,,"We found that Word2Vec generate large absolute value vectors when numIterations are large, and if numIterations are large enough (>20), the vector's value many be *infinity(or -**infinity)***, resulting in useless vectors.

In normal situations, vectors values are mainly around -1.0~1.0 when numIterations = 1.

The bug is shown on spark 2.0.X, 2.1.X, 2.2.X, 2.3.X, 2.4.X

There are already issues report this bug: https://issues.apache.org/jira/browse/SPARK-5261 , but the bug fix works seems missing.

Other people's reports:

[https://stackoverflow.com/questions/49741956/infinity-vectors-in-spark-mllib-word2vec]

[http://apache-spark-user-list.1001560.n3.nabble.com/word2vec-outputs-Infinity-Infinity-vectors-with-increasing-iterations-td29020.html]

=======================================================

Here are the code to reproduce the issue. You can download title.akas.tsv from [https://datasets.imdbws.com/] and upload to hdfs.

 
{code:java}
import org.apache.spark.sql.SparkSession
import org.apache.spark.ml.feature.Word2Vec

case class Sentences(name: String, words: Array[String])

import spark.implicits._

// IMDB raw data title.akas.tsv download from https://datasets.imdbws.com/
val dataset = spark.read
  .option(""header"", ""true"").option(""sep"", ""\t"")
  .option(""quote"", """").option(""nullValue"", ""\\N"")
  .csv(""/tmp/word2vec/title.akas.tsv"")
  .filter(""region = 'US' or language = 'en'"")
  .select(""title"")
  .as[String]
  .map(s => Sentences(s, s.split(' ')))
  .persist()

println(""Training model..."")
val word2Vec = new Word2Vec()
  .setInputCol(""words"")
  .setOutputCol(""vector"")
  .setVectorSize(64)
  .setWindowSize(4)
  .setNumPartitions(50)
  .setMinCount(5)
  .setMaxIter(20)
val model = word2Vec.fit(dataset)

model.getVectors.show()
{code}
When set maxIter to 30, you will get the result.
{code:java}
scala> model.getVectors.show()
+-------------+--------------------+
|         word|              vector|
+-------------+--------------------+
|     Unspoken|[-Infinity,-Infin...|
|       Talent|[Infinity,-Infini...|
|    Hourglass|[1.09657520526310...|
|Nickelodeon's|[2.20436549446219...|
|      Priests|[-1.9625896848389...|
|    Religion:|[-3.8815759928213...|
|           Bu|[-7.9722236466752...|
|      Totoro:|[-4.1829056206528...|
|     Trouble,|[2.51985378203136...|
|       Hatter|[8.49108115961009...|
|          '79|[-5.4560309784650...|
|         Vile|[-1.2059769646379...|
|         9/11|[Infinity,-Infini...|
|      Santino|[6.30405421282099...|
|      Motives|[1.96207712570869...|
|          '13|[-1.7641987324084...|
|       Fierce|[-Infinity,Infini...|
|       Stover|[5.10057474120744...|
|          'It|[1.08629989605664...|
|        Butts|[Infinity,Infinit...|
+-------------+--------------------+
only showing top 20 rows
{code}
In this case, set maxIter to 20 may not generate Infinity but very large absolute values. It depends on the training data sample and other configurations.
{code:java}
scala> model.getVectors.show(2,false)
+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|word    |vector                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Unspoken|[-8.345756381631837E26,-4.521902763541592E26,-2.3382486258889084E27,-1.0244081299466769E27,-2.0078509112460803E27,-1.6760533100889865E27,-2.582670788770659E27,-3.38100521565687E26,1.7553847873565714E27,-1.170131062449021E27,-1.6565472801835883E27,-1.5594244347657445E27,-2.5150639513558596E26,1.949539129915606E27,-7.580918216717454E26,1.2361994783015613E27,-3.152053008864166E27,-8.185652662597534E26,-5.4443628225426E25,2.245579525466733E26,-1.97655047590181E27,2.8597275293150673E26,-1.1006336920210832E27,1.6166580407985987E27,1.5272882143409825E26,-1.0115330404529906E27,-1.8895683222101184E27,2.6156506156954E27,-1.698058504881491E27,-1.5132098806248563E27,3.7327358519511804E27,1.3356636582642166E27,2.3614379909704805E26,8.96912646624494E26,1.5518857669716535E27,-3.05221863964144E27,4.399680909202177E26,-2.607914789100649E27,-1.4080384994067242E27,2.7666078487221474E27,6.946950108699123E26,-1.1122679059344192E27,-2.3621557537823886E27,9.433206702172274E26,-2.3704690372536228E27,2.5086034219659006E27,2.0173186657484236E27,-1.8448836672357273E27,-1.5081404202054957E27,2.641836064055936E26,-5.613083015733733E26,-2.1296579720982533E26,-1.6550184140347592E27,-1.9152898718506886E27,1.25699596863538E27,-2.0774912070471012E27,-1.5454685136432914E27,-2.479843324641509E27,1.5560216745669318E27,-2.2176656540799786E27,-9.628781296451031E26,1.3663974096305426E27,1.6326327735924786E27,-1.9533865304335714E27]|
|Talent  |[1.3996313289146157E31,-2.216329024373106E31,1.0729251707928603E31,-4.007120754159977E31,-7.217488429248302E30,3.579654497535965E31,2.7979270365837212E31,4.333613174196825E31,3.2947832174019738E31,-1.770444782887265E31,-1.1996572271408077E31,1.9686960444755403E31,-5.211369239778517E31,4.559579301984929E31,8.789691017490939E30,-3.3896103915518896E31,-2.842517781869879E31,3.653230690058367E31,1.6690004323711066E31,-1.1803405268246773E31,4.577673536512265E31,3.9686553942166427E31,-2.0779652882517364E31,9.553626958941078E29,-1.1967228014988571E31,2.667234660143298E31,-5.082234231802067E29,-5.053934698852727E31,2.911363689445293E31,4.57440169967406E31,2.296044625777839E31,3.4719839372636273E31,-4.753091634806606E30,-2.2139650908254315E31,5.747913246328898E31,-4.027332301367786E31,-3.3981312029599884E30,-3.235915541756495E31,-3.690297564613571E31,3.6645060993927487E31,2.32138854666024E31,-4.79833731565554E31,2.4538652976104142E31,4.91394707312416E30,2.2888500664401483E31,8.433142525511996E30,-2.3447174299865074E31,-3.9894235308718024E31,1.6571656530599892E31,3.743449438983912E31,5.619889452742693E31,2.0932366809902723E31,-2.2306515916821173E30,-4.2788883664425833E30,-8.754273117753689E30,-3.8767150140313846E30,-3.7649840346087072E31,-3.604430948638639E31,5.083292737026576E31,2.92915351645125E31,5.971055806972711E31,1.4773152095869043E31,5.12252479772471E31,3.035571146004139E31]                     |
+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
only showing top 2 rows

{code}
 "," 2.0.X, 2.1.X, 2.2.X, 2.3.X, 2.4.X",holden,viirya,yhao2014,zhongyu09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 00:33:54 UTC 2019,,,,,,,,,,"0|i3va3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/18 08:32;zhongyu09;Anyone else?;;;","09/Jul/18 05:10;viirya;Is it possible you can provide an example dataset and code to reproduce this issue?;;;","11/Jul/18 10:30;zhongyu09;It is very easy to reproduce. We have about 600,000 words and 10,000,000 dataset. Just set numIterations = 20 and you will find it.;;;","15/Jul/19 14:48;zhongyu09;Not any update for one year?;;;","15/Aug/19 16:34;holden;[~zhongyu09]specific code & data which leads to repro can help.;;;","24/Aug/19 13:22;viirya;I tried to run word2vec with Quora Question Pairs dataset. Set max iteration 20, but can't reproduce this.
;;;","23/Oct/19 07:26;yhao2014;I also get this question, and my spark version is 2.1.0. I used 1000w record for train and words size is about 100w. When the numIterations>10, the vectors generated contain *infinity* and *NaN*.;;;","20/Nov/19 05:25;zhongyu09;Hi [~viirya] and [~holden], I put data and code to reproduce this issues.;;;","20/Nov/19 05:28;viirya;[~zhongyu09] Thanks! I will look into this and see if I can reproduce it.;;;","06/Dec/19 00:33;viirya;Issue resolved by pull request 26722
[https://github.com/apache/spark/pull/26722];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test: StreamingContextSuite ""stop slow receiver gracefully""",SPARK-24663,13168505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,vanzin,vanzin,27/Jun/18 00:05,14/Sep/19 21:36,13/Jul/23 08:45,11/Sep/19 20:32,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,,2.4.5,3.0.0,,,,Tests,,,,0,,,,"This is another test that sometimes fails on our build machines, although I can't find failures on the riselab jenkins servers. Failure looks like:

{noformat}
org.scalatest.exceptions.TestFailedException: 0 was not greater than 0
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
      at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$24.apply$mcV$sp(StreamingContextSuite.scala:356)
      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$24.apply(StreamingContextSuite.scala:335)
      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$24.apply(StreamingContextSuite.scala:335)
{noformat}

The test fails in about 2s, while a successful run generally takes 15s. Looking at the logs, the receiver hasn't even started when things fail, which points at a race during test initialization.",,dongjoon,kabhwan,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 14 21:36:37 UTC 2019,,,,,,,,,,"0|i3v9vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/19 22:03;dongjoon;I hit the same issue in Riselab Jenkins today.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/6134/;;;","27/Aug/19 10:18;kabhwan;Hit this again.

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/109802/testReport]

 ;;;","11/Sep/19 20:32;vanzin;Issue resolved by pull request 25725
[https://github.com/apache/spark/pull/25725];;;","14/Sep/19 21:36;dongjoon;This is backported to branch-2.4 via https://github.com/apache/spark/commit/637a6c2750be8d4f42b1fd11c4cca8d0067e80d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHS is not showing properly errors when downloading logs,SPARK-24660,13168362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mgaido,mgaido,26/Jun/18 14:00,27/Jun/18 21:26,13/Jul/23 08:45,27/Jun/18 21:26,2.3.1,,,,,,,,,2.4.0,,,,,Web UI,,,,0,,,,"The History Server is not showing properly errors which happen when trying to download logs. In particular, when downloading logs for which the user is not authorized, the user sees a File not found error, instead of the unauthorized response.

Similarly, trying to download logs from a non-existing application returns a server error, instead of a 404 message.",,apachespark,mgaido,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 27 21:26:36 UTC 2018,,,,,,,,,,"0|i3v90f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/18 14:16;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21644;;;","27/Jun/18 21:26;vanzin;Issue resolved by pull request 21644
[https://github.com/apache/spark/pull/21644];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericArrayData.equals should respect element type differences,SPARK-24659,13168324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,26/Jun/18 11:01,27/Jun/18 02:28,13/Jul/23 08:45,27/Jun/18 02:28,2.3.0,2.3.1,2.4.0,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Right now, Spark SQL's {{GenericArrayData.equals}} doesn't always respect element type differences, due to a caveat in Scala's {{==}} operator.

e.g. {{new GenericArrayData(Array[Int](123)).equals(new GenericArrayData(Array[Long](123L)))}} currently returns true. But that's against the semantics of Spark SQL's array type, where {{array<int>}} and {{array<long>}} are considered to be incompatible types and thus should never be equal.

This ticket proposes to fix the implementation of {{GenericArrayData.equals}} so that it's more aligned to Spark SQL's array type semantics.",,apachespark,cloud_fan,maropu,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 27 02:28:17 UTC 2018,,,,,,,,,,"0|i3v8rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/18 11:09;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/21643;;;","27/Jun/18 02:28;cloud_fan;Issue resolved by pull request 21643
[https://github.com/apache/spark/pull/21643];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test ""JoinSuite.test SortMergeJoin (with spill)""",SPARK-24653,13168155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,25/Jun/18 21:17,12/Dec/22 18:10,13/Jul/23 08:45,01/Aug/18 07:48,2.4.0,,,,,,,,,2.4.0,,,,,Tests,,,,0,,,,"We've run into failures in this test in our internal jobs a few times. They look like this:

{noformat}
java.lang.AssertionError: assertion failed: expected full outer join to not spill, but did
      at scala.Predef$.assert(Predef.scala:170)
      at org.apache.spark.TestUtils$.assertNotSpilled(TestUtils.scala:189)
      at org.apache.spark.sql.JoinSuite$$anonfun$23$$anonfun$apply$mcV$sp$16.apply$mcV$sp(JoinSuite.scala:734)
      at org.apache.spark.sql.test.SQLTestUtils$class.withSQLConf(SQLTestUtils.scala:108)
{noformat}

I looked on the riselab jenkins and couldn't find a failure, so filing with a low priority.

I did notice a possible race in the code that could explain the failure. Will send a PR.",,apachespark,kiszk,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 01 07:48:35 UTC 2018,,,,,,,,,,"0|i3v7qn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/18 16:08;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21639;;;","01/Aug/18 07:48;gurwls223;Issue resolved by pull request 21639
[https://github.com/apache/spark/pull/21639];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLMetrics counters are not thread safe,SPARK-24648,13168030,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dbkerkela,dbkerkela,dbkerkela,25/Jun/18 12:29,25/Jun/18 21:43,13/Jul/23 08:45,25/Jun/18 21:43,2.3.1,,,,,,,,,2.4.0,,,,,Project Infra,,,,0,,,,"The += operator is not atomic, so for broadcast hash joins there have been discrepancies observed.",,apachespark,dbkerkela,mgaido,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 12:56:10 UTC 2018,,,,,,,,,,"0|i3v6z3:",9223372036854775807,,,,,juliuszsompolski,,,,,,,,,,,,,,,,,,"25/Jun/18 12:30;dbkerkela;I have a PR prepared for this.;;;","25/Jun/18 12:56;apachespark;User 'dbkerkela' has created a pull request for this issue:
https://github.com/apache/spark/pull/21634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip parsing when csvColumnPruning enabled and partitions scanned only,SPARK-24645,13167955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,25/Jun/18 04:17,12/Dec/22 18:10,13/Jul/23 08:45,28/Jun/18 01:20,2.3.1,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"I hit the bug below when parsing csv data;
{code:java}
scala> val dir = ""/tmp/spark-csv/csv""
scala> spark.range(10).selectExpr(""id % 2 AS p"", ""id"").write.mode(""overwrite"").partitionBy(""p"").csv(dir)
scala> spark.read.csv(dir).selectExpr(""sum(p)"").collect()
18/06/25 13:12:51 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5)
java.lang.NullPointerException
at org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(UnivocityParser.scala:197) 
at org.apache.spark.sql.execution.datasources.csv.UnivocityParser.parse(UnivocityParser.scala:190)
at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$5.apply(UnivocityParser.scala:309)
at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$5.apply(UnivocityParser.scala:309)
at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:61)
...

{code}
 ",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 28 01:20:23 UTC 2018,,,,,,,,,,"0|i3v6if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/18 04:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21631;;;","28/Jun/18 01:20;gurwls223;Issue resolved by pull request 21631
[https://github.com/apache/spark/pull/21631];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
arrays_zip function's code generator splits input processing incorrectly,SPARK-24633,13167785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,bersprockets,bersprockets,23/Jun/18 01:34,25/Jun/18 15:45,13/Jul/23 08:45,25/Jun/18 15:44,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"This works:
{noformat}
scala> val df = spark.read.parquet(""many_arrays_per_row"")
df: org.apache.spark.sql.DataFrame = [k0: array<bigint>, k1: array<bigint> ... 98 more fields]
scala> df.selectExpr(""arrays_zip(k0, k1, k2)"").show(truncate=false)
+----------------------------------------+
|arrays_zip(k0, k1, k2)                  |
+----------------------------------------+
|[[6583, 1312, 7460], [668, 1626, 4129]] |
|[[5415, 5251, 1514], [1631, 2224, 2553]]|
+----------------------------------------+
{noformat}
If I add one more array to the parameter list, I get this:
{noformat}
scala> df.selectExpr(""arrays_zip(k0, k1, k2, k3)"").show(truncate=false)
18/06/22 18:06:41 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 92, Column 35: Unknown variable or type ""scan_row_0""
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 92, Column 35: Unknown variable or type ""scan_row_0""
        at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11821)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6521)
        at org.codehaus.janino.UnitCompiler.access$13100(UnitCompiler.java:212)
        at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:6133)
.. much exception trace...
18/06/22 18:06:41 WARN WholeStageCodegenExec: Whole-stage codegen disabled for plan (id=1):
 *(1) LocalLimit 21
+- *(1) Project [cast(arrays_zip(k0#375, k1#376, k2#387, k3#398) as string) AS arrays_zip(k0, k1, k2, k3)#619]
   +- *(1) FileScan parquet [k0#375,k1#376,k2#387,k3#398] Batched: false, Format: Parquet, Location: InMemoryFileIndex[file:/Users/brobbins/github/spark_upstream/many_arrays_per_row], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<k0:array<bigint>,k1:array<bigint>,k2:array<bigint>,k3:array<bigint>>

+----------------------------------------------------+
|arrays_zip(k0, k1, k2, k3)                          |
+----------------------------------------------------+
|[[6583, 1312, 7460, 3712], [668, 1626, 4129, 2815]] |
|[[5415, 5251, 1514, 1580], [1631, 2224, 2553, 7555]]|
+----------------------------------------------------+
{noformat}
I still got the answer!

I add a 5th parameter:
{noformat}
scala> df.selectExpr(""arrays_zip(k0, k1, k2, k3, k4)"").show(truncate=false)
18/06/22 18:07:53 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 97, Column 35: Unknown variable or type ""scan_row_0""
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 97, Column 35: Unknown variable or type ""scan_row_0""
        at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11821)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6521)
        at org.codehaus.janino.UnitCompiler.access$13100(UnitCompiler.java:212)
        at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:6133)
        at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:6130)
        at org.codehaus.janino.Java$Package.accept(Java.java:4077)
.. much exception trace...
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 73, Column 21: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 73, Column 21: Unknown variable or type ""i""
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scal\
a:1361)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1423)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1420)
  at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
  at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
  ... 31 more
scala> 
{noformat}
This time, no result.

It looks like the generated code is expecting the input row to be in a parameter (either i or scan_row_x), but that parameter is not passed to the input handler function (see lines 069 and 073)
{noformat}
/* 069 */   private int getValuesAndCardinalities_0_1(ArrayData[] arrVals_0, int biggestCardinality_0) {
/* 070 */
/* 071 */
/* 072 */     if (biggestCardinality_0 != -1) {
/* 073 */       boolean isNull_6 = i.isNullAt(4);
/* 074 */       ArrayData value_6 = isNull_6 ?
/* 075 */       null : (i.getArray(4));
/* 076 */       if (!isNull_6) {
/* 077 */         arrVals_0[4] = value_6;
/* 078 */         biggestCardinality_0 = Math.max(biggestCardinality_0, value_6.numElements());
/* 079 */       } else {
/* 080 */         biggestCardinality_0 = -1;
/* 081 */       }
/* 082 */     }
/* 083 */
/* 084 */     return biggestCardinality_0;
/* 085 */
/* 086 */   }
{noformat}
Here's the scan_row_0 case:
{noformat}
/* 095 */   private int project_getValuesAndCardinalities_0_1(ArrayData[] project_arrVals_0, int project_biggestCardinality_0) {
/* 096 */     if (project_biggestCardinality_0 != -1) {
/* 097 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 098 */       ArrayData scan_value_3 = scan_isNull_3 ?
/* 099 */       null : (scan_row_0.getArray(3));
/* 100 */       if (!scan_isNull_3) {
/* 101 */         project_arrVals_0[3] = scan_value_3;
/* 102 */         project_biggestCardinality_0 = Math.max(project_biggestCardinality_0, scan_value_3.numElements());
/* 103 */       } else {
/* 104 */         project_biggestCardinality_0 = -1;
/* 105 */       }
/* 106 */     }
{noformat}
I am marking this as minor since this function is new and not in a released version of Spark.

cc [~DylanGuedes]",Mac OS High Sierra,apachespark,bersprockets,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 15:44:58 UTC 2018,,,,,,,,,,"0|i3v5ov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/18 12:22;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21621;;;","25/Jun/18 15:44;cloud_fan;Issue resolved by pull request 21621
[https://github.com/apache/spark/pull/21621];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebUI - application 'name' urls point to http instead of https (even when ssl enabled),SPARK-24621,13167396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,toopt4,toopt4,21/Jun/18 13:26,11/Mar/19 00:31,13/Jul/23 08:45,11/Mar/19 00:28,2.3.1,,,,,,,,,3.0.0,,,,,Web UI,,,,0,,,,"See attached

ApplicationID correctly points to DNS url
but Name points to IP address

Update: I found setting SPARK_PUBLIC_DNS to DNS hostname will make Name point to DNS. BUT it will use http instead of https!",,apachespark,gsomogyi,mgaido,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2750,,,,,,,,,,,"21/Jun/18 13:27;toopt4;spark_master-one-app.png;https://issues.apache.org/jira/secure/attachment/12928630/spark_master-one-app.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 11 00:28:53 UTC 2019,,,,,,,,,,"0|i3v3an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/18 06:22;toopt4;[https://github.com/apache/spark/pull/21514/commits] 

 

[core/src/main/scala/org/apache/spark/deploy/master/Master.scala|https://github.com/apache/spark#diff-29dffdccd5a7f4c8b496c293e87c8668]

 

val SSL_ENABLED = conf.getBoolean(""spark.ssl.enabled"", false)
 var uriScheme = ""http://""
 if (SSL_ENABLED)

{ uriScheme = ""https://"" }

masterWebUiUrl = uriScheme + masterPublicAddress + "":"" + webUi.boundPort
 //masterWebUiUrl = ""http://"" + masterPublicAddress + "":"" + webUi.boundPort

 

 ;;;","30/Jun/18 06:24;apachespark;User 'tooptoop4' has created a pull request for this issue:
https://github.com/apache/spark/pull/21514;;;","15/Feb/19 23:27;toopt4;:(;;;","06/Mar/19 12:39;gsomogyi;I've analyzed through this issue with the latest master branch and here are my findings:
* Steps what I've done: https://gist.github.com/gaborgsomogyi/03bade548902f74cac7d0962f76c76f1
* Secure master/worker/spark-shell connection works fine
* On the spark master webpage ""Workers"" section -> ""Worker Id"" field is pointing to an URL where scheme not added (so http) and port is 8080 (http port)
* On the spark master webpage ""Running Applications"" section -> ""Name"" field is pointing to an URL where scheme not added (so http) and port is 4040 (http port)

In both cases when one clicks on the mentioned unsecure links the request forwarded to the secure page.
Please see the functionality [here|https://github.com/apache/spark/blob/3fcbc7fb9f7b58b040a85289a82cf551d51bac37/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala#L328].
From my perspective it's more like a cosmetic change than a heavy bugfix.

[~toopt4] I think the commented fix is just bad because boundPort is pointing to the http port but in secure case securePort has to be used.
;;;","11/Mar/19 00:28;srowen;Issue resolved by pull request 23991
[https://github.com/apache/spark/pull/23991];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache with UDF could not be matched with subsequent dependent caches,SPARK-24613,13167278,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,20/Jun/18 23:16,27/Jun/18 20:23,13/Jul/23 08:45,21/Jun/18 18:48,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"When caching a query, we generate its execution plan from the query's logical plan. However, the logical plan we get from the Dataset has already been analyzed, and when we try the get the execution plan, this already analyzed logical plan will be analyzed again in the new QueryExecution object, and unfortunately some rules have side effects if applied multiple times, which in this case, is the {{HandleNullInputsForUDF}} rule. The re-analyzed plan now has an extra null-check and can't be matched against the same plan. The following test would fail since {{df2}}'s execution plan inside the CacheManager does not depend on {{df1}}.
{code:java}
test(""cache UDF result correctly 2"") {
  val expensiveUDF = udf({x: Int => Thread.sleep(10000); x})
  val df = spark.range(0, 10).toDF(""a"").withColumn(""b"", expensiveUDF($""a""))
  val df2 = df.agg(sum(df(""b"")))

  df.cache()
  df.count()
  df2.cache()

  // udf has been evaluated during caching, and thus should not be re-evaluated here
  failAfter(5 seconds) {
    df2.collect()
  }
}
{code}

While it might be worth re-visiting such analysis rules, we can make also fix the CacheManager to avoid these potential problems.",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 20 23:26:05 UTC 2018,,,,,,,,,,"0|i3v2kf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/18 23:26;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21602;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wholeTextFiles broken for small files,SPARK-24610,13167201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Dhruve Ashar,Dhruve Ashar,Dhruve Ashar,20/Jun/18 16:22,19/Oct/18 21:46,13/Jul/23 08:45,12/Jul/18 20:37,2.2.1,2.3.1,,,,,,,,2.4.0,,,,,Input/Output,,,,0,,,,"Spark is unable to read small files using the wholeTextFiles method when split size related configs are specified - either explicitly or if they are contained in other config files like hive-site.xml.

For small sized files, the computed maxSplitSize by `WholeTextFileInputFormat`  is way smaller than the default or commonly used split size of 64/128M and spark throws an exception while trying to read them.  

 

To reproduce the issue: 
{code:java}
$SPARK_HOME/bin/spark-shell --master yarn --deploy-mode client --conf ""spark.hadoop.mapreduce.input.fileinputformat.split.minsize.per.node=123456""

scala> sc.wholeTextFiles(""file:///etc/passwd"").count
java.io.IOException: Minimum split size pernode 123456 cannot be larger than maximum split size 9962
at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:200)
at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:50)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2096)
at org.apache.spark.rdd.RDD.count(RDD.scala:1158)
... 48 elided


// For hdfs
sc.wholeTextFiles(""smallFile"").count
java.io.IOException: Minimum split size pernode 123456 cannot be larger than maximum split size 15
at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:200)
at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:50)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2096)
at org.apache.spark.rdd.RDD.count(RDD.scala:1158)
... 48 elided{code}",,apachespark,Dhruve Ashar,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25753,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 20 18:42:05 UTC 2018,,,,,,,,,,"0|i3v23b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/18 18:42;apachespark;User 'dhruve' has created a pull request for this issue:
https://github.com/apache/spark/pull/21601;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in comments,SPARK-24603,13167112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,fokko,fokko,fokko,20/Jun/18 09:13,12/Dec/22 18:10,13/Jul/23 08:45,28/Jun/18 02:01,2.3.1,,,,,,,,,2.2.3,2.3.2,2.4.0,,,Spark Core,,,,0,,,,The findTightestCommonTypeOfTwo has been renamed to findTightestCommonType,,apachespark,dongjoon,fokko,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 02 19:17:55 UTC 2019,,,,,,,,,,"0|i3v1jj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/18 09:17;apachespark;User 'Fokko' has created a pull request for this issue:
https://github.com/apache/spark/pull/21597;;;","28/Jun/18 02:01;gurwls223;Issue resolved by pull request 21597
[https://github.com/apache/spark/pull/21597];;;","02/Jan/19 19:17;dongjoon;I removed 2.2.2 and added 2.2.3 since this wasn't released as a part of Apache Spark 2.2.2.
- https://dist.apache.org/repos/dist/release/spark/spark-2.2.2/spark-2.2.2.tgz;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Jackson version to 2.9.6,SPARK-24601,13167100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fokko,fokko,fokko,20/Jun/18 08:18,12/Dec/22 18:10,13/Jul/23 08:45,05/Oct/18 08:41,2.3.1,,,,,,,,,3.0.0,,,,,Spark Core,,,,2,release-notes,,,"The Jackson version is lacking behind, and therefore I have to add a lot of exclusions to the SBT files: 

```
Caused by: com.fasterxml.jackson.databind.JsonMappingException: Incompatible Jackson version: 2.9.5
	at com.fasterxml.jackson.module.scala.JacksonModule$class.setupModule(JacksonModule.scala:64)
	at com.fasterxml.jackson.module.scala.DefaultScalaModule.setupModule(DefaultScalaModule.scala:19)
	at com.fasterxml.jackson.databind.ObjectMapper.registerModule(ObjectMapper.java:751)
	at org.apache.spark.rdd.RDDOperationScope$.<init>(RDDOperationScope.scala:82)
	at org.apache.spark.rdd.RDDOperationScope$.<clinit>(RDDOperationScope.scala)
```",,apachespark,dongjoon,fokko,h_o,maropu,matijhs,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25455,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,Spark's Jackson dependency has been updated from 2.6.x to 2.9.x. User applications that inherit Spark's Jackson version should note that various Jackson behaviors changed between these releases.,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 17 16:06:00 UTC 2018,,,,,,,,,,"0|i3v1gv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/18 08:47;apachespark;User 'Fokko' has created a pull request for this issue:
https://github.com/apache/spark/pull/21596;;;","20/Jun/18 09:52;matijhs;Hi, nice to see this ticket as I saw a lot of times that transitive jackson jars from spark are causing troubles in other components.;;;","05/Oct/18 08:41;gurwls223;Fixed in https://github.com/apache/spark/pull/21596;;;","17/Oct/18 16:06;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce metrics for YARN executor allocation problems ,SPARK-24594,13166918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,19/Jun/18 12:17,17/May/20 18:14,13/Jul/23 08:45,24/Jul/18 01:33,2.4.0,,,,,,,,,2.4.0,,,,,Spark Core,YARN,,,0,,,,Within SPARK-16630 it come up to introduce metrics for  YARN executor allocation problems.,,apachespark,attilapiros,jerryshao,mgaido,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25277,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 24 01:33:42 UTC 2018,,,,,,,,,,"0|i3v0cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/18 12:44;attilapiros;I am working on this.;;;","25/Jun/18 14:23;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/21635;;;","24/Jul/18 01:33;jerryshao;Issue resolved by pull request 21635
[https://github.com/apache/spark/pull/21635];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutputCommitCoordinator may allow duplicate commits,SPARK-24589,13166828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,19/Jun/18 01:31,22/Jun/18 21:08,13/Jul/23 08:45,21/Jun/18 21:17,2.2.1,2.3.1,,,,,,,,2.1.3,2.2.2,2.3.2,2.4.0,,Spark Core,,,,0,,,,"This is a sibling bug to SPARK-24552. While investigating the source of that bug, it was found that currently the output committer allows duplicate commits when there are stage retries, and the task with the task attempt number (one in each stage that currently has running tasks) try to commit their output.

This can lead to duplicate data in the output.",,apachespark,irashid,misutoth,riza,tgraves,vanzin,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 21 21:48:05 UTC 2018,,,,,,,,,,"0|i3uzsf:",9223372036854775807,,,,,,,,,,,,,2.2.2,2.3.2,,,,,,,,,"19/Jun/18 01:32;vanzin;[~tgraves] fyi;;;","19/Jun/18 01:33;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21577;;;","21/Jun/18 21:48;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21607;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingSymmetricHashJoinExec should require HashClusteredPartitioning from children,SPARK-24588,13166819,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,cloud_fan,cloud_fan,19/Jun/18 00:41,21/Jun/18 22:40,13/Jul/23 08:45,21/Jun/18 22:40,2.3.0,2.3.1,,,,,,,,2.3.2,2.4.0,,,,Structured Streaming,,,,0,correctness,,,"In [https://github.com/apache/spark/pull/19080], we simplified the distribution/partitioning framework, and make all the join-like operators require HashClusteredDistribution from children. Unfortunately streaming join operator was missed.

This can cause wrong result. Think about
{code:java}
val input1 = MemoryStream[Int]
val input2 = MemoryStream[Int]

val df1 = input1.toDF.select('value as 'a, 'value * 2 as 'b)
val df2 = input2.toDF.select('value as 'a, 'value * 2 as 'b).repartition('b)
val joined = df1.join(df2, Seq(""a"", ""b"")).select('a)
{code}
The physical plan is
{code:java}
*(3) Project [a#5, b#6, c#7, c#14]
+- StreamingSymmetricHashJoin [a#5, b#6], [a#12, b#13], Inner, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = <unknown>, runId = 5a1ab77a-ed5c-4f0b-8bcb-fc5637152b97, opId = 0, ver = 0, numPartitions = 5], 0, state cleanup [ left = null, right = null ]
   :- Exchange hashpartitioning(a#5, b#6, 5)
   :  +- *(1) Project [value#1 AS a#5, (value#1 * 2) AS b#6, (value#1 * 3) AS c#7]
   :     +- StreamingRelation MemoryStream[value#1], [value#1]
   +- Exchange hashpartitioning(b#13, 5)
      +- *(2) Project [value#3 AS a#12, (value#3 * 3) AS b#13, (value#3 * 4) AS c#14]
         +- StreamingRelation MemoryStream[value#3], [value#3]
{code}
The left table is hash partitioned by a, b, while the right table is hash partitioned by b. This means, we may have a matching record that is in different partitions, which should be in the output but not.",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 19 00:49:05 UTC 2018,,,,,,,,,,"0|i3uzqf:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"19/Jun/18 00:49;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21587;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upcast should not allow casting from string to other types,SPARK-24586,13166810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,18/Jun/18 22:30,22/May/19 04:30,13/Jul/23 08:45,22/May/19 03:36,2.4.0,,,,,,,,,3.0.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 22 03:36:27 UTC 2019,,,,,,,,,,"0|i3uzof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/18 23:25;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21586;;;","22/May/19 03:36;cloud_fan;Issue resolved by pull request 21586
[https://github.com/apache/spark/pull/21586];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong schema type in InsertIntoDataSourceCommand,SPARK-24583,13166753,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,18/Jun/18 17:29,19/Jun/18 22:27,13/Jul/23 08:45,19/Jun/18 22:27,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"For a DataSource table, whose schema contains a field with ""nullable=false"", while user tries to insert a NULL value into this field, the input dataFrame will return an incorrect value or throw NullPointerException. And that's because, the schema nullability of the input relation has been overridden bluntly with the destination schema by the code below in {{InsertIntoDataSourceCommand}}:
{code:java}
  override def run(sparkSession: SparkSession): Seq[Row] = {
    val relation = logicalRelation.relation.asInstanceOf[InsertableRelation]
    val data = Dataset.ofRows(sparkSession, query)
    // Apply the schema of the existing table to the new data.
    val df = sparkSession.internalCreateDataFrame(data.queryExecution.toRdd, logicalRelation.schema)
    relation.insert(df, overwrite)

    // Re-cache all cached plans(including this relation itself, if it's cached) that refer to this
    // data source relation.
    sparkSession.sharedState.cacheManager.recacheByPlan(sparkSession, logicalRelation)

    Seq.empty[Row]
  }
{code}",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 20:56:06 UTC 2018,,,,,,,,,,"0|i3uzbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/18 20:56;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21585;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading remote cache block behavior changes and causes timeout issue,SPARK-24578,13166723,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,wbzhao,wbzhao,wbzhao,18/Jun/18 14:55,20/Sep/18 17:23,13/Jul/23 08:45,20/Jun/18 21:27,2.3.0,2.3.1,,,,,,,,2.3.2,2.4.0,,,,Spark Core,,,,0,,,,"After Spark 2.3, we observed lots of errors like the following in some of our production job
{code:java}
18/06/15 20:59:42 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=91672904003, chunkIndex=0}, buffer=org.apache.spark.storage.BlockManagerManagedBuffer@783a9324} to /172.22.18.7:60865; closing connection
java.io.IOException: Broken pipe
at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
at sun.nio.ch.IOUtil.write(IOUtil.java:65)
at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
at org.apache.spark.network.protocol.MessageWithHeader.writeNioBuffer(MessageWithHeader.java:156)
at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:142)
at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:123)
at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:355)
at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:224)
at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:382)
at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:934)
at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:362)
at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:901)
at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1321)
at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:768)
at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:749)
at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:768)
at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:749)
at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117)
at io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:768)
at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:749)
at io.netty.channel.DefaultChannelPipeline.flush(DefaultChannelPipeline.java:983)
at io.netty.channel.AbstractChannel.flush(AbstractChannel.java:248)
at io.netty.channel.nio.AbstractNioByteChannel$1.run(AbstractNioByteChannel.java:284)
at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
{code}
 

Here is a small reproducible for a small cluster of 2 executors (say host-1 and host-2) each with 8 cores. Here, the memory of driver and executors are not an import factor here as long as it is big enough, say 20G. 
{code:java}
val n = 100000000
val df0 = sc.parallelize(1 to n).toDF
val df = df0.withColumn(""x0"", rand()).withColumn(""x0"", rand()
).withColumn(""x1"", rand()
).withColumn(""x2"", rand()
).withColumn(""x3"", rand()
).withColumn(""x4"", rand()
).withColumn(""x5"", rand()
).withColumn(""x6"", rand()
).withColumn(""x7"", rand()
).withColumn(""x8"", rand()
).withColumn(""x9"", rand())

df.cache; df.count

(1 to 10).toArray.par.map { i => println(i); df.groupBy(""x1"").agg(count(""value"")).show() }
{code}
 

In the above example, we generate a random DataFrame of size around 7G; cache it and then perform a parallel DataFrame operations by using `array.par.map`. Because of the parallel computation, with high possibility, some task could be scheduled to a host-2 where it needs to read the cache block data from host-1. This follows the code path of [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L691] and then tries to transfer a big block (~ 500MB) of cache block from host-1 to host-2. Often, this big transfer makes the cluster suffer time out issue (it will retry 3 times, each with 120s timeout, and then do recompute to put the cache block into the local MemoryStore).

We couldn't to reproduce the same issue in Spark 2.2.1. From the log of Spark 2.2.1, we found that 
{code:java}
18/06/16 17:23:47 DEBUG BlockManager: Getting local block rdd_3_0 
18/06/16 17:23:47 TRACE BlockInfoManager: Task 0 trying to acquire read lock for rdd_3_0 
18/06/16 17:23:47 DEBUG BlockManager: Block rdd_3_0 was not found 
18/06/16 17:23:47 DEBUG BlockManager: Getting remote block rdd_3_0 
18/06/16 17:23:47 DEBUG BlockManager: Block rdd_3_0 not found 
18/06/16 17:23:47 TRACE BlockInfoManager: Task 0 trying to put rdd_3_0 
18/06/16 17:23:47 TRACE BlockInfoManager: Task 0 trying to acquire read lock for rdd_3_0 
18/06/16 17:23:47 TRACE BlockInfoManager: Task 0 trying to acquire write lock for rdd_3_0 
18/06/16 17:23:47 TRACE BlockInfoManager: Task 0 acquired write lock for rdd_3_0 
18/06/16 17:23:58 INFO MemoryStore: Block rdd_3_0 stored as values in memory (estimated size 538.2 MB, free 11.1 GB)
{code}
That is, when a task is scheduled to a host-2 where it needs to read the cache block rdd_3_0 data from host-1, the endpoint of `master.getLocations(..)` ( see [https://github.com/apache/spark/blob/v2.2.1/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L622]) reports a remote cache block is not found and triggered the recompute.  

-I believe this behavior change is introduced by this change set  [https://github.com/apache/spark/commit/e1960c3d6f380b0dfbba6ee5d8ac6da4bc29a698#diff-2b643ea78c1add0381754b1f47eec132]- 

We have two questions here
 # what is the right behavior, should we re-compute or should we transfer block from remote?
 # if we should transfer from remote, why the performance is so bad for cache block?

 ",,apachespark,attilapiros,curtishoward,icexelloss,irashid,java8964,maropu,mingmwang,riza,toopt4,vanzin,wbzhao,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 20 21:27:15 UTC 2018,,,,,,,,,,"0|i3uz53:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"18/Jun/18 15:14;wbzhao;An easier reproduciable cluster setting is 10 executors each with 2 cores and 15G memory.;;;","18/Jun/18 15:23;icexelloss;cc @gatorsmile [~cloud_fan]

We found this when switching from 2.2.1 to 2.3.0 in one of our applications. The implication is pretty bad - the time outs significantly hurt the performance (20s to several minutes for some jobs). This could affect other Spark 2.3 users too because it's pretty easy to reproduce.;;;","19/Jun/18 01:37;irashid;[~wbzhao] [~icexelloss] you're saying this is *without* touching the value of ""spark.maxRemoteBlockSizeFetchToMem"", right?  If you aren't turning on fetch-to-disk, then the behavior shouldn't change in 2.3.0 (though of course there may be bugs)


cc [~jerryshao] [~attilapiros];;;","19/Jun/18 03:03;wbzhao;[~irashid] We didn't touch ""spark.maxRemoteBlockSizeFetchToMem"". You are right. After digging more details, I don't think that commit is relevant,;;;","19/Jun/18 03:12;wbzhao;For now, we could reproduce this issue in completely different env and different distribution of Spark 2.3.0. We also observed that by setting spark.locality.wait to be a big number, e.g. 60s could help but this reduces the our system throughput significantly.;;;","19/Jun/18 03:24;irashid;btw to answer your initial questions:

{quote}
1. what is the right behavior, should we re-compute or should we transfer block from remote?
2. if we should transfer from remote, why the performance is so bad for cache block?
{quote}

Spark should try to fetch from the remote, and if that fails for some reason, it should fall back to recomputing.  so you should actually see a similar sequence of events in the logs in spark 2.3.0 as you do in your spark 2.2.1 snippet -- the difference being the remote fetch fails in 2.3.0, and so instead it does the recomputation.  The real issue here is why the remote fetches are failing.  Unfortunately there isn't a ton of info in that stack trace -- are there any other warning messages before that in the logs?

I can see how setting spark.locality.wait lets you workaround this, but its definitely not an ideal solution.

I wouldn't rule out that commit you mentioned as the issue -- its certainly possible.;;;","19/Jun/18 03:49;wbzhao;Hi [~irashid], many thanks for clarifying my questions. I tried to compare the logs between Spark 2.2.1 and 2.3.0 to see if there something interesting but out of luck so far. I added a few logging to benchmark how much time we need to transfer ~500MB data in the [https://github.com/apache/spark/blob/v2.3.0/common/network-common/src/main/java/org/apache/spark/network/protocol/MessageWithHeader.java#L142]. In Spark 2.2.1, it only took <= 2s, but in Spark 2.3.0, it is very slow and never got finish with 120s and thus timeout. 

Are you able to reproduce the issue?;;;","19/Jun/18 14:41;wbzhao;After digging more details, this commit  [https://github.com/apache/spark/commit/16612638f0539f197eb7deb1be2ec53fed60d707#diff-a1f697fbd4610dff4eb8ff848e8cea2a] came to my attention. I reverted this commit in our Spark distribution and found out the issue went away.

I also examined the log to verify that performance of [https://github.com/apache/spark/blob/v2.3.0/common/network-common/src/main/java/org/apache/spark/network/protocol/MessageWithHeader.java#L142] is back to normal. It is able to transfer the 500MB data in 2s~3s. 

However, it is still not quite clear why that commit is causing the issue.;;;","19/Jun/18 16:41;attilapiros;The copyByteBuf() along with transferTo() is called several times on a huge byteBuf. If we have a really huge chunk from small chunks we are re-merging the small chunks again and again by [https://github.com/apache/spark/blob/v2.3.0/common/network-common/src/main/java/org/apache/spark/network/protocol/MessageWithHeader.java#L140|https://github.com/apache/spark/blob/v2.3.0/common/network-common/src/main/java/org/apache/spark/network/protocol/MessageWithHeader.java#L140.] although we need only a very small part of it.

Is it possible the following line would help?
{code:java}
ByteBuffer buffer = buf.nioBuffer(0, Math.min(buf.readableBytes(), NIO_BUFFER_LIMIT));{code};;;","19/Jun/18 18:41;attilapiros;I have written a small test I know it is a bit naive but I still thing it shows us something:

[https://gist.github.com/attilapiros/730d67b62317d14f5fd0f6779adea245]

And the result is:
{noformat}
n = 500 
duration221 = 2 
duration221.new = 0 
duration230 = 5242 
duration230.new = 7
{noformat}
My guess the receiver timeouts and the sender is writing into a closed socket.;;;","19/Jun/18 19:18;irashid;I think [~attilapiros] may be right -- can you send a PR to fix?

We were able to reproduce the issue with the code you gave, and noticed that the errors occur exactly at the two minute timeout.  The receiver closes the connection because of the timeout, so then the sender just has a generic failure that the connection was closed.

Sender:
{noformat}
18/06/18 10:12:08 ERROR server.TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=879251518000, chunkIndex=0}, buffer=org.apache.spark.storage.BlockManagerManagedBuffer@19b0a640} to /xxxxxx:38710; closing connection
java.io.IOException: Connection reset by peer
{noformat}

Receiver:
{noformat}
18/06/18 10:12:08 ERROR server.TransportChannelHandler: Connection to xxx.com/xxxx:38765 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.
18/06/18 10:12:08 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from xxxxx.com/xxxxxxxx:38765 is closed
18/06/18 10:12:08 INFO shuffle.RetryingBlockFetcher: Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
{noformat}

At first I found it hard to believe this slow down would lead to the connection appearing totally quiet -- but i realized that there is actually a lot of concurrent activity going on, as all the executors are also sending and receiving many more blocks from each other. So if you're simultaneously dealing with a bunch of large blocks, and we put this big slow down inside the netty threads, this could lead to things appearing idle.

[~wbzhao] does this match the cases you see as well?;;;","19/Jun/18 19:21;wbzhao;Hi [~attilapiros], I guess what you suggest is 
{code:java}
ByteBuffer buffer = buf.nioBuffer(buf.readerIndex(), Math.min(buf.readableBytes(), NIO_BUFFER_LIMIT));{code}
Will report back how my test goes.;;;","19/Jun/18 19:23;wbzhao;[~irashid] Yes, that is exactly what I saw in our side.;;;","19/Jun/18 19:43;vanzin;Attila's suggestion looks good, but I wonder what caused this to show up in 2.3... the code he's changing has been there since 2.0, so my guess would be something changed in Netty (which was upgraded in 2.3). Anyway, just curious about the underlying cause of the change.;;;","19/Jun/18 19:54;attilapiros;[~wbzhao] Yes, you are right, readerIndex is the first (I just checked skipBytes), thanks.

[~irashid] Yes, I can create a PR soon.

[~vanzin] Netty was upgraded between 2.2.1 and 2.3.0: from 4.0.43.Final to 4.1.17.Final. Could it be?;;;","19/Jun/18 19:58;wbzhao;Hi [~vanzin].  the commit [https://github.com/apache/spark/commit/16612638f0539f197eb7deb1be2ec53fed60d707#diff-a1f697fbd4610dff4eb8ff848e8cea2a] is only included  since 2.3.0.  If I understand right, the root cause is that we don't do `consolidateIfNeeded` anymore for many small chunks which causes the buf.notBuffer() has bad performance in the case that we have to call `copyByteBuf()` many times. In my example, the many times = 500MB / 256 KB.

[~attilapiros] I applied 
{code:java}
ByteBuffer buffer = buf.nioBuffer(buf.readerIndex(), Math.min(buf.readableBytes(), NIO_BUFFER_LIMIT));{code}
in our spark distribution and ran through the test code in this jira. This works fine. 

 ;;;","19/Jun/18 20:03;vanzin;Ah, I see. That makes sense. (I actually took at look at netty changes and didn't find anything in this area, so was starting to wonder.);;;","19/Jun/18 20:05;wbzhao;[~attilapiros] if don't mind, I could create a PR for it :);;;","19/Jun/18 20:25;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/21592;;;","19/Jun/18 20:36;attilapiros;[~wbzhao] oh sorry I read your comment late, definitely without your help (pointing to the right commit caused the problem) it would took much much longer. If you like please create another PR, it is fine for me.;;;","19/Jun/18 20:44;apachespark;User 'WenboZhao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21593;;;","19/Jun/18 20:46;wbzhao;woop, [~attilapiros], sorry, I didn't know you have created a PR. ;;;","20/Jun/18 15:45;irashid;Given the severity of the issue and that its a regression in 2.3, I've set the target version and made it a blocker;;;","20/Jun/18 21:27;zsxwing;Issue resolved by pull request 21593
[https://github.com/apache/spark/pull/21593];;;",,,,,,,,,,,,,,,,,,,,
SBT Java checkstyle affecting the build,SPARK-24573,13166582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Jun/18 04:41,12/Dec/22 18:10,13/Jul/23 08:45,18/Jun/18 07:33,2.4.0,,,,,,,,,2.4.0,,,,,Project Infra,,,,0,,,,"Seems checkstyle affects the build in Jenkins. I can't reproduce in my local but it can only be reproduced in Jenkins.

When PR contains Java, this consistently fails on compilation as below:

{code}
[warn] /home/jenkins/workspace/SparkPullRequestBuilder/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocatorSuite.scala:160: non-variable type argument org.apache.spark.deploy.k8s.KubernetesExecutorSpecificConf in type org.apache.spark.deploy.k8s.KubernetesConf[org.apache.spark.deploy.k8s.KubernetesExecutorSpecificConf] is unchecked since it is eliminated by erasure
[warn]         if (!argument.isInstanceOf[KubernetesConf[KubernetesExecutorSpecificConf]]) {
[warn]                                   ^
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at sbt.ConcurrentRestrictions$$anon$4.take(ConcurrentRestrictions.scala:188)
	at sbt.Execute.next$1(Execute.scala:85)
	at sbt.Execute.processAll(Execute.scala:88)
	at sbt.Execute.runKeep(Execute.scala:68)
	at sbt.EvaluateTask$.liftedTree1$1(EvaluateTask.scala:359)
	at sbt.EvaluateTask$.run$1(EvaluateTask.scala:358)
	at sbt.EvaluateTask$.runTask(EvaluateTask.scala:378)
	at sbt.Aggregation$$anonfun$3.apply(Aggregation.scala:69)
	at sbt.Aggregation$$anonfun$3.apply(Aggregation.scala:67)
	at sbt.EvaluateTask$.withStreams(EvaluateTask.scala:314)
	at sbt.Aggregation$.timedRun(Aggregation.scala:67)
	at sbt.Aggregation$.runTasks(Aggregation.scala:76)
	at sbt.Aggregation$$anonfun$applyTasks$1.apply(Aggregation.scala:37)
	at sbt.Aggregation$$anonfun$applyTasks$1.apply(Aggregation.scala:36)
	at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:61)
	at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:61)
	at sbt.Aggregation$$anonfun$evaluatingParser$4$$anonfun$apply$5.apply(Aggregation.scala:158)
	at sbt.Aggregation$$anonfun$evaluatingParser$4$$anonfun$apply$5.apply(Aggregation.scala:157)
	at sbt.Act$$anonfun$sbt$Act$$actParser0$1$$anonfun$sbt$Act$$anonfun$$evaluate$1$1$$anonfun$apply$10.apply(Act.scala:253)
	at sbt.Act$$anonfun$sbt$Act$$actParser0$1$$anonfun$sbt$Act$$anonfun$$evaluate$1$1$$anonfun$apply$10.apply(Act.scala:250)
	at sbt.Command$.process(Command.scala:93)
	at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:96)
	at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:96)
	at sbt.State$$anon$1.runCmd$1(State.scala:183)
	at sbt.State$$anon$1.process(State.scala:187)
	at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:96)
	at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:96)
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)
	at sbt.MainLoop$.next(MainLoop.scala:96)
	at sbt.MainLoop$.run(MainLoop.scala:89)
	at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:68)
	at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:63)
	at sbt.Using.apply(Using.scala:24)
	at sbt.MainLoop$.runWithNewLog(MainLoop.scala:63)
	at sbt.MainLoop$.runAndClearLast(MainLoop.scala:46)
	at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:30)
	at sbt.MainLoop$.runLogged(MainLoop.scala:22)
	at sbt.StandardMain$.runManaged(Main.scala:61)
	at sbt.xMain.run(Main.scala:35)
	at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)
	at xsbt.boot.Launch$.withContextLoader(Launch.scala:128)
	at xsbt.boot.Launch$.run(Launch.scala:109)
	at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:35)
	at xsbt.boot.Launch$.launch(Launch.scala:117)
	at xsbt.boot.Launch$.apply(Launch.scala:18)
	at xsbt.boot.Boot$.runImpl(Boot.scala:41)
	at xsbt.boot.Boot$.main(Boot.scala:17)
	at xsbt.boot.Boot.main(Boot.scala)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.tools.nsc.symtab.classfile.ClassfileParser$ConstantPool.<init>(ClassfileParser.scala:170)
	at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader$classfileParser$.newConstantPool(SymbolLoaders.scala:317)
	at scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:139)
	at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader$$anonfun$doComplete$2.apply$mcV$sp(SymbolLoaders.scala:347)
	at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader$$anonfun$doComplete$2.apply(SymbolLoaders.scala:347)
	at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader$$anonfun$doComplete$2.apply(SymbolLoaders.scala:347)
	at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)
	at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:347)
	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)
	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:227)
	at scala.reflect.internal.Symbols$Symbol.typeParams(Symbols.scala:1733)
	at scala.reflect.internal.Types$class.isRawIfWithoutArgs(Types.scala:3756)
	at scala.reflect.internal.SymbolTable.isRawIfWithoutArgs(SymbolTable.scala:16)
	at scala.reflect.internal.tpe.TypeMaps$$anon$1.apply(TypeMaps.scala:328)
	at scala.reflect.internal.tpe.TypeMaps$TypeMap.applyToSymbolInfo(TypeMaps.scala:218)
	at scala.reflect.internal.tpe.TypeMaps$TypeMap.loop$1(TypeMaps.scala:227)
	at scala.reflect.internal.tpe.TypeMaps$TypeMap.noChangeToSymbols(TypeMaps.scala:229)
	at scala.reflect.internal.tpe.TypeMaps$TypeMap.mapOver(TypeMaps.scala:243)
	at scala.reflect.internal.tpe.TypeMaps$TypeMap.mapOver(TypeMaps.scala:128)
	at scala.reflect.internal.tpe.TypeMaps$$anon$1.apply(TypeMaps.scala:338)
	at scala.reflect.internal.tpe.TypeMaps$$anon$1.apply(TypeMaps.scala:325)
	at scala.reflect.internal.Symbols$Symbol.modifyInfo(Symbols.scala:1542)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$cookJavaRawInfo$2.apply(Symbols.scala:1690)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$cookJavaRawInfo$2.apply(Symbols.scala:1690)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at scala.reflect.internal.Symbols$Symbol.cookJavaRawInfo(Symbols.scala:1690)
	at scala.tools.nsc.typechecker.Infer$Inferencer.checkAccessible(Infer.scala:270)
	at scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$makeAccessible(Typers.scala:559)
	at scala.tools.nsc.typechecker.Typers$Typer$$anonfun$106.apply(Typers.scala:4757)
	at scala.tools.nsc.typechecker.Typers$Typer$$anonfun$106.apply(Typers.scala:4757)
[error] java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
[error] Use 'last' for the full log.
[error] running /home/jenkins/workspace/SparkPullRequestBuilder/build/sbt -Phadoop-2.6 -Pkubernetes -Phive-thriftserver -Pflume -Pkinesis-asl -Pyarn -Pkafka-0-8 -Phive -Pmesos test:package streaming-kafka-0-8-assembly/assembly streaming-flume-assembly/assembly streaming-kinesis-asl-assembly/assembly ; received return code 1
{code} 


See also https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/91980/console",,apachespark,Simon_poortman@icloud.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 07:33:16 UTC 2018,,,,,,,,,,"0|i3uy9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/18 04:48;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21579;;;","18/Jun/18 07:33;gurwls223;Issue resolved by pull request 21579
[https://github.com/apache/spark/pull/21579];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""eager execution"" for R shell, IDE",SPARK-24572,13166518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian555,felixcheung,felixcheung,16/Jun/18 07:09,25/Oct/18 06:43,13/Jul/23 08:45,25/Oct/18 06:43,2.4.0,,,,,,,,,3.0.0,,,,,SparkR,,,,0,,,,"like python in SPARK-24215

we could also have eager execution when SparkDataFrame is returned to the R shell",,adrian555,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 20 20:58:48 UTC 2018,,,,,,,,,,"0|i3uxvr:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"11/Sep/18 22:56;adrian555;[~felixcheung] are you thinking of using the same config introduced by the SPARK-24215 and doing something like below in red?
{code:java}
setMethod(""show"", ""SparkDataFrame"",
          function(object) {
            if (identical(sparkR.conf(""spark.sql.repl.eagerEval.enabled"", ""false"")[[1]], ""true"")) {
              return(showDF(object))
            }
            cols <- lapply(dtypes(object), function(l) {
              paste(l, collapse = "":"")
            })
            s <- paste(cols, collapse = "", "")
            cat(paste(class(object), ""["", s, ""]\n"", sep = """"))
          })
{code}
 ;;;","17/Sep/18 06:14;felixcheung;thanks! very close -  showDF doesn't return anything so we should refactor this slight as 

 
{code:java}
setMethod(""show"", ""SparkDataFrame"",
          function(object) {
            if (identical(sparkR.conf(""spark.sql.repl.eagerEval.enabled"", ""false"")[[1]], ""true"")) {
              showDF(object)
            } else {
              cols <- lapply(dtypes(object), function(l) {
                paste(l, collapse = "":"")
              })
              s <- paste(cols, collapse = "", "")
              cat(paste(class(object), ""["", s, ""]\n"", sep = """"))
            }
          })

{code}
 ;;;","18/Sep/18 23:14;adrian555;thanks [~felixcheung], raised PR https://github.com/apache/spark/pull/22455.;;;","20/Sep/18 20:58;apachespark;User 'adrian555' has created a pull request for this issue:
https://github.com/apache/spark/pull/22455;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Aggregator with output type Option[Boolean] creates column of type Row,SPARK-24569,13166479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,jconwell,jconwell,15/Jun/18 22:30,07/Jul/18 02:55,13/Jul/23 08:45,07/Jul/18 02:54,2.3.1,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Spark SQL Aggregator that returns an output column of Option[Boolean] creates a column of type StructField(<col_name>,StructType(StructField(value,BooleanType,true)),true) instead of StructField(<col_name>,BooleanType,true).  

In other words it puts a Row instance into the new column

 

Reproduction

 
{code:java}
class OptionBooleanAggregatorTest extends BaseFreeSpec {

  val ss: SparkSession = getSparkSession

  ""test option"" in {
    import ss.implicits._

    val df = List(
      Thing(""bob"", Some(true)),
      Thing(""bob"", Some(false)),
      Thing(""bob"", None))
      .toDF()

    val group = df
      .groupBy(""name"")
      .agg(OptionBooleanAggregator(""isGood"").toColumn.alias(""isGood""))
      .cache()

    assert(group.schema(""name"").dataType == StringType)

    //this will fail
    assert(group.schema(""isGood"").dataType == BooleanType)
  }
}

case class Thing(name: String, isGood: Option[Boolean])

case class OptionBooleanAggregator(colName: String) extends Aggregator[Row, Option[Boolean], Option[Boolean]] {

  override def zero: Option[Boolean] = Option.empty[Boolean]

  override def reduce(buffer: Option[Boolean], row: Row): Option[Boolean] = {
    val index = row.fieldIndex(colName)
    val value = if (row.isNullAt(index))
      Option.empty[Boolean]
    else
      Some(row.getBoolean(index))
    merge(buffer, value)
  }

  override def merge(b1: Option[Boolean], b2: Option[Boolean]): Option[Boolean] = {
    if ((b1.isDefined && b1.get) || (b2.isDefined && b2.get)) {
      Some(true)
    }
    else if (b1.isDefined) {
      b1
    }
    else
      b2
  }

  override def finish(reduction: Option[Boolean]): Option[Boolean] = reduction
  override def bufferEncoder: Encoder[Option[Boolean]] = OptionalBoolEncoder
  override def outputEncoder: Encoder[Option[Boolean]] = OptionalBoolEncoder

  def OptionalBoolEncoder: org.apache.spark.sql.Encoder[Option[Boolean]] = org.apache.spark.sql.catalyst.encoders.ExpressionEncoder()
}
{code}
 ",OSX,apachespark,cloud_fan,jconwell,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 07 02:54:47 UTC 2018,,,,,,,,,,"0|i3uxn3:",9223372036854775807,,,,,jconwell,,,,,,,,,,,,,,,,,,"22/Jun/18 06:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21611;;;","07/Jul/18 02:54;cloud_fan;Issue resolved by pull request 21611
[https://github.com/apache/spark/pull/21611];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow running PySpark shell without Hive,SPARK-24563,13166164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,icexelloss,icexelloss,icexelloss,14/Jun/18 18:03,14/Jun/18 20:16,13/Jul/23 08:45,14/Jun/18 20:16,2.4.0,,,,,,,,,2.4.0,,,,,PySpark,,,,0,,,,"A previous commit: 

[https://github.com/apache/spark/commit/b3417b731d4e323398a0d7ec6e86405f4464f4f9#diff-3b5463566251d5b09fd328738a9e9bc5]

disallows running PySpark shell without Hive.

Per discussion on mailing list, the behavior change is unintended.",,apachespark,icexelloss,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 14 20:16:39 UTC 2018,,,,,,,,,,"0|i3uvfb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"14/Jun/18 18:19;icexelloss;Will submit a PR soon;;;","14/Jun/18 19:12;apachespark;User 'icexelloss' has created a pull request for this issue:
https://github.com/apache/spark/pull/21569;;;","14/Jun/18 20:16;vanzin;Issue resolved by pull request 21569
[https://github.com/apache/spark/pull/21569];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReusedExchange should rewrite output partitioning also when child's partitioning is RangePartitioning,SPARK-24556,13166031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,14/Jun/18 07:56,19/Jun/18 17:53,13/Jul/23 08:45,19/Jun/18 17:53,2.3.2,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Currently, ReusedExchange would rewrite output partitioning if child's partitioning is HashPartitioning, but it does not do the same when child's partitioning is RangePartitioning, sometimes, it could introduce extra shuffle, see:
{code:java}
val df = Seq(1 -> ""a"", 3 -> ""c"", 2 -> ""b"").toDF(""i"", ""j"")
val df1 = df.as(""t1"")
val df2 = df.as(""t2"")
val t = df1.orderBy(""j"").join(df2.orderBy(""j""), $""t1.i"" === $""t2.i"", ""right"")
t.cache.orderBy($""t2.j"").explain
{code}
Before fix:
{code:sql}
== Physical Plan ==
*(1) Sort [j#14 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(j#14 ASC NULLS FIRST, 200)
   +- InMemoryTableScan [i#5, j#6, i#13, j#14]
         +- InMemoryRelation [i#5, j#6, i#13, j#14], CachedRDDBuilder...
               +- *(2) BroadcastHashJoin [i#5], [i#13], RightOuter, BuildLeft
                  :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as...
                  :  +- *(1) Sort [j#6 ASC NULLS FIRST], true, 0
                  :     +- Exchange rangepartitioning(j#6 ASC NULLS FIRST, 200)
                  :        +- LocalTableScan [i#5, j#6]
                  +- *(2) Sort [j#14 ASC NULLS FIRST], true, 0
                     +- ReusedExchange [i#13, j#14], Exchange rangepartitioning(j#6 ASC NULLS FIRST, 200)
{code}
Better plan should avoid ""Exchange rangepartitioning(j#14 ASC NULLS FIRST, 200)"", like:
{code:sql}
== Physical Plan ==
*(1) Sort [j#14 ASC NULLS FIRST], true, 0
+- InMemoryTableScan [i#5, j#6, i#13, j#14]
      +- InMemoryRelation [i#5, j#6, i#13, j#14], CachedRDDBuilder...
            +- *(2) BroadcastHashJoin [i#5], [i#13], RightOuter, BuildLeft
               :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
               :  +- *(1) Sort [j#6 ASC NULLS FIRST], true, 0
               :     +- Exchange rangepartitioning(j#6 ASC NULLS FIRST, 200)
               :        +- LocalTableScan [i#5, j#6]
               +- *(2) Sort [j#14 ASC NULLS FIRST], true, 0
                  +- ReusedExchange [i#13, j#14], Exchange rangepartitioning(j#6 ASC NULLS FIRST, 200)
{code}",,apachespark,cloud_fan,maropu,mgaido,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 19 17:53:15 UTC 2018,,,,,,,,,,"0|i3uulr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/18 08:50;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/21564;;;","19/Jun/18 17:53;cloud_fan;Issue resolved by pull request 21564
[https://github.com/apache/spark/pull/21564];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job UI redirect causing http 302 error,SPARK-24553,13165934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sjkallman,sjkallman,sjkallman,13/Jun/18 20:45,23/Nov/18 07:23,13/Jul/23 08:45,27/Jun/18 22:38,2.2.1,2.3.0,2.3.1,,,,,,,2.4.0,2.4.1,,,,Web UI,,,,0,,,,"When on spark UI port 4040 jobs or stages tab, the href links for the individual jobs or stages are missing a '/' before the '?id' this causes a redirect to the address with a '/' which is breaking the use of a reverse proxy

 

localhost:4040/jobs/job?id=2 --> localhost:4040/jobs/job/?id=2

localhost:4040/stages/stage?id=3&attempt=0 --> localhost:4040/stages/stage/?id=3&attempt=0

 

Updated with Pull Request --> https://github.com/apache/spark/pull/21600",,apachespark,dongjoon,sjkallman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 23 07:23:40 UTC 2018,,,,,,,,,,"0|i3uu07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/18 15:35;apachespark;User 'SJKallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/21600;;;","22/Nov/18 06:49;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/23116;;;","22/Nov/18 06:49;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/23116;;;","23/Nov/18 02:38;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/23121;;;","23/Nov/18 07:23;dongjoon;Followup PR lands on `master` and `branch-2.4`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task attempt numbers are reused when stages are retried,SPARK-24552,13165916,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,rdblue,rdblue,13/Jun/18 19:47,26/Jun/18 00:08,13/Jul/23 08:45,26/Jun/18 00:08,2.1.1,2.2.0,2.2.1,2.3.0,2.3.1,,,,,2.2.2,2.3.2,2.4.0,,,Spark Core,,,,0,,,,"When stages are retried due to shuffle failures, task attempt numbers are reused. This causes a correctness bug in the v2 data sources write path.

Data sources (both the original and v2) pass the task attempt to writers so that writers can use the attempt number to track and clean up data from failed or speculative attempts. In the v2 docs for DataWriterFactory, the attempt number's javadoc states that ""Implementations can use this attempt number to distinguish writers of different task attempts.""

When two attempts of a stage use the same (partition, attempt) pair, two tasks can create the same data and attempt to commit. The commit coordinator prevents both from committing and will abort the attempt that finishes last. When using the (partition, attempt) pair to track data, the aborted task may delete data associated with the (partition, attempt) pair. If that happens, the data for the task that committed is also deleted as well, which is a correctness bug.

For a concrete example, I have a data source that creates files in place named with {{part-<partition>-<attempt>-<uuid>.<format>}}. Because these files are written in place, both tasks create the same file and the one that is aborted deletes the file, leading to data corruption when the file is added to the table.",,apachespark,irashid,jiangxb1987,maropu,rdblue,riza,tgraves,vanzin,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 26 00:08:39 UTC 2018,,,,,,,,,,"0|i3utw7:",9223372036854775807,,,,,,,,,,,,,2.1.3,2.2.2,2.3.2,,,,,,,,"13/Jun/18 19:51;rdblue;cc [~vanzin], [~henryr], [~cloud_fan];;;","13/Jun/18 19:55;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21558;;;","13/Jun/18 21:15;irashid;I wouldn't call this a bug in the scheduler, though I agree its definitely confusing and not clearly documented at all.  I think this has always been the meaning of the attempt number.  It might be confusing for other cases to change its meaning now -- eg. its useful to know how many failed attempts you had within one taskset, and some code out there might be using max(attemptNumber) now as a proxy for that.

For sure we should improve the documentation.

[~markhamstra] [~tgraves] [~jiangxb1987] for some more opinions on the scheduler side.;;;","13/Jun/18 21:43;jiangxb1987;IIUC stageAttemptId + taskAttemptNumber shall probably define a unique task attempt, and it carries enough information to know how many failed attempts you had previously.;;;","14/Jun/18 14:00;tgraves;I agree, I don't think changing the attempt number at this point does much help and could cause confusion.  I would rather see something like this change if we do major reworking of the scheduler.;;;","14/Jun/18 14:02;tgraves;Note if this is a correctness bug and can cause data loss/corruption it needs to be a blocker, changed to blocker, if I am misunderstanding please update.;;;","14/Jun/18 14:12;tgraves;sorry just realized the v2 api is still marked experiment so downgrading to critical;;;","19/Jun/18 01:32;vanzin;I forked the output commiter issue into SPARK-24589 so that we have a separate record of each issue.;;;","19/Jun/18 01:37;vanzin;Added some target versions. We should take the chance to fix this now.;;;","21/Jun/18 14:28;tgraves;this is actually a problem with hadoop committers, v1 and v2;;;","21/Jun/18 14:47;tgraves;more details on hadoop committer side:

So I think the commit/delete thing is also an issue for existing v1 and hadoop committers as well. So this doesn't fully solve the problem. spark uses a file format like (HadoopMapReduceWriteConfigUtil/HadoopMapRedWriteConfigUtil):
{code:java}
{date}_{rddid}_{m/r}_{partitionid}_{task attempt number}
{code}
I believe the same fix as the v2 would work using the taskAttemptId instead of the attemptNumber.

In the case we have the stage failure and a second stage attempt the task attempt number could be the same and thus both tasks write to the same place. If one of them fails or is told not to commit it could delete the output which is being used by both.

Need to think through all the scenarios to make sure its covered.;;;","21/Jun/18 20:00;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21606;;;","22/Jun/18 20:08;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21615;;;","22/Jun/18 20:26;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21616;;;","26/Jun/18 00:08;vanzin;Giving credit to Ryan since he found the issue and provided the initial fix, although the final fix was a little more extensive.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaPairRDD to Dataset<Row> in SPARK generates ambiguous results,SPARK-24548,13165816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,jacksoncoutinho,jacksoncoutinho,13/Jun/18 11:39,18/Jun/18 18:02,13/Jul/23 08:45,18/Jun/18 18:01,2.3.0,,,,,,,,,2.4.0,,,,,Java API,SQL,,,0,,,,"I have data in below JavaPairRDD :
{quote}JavaPairRDD<String,Tuple2<String,String>> MY_RDD;
{quote}
I tried using below code:
{quote}Encoder<Tuple2<String, Tuple2<String,String>>> encoder2 =
Encoders.tuple(Encoders.STRING(), Encoders.tuple(Encoders.STRING(),Encoders.STRING()));
Dataset<Row> newDataSet = spark.createDataset(JavaPairRDD.toRDD(MY_RDD),encoder2).toDF(""value1"",""value2"");

newDataSet.printSchema();
{quote}
{{root}}
{{ |-- value1: string (nullable = true)}}
{{ |-- value2: struct (nullable = true)}}
{{ | |-- value: string (nullable = true)}}
{{ | |-- value: string (nullable = true)}}

But after creating a StackOverflow question (""https://stackoverflow.com/questions/50834145/javapairrdd-to-datasetrow-in-spark""), i got to know that values in tuple should have distinguish field names, where in this case its generating same name. Cause of this I cannot select specific column under value2.","Using Windows 10, on 64bit machine with 16G of ram.",apachespark,cloud_fan,jacksoncoutinho,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 18:01:47 UTC 2018,,,,,,,,,,"0|i3ut9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/18 09:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21576;;;","18/Jun/18 18:01;cloud_fan;Issue resolved by pull request 21576
[https://github.com/apache/spark/pull/21576];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Function hour not working as expected for hour 2 in PySpark,SPARK-24545,13165806,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,eblanco,eblanco,13/Jun/18 10:37,05/Apr/19 08:08,13/Jul/23 08:45,05/Apr/19 08:08,2.2.1,,,,,,,,,,,,,,Java API,,,,0,,,,"Hello,

I tried to get the hour out of a date and it works except if the hour is 2. It works well in Scala but in PySpark it shows hour 3 instead of hour 2.

Example:
{code:java}
from pyspark.sql.functions import *
 columns = [""id"",""date""]
 vals = [(4,""2016-03-27 02:00:00"")]
 df = sqlContext.createDataFrame(vals, columns)
 df.withColumn(""hours"", hour(col(""date""))).show(){code}
|id|date|hours|
|4|2016-03-27 2:00:00|3|

It works as expected for other hours.

Also, if you change the year or month apparently it works well. ",,eblanco,kiszk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/18 11:52;eblanco;image-2018-06-13-13-52-06-165.png;https://issues.apache.org/jira/secure/attachment/12927636/image-2018-06-13-13-52-06-165.png","13/Jun/18 11:53;eblanco;image-2018-06-13-13-53-21-185.png;https://issues.apache.org/jira/secure/attachment/12927638/image-2018-06-13-13-53-21-185.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 08:08:38 UTC 2019,,,,,,,,,,"0|i3ut7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/18 11:48;eblanco;Ok apparenlty this is due to a change of hour that was that day form 2 AM to 3 AM in Europe :);;;","13/Jun/18 11:53;eblanco;!image-2018-06-13-13-52-06-165.png!

In Scala this does not happens! In PySpark it does say hour 3 instead of 2:

 

!image-2018-06-13-13-53-21-185.png!;;;","05/Apr/19 08:08;eblanco;Field date was in String format so the change happened because it converted it to timestamp. Also, timezones of python and java where different.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query with nonsensical LIMIT hits AssertionError,SPARK-24536,13165702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,alex.behm,alex.behm,12/Jun/18 23:23,31/Jul/18 15:26,13/Jul/23 08:45,31/Jul/18 15:26,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,beginner,spree,,"SELECT COUNT(1) FROM t LIMIT CAST(NULL AS INT)

fails in the QueryPlanner with:
{code}
java.lang.AssertionError: assertion failed: No plan for GlobalLimit null
{code}

I think this issue should be caught earlier during semantic analysis.
",,alex.behm,apachespark,ksunitha,maropu,nsheth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 18 17:00:35 UTC 2018,,,,,,,,,,"0|i3uskn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/18 20:32;nsheth;I'd like to take a shot at this, if no one else is;;;","18/Jul/18 17:00;apachespark;User 'mauropalsgraaf' has created a pull request for this issue:
https://github.com/apache/spark/pull/21807;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveExternalCatalogVersionsSuite failing due to missing 2.2.0 version,SPARK-24531,13165596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,mgaido,mgaido,12/Jun/18 15:23,15/Jun/18 16:43,13/Jul/23 08:45,13/Jun/18 22:19,2.4.0,,,,,,,,,2.2.2,2.3.2,2.4.0,,,Tests,,,,0,,,,We have many build failures caused by HiveExternalCatalogVersionsSuite failing because Spark 2.2.0 is not present anymore in the mirrors.,,apachespark,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24532,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 19:31:04 UTC 2018,,,,,,,,,,"0|i3urx3:",9223372036854775807,,,,,,,,,,,,,2.2.2,2.3.2,,,,,,,,,"12/Jun/18 15:29;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21540;;;","12/Jun/18 19:31;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21543;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sphinx doesn't render autodoc_docstring_signature correctly (with Python 2?) and pyspark.ml docs are broken,SPARK-24530,13165595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,mengxr,mengxr,12/Jun/18 15:22,12/Dec/22 18:11,13/Jul/23 08:45,11/Jul/18 02:11,2.4.0,,,,,,,,,2.3.2,2.4.0,,,,ML,PySpark,,,1,,,,"I generated python docs from master locally using `make html`. However, the generated html doc doesn't render class docs correctly. I attached the screenshot from Spark 2.3 docs and master docs generated on my local. Not sure if this is because my local setup.

cc: [~dongjoon] Could you help verify?

 

The followings are our released doc status. Some recent docs seems to be broken.

*2.1.x*

(O) [https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(O) [https://spark.apache.org/docs/2.1.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(X) [https://spark.apache.org/docs/2.1.2/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]

*2.2.x*
(O) [https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(X) [https://spark.apache.org/docs/2.2.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]

*2.3.x*
(O) [https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(X) [https://spark.apache.org/docs/2.3.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]",,apachespark,dongjoon,jerryshao,mengxr,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/18 15:23;mengxr;Screen Shot 2018-06-12 at 8.23.18 AM.png;https://issues.apache.org/jira/secure/attachment/12927479/Screen+Shot+2018-06-12+at+8.23.18+AM.png","12/Jun/18 15:23;mengxr;Screen Shot 2018-06-12 at 8.23.29 AM.png;https://issues.apache.org/jira/secure/attachment/12927480/Screen+Shot+2018-06-12+at+8.23.29+AM.png","13/Jun/18 22:15;dongjoon;image-2018-06-13-15-15-51-025.png;https://issues.apache.org/jira/secure/attachment/12927730/image-2018-06-13-15-15-51-025.png","13/Jun/18 03:11;dongjin;pyspark-ml-doc-utuntu18.04-python2.7-sphinx-1.7.5.png;https://issues.apache.org/jira/secure/attachment/12927575/pyspark-ml-doc-utuntu18.04-python2.7-sphinx-1.7.5.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 01 03:38:47 UTC 2019,,,,,,,,,,"0|i3urwv:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"13/Jun/18 03:12;dongjin;In my case, it works correctly on current master (commit 9786ce6). My environment is Ubuntu 18.04, Python 2.7, and Sphinx 1.7.5.

!pyspark-ml-doc-utuntu18.04-python2.7-sphinx-1.7.5.png! ;;;","13/Jun/18 22:23;dongjoon;Hi, [~mengxr] .

I got the following locally. It generated correctly.  !image-2018-06-13-15-15-51-025.png!

However, as you pointed out, I found that the following status. Some docs are broken.

2.1.x

(O) [https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(O) [https://spark.apache.org/docs/2.1.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(X) [https://spark.apache.org/docs/2.1.2/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]

2.2.x
(O) [https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(X) [https://spark.apache.org/docs/2.2.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]

2.3.x
(O) [https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression]
(X) [https://spark.apache.org/docs/2.3.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression];;;","14/Jun/18 08:37;dongjin;OMG, I am sorry; I was misunderstanding. The documentation is also broken in my environment.;;;","24/Jun/18 06:39;gurwls223;Probably it's related with Sphinx's version just from my vague rough memory. I just felt like I better mention it here at least ...;;;","25/Jun/18 18:30;mengxr;[~dongjoon] [~hyukjin.kwon] Could you report your system, Python, and Sphinx version?

Mine is: macOS, python 2.7, sphinx 1.6.3.;;;","26/Jun/18 08:47;gurwls223;macOS, Python 2.7.14, Sphinx 1.4.1 shows:

{code}
class pyspark.ml.classification.LogisticRegression(*args, **kwargs)[source]
Logistic regression. This class supports multinomial logistic (softmax) and binomial logistic regression.
{code}
;;;","26/Jun/18 08:50;gurwls223;I have another computer: macOS, Python 2.7.14, Sphinx 1.7.2 shows:

{code}
class pyspark.ml.classification.LogisticRegression(*args, **kwargs)[source]
Logistic regression. This class supports multinomial logistic (softmax) and binomial logistic regression.
{code}

I think we need [~dongjoon]'s input.;;;","26/Jun/18 21:42;dongjoon;[~mengxr] and [~hyukjin.kwon]. My environment is macOS, *python 3*, Sphinx v1.6.3.  
{code}
~/s/p/docs:master$ make html
sphinx-build -b html -d _build/doctrees   . _build/html
Running Sphinx v1.6.3
making output directory...
...
{code}

According to the above reports, many combinations of Python 2.7 and Sphinx looks broken?;;;","27/Jun/18 03:11;mengxr;Confirmed that macOS, python 3, and Sphinx v1.6.6 can produce correct doc on my machine. I didn't find any reports on Sphinx github. So if we could make a minimal reproducible example, we should report the issue to Sphinx. On our side, we should update the release procedure doc to use Python 3 to generate docs. We should also update the official docs that are broken (2.1.2, 2.2.1, 2.3.1).

[~hyukjin.kwon] Do you have time to take this ticket? (feel free to say no if you are busy:)

cc: [~smilegator]

 ;;;","27/Jun/18 04:01;gurwls223;Will take a look on this weekends. Please go ahead if anyone finds some time till then :-).;;;","27/Jun/18 04:11;smilegator;[~hyukjin.kwon]  Thanks for helping this!;;;","28/Jun/18 17:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21659;;;","30/Jun/18 09:16;gurwls223;[~vanzin] and [~jerryshao] FYI.;;;","02/Jul/18 03:19;jerryshao;[~hyukjin.kwon], Spark 2.1.3 and 2.2.2 are on vote, can you please fix the issue and leave the comments in the related threads.;;;","02/Jul/18 04:25;gurwls223;Yea, will post a email to related threads, and try to deal with it very soon. [~mengxr], mind if I set the priority to Critical since we have a workaround to get through this anyway?;;;","02/Jul/18 18:06;gurwls223;related PRs are also open in https://github.com/apache/spark-website/pulls;;;","04/Jul/18 01:55;jerryshao;Hi [~hyukjin.kwon] what is the current status of this JIRA, do you have an ETA about it?;;;","04/Jul/18 02:22;gurwls223;There's workaround for this. To cut it short, Sphinx for Python 3 is required and installed e.g., {{sudo pip3 install sphinx}} but Sphinx for Python 2 should be removed first before installing Sphinx for Python 3 for sure if Sphinx for Python 2 is already installed.

Currently, whether it uses Sphinx for Pythom 3 or not can be manually checked by, {{cd python/docs && make clean html}}, checking if the keywords arguments are shown in the equaivelent link above before making the release documentation for Python API.;;;","04/Jul/18 02:24;gurwls223;[~mengxr], I lowered the priority to {{Critical}} for now since I believe this doesn't block the release although it's critical. Please revert my action if you think differently. I don't mind.;;;","11/Jul/18 02:11;gurwls223;Issue resolved by pull request 21659
[https://github.com/apache/spark/pull/21659];;;","21/Aug/18 10:58;gurwls223;Note to myself:

This ended up with misconfiguration and mistakes in docstrings within PySpark. We should configure differently but then it causes doc generation to be broken currently.
We should fix docs and configuration, and check the built documentation closely to fix this cleanly, and then should revert the current changes within {{Makefile}}

Also see https://github.com/sphinx-doc/sphinx/issues/5142#issuecomment-414634234;;;","02/Oct/18 07:14;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22607;;;","02/Oct/18 07:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22607;;;","01/May/19 03:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/24503;;;",,,,,,,,,,,,,,,,,,,,
Spaces in the build dir causes failures in the build/mvn script,SPARK-24526,13165490,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tleftwich,tleftwich,tleftwich,12/Jun/18 04:40,12/Dec/22 18:10,13/Jul/23 08:45,18/Jun/18 16:35,2.3.0,,,,,,,,,2.4.0,,,,,Build,,,,0,,,,"If you are running make-distribution in a path that contains a space in it the build/mvn script will fail:
{code:bash}
mkdir /tmp/test\ spaces
cd /tmp/test\ spaces
git clone https://github.com/apache/spark.git
cd spark
# Remove all mvn references in PATH so the script will download mvn to the local dir
./build/mvn -DskipTests clean package{code}

You will get the following errors:
{code:bash}
Using `mvn` from path: /tmp/test spaces/spark/build/apache-maven-3.3.9/bin/mvn
./build/mvn: line 157: /tmp/test: No such file or directory
{code}",,apachespark,tleftwich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 16:35:39 UTC 2018,,,,,,,,,,"0|i3ur9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/18 04:50;apachespark;User 'trystanleftwich' has created a pull request for this issue:
https://github.com/apache/spark/pull/21534;;;","18/Jun/18 16:35;gurwls223;Issue resolved by pull request 21534
[https://github.com/apache/spark/pull/21534];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Double braces in link,SPARK-24520,13165378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,fokko,fokko,fokko,11/Jun/18 17:43,11/Jun/18 22:12,13/Jul/23 08:45,11/Jun/18 22:12,2.3.0,,,,,,,,,2.4.0,,,,,Documentation,,,,0,,,,"Double braces in the markdown, which break the link",,apachespark,fokko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 11 22:12:43 UTC 2018,,,,,,,,,,"0|i3uqkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/18 17:45;apachespark;User 'Fokko' has created a pull request for this issue:
https://github.com/apache/spark/pull/21528;;;","11/Jun/18 22:12;srowen;Issue resolved by pull request 21528
[https://github.com/apache/spark/pull/21528];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark.ui.filters not applied to /sqlserver/ url,SPARK-24506,13165114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,toopt4,toopt4,09/Jun/18 13:20,21/Jun/18 12:49,13/Jul/23 08:45,12/Jun/18 23:53,2.3.0,,,,,,,,,2.2.2,2.3.2,2.4.0,,,Web UI,,,,0,,,,"With Spark.ui.filters applied, the web ui's for master/history/worker/storage/executors/stages.etc prompt for http auth but /sqlserver/ tab is not prompting for http auth",,apachespark,mgaido,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-19848,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 10 10:52:06 UTC 2018,,,,,,,,,,"0|i3uoy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/18 10:52;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException when trying to execute Union plan with Stream of children,SPARK-24500,13165005,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,bograd,bograd,08/Jun/18 19:25,27/Oct/20 11:01,13/Jul/23 08:45,13/Jun/18 14:14,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"To reproduce:
{code}
import org.apache.spark.sql.catalyst.plans.logical._
def range(i: Int) = Range(1, i, 1, 1)
val union = Union(Stream(range(3), range(5), range(7)))
spark.sessionState.planner.plan(union).next().execute()
{code}

produces

{code}
java.lang.UnsupportedOperationException
  at org.apache.spark.sql.execution.PlanLater.doExecute(SparkStrategies.scala:55)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
{code}

The SparkPlan looks like this:

{code}
:- Range (1, 3, step=1, splits=1)
:- PlanLater Range (1, 5, step=1, splits=Some(1))
+- PlanLater Range (1, 7, step=1, splits=Some(1))
{code}

So not all of it was planned (some PlanLater still in there).
This appears to be a longstanding issue.
I traced it to the use of var in TreeNode.
For example in mapChildren:
{code}
        case args: Traversable[_] => args.map {
          case arg: TreeNode[_] if containsChild(arg) =>
            val newChild = f(arg.asInstanceOf[BaseType])
            if (!(newChild fastEquals arg)) {
              changed = true
{code}

If args is a Stream then changed will never be set here, ultimately causing the method to return the original plan.",,apachespark,bograd,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33260,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 10:03:06 UTC 2018,,,,,,,,,,"0|i3uo9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/18 10:03;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/21539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeJoin with duplicate keys wrong results,SPARK-24495,13164904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,bograd,bograd,08/Jun/18 11:47,14/Jun/18 16:24,13/Jul/23 08:45,14/Jun/18 16:23,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,correctness,,,"To reproduce:
{code:java}
// the bug is in SortMergeJoin but the Shuffles are correct. with the default 200 it might split the data in such small partitions that the SortMergeJoin cannot return wrong results anymore
spark.conf.set(""spark.sql.shuffle.partitions"", ""1"")
// disable this, otherwise it would filter results before join, hiding the bug
spark.conf.set(""spark.sql.constraintPropagation.enabled"", ""false"")
sql(""select id as a1 from range(1000)"").createOrReplaceTempView(""t1"")
sql(""select id * 2 as b1, -id as b2 from range(1000)"").createOrReplaceTempView(""t2"")

spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")
sql(""""""select b1, a1, b2 FROM t1 INNER JOIN t2 ON b1 = a1 AND b2 = a1"""""").show
{code}
In the results, it's expected that all columns are equal (see join condition).

But the result is:
{code:java}
+---+---+---+
| b1| a1| b2|
+---+---+---+
|  0|  0|  0|
|  2|  2| -1|
|  4|  4| -2|
|  6|  6| -3|
|  8|  8| -4|
....
{code}
I traced it to {{EnsureRequirements.reorder}} which was introduced by [https://github.com/apache/spark/pull/16985] and [https://github.com/apache/spark/pull/20041]

It leads to an incorrect plan:
{code:java}
== Physical Plan ==
*(5) Project [b1#735672L, a1#735669L, b2#735673L]
+- *(5) SortMergeJoin [a1#735669L, a1#735669L], [b1#735672L, b1#735672L], Inner
   :- *(2) Sort [a1#735669L ASC NULLS FIRST, a1#735669L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(a1#735669L, a1#735669L, 1)
   :     +- *(1) Project [id#735670L AS a1#735669L]
   :        +- *(1) Range (0, 1000, step=1, splits=8)
   +- *(4) Sort [b1#735672L ASC NULLS FIRST, b2#735673L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(b1#735672L, b2#735673L, 1)
         +- *(3) Project [(id#735674L * 2) AS b1#735672L, -id#735674L AS b2#735673L]
            +- *(3) Range (0, 1000, step=1, splits=8)
{code}
The SortMergeJoin keys are wrong: key b2 is missing completely.",,apachespark,bograd,kevinyu98,maropu,mgaido,Qin Yao,smilegator,Teng Peng,toopt4,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 14 16:24:40 UTC 2018,,,,,,,,,,"0|i3unnj:",9223372036854775807,,,,,,,,,,,,,2.3.2,2.4.0,,,,,,,,,"11/Jun/18 18:08;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21529;;;","14/Jun/18 16:24;smilegator;We might need to release 2.3.2 since this is a serious bug. The impact is large. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyzer throws when generator is aliased multiple times,SPARK-24488,13164779,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bkrieger,bkrieger,bkrieger,07/Jun/18 20:08,20/Jul/18 22:46,13/Jul/23 08:45,20/Jul/18 22:46,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Currently, the Analyzer throws an exception if your try to nest a generator. However, it special cases generators ""nested"" in an alias, and allows that. If you try to alias a generator twice, it is not caught by the special case, so an exception is thrown:

 
{code:java}
scala> Seq((""a"", ""b""))
    .toDF(""col1"",""col2"")
    .select(functions.array('col1,'col2).as(""arr""))
    .select(functions.explode('arr).as(""first"").as(""second""))
    .collect()
org.apache.spark.sql.AnalysisException: Generators are not supported when it's nested in expressions, but got: explode(arr) AS `first`;
at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator$$anonfun$apply$23.applyOrElse(Analyzer.scala:1604)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator$$anonfun$apply$23.applyOrElse(Analyzer.scala:1601)
{code}
 

In reality, aliasing twice is fine, so we can fix this by trimming non top-level aliases.",,apachespark,bkrieger,maropu,mgaido,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 07 20:18:04 UTC 2018,,,,,,,,,,"0|i3umvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/18 20:17;bkrieger;CC [~cloud_fan] [~rxin@databricks.com];;;","07/Jun/18 20:17;bkrieger;Made https://github.com/apache/spark/pull/21508 to address this issue.;;;","07/Jun/18 20:18;apachespark;User 'bkrieger' has created a pull request for this issue:
https://github.com/apache/spark/pull/21508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slow performance reading ArrayType columns,SPARK-24486,13164700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,lucacanali,lucacanali,07/Jun/18 14:34,12/Mar/19 10:08,13/Jul/23 08:45,12/Mar/19 10:08,2.3.0,,,,,,,,,3.0.0,,,,,Spark Core,SQL,,,1,,,,"We have found an issue of slow performance in one of our applications when running on Spark 2.3.0 (the same workload does not have a performance issue on Spark 2.2.1). We suspect a regression in the area of handling columns of ArrayType. I have built a simplified test case showing a manifestation of the issue to help with troubleshooting:

 

 
{code:java}
// prepare test data
val stringListValues=Range(1,30000).mkString("","")
sql(s""select 1 as myid, Array($stringListValues) as myarray from range(20000)"").repartition(1).write.parquet(""file:///tmp/deleteme1"")

// run test
spark.read.parquet(""file:///tmp/deleteme1"").limit(1).show(){code}

Performance measurements:

 

On a desktop-size test system, the test runs in about 2 sec using Spark 2.2.1 (runtime goes down to subsecond in subsequent runs) and takes close to 20 sec on Spark 2.3.0

 

Additional drill-down using Spark task metrics data, show that in Spark 2.2.1 only 2 records are read by this workload, while on Spark 2.3.0 all rows in the file are read, which appears anomalous.

Example:
{code:java}
bin/spark-shell --master local[*] --driver-memory 2g --packages ch.cern.sparkmeasure:spark-measure_2.11:0.11
val stageMetrics = ch.cern.sparkmeasure.StageMetrics(spark) 
stageMetrics.runAndMeasure(spark.read.parquet(""file:///tmp/deleteme1"").limit(1).show())
{code}
 

 

Selected metrics from Spark 2.3.0 run:

 
{noformat}
elapsedTime => 17849 (18 s)
sum(numTasks) => 11
sum(recordsRead) => 20000
sum(bytesRead) => 1136448171 (1083.0 MB){noformat}
 

 

From Spark 2.2.1 run:

 
{noformat}
elapsedTime => 1329 (1 s)
sum(numTasks) => 2
sum(recordsRead) => 2
sum(bytesRead) => 269162610 (256.0 MB)
{noformat}
 

Note: Using Spark built from master (as I write this, June 7th 2018) shows the same behavior as found in Spark 2.3.0

 ",,kiszk,ksunitha,lucacanali,maropu,mgaido,riza,Teng Peng,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 12 10:08:10 UTC 2019,,,,,,,,,,"0|i3umef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/18 15:55;kiszk;Thank you for reporting a problem.
Could you please let us know which value is shown for each of three results in `sum(...)`?;;;","07/Jun/18 20:13;lucacanali;Thanks for your comment.

In all 3 cases (spark 2.2.1, 2.3.0 and latest version from master) I am using a simple test workload to investigate the issue, that is:
spark.read.parquet(""file:///tmp/deleteme1"").limit(1).show()
The output is simply the first row of the test table, that is an int value ""1"" and an array of 30000 int elements.

I'll be happy to provide additional info on the tests and workload. BTW, it should be straighforward to reproduce this issue in a test environemnt if you can spare the time.

 ;;;","22/Sep/18 15:57;yumwang;[~lucacanali]  May be caused by SPARK-23023. Cloud you use {{collect()}} to test you case. Below is my benchmark:

code:
{code:java}
val benchmark = new Benchmark(""read parquet"", 1, minNumIters = 10)
benchmark.addCase(""show"", 5) { _ =>
  spark.read.parquet(""file:///tmp/deleteme1"").limit(1).show()
}
benchmark.addCase(""collect"", 5) { _ =>
  spark.read.parquet(""file:///tmp/deleteme1"").limit(1).collect()
}
benchmark.run()
{code}
{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_151-b12 on Mac OS X 10.12.6
Intel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz

read parquet:                            Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
show                                        14578 / 15174          0.0 14578205056.0       1.0X
collect                                        252 /  271          0.0   251586336.0      57.9X
{noformat};;;","12/Mar/19 10:08;lucacanali;Thanks [~yumwang] for looking at this. Indeed I confirm that using collect instead of show is faster. In addition, testing on Spark master (March 12 2019) I see that show works fast there too (I have not yet looked at which PR fixed this).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Power Iteration Clustering is giving incorrect clustering results when there are mutiple leading eigen values.,SPARK-24484,13164655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,shahid,shahid,07/Jun/18 10:15,23/Jan/19 02:49,13/Jul/23 08:45,23/Jan/19 00:29,2.4.0,,,,,,,,,3.0.0,,,,,ML,MLlib,,,0,,,,"When there are multiple leading eigen values of the normalized affinity matrix, power iteration clustering gives incorrect results.

We should either give an error or warning to the user when PIC doesn't converges ( ie. 
when |\lambda_1/\lambda_2| = 1 )

{code:java}
test(""Fail to converge: Multiple leading eigen values"") {
    /*
         Graph:

         2
       /
     /
    1        3 - - 4

    Adjacency matrix:

      [(0, 1, 0, 0),
      (1, 0, 0, 0),
 A =  (0, 0, 0, 1),
       (0, 0, 1, 0)]

    */

    val data = Seq[(Long, Long, Double)](
      (1, 2, 1.0),
      (3, 4, 1.0)

    ).toDF(""src"", ""dst"", ""weight"")

    val result = new PowerIterationClustering()
      .setK(2)
      .setMaxIter(20)
      .setInitMode(""random"")
      .setWeightCol(""weight"")
      .assignClusters(data)
      .select('id, 'cluster)

    val predictions = Array.fill(2)(mutable.Set.empty[Long])
    result.collect().foreach {
      case Row(id: Long, cluster: Integer) => predictions(cluster) += id
    }

    assert(predictions.toSet == Set(Array(1, 2).toSet, Array(3, 4).toSet))
  }
 {code}",,apachespark,mgaido,shahid,Teng Peng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 23 00:29:36 UTC 2019,,,,,,,,,,"0|i3um4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/18 04:43;shahid;I have raised a PR, https://github.com/apache/spark/pull/21627;;;","25/Jun/18 04:44;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/21627;;;","23/Jan/19 00:29;srowen;Issue resolved by pull request 21627
[https://github.com/apache/spark/pull/21627];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.net.SocketTimeoutException: Read timed out under jets3t while running the Spark Structured Streaming,SPARK-24476,13164501,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,abharath9,abharath9,06/Jun/18 16:22,15/Jun/18 15:30,13/Jul/23 08:45,15/Jun/18 15:30,2.3.0,,,,,,,,,,,,,,Structured Streaming,,,,0,,,,"We are working on spark streaming application using spark structured streaming with checkpointing in s3. When we start the application, the application runs just fine for sometime  then it crashes with the error mentioned below. The amount of time it will run successfully varies from time to time, sometimes it will run for 2 days without any issues then crashes, sometimes it will crash after 4hrs/ 24hrs. 

Our streaming application joins(left and inner) multiple sources from kafka and also s3 and aurora database.

Can you please let us know how to solve this problem?
Is it possible to somehow tweak the SocketTimeout-Time? 
Here, I'm pasting the few line of complete exception log below. Also attached the complete exception to the issue.

*_Exception:_*

*_Caused by: java.net.SocketTimeoutException: Read timed out_*

        _at java.net.SocketInputStream.socketRead0(Native Method)_

        _at java.net.SocketInputStream.read(SocketInputStream.java:150)_

        _at java.net.SocketInputStream.read(SocketInputStream.java:121)_

        _at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)_

        _at sun.security.ssl.InputRecord.read(InputRecord.java:503)_

        _at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:954)_

        _at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1343)_

        _at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1371)_

        _at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1355)_

        _at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:553)_

        _at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:412)_

 ",,abharath9,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/18 16:23;abharath9;socket-timeout-exception;https://issues.apache.org/jira/secure/attachment/12926753/socket-timeout-exception",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 15 15:01:46 UTC 2018,,,,,,,,,,"0|i3ul67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/18 13:07;stevel@apache.org;Switch from s3n to the s3a connector, see if it goes away...if not look at the hadoop s3a documentation about tuning this stuff. ;;;","12/Jun/18 14:48;abharath9;[~stevel@apache.org] Our initial try was enable the speculative execution, reran the job and it is working fine. Then replaced s3n with s3a, reran the job. It is working fine too. Right now we are running two jobs simultaneously with these two different settings, nothing got failed. Do you recommend enabling speculative execution even after changing s3n from s3a?;;;","15/Jun/18 15:01;stevel@apache.org;* Use S3A, as S3n is unsupported and deleted from the recent versions of Hadoop. Nobody tests it either.
* Don't know about speculation. S3 in general isn't good here as the speculation code in (hadoop, spark, hive, ...) assumes that renames are fast and, for directories, atomic. You can get into serious trouble there. I haven't looked at what Spark Streaming's commit protocol is in any detail; still on my TODO list. 

My recommendation, stay with S3A, close this as cannot reproduce for now
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecimalType `adjustPrecisionScale` might fail when scale is negative,SPARK-24468,13164133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,birdstorm,birdstorm,05/Jun/18 09:21,09/Jun/18 01:53,13/Jul/23 08:45,09/Jun/18 01:53,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"Hi, I am using MySQL JDBC Driver along with Spark to do some sql queries.

When multiplying a LongType with a decimal in scientific notation, say
{code:java}
spark.sql(""select some_int * 2.34E10 from t""){code}
, decimal 2.34E10 will be treated as decimal(3,-8), and some_int will be casted as decimal(20,0).

 

So according to the rules in comments:
{code:java}
/*
 *   Operation    Result Precision                        Result Scale
 *   ------------------------------------------------------------------------
 *   e1 + e2      max(s1, s2) + max(p1-s1, p2-s2) + 1     max(s1, s2)
 *   e1 - e2      max(s1, s2) + max(p1-s1, p2-s2) + 1     max(s1, s2)
 *   e1 * e2      p1 + p2 + 1                             s1 + s2
 *   e1 / e2      p1 - s1 + s2 + max(6, s1 + p2 + 1)      max(6, s1 + p2 + 1)
 *   e1 % e2      min(p1-s1, p2-s2) + max(s1, s2)         max(s1, s2)
 *   e1 union e2  max(s1, s2) + max(p1-s1, p2-s2)         max(s1, s2)
*/
{code}
their multiplication will be decimal(3+20+1,-8+0) and thus fails the assert assumption (scale>=0) on DecimalType.scala:166.

 

My current workaround is to set spark.sql.decimalOperations.allowPrecisionLoss to false.

 ",,apachespark,birdstorm,cloud_fan,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 09 01:53:08 UTC 2018,,,,,,,,,,"0|i3uiwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/18 12:01;mgaido;Thanks for reporting this. I will submit soon a fix. Thanks.;;;","05/Jun/18 16:23;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21499;;;","09/Jun/18 01:53;cloud_fan;Issue resolved by pull request 21499
[https://github.com/apache/spark/pull/21499];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TextSocketMicroBatchReader no longer works with nc utility,SPARK-24466,13164046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,05/Jun/18 02:45,12/Dec/22 18:11,13/Jul/23 08:45,13/Jun/18 04:35,2.4.0,,,,,,,,,2.4.0,,,,,Structured Streaming,,,,0,,,,"While playing with Spark 2.4.0-SNAPSHOT, I found nc command exits before reading actual data so the query also exits with error.
 
The reason is due to launching temporary reader for reading schema, and closing reader, and re-opening reader. While reliable socket server should be able to handle this without any issue, nc command normally can't handle multiple connections and simply exits when closing temporary reader.
 
Given that socket source is expected to be used from examples on official document or some experiments, which we tend to simply use netcat, this is better to be treated as bug, though this is a kind of limitation on netcat.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 13 04:35:17 UTC 2018,,,,,,,,,,"0|i3uidb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"05/Jun/18 02:46;kabhwan;I'm working on this. Will provide the patch sooner.;;;","05/Jun/18 08:54;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/21497;;;","13/Jun/18 04:35;gurwls223;Issue resolved by pull request 21497
[https://github.com/apache/spark/pull/21497];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix error recovering from the failure in a no-data batch,SPARK-24453,13163557,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,01/Jun/18 23:01,13/Jul/18 05:26,13/Jul/23 08:45,05/Jun/18 08:08,2.4.0,,,,,,,,,2.4.0,,,,,Structured Streaming,,,,0,,,,"```
java.lang.AssertionError: assertion failed: Concurrent update to the log. Multiple streaming jobs detected for 159897
```

The error occurs when we are recovering from a failure in a no-data batch (say X) that has been planned (i.e. written to offset log) but not executed (i.e. not written to commit log). Upon recovery, the following sequence of events happen.

- `MicroBatchExecution.populateStartOffsets` sets `currentBatchId` to X. Since there was no data in the batch, the `availableOffsets` is same as `committedOffsets`, so `isNewDataAvailable` is false.
- When MicroBatchExecution.constructNextBatch is called, ideally it should immediately return true because the next batch has already been constructed. However, the check of whether the batch has been constructed was `if (isNewDataAvailable) return true`. Since the planned batch is a no-data batch, it escaped this check and proceeded to plan the same batch X once again. And if there is new data since the failure, it does plan a new batch, and try to write new offsets to the `offsetLog` as batchId X, and fail with the above error.

The correct solution is to check the offset log whether the currentBatchId is the latest or not.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 05 08:08:29 UTC 2018,,,,,,,,,,"0|i3ufcn:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"04/Jun/18 19:49;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/21491;;;","05/Jun/18 08:08;tdas;Issue resolved by pull request 21491
[https://github.com/apache/spark/pull/21491];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long = int*int or long = int+int may cause overflow.,SPARK-24452,13163514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,01/Jun/18 19:54,15/Jun/18 22:06,13/Jul/23 08:45,15/Jun/18 22:06,2.4.0,,,,,,,,,2.3.2,2.4.0,,,,Spark Core,SQL,,,0,,,,"The following assignment cause overflow in right hand side. As a result, the result may be negative.
{code:java}
long = int*int
long = int+int{code}
",,apachespark,cloud_fan,kiszk,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 15 22:06:43 UTC 2018,,,,,,,,,,"0|i3uf33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/18 20:37;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21481;;;","15/Jun/18 22:06;cloud_fan;Issue resolved by pull request 21481
[https://github.com/apache/spark/pull/21481];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Library path with special characters breaks Spark on YARN,SPARK-24446,13163287,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,31/May/18 21:22,17/May/20 18:14,13/Jul/23 08:45,27/Jun/18 17:58,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,YARN,,,0,,,,"When YARN runs the application's main command, it does it like this:

{code}
bash -c ""<your command goes here>""
{code}

The way Spark injects the library path into that command makes it look like this:

{code}
bash -c ""LD_LIBRARY_PATH=""/foo:/bar:/baz:$LD_LIBRARY_PATH"" <rest of the command>""
{code}

So that works kinda out of luck, because the concatenation of the strings creates a proper final command... except if you have something like a space or an ampersand in the library path, in which case all containers will fail with a cryptic message like the following:

{noformat}
WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Executor for container container_1475411358336_0010_01_000002 exited because of a YARN event (e.g., pre-emption) and not because of an error in the running job.
{noformat}

And no useful log output.
",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 31 22:42:05 UTC 2018,,,,,,,,,,"0|i3udov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/18 22:42;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21476;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update configuration definition for spark.blacklist.killBlacklistedExecutors,SPARK-24416,13162725,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sanket991,sanket991,sanket991,29/May/18 20:47,12/Jun/18 18:55,13/Jul/23 08:45,12/Jun/18 18:55,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"spark.blacklist.killBlacklistedExecutors is defined as 

(Experimental) If set to ""true"", allow Spark to automatically kill, and attempt to re-create, executors when they are blacklisted. Note that, when an entire node is added to the blacklist, all of the executors on that node will be killed.

I presume the killing of blacklisted executors only happens after the stage completes successfully and all tasks have completed or on fetch failures (updateBlacklistForFetchFailure/updateBlacklistForSuccessfulTaskSet). It is confusing because the definition states that the executor will be attempted to be recreated as soon as it is blacklisted. This is not true while the stage is in progress and an executor is blacklisted, it will not attempt to cleanup until the stage finishes.",,apachespark,irashid,riza,sanket991,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 18:55:32 UTC 2018,,,,,,,,,,"0|i3ua7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/18 22:21;apachespark;User 'redsanket' has created a pull request for this issue:
https://github.com/apache/spark/pull/21475;;;","12/Jun/18 18:55;irashid;Issue resolved by pull request 21475
[https://github.com/apache/spark/pull/21475];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage page aggregated executor metrics wrong when failures ,SPARK-24415,13162690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ankur.gupta,tgraves,tgraves,29/May/18 19:15,27/Dec/18 11:15,13/Jul/23 08:45,05/Sep/18 16:52,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,Web UI,,,,0,,,,"Running with spark 2.3 on yarn and having task failures and blacklisting, the aggregated metrics by executor are not correct.  In my example it should have 2 failed tasks but it only shows one.    Note I tested with master branch to verify its not fixed.

I will attach screen shot.

To reproduce:

$SPARK_HOME/bin/spark-shell --master yarn --deploy-mode client --executor-memory=2G --num-executors=1 --conf ""spark.blacklist.enabled=true"" --conf ""spark.blacklist.stage.maxFailedTasksPerExecutor=1"" --conf ""spark.blacklist.stage.maxFailedExecutorsPerNode=1""  --conf ""spark.blacklist.application.maxFailedTasksPerExecutor=2"" --conf ""spark.blacklist.killBlacklistedExecutors=true""

import org.apache.spark.SparkEnv 

sc.parallelize(1 to 10000, 10).map \{ x => if (SparkEnv.get.executorId.toInt >= 1 && SparkEnv.get.executorId.toInt <= 4) throw new RuntimeException(""Bad executor"") else (x % 3, x) }.reduceByKey((a, b) => a + b).collect()",,ankur.gupta,apachespark,maropu,mgaido,shahid,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25284,SPARK-24539,SPARK-25910,,,,,,,,,,,,,,,,,"29/May/18 19:16;tgraves;Screen Shot 2018-05-29 at 2.15.38 PM.png;https://issues.apache.org/jira/secure/attachment/12925620/Screen+Shot+2018-05-29+at+2.15.38+PM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 05 16:52:41 UTC 2018,,,,,,,,,,"0|i3ua07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/18 14:00;tgraves;this might actually be an order of events type thing.  You will note that the config I have is stage.maxFailedTasksPerExecutor=1 so it should really only have 1 failed task, but looking at the log it seems it starts the second task before totally handling the blacklist from the first failure:

 

18/05/30 13:57:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, gsrd259n13.red.ygrid.yahoo.com, executor 2, partition 0, PROCESS_LOCAL, 7746 bytes)
[Stage 2:> (0 + 1) / 10]18/05/30 13:57:20 INFO BlockManagerMasterEndpoint: Registering block manager gsrd259n13.red.ygrid.yahoo.com:43203 with 912.3 MB RAM, BlockManagerId(2, gsrd259n13.red.ygrid.yahoo.com, 43203, None)
18/05/30 13:57:21 INFO Client: Application report for application_1526529576371_25524 (state: RUNNING)
18/05/30 13:57:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on gsrd259n13.red.ygrid.yahoo.com:43203 (size: 1941.0 B, free: 912.3 MB)
18/05/30 13:57:21 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, gsrd259n13.red.ygrid.yahoo.com, executor 2, partition 1, PROCESS_LOCAL, 7747 bytes)
18/05/30 13:57:21 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, gsrd259n13.red.ygrid.yahoo.com, executor 2): java.lang.RuntimeException: Bad executor

....

18/05/30 13:57:21 INFO TaskSetBlacklist: Blacklisting executor 2 for stage 2

18/05/30 13:57:21 INFO YarnScheduler: Cancelling stage 2
18/05/30 13:57:21 INFO YarnScheduler: Stage 2 was cancelled
18/05/30 13:57:21 INFO DAGScheduler: ShuffleMapStage 2 (map at <console>:26) failed in 12.063 s due to Job aborted due to stage failure:

18/05/30 13:57:21 INFO DAGScheduler: Job 1 failed: collect at <console>:26, took 12.069052 s

 

The thing is though that the executors page shows that it had 2 task failures on that node, its just in the aggregated metrics for that stage that doesn't have it.;;;","30/May/18 14:05;tgraves;It also looks like in the history server they show up properly in the aggregated metrics, although if you look at the Tasks (for all stages) column on the jobs page, it only lists a single task failure where it should list 2.;;;","30/May/18 14:22;tgraves;ok so the issue here is in the AppStatusListener where its only updating the task metrics for liveStages.  It gets the second taskEnd event after it cancelled stage 2 so its no longer in the live stages array.  ;;;","13/Jun/18 20:19;ankur.gupta;I am planning to work on this JIRA;;;","23/Aug/18 19:57;apachespark;User 'ankuriitg' has created a pull request for this issue:
https://github.com/apache/spark/pull/22209;;;","05/Sep/18 16:52;vanzin;Issue resolved by pull request 22209
[https://github.com/apache/spark/pull/22209];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stages page doesn't show all task attempts when failures,SPARK-24414,13162688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,tgraves,tgraves,29/May/18 19:06,24/Sep/18 05:04,13/Jul/23 08:45,31/May/18 17:05,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Web UI,,,,0,,,,"If you have task failures, the StagePage doesn't render all the task attempts properly.  It seems to make the table the size of the total number of successful tasks rather then including all the failed tasks.

Even though the table size is smaller, if you sort by various columns you can see all the tasks are actually there, it just seems the size of the table is wrong.",,apachespark,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25503,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 31 17:05:55 UTC 2018,,,,,,,,,,"0|i3u9zr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/May/18 19:12;tgraves;to reproduce this simply start a shell:

$SPARK_HOME/bin/spark-shell --num-executors 5  --master yarn --deploy-mode client

Run something that gets some tasks failures but not all:

sc.parallelize(1 to 10000, 10).map { x =>
 | if (SparkEnv.get.executorId.toInt >= 1 && SparkEnv.get.executorId.toInt <= 4) throw new RuntimeException(""Bad executor"")
 | else (x % 3, x)
 | }.reduceByKey((a, b) => a + b).collect()

 

Go to the stages page and you will only see 10 tasks rendered when it should has 21 total between succeeded and failed. ;;;","29/May/18 20:27;tgraves;looks like this was broken by SPARK-23147, so we probably need to find a different solution.

[~vanzin] [~jerryshao];;;","29/May/18 20:29;vanzin;I was going to take a stab at this next.;;;","29/May/18 20:55;tgraves;I am looking to see if we can just return an empty table in the case the tasks aren't initialized yet. If you get to it first thats fine or had something else in mind ;;;","29/May/18 20:57;vanzin;Yeah that's the direction I ended up in. Taking the chance to clean up the code a bit around this area...;;;","29/May/18 21:02;tgraves;also just an fyi I also filed SPARK-24415, not sure if they are related as I haven't dug into that one yet.  ;;;","29/May/18 21:11;vanzin;After a quick look at the code they don't seem related.;;;","29/May/18 21:28;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21457;;;","31/May/18 17:05;vanzin;Issue resolved by pull request 21457
[https://github.com/apache/spark/pull/21457];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggreate on Decimal Types does not work,SPARK-24401,13162332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jomach,jomach,28/May/18 06:33,29/May/18 05:49,13/Jul/23 08:45,29/May/18 05:49,2.2.0,,,,,,,,,2.3.0,,,,,SQL,,,,1,,,,"Hi, 

I think I found a really ugly bug in spark when performing aggregations with Decimals

To reproduce: 

 
{code:java}
val df = spark.read.parquet(""attached file"")
val first_agg = fact_df.groupBy(""id1"", ""id2"", ""start_date"").agg(mean(""projection_factor"").alias(""projection_factor""))
first_agg.show
val second_agg = first_agg.groupBy(""id1"",""id2"").agg(max(""projection_factor"").alias(""maxf""), min(""projection_factor"").alias(""minf""))
second_agg.show
{code}
First aggregation works fine the second aggregation seems to be summing instead of max value. I tried with spark 2.2.0 and 2.3.0 same problem.

The dataset as circa 800 Rows and the projection_factor has values from 0 until 100. the result should not be bigger that 5 but with get 265820543091454.... as result back.

 

 

As Code not 100% the same but I think there is really a bug there: 

 
{code:java}
BigDecimal [] objects = new BigDecimal[]{
        new BigDecimal(3.5714285714D),
        new BigDecimal(3.5714285714D),
        new BigDecimal(3.5714285714D),
        new BigDecimal(3.5714285714D)};
Row dataRow = new GenericRow(objects);
Row dataRow2 = new GenericRow(objects);
StructType structType = new StructType()
        .add(""id1"", DataTypes.createDecimalType(38,10), true)
        .add(""id2"", DataTypes.createDecimalType(38,10), true)
        .add(""id3"", DataTypes.createDecimalType(38,10), true)
        .add(""id4"", DataTypes.createDecimalType(38,10), true);

final Dataset<Row> dataFrame = sparkSession.createDataFrame(Arrays.asList(dataRow,dataRow2), structType);
System.out.println(dataFrame.schema());
dataFrame.show();
final Dataset<Row>  df1 = dataFrame.groupBy(""id1"",""id2"")
        .agg( mean(""id3"").alias(""projection_factor""));
df1.show();
final Dataset<Row> df2 = df1
        .groupBy(""id1"")
        .agg(max(""projection_factor""));

df2.show();
{code}
 

The df2 should have:
{code:java}
+------------+----------------------+
| id1|max(projection_factor)|
+------------+----------------------+
|3.5714285714| 3.5714285714|
+------------+----------------------+

{code}
instead it returns: 
{code:java}
+------------+----------------------+
| id1|max(projection_factor)|
+------------+----------------------+
|3.5714285714| 0.00035714285714|
+------------+----------------------+
{code}
 

 ",,bombatkar.vivek@gmail.com,jomach,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 05:49:58 UTC 2018,,,,,,,,,,"0|i3u7sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/18 06:40;jomach;Probably is worth to mention that my dataset is coming from a oracle DB;;;","28/May/18 15:19;mgaido;I followed the repro steps using the file you attached and I was unable to reproduce on current master. May you please check the repro code and try it on current master branch?;;;","29/May/18 02:16;maropu;I didn't narrow down  and I'm not sure that this is related to this issue though, I found v2.2.0 had an incorrect result (fixed in v2.2.1);
As marco said, I'd be greated if you try on master;
{code}
// v2.2.0
scala> second_agg.show
+----+-----+----------------+-----+                                             
| id1|  id2|            maxf| minf|
+----+-----+----------------+-----+
|1498|88586|0.00238636363635|3E-14|
+----+-----+----------------+-----+

// v2.2.1
scala> second_agg.show
+----+-----+----------------+----------------+                                  
| id1|  id2|            maxf|            minf|
+----+-----+----------------+----------------+
|1498|88586|3.95833333330000|2.00000000000000|
+----+-----+----------------+----------------+

// v2.3.0
scala> second_agg.show
+----+-----+----------------+----------------+
| id1|  id2|            maxf|            minf|
+----+-----+----------------+----------------+
|1498|88586|3.95833333330000|2.00000000000000|
+----+-----+----------------+----------------+

// master
scala> second_agg.show
+----+-----+----------------+----------------+
| id1|  id2|            maxf|            minf|
+----+-----+----------------+----------------+
|1498|88586|3.95833333330000|2.00000000000000|
+----+-----+----------------+----------------+
{code};;;","29/May/18 05:48;jomach;Yes, I just tried my code and it works on spark 2.3.0;;;","29/May/18 05:49;jomach;Was already Fixed on 2.3.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reused Exchange is used where it should not be,SPARK-24399,13162296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,vrbad,vrbad,27/May/18 13:29,16/Jun/18 09:44,13/Jul/23 08:45,16/Jun/18 09:44,2.3.0,,,,,,,,,2.3.1,,,,,SQL,,,,0,correctness,,,"Reused Exchange produces wrong result. Here is the code to reproduce the issue:
{code:java}
 
import org.apache.spark.sql.functions.{sum, lit}
import org.apache.spark.sql.expressions.Window


val row1 = (1, 3, 4, 50)
val row2 = (2, 2, 2, 250)
val row3 = (3, 2, 4, 250)
val row4 = (4, 3, 1, 350)
val data = Seq(row1, row2, row3, row4)


val df = data.toDF(""id"", ""pFilter"", ""secondFilter"", ""metricToAgg"").cache()

val w = Window.partitionBy($""id"")

val firstUnionPart = df.withColumn(""activity_sum"", sum(""metricToAgg"").over(w))
  .filter($""activity_sum"" > 50)
  .filter($""pFilter"".isin(2, 3))
  .agg(sum($""metricToAgg""))
  .withColumn(""t"", lit(""first_union_part""))

val secondUnionPart = df.withColumn(""activity_sum"",sum(""metricToAgg"").over(w))
  .filter($""activity_sum"" > 50)
  .filter($""secondFilter"".isin(2, 3))
  .agg(sum($""metricToAgg""))
  .withColumn(""t"", lit(""second_union_part""))

val finalDF = firstUnionPart.union(secondUnionPart)
finalDF.show()

+----------------+-----------------+ 
|sum(metricToAgg)| t               | 
+----------------+-----------------+ 
| 850            | first_union_part| 
| 850            |second_union_part| 
+----------------+-----------------+
{code}
 

The second row is wrong, it should be 250, instead of 850, which you can see if you show both unionParts separately:
{code:java}
firstUnionPart.show() 
+----------------+----------------+ 
|sum(metricToAgg)|               t| 
+----------------+----------------+ 
|             850|first_union_part| 
+----------------+----------------+

secondUnionPart.show()
+----------------+-----------------+
|sum(metricToAgg)|                t|
+----------------+-----------------+
|             250|second_union_part|
+----------------+-----------------+{code}
 

The ReusedExchange replaced the part of the query plan in the second branch of the union by the query plan from the first branch as you can see from explain() function.

I did some inspection and it appears that both sub-plans have the same canonicalized plans and therefore the ReusedExchange takes place. But I don't think they should have the same canonicalized plan, since according to the notes in the source code only plans that evaluate to the same result can have same canonicalized plans. And the two sub-plans in this query lead in principle to different results, because in the second union there is filter on different column than in the first union.

 

Interesting think happens when we change the name of the second column from ""pFilter"" to ""kFilter"". In this case query works fine and produces correct result, as you can see here:
{code:java}
import org.apache.spark.sql.functions.{sum, lit}
import org.apache.spark.sql.expressions.Window


val row1 = (1, 3, 4, 50)
val row2 = (2, 2, 2, 250)
val row3 = (3, 2, 4, 250)
val row4 = (4, 3, 1, 350)
val data = Seq(row1, row2, row3, row4)


val df = data.toDF(""id"", ""kFilter"", ""secondFilter"", ""metricToAgg"").cache()

val w = Window.partitionBy($""id"")

val firstUnionPart = df.withColumn(""activity_sum"", sum(""metricToAgg"").over(w))
  .filter($""activity_sum"" > 50)
  .filter($""kFilter"".isin(2, 3))
  .agg(sum($""metricToAgg""))
  .withColumn(""t"", lit(""first_union_part""))

val secondUnionPart = df.withColumn(""activity_sum"",sum(""metricToAgg"").over(w))
  .filter($""activity_sum"" > 50)
  .filter($""secondFilter"".isin(2, 3))
  .agg(sum($""metricToAgg""))
  .withColumn(""t"", lit(""second_union_part""))

val finalDF = firstUnionPart.union(secondUnionPart)

finalDF.show()

+----------------+-----------------+
|sum(metricToAgg)|                t|
+----------------+-----------------+
|             850| first_union_part|
|             250|second_union_part|
+----------------+-----------------+{code}
 

The result is now correct and the only think we changed is a name of one column. The ReusedExchange does not happen here and I checked that the canonicalized plans now really differ.

 

The key points to reproduce this bug are:
 # Use union (or some operator with multiple branches)
 # Use cache to have InMemoryTableScan
 # Use operator that forces Exchange in the plan (in this case window function call)
 # Use column names that will have specific alphabetical order

 

 ",,maropu,vrbad,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 05:54:43 UTC 2018,,,,,,,,,,"0|i3u7kv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/May/18 13:38;vrbad;I did some more inspection to see why changing the column name has impact on the correctness of the result. It appears that the attributes in the InMemoryTableScan is an ArrayBuffer which is ordered alphabetically. In the first version that is not working fine we have in the SparkPlan:

In the first branch we get this node
{code:java}
+- InMemoryTableScan [id#2028, metricToAgg#2031, pFilter#2029]{code}
In the second branch we get this node
{code:java}
+- InMemoryTableScan [id#2028, metricToAgg#2031, secondFilter#2030]{code}
 

After canonicalisation we get same plans and we obtain incorrect result. On the other hand in the second query that produced correct result (where we changed the column name) we get in the first branch:
{code:java}
+- InMemoryTableScan [id#2405, kFilter#2406, metricToAgg#2408]  {code}
 And in the second branch we get
{code:java}
+- InMemoryTableScan [id#2405, metricToAgg#2408, secondFilter#2407]{code}
 

As you can see the order of attributes in the first branch changed and the canonicalized plans will now be different and consequently the ReusedExchange does not happen.

If we take a look at the InMemoryStrategy, it is using the pruneFilterProject function. This function is using (projectSet ++ filterSet).toSeq as argument for the ScanBuilder. I believe that this sequence creation can change the order of the attributes. It creates an ArrayBuffer form a set where the order is not guaranteed.;;;","29/May/18 05:54;maropu;I checked on master and branch-2.3 and this issue already has been fixed there.

Also, the vote for the v2.3.1 release has started, so it will be available soon.

http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Spark-2-3-1-RC2-td24020.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"from_json should support arrays of primitives, and more generally all JSON ",SPARK-24391,13162118,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,skimbrel,skimbrel,25/May/18 16:05,12/Dec/22 18:11,13/Jul/23 08:45,13/Aug/18 12:14,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"https://issues.apache.org/jira/browse/SPARK-19849 and https://issues.apache.org/jira/browse/SPARK-21513 brought support for more column types to functions.to_json/from_json, but I also have cases where I'd like to simply (de)serialize an array of primitives to/from JSON when outputting to certain destinations, which does not work:
{code:java}
scala> import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions._

scala> import spark.implicits._
import spark.implicits._

scala> val df = Seq(""[1, 2, 3]"").toDF(""a"")
df: org.apache.spark.sql.DataFrame = [a: string]

scala> val schema = new ArrayType(IntegerType, false)
schema: org.apache.spark.sql.types.ArrayType = ArrayType(IntegerType,false)

scala> df.select(from_json($""a"", schema))
org.apache.spark.sql.AnalysisException: cannot resolve 'jsontostructs(`a`)' due to data type mismatch: Input schema array<int> must be a struct or an array of structs.;;
'Project [jsontostructs(ArrayType(IntegerType,false), a#3, Some(America/Los_Angeles)) AS jsontostructs(a)#10]

scala> val arrayDf = Seq(Array(1, 2, 3)).toDF(""arr"")
arrayDf: org.apache.spark.sql.DataFrame = [arr: array<int>]

scala> arrayDf.select(to_json($""arr""))
org.apache.spark.sql.AnalysisException: cannot resolve 'structstojson(`arr`)' due to data type mismatch: Input type array<int> must be a struct, array of structs or a map or array of map.;;
'Project [structstojson(arr#19, Some(America/Los_Angeles)) AS structstojson(arr)#26]
{code}",,apachespark,maropu,skimbrel,Teng Peng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22228,,,,,,,SPARK-25252,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 28 03:51:49 UTC 2018,,,,,,,,,,"0|i3u6hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/18 17:33;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21439;;;","13/Aug/18 12:14;gurwls223;Issue resolved by pull request 21439
[https://github.com/apache/spark/pull/21439];;;","24/Aug/18 19:30;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22226;;;","28/Aug/18 03:51;gurwls223;For to_json, separate JIRA was filed in SPARK-25252;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivially-true EqualNullSafe should be handled like EqualTo in Dataset.join,SPARK-24385,13161869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,manus,manus,24/May/18 19:26,12/Jan/20 00:07,13/Jul/23 08:45,03/Jul/18 04:20,2.2.1,2.3.0,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"Dataset.join(right: Dataset[_], joinExprs: Column, joinType: String) has special logic for resolving trivially-true predicates to both sides. It currently handles regular equals but not null-safe equals; the code should be updated to also handle null-safe equals.

Pyspark example:
{code:java}
df = spark.range(10)
df.join(df, 'id').collect() # This works.
df.join(df, df['id'] == df['id']).collect() # This works.
df.join(df, df['id'].eqNullSafe(df['id'])).collect() # This fails!!!

# This is a workaround that works.
df2 = df.withColumn('id', F.col('id'))
df.join(df2, df['id'].eqNullSafe(df2['id'])).collect(){code}
The relevant code in Dataset.join should look like this:
{code:java}
// Otherwise, find the trivially true predicates and automatically resolves them to both sides.
// By the time we get here, since we have already run analysis, all attributes should've been
// resolved and become AttributeReference.
val cond = plan.condition.map { _.transform {
  case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) =>
    catalyst.expressions.EqualTo(
      withPlan(plan.left).resolve(a.name),
      withPlan(plan.right).resolve(b.name))
  // This case is new!!!
  case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) =>
    catalyst.expressions.EqualNullSafe(
      withPlan(plan.left).resolve(a.name),
      withPlan(plan.right).resolve(b.name))
}}
{code}",,apachespark,cloud_fan,manus,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 03 04:20:43 UTC 2018,,,,,,,,,,"0|i3u4yv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/May/18 12:40;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21449;;;","21/Jun/18 16:00;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21605;;;","03/Jul/18 04:20;cloud_fan;Issue resolved by pull request 21605
[https://github.com/apache/spark/pull/21605];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-submit --py-files with .py files doesn't work in client mode before context initialization,SPARK-24384,13161838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,24/May/18 17:34,12/Dec/22 17:51,13/Jul/23 08:45,30/May/18 17:51,2.3.0,2.4.0,,,,,,,,2.3.1,2.4.0,,,,PySpark,Spark Submit,,,0,,,,"In case the given Python file is .py file (zip file seems fine), seems the python path is dynamically added after the context is got initialized.

with this pyFile:

{code}
$ cat /home/spark/tmp.py
def testtest():
    return 1
{code}

This works:

{code}
$ cat app.py
import pyspark
pyspark.sql.SparkSession.builder.getOrCreate()
import tmp
print(""************************%s"" % tmp.testtest())

$ ./bin/spark-submit --master yarn --deploy-mode client --py-files /home/spark/tmp.py app.py
...
************************1
{code}

but this doesn't:

{code}
$ cat app.py
import pyspark
import tmp
pyspark.sql.SparkSession.builder.getOrCreate()
print(""************************%s"" % tmp.testtest())

$ ./bin/spark-submit --master yarn --deploy-mode client --py-files /home/spark/tmp.py app.py
Traceback (most recent call last):
  File ""/home/spark/spark/app.py"", line 2, in <module>
    import tmp
ImportError: No module named tmp
{code}

See https://issues.apache.org/jira/browse/SPARK-21945?focusedCommentId=16488486&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16488486",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 30 17:51:45 UTC 2018,,,,,,,,,,"0|i3u4rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/18 18:18;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21426;;;","30/May/18 17:51;vanzin;Issue resolved by pull request 21426
[https://github.com/apache/spark/pull/21426];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make --py-files work in non pyspark application,SPARK-24377,13161682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,24/May/18 06:46,12/Dec/22 18:10,13/Jul/23 08:45,29/May/18 02:49,2.3.0,,,,,,,,,2.4.0,,,,,Spark Submit,,,,0,,,,"For some Spark applications, though they're a java program, they require not only jar dependencies, but also python dependencies. One example is Livy remote SparkContext application, this application is actually a embedded REPL for Scala/Python/R, so it will not only load in jar dependencies, but also python and R deps.

Currently for a Spark application, --py-files can only be worked for a pyspark application, so it will not be worked in the above case. So here propose to remove such restriction.

Also we tested that ""spark.submit.pyFiles"" only supports quite limited scenario (client mode with local deps), so here also expand the usage of ""spark.submit.pyFiles"" to be alternative of --py-files.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 02:49:20 UTC 2018,,,,,,,,,,"0|i3u3tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/18 07:08;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21420;;;","29/May/18 02:49;gurwls223;Issue resolved by pull request 21420
[https://github.com/apache/spark/pull/21420];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""df.cache() df.count()"" no longer eagerly caches data when the analyzed plans are different after re-analyzing the plans",SPARK-24373,13161595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mgaido,wbzhao,wbzhao,23/May/18 21:44,30/May/18 21:19,13/Jul/23 08:45,28/May/18 05:57,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,2,,,,"Here is the code to reproduce in local mode
{code:java}
scala> val df = sc.range(1, 2).toDF
df: org.apache.spark.sql.DataFrame = [value: bigint]

scala> val myudf = udf({x: Long => println(""xxxx""); x + 1})
myudf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))

scala> val df1 = df.withColumn(""value1"", myudf(col(""value"")))
df1: org.apache.spark.sql.DataFrame = [value: bigint, value1: bigint]

scala> df1.cache
res0: df1.type = [value: bigint, value1: bigint]

scala> df1.count
res1: Long = 1 

scala> df1.count
res2: Long = 1

scala> df1.count
res3: Long = 1
{code}
 

in Spark 2.2, you could see it prints ""xxxx"". 

In the above example, when you do explain. You could see
{code:java}
scala> df1.explain(true)
== Parsed Logical Plan ==
'Project [value#2L, UDF('value) AS value1#5]
+- AnalysisBarrier
+- SerializeFromObject [input[0, bigint, false] AS value#2L]
+- ExternalRDD [obj#1L]

== Analyzed Logical Plan ==
value: bigint, value1: bigint
Project [value#2L, if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L]
+- SerializeFromObject [input[0, bigint, false] AS value#2L]
+- ExternalRDD [obj#1L]

== Optimized Logical Plan ==
InMemoryRelation [value#2L, value1#5L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
+- *(1) Project [value#2L, UDF(value#2L) AS value1#5L]
+- *(1) SerializeFromObject [input[0, bigint, false] AS value#2L]
+- Scan ExternalRDDScan[obj#1L]

== Physical Plan ==
*(1) InMemoryTableScan [value#2L, value1#5L]
+- InMemoryRelation [value#2L, value1#5L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
+- *(1) Project [value#2L, UDF(value#2L) AS value1#5L]
+- *(1) SerializeFromObject [input[0, bigint, false] AS value#2L]
+- Scan ExternalRDDScan[obj#1L]

{code}
but the ImMemoryTableScan is mising in the following explain()
{code:java}
scala> df1.groupBy().count().explain(true)
== Parsed Logical Plan ==
Aggregate [count(1) AS count#170L]
+- Project [value#2L, if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L]
+- SerializeFromObject [input[0, bigint, false] AS value#2L]
+- ExternalRDD [obj#1L]

== Analyzed Logical Plan ==
count: bigint
Aggregate [count(1) AS count#170L]
+- Project [value#2L, if (isnull(value#2L)) null else if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L]
+- SerializeFromObject [input[0, bigint, false] AS value#2L]
+- ExternalRDD [obj#1L]

== Optimized Logical Plan ==
Aggregate [count(1) AS count#170L]
+- Project
+- SerializeFromObject [input[0, bigint, false] AS value#2L]
+- ExternalRDD [obj#1L]

== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[count(1)], output=[count#170L])
+- Exchange SinglePartition
+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#175L])
+- *(1) Project
+- *(1) SerializeFromObject [input[0, bigint, false] AS value#2L]
+- Scan ExternalRDDScan[obj#1L]
{code}
 

 ",,apachespark,aweise,clembou,icexelloss,maropu,mgaido,smilegator,tgraves,TomaszGaweda,vanzin,wbzhao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 30 21:19:08 UTC 2018,,,,,,,,,,"0|i3u3a7:",9223372036854775807,,,,,,,,,,,,,2.3.1,,,,,,,,,,"23/May/18 22:13;icexelloss;We found after upgrading to Spark 2.3, many of our production systems runs slower. This is founded to be caused by ""df.cache(); df.count()"" no longer eagerly cache df correctly. I think this might be a regression from 2.2. 

I think any one uses  ""df.cache() df.count()"" to cache data eagerly will be affected.;;;","24/May/18 14:05;icexelloss;This is a reproduce:
{code:java}
val myUDF = udf((x: Long) => { println(""xxxx""); x + 1 })

val df1 = spark.range(0, 1).toDF(""s"").select(myUDF($""s""))
df1.cache()
df1.count()
// No xxxx printed
{code}
It appears the issue is related to UDF:
{code:java}
val df1 = spark.range(0, 1).toDF(""s"").select(myUDF($""s""))
df1.cache()
df1.groupBy().count().explain()

== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[count(1)])
+- Exchange SinglePartition
  +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])
    +- *(1) Project
      +- *(1) Range (0, 1, step=1, splits=2)
{code}
Without UDF it uses ""count"" materialize cache:
{code:java}
val df1 = spark.range(0, 1).toDF(""s"").select($""s"" + 1)
df1.cache()
df1.groupBy().count().explain()

== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[count(1)])
  +- Exchange SinglePartition
    +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])
      +- *(1) InMemoryTableScan
        +- InMemoryRelation [(s + 1)#179L], CachedRDDBuilder(true,10000,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [(id#175L + 1) AS (s + 1)#179L]
          +- *(1) Range (0, 1, step=1, splits=2) ,None)
            +- *(1) Project [(id#175L + 1) AS (s + 1)#179L]
               +- *(1) Range (0, 1, step=1, splits=2)

{code}
 ;;;","24/May/18 14:46;aweise;We are also facing increased runtime duration for our SQL jobs (after upgrading from 2.2.1 to 2.3.0), but didn't trace it down to the root cause. This issue sounds reasonable to me, as we are also using cache() + count() quite often.;;;","24/May/18 15:37;wbzhao;I turned on the log trace of RuleExecutor and found that in my example of df1.count() after cache. 

 
{code:java}
scala> df1.groupBy().count().explain(true)

=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$HandleNullInputsForUDF ===
Aggregate [count(1) AS count#40L] Aggregate [count(1) AS count#40L]
!+- Project [value#2L, if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L] +- Project [value#2L, if (isnull(value#2L)) null else if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L]
+- SerializeFromObject [input[0, bigint, false] AS value#2L] +- SerializeFromObject [input[0, bigint, false] AS value#2L]
+- ExternalRDD [obj#1L] +- ExternalRDD [obj#1L]
{code}
 that is node
{code:java}
Project [value#2L, if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L]{code}
becomes 
{code:java}
Project [value#2L, if (isnull(value#2L)) null else if (isnull(value#2L)) null else UDF(value#2L) AS value1#5L]{code}
 

This will cause a miss in the CacheManager?

which could be confirmed by later applying ColumnPrunning rule's log trace.  

May question is: is that supposed protected by AnalysisBarrier ?;;;","24/May/18 16:57;vanzin;This  could be the same as SPARK-23309.;;;","24/May/18 18:23;wbzhao;It is not apparently to me that they are the same issue though;;;","24/May/18 21:17;wbzhao;I guess we should use `planWithBarrier` in the 'RelationalGroupedDataset' or other similar places. Any suggestion?;;;","25/May/18 10:51;mgaido;[~wbzhao] yes, I do agree with you. That is the problem.;;;","25/May/18 14:12;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21432;;;","25/May/18 17:54;smilegator;[~icexelloss] [~aweise] Are you also using the Dataset APIs groupBy(), rollup(), cube(), rollup, pivot() and groupByKey()?;;;","25/May/18 17:56;icexelloss;We use groupby() and pivot();;;","25/May/18 17:59;smilegator;BTW, I plan to continue my work of https://github.com/apache/spark/pull/18717, which will add an eager persist/cache API. ;;;","25/May/18 18:24;mgaido;[~smilegator] I think an eager API is not related to the problem experienced here, though.;;;","25/May/18 18:52;TomaszGaweda;[~LI,Xiao]  That is a good idea :) Eager caching is useful, many times I see additional count just to cache eagerly;;;","25/May/18 20:18;smilegator;{code}
  def count(): Long = withAction(""count"", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }
{code}

Many Spark users are using df.count() after df.cache() for achieving eager caching.  Since our count() API is using `groupBy()`, the impact becomes much bigger. The count() API will not trigger the data materialization when the plans are different after multiple rounds of plan analysis. ;;;","25/May/18 21:09;mgaido;[~smilegator] yes, you're right, the impact would be definitely lower.;;;","25/May/18 21:50;smilegator;In the above example, each time when we re-analyze the plan that is recreated through the Dataset APIs count(), groupBy(), rollup(), cube(), rollup, pivot() and groupByKey(), the Analyzer rule HandleNullInputsForUDF will add the extra IF expression above the UDF in the previously resolved sub-plan. Note, this is not the only rule that could change the analyzed plans if we re-run the analyzer.

This is a regression introduced by [https://github.com/apache/spark/pull/17770]. We replaced the original solution (based on the analyzed flag) by the AnalysisBarrier. However, we did not add the AnalysisBarrier on the APIs of RelationalGroupedDataset and KeyValueGroupedDataset.

To fix it, we will changes the plan again. We might face some unknown issues. How about adding a temporary flag in Spark 2.3.1? If anything unexpected happens, our users still can change it back to the Spark 2.3.0 behavior?;;;","25/May/18 21:54;icexelloss;[~smilegator] do you mean that add AnalysisBarrier to RelationalGroupedDataset and KeyValueGroupedDataset could lead to new bugs?;;;","29/May/18 13:53;mgaido;[~wbzhao] as I answered on the PR, the fix is complete and includes also {{flatMapGroupsInPandas}}.;;;","29/May/18 14:18;wbzhao;[~mgaido] Thanks. I didn't look the comment carefully. ;;;","30/May/18 21:15;smilegator;[~icexelloss] This is still possible since the query plans are changed. I am also fine to do it without a flag. If you apply the fix to your internal fork, I would suggest to add a flag. At least, you can turn it off when anything unexpected happens. ;;;","30/May/18 21:19;icexelloss;[~smilegator] Thank you for the suggestion.;;;",,,,,,,,,,,,,,,,,,,,,,
A bug when having multiple distinct aggregations,SPARK-24369,13161551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,smilegator,smilegator,23/May/18 18:37,04/Jun/18 04:58,13/Jul/23 08:45,04/Jun/18 04:58,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"{code}
SELECT corr(DISTINCT x, y), corr(DISTINCT y, x), count(*) FROM
(VALUES
   (1, 1),
   (2, 2),
   (2, 2)
) t(x, y)
{code}

It returns 

{code}
java.lang.RuntimeException
You hit a query analyzer bug. Please report your query to Spark user mailing list.
{code}",,apachespark,cloud_fan,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 03 03:57:05 UTC 2018,,,,,,,,,,"0|i3u30f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/18 04:17;smilegator;cc [~maropu] Are you interested in this?;;;","24/May/18 04:42;maropu;ok, I will look into this. Thanks!;;;","25/May/18 22:32;smilegator;Thanks!;;;","28/May/18 15:05;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/21443;;;","30/May/18 16:26;cloud_fan;Issue resolved by pull request 21443
[https://github.com/apache/spark/pull/21443];;;","03/Jun/18 03:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky tests: org.apache.spark.sql.execution.datasources.csv.UnivocityParserSuite,SPARK-24368,13161484,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,smilegator,smilegator,23/May/18 15:56,16/Aug/18 15:50,13/Jul/23 08:45,25/May/18 04:39,2.4.0,,,,,,,,,2.4.0,,,,,SQL,Tests,,,0,,,,"org.apache.spark.sql.execution.datasources.csv.UnivocityParserSuite failed very often.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/91020/testReport/org.apache.spark.sql.execution.datasources.csv/UnivocityParserSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/history/



{code}
org.apache.spark.sql.execution.datasources.csv.UnivocityParserSuite *** ABORTED *** (1 millisecond)
[info]   java.lang.IllegalStateException: LiveListenerBus is stopped.
[info]   at org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:97)
[info]   at org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:80)
[info]   at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:93)
[info]   at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:120)
[info]   at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:120)
[info]   at scala.Option.getOrElse(Option.scala:121)
[info]   at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:120)
[info]   at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:119)
[info]   at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:286)
[info]   at org.apache.spark.sql.test.TestSparkSession.sessionState$lzycompute(TestSQLContext.scala:42)
[info]   at org.apache.spark.sql.test.TestSparkSession.sessionState(TestSQLContext.scala:41)
[info]   at org.apache.spark.sql.SparkSession$$anonfun$1$$anonfun$apply$1.apply(SparkSession.scala:95)
[info]   at org.apache.spark.sql.SparkSession$$anonfun$1$$anonfun$apply$1.apply(SparkSession.scala:95)
[info]   at scala.Option.map(Option.scala:146)
[info]   at org.apache.spark.sql.SparkSession$$anonfun$1.apply(SparkSession.scala:95)
[info]   at org.apache.spark.sql.SparkSession$$anonfun$1.apply(SparkSession.scala:94)
[info]   at org.apache.spark.sql.internal.SQLConf$.get(SQLConf.scala:125)
[info]   at org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:84)
[info]   at org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:40)
[info]   at org.apache.spark.sql.execution.datasources.csv.UnivocityParserSuite.<init>(UnivocityParserSuite.scala:30)
[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[info]   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[info]   at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
[info]   at java.lang.Class.newInstance(Class.java:442)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:435)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 23 19:37:08 UTC 2018,,,,,,,,,,"0|i3u2lj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/18 15:56;smilegator;cc [~maxgekk]
;;;","23/May/18 17:58;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21414;;;","23/May/18 19:37;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21415;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Files deletion after globbing may fail StructuredStreaming jobs,SPARK-24364,13161366,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,23/May/18 10:15,12/Dec/22 18:10,13/Jul/23 08:45,24/May/18 05:22,2.3.0,2.4.0,,,,,,,,2.3.1,2.4.0,,,,Structured Streaming,,,,0,,,,"This is related with SPARK-17599. SPARK-17599 checked the directory only but actually it can file on another FileSystem operation when the file is not found. For example see:

{code}
Error occurred while processing: File does not exist: /rel/00171151/input/PJ/part-00136-b6403bac-a240-44f8-a792-fc2e174682b7-c000.csv
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

java.io.FileNotFoundException: File does not exist: /rel/00171151/input/PJ/part-00136-b6403bac-a240-44f8-a792-fc2e174682b7-c000.csv
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1251)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1236)
	at org.apache.hadoop.hdfs.DFSClient.getBlockLocations(DFSClient.java:1294)
	at org.apache.hadoop.hdfs.DistributedFileSystem$2.doCall(DistributedFileSystem.java:242)
	at org.apache.hadoop.hdfs.DistributedFileSystem$2.doCall(DistributedFileSystem.java:238)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:249)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:229)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles$3.apply(InMemoryFileIndex.scala:314)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles$3.apply(InMemoryFileIndex.scala:297)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(InMemoryFileIndex.scala:297)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$1.apply(InMemoryFileIndex.scala:174)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$1.apply(InMemoryFileIndex.scala:173)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(InMemoryFileIndex.scala:173)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:126)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:91)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:67)
	at org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:161)
	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$tempFileIndex$1(DataSource.scala:152)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:166)
	at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:261)
	at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:94)
	at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:33)
	at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:196)
	at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:206)
	at com.hwx.StreamTest$.main(StreamTest.scala:97)
	at com.hwx.StreamTest.main(StreamTest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:906)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /rel/00171151/input/PJ/part-00136-b6403bac-a240-44f8-a792-fc2e174682b7-c000.csv
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)
	at org.apache.hadoop.ipc.Client.call(Client.java:1498)
	at org.apache.hadoop.ipc.Client.call(Client.java:1398)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:272)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:290)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:202)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:184)
	at com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1249)
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27676,,,,,,,,,SPARK-17599,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 05:22:44 UTC 2018,,,,,,,,,,"0|i3u1vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/18 10:22;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21408;;;","24/May/18 05:22;gurwls223;Issue resolved by pull request 21408
[https://github.com/apache/spark/pull/21408];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
offsetLog/commitLog purge thresholdBatchId should be computed with current committed epoch but not currentBatchId in CP mode,SPARK-24351,13161105,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivoson,ivoson,ivoson,22/May/18 15:45,01/Jun/18 17:48,13/Jul/23 08:45,01/Jun/18 17:48,2.3.0,,,,,,,,,2.4.0,,,,,Structured Streaming,,,,0,,,,"In structured streaming, there is a conf spark.sql.streaming.minBatchesToRetain which is set to specify 'The minimum number of batches that must be retained and made recoverable' as described in [SQLConf|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L802]. In continuous processing, the metadata purge is triggered when an epoch is committed in [ContinuousExecution|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala#L306]. 
 Since currentBatchId increases independently in cp mode, the current committed epoch may be far behind currentBatchId if some task hangs for some time. It is not safe to discard the metadata with thresholdBatchId computed based on currentBatchId because we may clean all the metadata in the checkpoint directory.",,apachespark,ivoson,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 01 17:48:10 UTC 2018,,,,,,,,,,"0|i3u09b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/May/18 15:52;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/21400;;;","01/Jun/18 17:48;zsxwing;Issue resolved by pull request 21400
[https://github.com/apache/spark/pull/21400];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ClassCastException in ""array_position"" function",SPARK-24350,13161104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,wajda,wajda,22/May/18 15:44,24/May/18 21:11,13/Jul/23 08:45,24/May/18 21:11,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"When calling {{array_position}} function with a wrong type of the 1st operand a {{ClassCastException}} is thrown instead of {{AnalysisException}}

Example:

{code:sql}
select array_position('foo', 'bar')
{code}

{noformat}
java.lang.ClassCastException: org.apache.spark.sql.types.StringType$ cannot be cast to org.apache.spark.sql.types.ArrayType
	at org.apache.spark.sql.catalyst.expressions.ArrayPosition.inputTypes(collectionOperations.scala:1398)
	at org.apache.spark.sql.catalyst.expressions.ExpectsInputTypes$class.checkInputDataTypes(ExpectsInputTypes.scala:44)
	at org.apache.spark.sql.catalyst.expressions.ArrayPosition.checkInputDataTypes(collectionOperations.scala:1401)
	at org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:168)
	at org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:168)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveAliases$$assignAliases$1$$anonfun$apply$3.applyOrElse(Analyzer.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveAliases$$assignAliases$1$$anonfun$apply$3.applyOrElse(Analyzer.scala:252)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
{noformat}",,apachespark,maropu,wajda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23919,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 15:52:06 UTC 2018,,,,,,,,,,"0|i3u093:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"22/May/18 15:52;apachespark;User 'wajda' has created a pull request for this issue:
https://github.com/apache/spark/pull/21401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"scala.MatchError in the ""element_at"" expression",SPARK-24348,13161078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wajda,wajda,wajda,22/May/18 14:21,22/May/18 20:07,13/Jul/23 08:45,22/May/18 20:07,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"When calling {{element_at}} with a wrong first operand type a {{scala.MatchError}} is thrown instead of {{AnalysisException}}

*Example:*
{code:sql}
select element_at('foo', 1)
{code}

results in:
{noformat}
scala.MatchError: StringType (of class org.apache.spark.sql.types.StringType$)
	at org.apache.spark.sql.catalyst.expressions.ElementAt.inputTypes(collectionOperations.scala:1469)
	at org.apache.spark.sql.catalyst.expressions.ExpectsInputTypes$class.checkInputDataTypes(ExpectsInputTypes.scala:44)
	at org.apache.spark.sql.catalyst.expressions.ElementAt.checkInputDataTypes(collectionOperations.scala:1478)
	at org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:168)
	at org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:168)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveAliases$$assignAliases$1$$anonfun$apply$3.applyOrElse(Analyzer.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveAliases$$assignAliases$1$$anonfun$apply$3.applyOrElse(Analyzer.scala:252)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
{noformat}",,apachespark,maropu,wajda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23924,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 14:29:06 UTC 2018,,,,,,,,,,"0|i3u03b:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"22/May/18 14:29;apachespark;User 'wajda' has created a pull request for this issue:
https://github.com/apache/spark/pull/21395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL Thrift Server issue,SPARK-24344,13161019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,SilenceNJ,SilenceNJ,22/May/18 10:17,24/May/18 08:19,13/Jul/23 08:45,24/May/18 08:19,2.2.0,,,,,,,,,,,,,,SQL,,,,0,,,,"I want to use spark thrift server to operate the data in hive, and i have start spark thrift server successfully with port 10015,and the hive thrift server port is 10000 which is default.But when i use beeline to connect spark thrift server,the error is below:

!image-2018-05-22-18-15-55-200.png!

The process of thrift server:

!image-2018-05-22-18-17-01-271.png!",,maropu,mgaido,SilenceNJ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 08:19:26 UTC 2018,,,,,,,,,,"0|i3tzq7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,"22/May/18 11:55;mgaido;I moved to Major as Critical and Blocker are reserved for committers. Moreover, I'd recommend you to upload the log file for the error you are reporting. Honestly, I don't think this JIRA is actionable with so few details. Thanks.;;;","24/May/18 08:19;SilenceNJ;Have solve that with establish a new spark without CDH and put the missing jar;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Codegen compile error from predicate subquery,SPARK-24341,13160987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,juliuszsompolski,juliuszsompolski,22/May/18 08:25,08/Nov/18 02:23,13/Jul/23 08:45,07/Aug/18 07:44,2.3.1,,,,,,,,,2.4.0,,,,,SQL,,,,0,release-notes,,,"Ran on master:
{code}
drop table if exists juleka;
drop table if exists julekb;
create table juleka (a integer, b integer);
create table julekb (na integer, nb integer);
insert into juleka values (1,1);
insert into julekb values (1,1);
select * from juleka where (a, b) not in (select (na, nb) from julekb);
{code}

Results in:
{code}
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 27, Column 29: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 27, Column 29: Cannot compare types ""int"" and ""org.apache.spark.sql.catalyst.InternalRow""
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3936)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4806)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1415)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:92)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.generate(GeneratePredicate.scala:46)
	at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:380)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$boundCondition$lzycompute(BroadcastNestedLoopJoinExec.scala:99)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$boundCondition(BroadcastNestedLoopJoinExec.scala:97)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2$$anonfun$apply$3.apply(BroadcastNestedLoopJoinExec.scala:203)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2$$anonfun$apply$3.apply(BroadcastNestedLoopJoinExec.scala:203)
	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)
	at scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:46)
	at scala.collection.mutable.ArrayOps$ofRef.exists(ArrayOps.scala:186)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2.apply(BroadcastNestedLoopJoinExec.scala:203)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2.apply(BroadcastNestedLoopJoinExec.scala:202)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)
	at org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:49)
	at org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:126)
	at org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:111)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:349)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 27, Column 29: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 27, Column 29: Cannot compare types ""int"" and ""org.apache.spark.sql.catalyst.InternalRow""
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1466)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1531)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1528)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	... 30 more
{code}

Looks like invalid expression is introduced in RewritePredicateSubquery:
{code}
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RewritePredicateSubquery ===
!Filter NOT named_struct(a, a#83, b, b#84) IN (list#74 [])                                                           'Join LeftAnti, ((a#83 = named_struct(na, na, nb, nb)#87) || isnull((a#83 = named_struct(na, na, nb, nb)#87)))
!:  +- Project [named_struct(na, na#85, nb, nb#86) AS named_struct(na, na, nb, nb)#87]                               :- HiveTableRelation `default`.`juleka`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#83, b#84]
!:     +- HiveTableRelation `default`.`julekb`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [na#85, nb#86]   +- Project [named_struct(na, na#85, nb, nb#86) AS named_struct(na, na, nb, nb)#87]
!+- HiveTableRelation `default`.`juleka`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#83, b#84]              +- HiveTableRelation `default`.`julekb`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [na#85, nb#86]
{code}

It works when I run
{code}
select * from juleka where (a, b) not in (select na, nb from julekb);
{code}
so the error comes from tupling the columns in the subquery.",,apachespark,cloud_fan,juliuszsompolski,maropu,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 03:23:43 UTC 2018,,,,,,,,,,"0|i3tzj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/May/18 12:11;mgaido;This is an issue in the Optimizer, rather than a codegen issue. I'll look at this in more details in the next days.;;;","22/May/18 16:16;smilegator;cc [~dkbiswal] Could you take a look at this too?;;;","23/May/18 11:46;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21403;;;","07/Aug/18 07:44;cloud_fan;Issue resolved by pull request 21403
[https://github.com/apache/spark/pull/21403];;;","27/Sep/18 03:23;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22563;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java lint errors,SPARK-24323,13160540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,19/May/18 23:18,12/Dec/22 18:11,13/Jul/23 08:45,21/May/18 07:43,2.4.0,,,,,,,,,,,,,,SQL,,,,0,,,,"The following error occurs when run lint-java
{code:java}
[ERROR] src/main/java/org/apache/spark/sql/sources/v2/reader/InputPartition.java:[39] (sizes) LineLength: Line is longer than 100 characters (found 104).
[ERROR] src/main/java/org/apache/spark/sql/sources/v2/reader/InputPartitionReader.java:[26] (sizes) LineLength: Line is longer than 100 characters (found 110).
[ERROR] src/main/java/org/apache/spark/sql/sources/v2/reader/InputPartitionReader.java:[30] (sizes) LineLength: Line is longer than 100 characters (found 104).
{code}",,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 21 07:43:08 UTC 2018,,,,,,,,,,"0|i3tx5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/18 23:25;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21374;;;","21/May/18 07:43;gurwls223;Fixed in https://github.com/apache/spark/pull/21374;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache ORC to 1.4.4,SPARK-24322,13160530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,19/May/18 18:20,24/May/18 03:38,13/Jul/23 08:45,24/May/18 03:38,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,Build,,,,0,correctness,,,"ORC 1.4.4 includes [nine fixes|https://issues.apache.org/jira/issues/?filter=12342568&jql=project%20%3D%20ORC%20AND%20resolution%20%3D%20Fixed%20AND%20fixVersion%20%3D%201.4.4]. One of the issues is about `Timestamp` bug (ORC-306) which occurs when `native` ORC vectorized reader reads ORC column vector's sub-vector `times` and `nanos`. ORC-306 fixes this according to the [original definition|https://github.com/apache/hive/blob/master/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java#L45-L46] and the linked PR includes the updated interpretation on ORC column vectors. Note that `hive` ORC reader and ORC MR reader is not affected.

{code}
scala> spark.version
res0: String = 2.3.0
scala> spark.sql(""set spark.sql.orc.impl=native"")
scala> Seq(java.sql.Timestamp.valueOf(""1900-05-05 12:34:56.000789"")).toDF().write.orc(""/tmp/orc"")
scala> spark.read.orc(""/tmp/orc"").show(false)
+--------------------------+
|value                     |
+--------------------------+
|1900-05-05 12:34:55.000789|
+--------------------------+
{code}

This issue aims to update Apache Spark to use it.

*FULL LIST*

|| ID || TITLE ||
| ORC-281 | Fix compiler warnings from clang 5.0 | 
| ORC-301 | `extractFileTail` should open a file in `try` statement | 
| ORC-304 | Fix TestRecordReaderImpl to not fail with new storage-api | 
| ORC-306 | Fix incorrect workaround for bug in java.sql.Timestamp | 
| ORC-324 | Add support for ARM and PPC arch | 
| ORC-330 | Remove unnecessary Hive artifacts from root pom | 
| ORC-332 | Add syntax version to orc_proto.proto | 
| ORC-336 | Remove avro and parquet dependency management entries | 
| ORC-360 | Implement error checking on subtype fields in Java | ",,apachespark,cloud_fan,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 03:38:45 UTC 2018,,,,,,,,,,"0|i3tx3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/18 18:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21372;;;","24/May/18 03:38;cloud_fan;Issue resolved by pull request 21372
[https://github.com/apache/spark/pull/21372];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run-example can not print usage,SPARK-24319,13160403,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,bryanc,bryanc,18/May/18 18:02,14/Jun/18 21:55,13/Jul/23 08:45,14/Jun/18 21:55,2.4.0,,,,,,,,,2.4.0,,,,,Spark Submit,,,,0,,,,"Running ""bin/run-example"" with no args or with ""–help"" will not print usage and just gives the error
{noformat}
$ bin/run-example
Exception in thread ""main"" java.lang.IllegalArgumentException: Missing application resource.
    at org.apache.spark.launcher.CommandBuilderUtils.checkArgument(CommandBuilderUtils.java:241)
    at org.apache.spark.launcher.SparkSubmitCommandBuilder.buildSparkSubmitArgs(SparkSubmitCommandBuilder.java:181)
    at org.apache.spark.launcher.SparkSubmitCommandBuilder.buildSparkSubmitCommand(SparkSubmitCommandBuilder.java:296)
    at org.apache.spark.launcher.SparkSubmitCommandBuilder.buildCommand(SparkSubmitCommandBuilder.java:162)
    at org.apache.spark.launcher.Main.main(Main.java:86){noformat}

it looks like there is an env var in the script that shows usage, but it's getting preempted by something else",,apachespark,bryanc,kiszk,mgaido,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 14 21:55:15 UTC 2018,,,,,,,,,,"0|i3twbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/May/18 16:00;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/21450;;;","14/Jun/18 21:55;vanzin;Issue resolved by pull request 21450
[https://github.com/apache/spark/pull/21450];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collection functions interpreted execution doesn't work with complex types,SPARK-24313,13160293,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mgaido,mgaido,mgaido,18/May/18 09:15,23/May/18 14:27,13/Jul/23 08:45,22/May/18 13:09,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,correctness,,,"Several functions working on collection return incorrect result for complex data types in interpreted mode. In particular, we consider comple data types BINARY, ARRAY. The list of the affected functions is: {{array_contains}}, {{array_position}}, {{element_at}} and {{GetMapValue}}.",,apachespark,cloud_fan,huaxingao,kiszk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 23 09:06:07 UTC 2018,,,,,,,,,,"0|i3tvnb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/18 09:21;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21361;;;","22/May/18 13:09;cloud_fan;Issue resolved by pull request 21361
[https://github.com/apache/spark/pull/21361];;;","22/May/18 22:20;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21403;;;","23/May/18 09:06;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21407;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncEventQueue should handle an interrupt from a Listener,SPARK-24309,13160142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,irashid,irashid,irashid,17/May/18 20:36,26/Nov/19 18:18,13/Jul/23 08:45,21/May/18 23:36,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Scheduler,Spark Core,,,0,,,,"AsyncEventQueue does not properly handle an interrupt from a Listener -- the spark app won't even stop!

I observed this on an actual workload as the EventLoggingListener can generate an interrupt from the underlying hdfs calls:

{noformat}
18/05/16 17:46:36 WARN hdfs.DFSClient: Error transferring data from DatanodeInfoWithStorage[10.17.206.36:20002,DS-3adac910-5d0a-418b-b0f7-6332b35bf6a1,DISK] to DatanodeInfoWithStorage[10.17.206.42:20002,DS-2e7ed0aa-0e68-441e-b5b2-96ad4a9ce7a5,DISK]: 100000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.17.206.35:33950 remote=/10.17.206.36:20002]
18/05/16 17:46:36 WARN hdfs.DFSClient: DataStreamer Exception
java.net.SocketTimeoutException: 100000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.17.206.35:33950 remote=/10.17.206.36:20002]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2305)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$StreamerStreams.sendTransferBlock(DFSOutputStream.java:516)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1450)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1408)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1559)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1254)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:739)
18/05/16 17:46:36 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception
[... a few more of these ...]
18/05/16 17:46:36 INFO scheduler.AsyncEventQueue: Stopping listener queue eventLog.
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220)
        at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:439)
        at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:94)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
        at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:83)
        at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:79)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
        at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:78)
{noformat}

When this happens, the AsyncEventQueue will continue to pile up events in its queue, though its no longer processing them.  And then in the call to stop, it'll block on {{queue.put(POISON_PILL)}} forever, so the SparkContext won't stop.",,apachespark,irashid,junping_du,riza,umayr_nuna,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 19 23:29:55 UTC 2018,,,,,,,,,,"0|i3tuq7:",9223372036854775807,,,,,,,,,,,,,2.3.1,,,,,,,,,,"17/May/18 20:50;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/21356;;;","21/May/18 23:36;vanzin;Issue resolved by pull request 21356
[https://github.com/apache/spark/pull/21356];;;","19/Sep/18 23:29;umayr_nuna;Hi folks. I'm not sure this - or a similar - issue is resolved in 2.3.1. See SPARK-24523. In short, we still see an exception like:

{{18/09/19 22:08:28 ERROR Utils: Uncaught exception in thread pool-4-thread-1 java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.spark.scheduler.AsyncEventQueue.stop(AsyncEventQueue.scala:135) at org.apache.spark.scheduler.LiveListenerBus$$anonfun$stop$1.apply(LiveListenerBus.scala:219) at org.apache.spark.scheduler.LiveListenerBus$$anonfun$stop$1.apply(LiveListenerBus.scala:219) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.spark.scheduler.LiveListenerBus.stop(LiveListenerBus.scala:219) at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1922) at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1360) at org.apache.spark.SparkContext.stop(SparkContext.scala:1921)}}

To work around this problem, we are explicitly invoking sparkSession.stop() but that is - in some cases - causing the session to take ~2hrs to stop, considerably increasing the job runtime. Would appreciate any thoughts here.

 

CC [~irashid] [~vanzin]

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generateLDAData in ml.cluster.LDASuite didn't set seed correctly,SPARK-24300,13159842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lu.DB,mengxr,mengxr,16/May/18 21:44,04/Jun/18 23:08,13/Jul/23 08:45,04/Jun/18 23:08,2.3.0,,,,,,,,,2.4.0,,,,,ML,,,,0,,,,"[https://github.com/apache/spark/blob/0d63eb8888d17df747fb41d7ba254718bb7af3ae/mllib/src/test/scala/org/apache/spark/ml/clustering/LDASuite.scala]

 

generateLDAData uses the same RNG in all partitions to generate random data. This either causes duplicate rows in cluster mode or indeterministic behavior in local mode:
{code:java}
scala> val rng = new java.util.Random(10)
rng: java.util.Random = java.util.Random@78c5ef58

scala> sc.parallelize(1 to 10).map { i => Seq.fill(10)(rng.nextInt(10)) }.collect().mkString(""\n"")
res12: String =
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 9, 1, 8, 5, 0, 6, 3, 3, 8)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 0, 3, 0, 6, 6, 7, 8, 1, 4)
List(3, 9, 1, 8, 5, 0, 6, 3, 3, 8){code}
We should create one RNG per partition to make it safe.

 

cc: [~lu.DB] [~josephkb]",,apachespark,josephkb,lu.DB,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 04 23:08:40 UTC 2018,,,,,,,,,,"0|i3tsvj:",9223372036854775807,,,,,josephkb,,,,,,,,2.4.0,,,,,,,,,,"25/May/18 22:53;lu.DB;I will fix this issue.;;;","04/Jun/18 21:54;apachespark;User 'ludatabricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/21492;;;","04/Jun/18 23:08;mengxr;Issue resolved by pull request 21492
[https://github.com/apache/spark/pull/21492];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw SparkException when OOM in BroadcastExchangeExec,SPARK-24294,13159635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,16/May/18 07:47,23/May/18 20:12,13/Jul/23 08:45,23/May/18 20:12,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"When OutOfMemoryError thrown from BroadcastExchangeExec, scala.concurrent.Future will hit scala bug -- https://github.com/scala/bug/issues/9554, and hang until future timeout:

We could wrap the OOM inside SparkException to resolve this issue.",scala-2.11.8,apachespark,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 16 07:52:04 UTC 2018,,,,,,,,,,"0|i3trlj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/18 07:52;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/21342;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
semanticHash() returns different values for semantically the same IS IN,SPARK-24276,13159322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,maxgekk,maxgekk,15/May/18 06:00,30/May/18 22:32,13/Jul/23 08:45,30/May/18 22:32,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"When a plan is canonicalized any set-based operation, such as IS IN, should have its expressions ordered as the order of expressions does not matter in the evaluation of the operator.

For instance:

{code:scala}
val df = spark.createDataFrame(Seq((1, 2)))
val p1 = df.where('_1.isin(1, 2)).queryExecution.logical.canonicalized
val p2 = df.where('_1.isin(2, 1)).queryExecution.logical.canonicalized
val h1 = p1.semanticHash
val h2 = p2.semanticHash
{code}

{code}
df: org.apache.spark.sql.DataFrame = [_1: int, _2: int]
p1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
'Filter '_1 IN (1,2)
+- LocalRelation [_1#0, _2#1]

p2: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
'Filter '_1 IN (2,1)
+- LocalRelation [_1#0, _2#1]

h1: Int = -1384236508
h2: Int = 939549189
{code}",,apachespark,maxgekk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 12:31:05 UTC 2018,,,,,,,,,,"0|i3tpnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/18 12:31;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21331;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR java check breaks on openjdk,SPARK-24263,13158969,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,felixcheung,felixcheung,felixcheung,13/May/18 20:56,14/May/18 17:49,13/Jul/23 08:45,14/May/18 17:49,2.3.1,,,,,,,,,2.3.1,2.4.0,,,,SparkR,,,,0,,,,"testing with openjdk, noticed that it breaks because the version string is different

 

instead of

""java version \""1.8.0\""""""

it has

""openjdk version \""1.8.0_91\""""",,apachespark,felixcheung,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24255,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 14 17:49:48 UTC 2018,,,,,,,,,,"0|i3tnif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/18 21:00;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/21314;;;","14/May/18 17:49;vanzin;Issue resolved by pull request 21314
[https://github.com/apache/spark/pull/21314];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayWriter for Arrow produces wrong output,SPARK-24259,13158918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,viirya,viirya,13/May/18 00:28,15/May/18 15:49,13/Jul/23 08:45,15/May/18 14:15,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,PySpark,SQL,,,0,correctness,,,"Right now {{ArrayWriter}} used to output Arrow data for array type, doesn't do clear or reset after each batch. It produces wrong output.",,apachespark,cloud_fan,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 14:15:29 UTC 2018,,,,,,,,,,"0|i3tn7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/18 00:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/21312;;;","15/May/18 14:15;cloud_fan;Issue resolved by pull request 21312
[https://github.com/apache/spark/pull/21312];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongToUnsafeRowMap calculate the new size may be wrong,SPARK-24257,13158877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dzcxzl,dzcxzl,dzcxzl,12/May/18 11:15,24/May/18 03:27,13/Jul/23 08:45,24/May/18 03:27,2.0.0,2.1.0,2.2.0,2.3.0,,,,,,2.0.3,2.1.3,2.2.2,2.3.1,2.4.0,SQL,,,,0,correctness,,,"LongToUnsafeRowMap

Calculate the new size simply by multiplying by 2

At this time, the size of the application may not be enough to store data

Some data is lost and the data read out is dirty",,apachespark,cloud_fan,dzcxzl,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 03:27:22 UTC 2018,,,,,,,,,,"0|i3tmy7:",9223372036854775807,,,,,,,,,,,,,2.3.1,,,,,,,,,,"12/May/18 11:19;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/21311;;;","24/May/18 03:27;cloud_fan;Issue resolved by pull request 21311
[https://github.com/apache/spark/pull/21311];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Require Java 8 in SparkR description,SPARK-24255,13158838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,12/May/18 00:02,17/Nov/18 08:11,13/Jul/23 08:45,12/May/18 00:03,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SparkR,,,,0,,,,CRAN checks require that the Java version be set both in package description and checked during runtime.,,felixcheung,kiszk,shivaram,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24272,,,,,,SPARK-24263,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 17 08:11:05 UTC 2018,,,,,,,,,,"0|i3tmpj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/18 00:03;shivaram;Resolved by https://github.com/apache/spark/pull/21278;;;","14/May/18 18:32;smilegator;Thanks! [~shivaram];;;","13/Nov/18 03:20;felixcheung;[~shivaram] I'm thinking if this is handling all version cases?

*[kiszk|https://github.com/kiszk]*  found this with java version
{code:java}
$ ../OpenJDK-8/java -version
java version ""1.8.0_162""
Java(TM) SE Runtime Environment (build 1.8.0_162-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)

$ ../OpenJDK-8/java Version
jave.specification.version=1.8
jave.version=1.8.0_162
jave.version.split(""."")[0]=1

$ ../OpenJDK-9/java -version
openjdk version ""9""
OpenJDK Runtime Environment (build 9+181)
OpenJDK 64-Bit Server VM (build 9+181, mixed mode)

$ ../OpenJDK-9/java Version
jave.specification.version=9
jave.version=9
jave.version.split(""."")[0]=9

$ ../OpenJDK-11/java -version
openjdk version ""11.0.1"" 2018-10-16
OpenJDK Runtime Environment 18.9 (build 11.0.1+13)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.1+13, mixed mode)

$ ../OpenJDK-11/java Version
jave.specification.version=11
jave.version=11.0.1
jave.version.split(""."")[0]=11


$ ../OpenJ9-8/java -version
openjdk version ""1.8.0_192""
OpenJDK Runtime Environment (build 1.8.0_192-b12)
Eclipse OpenJ9 VM (build openj9-0.11.0, JRE 1.8.0 Windows 10 amd64-64-Bit Compressed References 20181019_105 (JIT enabled, AOT enabled)
OpenJ9   - 090ff9dc
OMR      - ea548a66
JCL      - 51609250b5 based on jdk8u192-b12)

$ ../OpenJ9-8/java Version
jave.specification.version=1.8
jave.version=1.8.0_192
jave.version.split(""."")[0]=1

$ ../OpenJ9-9/java -version
openjdk version ""9.0.4-adoptopenjdk""
OpenJDK Runtime Environment (build 9.0.4-adoptopenjdk+12)
Eclipse OpenJ9 VM (build openj9-0.9.0, JRE 9 Windows 8.1 amd64-64-Bit Compressed References 20180814_161 (JIT enabled, AOT enabled)
OpenJ9   - 24e53631
OMR      - fad6bf6e
JCL      - feec4d2ae based on jdk-9.0.4+12)

$ ../OpenJ9-9/java Version
jave.specification.version=9
jave.version=9.0.4-adoptopenjdk
jave.version.split(""."")[0]=9


$ ../OpenJ9-11/java -version
openjdk version ""11.0.1"" 2018-10-16
OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.1+13)
Eclipse OpenJ9 VM AdoptOpenJDK (build openj9-0.11.0, JRE 11 Windows 10 amd64-64-Bit Compressed References 20181020_83 (JIT enabled, AOT enabled)
OpenJ9   - 090ff9dc
OMR      - ea548a66
JCL      - f62696f378 based on jdk-11.0.1+13)

$ ../OpenJ9-11/java Version
jave.specification.version=11
jave.version=11.0.1
jave.version.split(""."")[0]=11


$ ../IBMJDK-8/java -version
java version ""1.8.0""
Java(TM) SE Runtime Environment (build pwa6480-20150129_02)
IBM J9 VM (build 2.8, JRE 1.8.0 Windows 8.1 amd64-64 Compressed References 20150116_231420 (JIT enabled, AOT enabled)
J9VM - R28_Java8_GA_20150116_2030_B231420
JIT  - tr.r14.java_20150109_82886.02
GC   - R28_Java8_GA_20150116_2030_B231420_CMPRSS
J9CL - 20150116_231420)
JCL - 20150123_01 based on Oracle jdk8u31-b12

$ ../IBMJDK-8/java Version
jave.specification.version=1.8
jave.version=1.8.0
jave.version.split(""."")[0]=1
{code};;;","16/Nov/18 05:12;shivaram;This is a great list -- I dont think we are able to handle all of these scenarios ? [~kiszk] do you know of any existing library that parses all the version strings ?;;;","17/Nov/18 08:11;kiszk;I do not know an existing library to parse output of {{java -version}}.
You may want to know the difference between OpenJDK and Oracle JDK, as shown [here|https://stackoverflow.com/questions/36445502/bash-command-to-check-if-oracle-or-openjdk-java-version-is-installed-on-linux] and [there|https://qiita.com/mao172/items/42aa841280dc5a4e9924].

Output of OpenJDK 12-ea.
{code}
$ ../OpenJDK-12/java -version
openjdk version ""12-ea"" 2019-03-19
OpenJDK Runtime Environment (build 12-ea+20)
OpenJDK 64-Bit Server VM (build 12-ea+20, mixed mode, sharing)

$ ../OpenJDK-12/java Version
jave.specification.version=12
jave.version=12-ea
jave.version.split(""."")[0]=12-ea
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not fail fast when dynamic resource allocation enabled with 0 executor,SPARK-24241,13158413,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,10/May/18 08:41,15/May/18 08:18,13/Jul/23 08:45,15/May/18 08:16,2.3.0,,,,,,,,,2.4.0,,,,,Spark Submit,,,,0,,,,"{code:java}
~/spark-2.3.0-bin-hadoop2.7$ bin/spark-sql --num-executors 0 --conf spark.dynamicAllocation.enabled=true
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=1024m; support was removed in 8.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1024m; support was removed in 8.0
Error: Number of executors must be a positive number
Run with --help for usage help or --verbose for debug output
{code}

Actually, we could start up with min executor number with 0 before ",,apachespark,jerryshao,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 08:18:12 UTC 2018,,,,,,,,,,"0|i3tk47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/18 09:04;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/21290;;;","15/May/18 08:18;jerryshao;Issue resolved by pull request 21290
https://github.com/apache/spark/pull/21290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: KafkaContinuousSourceSuite.subscribing topic by name from earliest offsets,SPARK-24239,13158368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,dongjoon,dongjoon,10/May/18 04:44,20/Feb/19 12:36,13/Jul/23 08:45,20/Feb/19 12:36,2.3.2,,,,,,,,,2.3.4,,,,,Structured Streaming,Tests,,,0,,,,"- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/360/
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/353/",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 12:36:34 UTC 2019,,,,,,,,,,"0|i3tju7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/19 23:58;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/23757;;;","13/Feb/19 04:09;maropu;Based on the discussion (https://github.com/apache/spark/pull/23757), I cherry-picked the three commits in branch-2.3: https://github.com/apache/spark/compare/02e989057fbf110b9048788a578c2620cb94a7ca...55d5a19c8e01de945c4c9e42752ed132df4b9110

I'll keep watching the Jenkins tests in branch-2.3 for a couple of days, and if the flaky tests gone, I'll close this.;;;","20/Feb/19 12:36;maropu;Resolved by https://github.com/apache/spark/pull/23757;;;","20/Feb/19 12:36;maropu;I closed this cuz it seems these test failures don't happen in the recent branch-2.3 test runs.

Thanks [~kabhwan]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
With Parquet 1.10 upgrade has errors in the vectorized reader,SPARK-24230,13158319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,ianoc,ianoc,09/May/18 23:07,24/May/18 13:01,13/Jul/23 08:45,24/May/18 13:01,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"When reading some parquet files can get an error like:

java.io.IOException: expecting more rows but reached last block. Read 0 out of 1194236

This happens when looking for a needle thats pretty rare in a large haystack.

 

The issue here I believe is that the total row count is calculated at

[https://github.com/apache/spark/blob/master/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L229]

 

But we pass the blocks we filtered via 

org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups

to the ParquetFileReader constructor.

 

However the ParquetFileReader constructor will filter the list of blocks again using

 

[https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L737]

 

if a block is filtered out by the latter method, and not the former the vectorized reader will believe it should see more rows than it will.

the fix I used locally is pretty straight forward:
{code:java}
for (BlockMetaData block : blocks) {
this.totalRowCount += block.getRowCount();
}
{code}
goes to
{code:java}
this.totalRowCount = this.reader.getRecordCount();
{code}
[~rdblue] do you know if this sounds right? The second filter method in the ParquetFileReader might filter more blocks leading to the count being off? ",,apachespark,dongjoon,ianoc,maropu,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 18:57:07 UTC 2018,,,,,,,,,,"0|i3tjjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/18 23:14;rdblue;Looks like I have a fix for this that I missed when submitting the patch for 1.10.0. Here it is:

{code:java}
--- a/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java
+++ b/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java
@@ -148,7 +148,8 @@ public abstract class SpecificParquetRecordReaderBase<T> extends RecordReader<Vo
     this.sparkSchema = StructType$.MODULE$.fromString(sparkRequestedSchemaString);
     this.reader = new ParquetFileReader(
         configuration, footer.getFileMetaData(), file, blocks, requestedSchema.getColumns());
-    for (BlockMetaData block : blocks) {
+    // use the blocks from the reader in case some do not match filters and will not be read
+    for (BlockMetaData block : reader.getRowGroups()) {
       this.totalRowCount += block.getRowCount();
     }
 
@@ -224,7 +225,8 @@ public abstract class SpecificParquetRecordReaderBase<T> extends RecordReader<Vo
     this.sparkSchema = new ParquetToSparkSchemaConverter(config).convert(requestedSchema);
     this.reader = new ParquetFileReader(
         config, footer.getFileMetaData(), file, blocks, requestedSchema.getColumns());
-    for (BlockMetaData block : blocks) {
+    // use the blocks from the reader in case some do not match filters and will not be read
+    for (BlockMetaData block : reader.getRowGroups()) {
       this.totalRowCount += block.getRowCount();
     }
   }
{code}

I'll submit a PR to fix this. Thanks for reporting it!;;;","09/May/18 23:18;ianoc;Great, thanks! 

 

 

(I think its probably worth just using the reader.getRecordCount since the identical logic is already in the file reader we want);;;","10/May/18 18:57;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/21295;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the lint error,SPARK-24228,13158311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,smilegator,smilegator,09/May/18 22:17,12/Dec/22 18:10,13/Jul/23 08:45,14/May/18 02:57,2.4.0,,,,,,,,,2.4.0,,,,,Build,,,,0,,,,"[ERROR] src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java:[21,8] (imports) UnusedImports: Unused import - java.io.ByteArrayInputStream.
[ERROR] src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java:[29,8] (imports) UnusedImports: Unused import - org.apache.spark.unsafe.Platform.",,apachespark,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 14 02:57:55 UTC 2018,,,,,,,,,,"0|i3tjhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/18 16:12;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21301;;;","14/May/18 02:57;gurwls223;Fixed in https://github.com/apache/spark/pull/21301;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark TypedAggregateExpression uses getSimpleName that is not safe in scala,SPARK-24216,13158052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shengzhixia,shengzhixia,shengzhixia,09/May/18 00:02,16/Jun/18 03:32,13/Jul/23 08:45,12/Jun/18 19:10,2.3.0,2.3.1,,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"When user create a aggregator object in scala and pass the aggregator to Spark Dataset's agg() method, Spark's will initialize TypedAggregateExpression with the nodeName field as aggregator.getClass.getSimpleName. However, getSimpleName is not safe in scala environment, depending on how user creates the aggregator object. For example, if the aggregator class full qualified name is ""com.my.company.MyUtils$myAgg$2$"", the getSimpleName will throw java.lang.InternalError ""Malformed class name"". This has been reported in scalatest [scalatest/scalatest#1044|https://github.com/scalatest/scalatest/pull/1044] and discussed in many scala upstream jiras such as SI-8110, SI-5425.

To fix this issue, we follow the solution in [scalatest/scalatest#1044|https://github.com/scalatest/scalatest/pull/1044] to add safer version of getSimpleName as a util method, and TypedAggregateExpression will invoke this util method rather than getClass.getSimpleName.",,apachespark,cloud_fan,maropu,shengzhixia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 19:10:48 UTC 2018,,,,,,,,,,"0|i3thw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/18 00:17;apachespark;User 'fangshil' has created a pull request for this issue:
https://github.com/apache/spark/pull/21276;;;","12/Jun/18 19:10;cloud_fan;Issue resolved by pull request 21276
[https://github.com/apache/spark/pull/21276];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingRelationV2/StreamingExecutionRelation/ContinuousExecutionRelation.toJSON should not fail,SPARK-24214,13158035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,08/May/18 22:06,09/May/18 18:32,13/Jul/23 08:45,09/May/18 18:32,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Structured Streaming,,,,0,,,,"We should overwrite ""otherCopyArgs"" to provide the SparkSession parameter otherwise TreeNode.toJSON cannot get the full constructor parameter list.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 09 18:32:43 UTC 2018,,,,,,,,,,"0|i3thsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/May/18 22:09;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/21275;;;","09/May/18 18:32;zsxwing;Issue resolved by pull request 21275
[https://github.com/apache/spark/pull/21275];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: StreamingOuterJoinSuite,SPARK-24211,13157956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,dongjoon,dongjoon,08/May/18 17:30,20/Feb/19 12:35,13/Jul/23 08:45,20/Feb/19 12:33,2.3.2,,,,,,,,,2.3.4,,,,,Structured Streaming,Tests,,,0,,,,"*windowed left outer join*
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/330/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/317/]

*windowed right outer join*
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/334/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/328/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/371/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.6/345/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/366/]

*left outer join with non-key condition violated*
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/337/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/366/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.6/386/]

*left outer early state exclusion on left*
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/375]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.6/385/]",,apachespark,dongjoon,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,SPARK-23408,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 12:35:28 UTC 2019,,,,,,,,,,"0|i3thav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/19 23:57;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/23757;;;","13/Feb/19 04:08;maropu;Based on the discussion (https://github.com/apache/spark/pull/23757), I cherry-picked the three commits in branch-2.3: https://github.com/apache/spark/compare/02e989057fbf110b9048788a578c2620cb94a7ca...55d5a19c8e01de945c4c9e42752ed132df4b9110

I'll keep watching the Jenkins tests in branch-2.3 for a couple of days, and if the flaky tests gone, I'll close this.;;;","20/Feb/19 12:33;maropu;Resolved by https://github.com/apache/spark/pull/23757;;;","20/Feb/19 12:35;maropu;I closed this cuz it seems these test failures don't happen in the recent branch-2.3 test runs.

Thanks [~kabhwan]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sc.addFile for local:/ path is broken,SPARK-24195,13157486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,XuanYuan,felixcheung,felixcheung,07/May/18 05:12,20/Jul/18 03:27,13/Jul/23 08:45,20/Jul/18 03:26,1.3.1,1.4.1,1.5.2,1.6.3,2.0.2,2.1.2,2.2.1,2.3.0,,2.4.0,,,,,Spark Core,,,,0,starter,,,"In changing SPARK-6300
https://github.com/apache/spark/commit/00e730b94cba1202a73af1e2476ff5a44af4b6b2

essentially the change to
new File(path).getCanonicalFile.toURI.toString

breaks when path is local:, as java.io.File doesn't handle it.

eg.

new File(""local:///home/user/demo/logger.config"").getCanonicalFile.toURI.toString
res1: String = file:/user/anotheruser/local:/home/user/demo/logger.config",,apachespark,felixcheung,holden,jerryshao,mgaido,riza,Teng Peng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 03:26:38 UTC 2018,,,,,,,,,,"0|i3teen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/18 01:57;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/21533;;;","20/Jul/18 03:26;jerryshao;Issue resolved by pull request 21533
[https://github.com/apache/spark/pull/21533];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lineSep shouldn't be required in JSON write,SPARK-24190,13157373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,05/May/18 16:52,24/Jun/18 00:41,13/Jul/23 08:45,24/Jun/18 00:41,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Currently, the lineSep option is required by JSON datasource in write if encoding is different from UTF-8. For example, the code:
{code:scala}
df.write.option(""encoding"", ""UTF-32BE"").json(file)
{code}

throws the exception:
{code}
requirement failed: The lineSep option must be specified for the UTF-32BE encoding
java.lang.IllegalArgumentException: requirement failed: The lineSep option must be specified for the UTF-32BE encoding
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.catalyst.json.JSONOptions$$anonfun$32.apply(JSONOptions.scala:118)
	at org.apache.spark.sql.catalyst.json.JSONOptions$$anonfun$32.apply(JSONOptions.scala:103)
	at scala.Option.map(Option.scala:146)
{code}

The restriction should NOT be applied to writing.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 05 17:10:05 UTC 2018,,,,,,,,,,"0|i3tdpj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/18 17:10;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21247;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonToStructs should not access SQLConf at executor side,SPARK-24169,13156783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/May/18 05:34,03/May/18 15:38,13/Jul/23 08:45,03/May/18 15:38,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 15:38:08 UTC 2018,,,,,,,,,,"0|i3tagv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 05:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21226;;;","03/May/18 15:38;cloud_fan;Issue resolved by pull request 21226
[https://github.com/apache/spark/pull/21226];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowExec should not access SQLConf at executor side,SPARK-24168,13156782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/May/18 05:30,04/May/18 00:27,13/Jul/23 08:45,04/May/18 00:27,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 05:32:06 UTC 2018,,,,,,,,,,"0|i3tagn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 05:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21225;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetFilters should not access SQLConf at executor side,SPARK-24167,13156779,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/May/18 05:20,04/May/18 01:29,13/Jul/23 08:45,04/May/18 01:29,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 05:27:09 UTC 2018,,,,,,,,,,"0|i3tafz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 05:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21224;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryTableScanExec should not access SQLConf at executor side,SPARK-24166,13156776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/May/18 05:04,03/May/18 12:11,13/Jul/23 08:45,03/May/18 12:11,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 12:11:19 UTC 2018,,,,,,,,,,"0|i3tafb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 05:08;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21223;;;","03/May/18 12:11;cloud_fan;Issue resolved by pull request 21223
[https://github.com/apache/spark/pull/21223];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF within when().otherwise() raises NullPointerException,SPARK-24165,13156746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mn-mikke,jingxuan001,jingxuan001,03/May/18 00:56,17/Jul/18 15:00,13/Jul/23 08:45,11/Jul/18 04:21,2.2.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"I have a UDF which takes java.sql.Timestamp and String as input column type and returns an Array of (Seq[case class], Double) as output. Since some of values in input columns can be nullable, I put the UDF inside a when($input.isNull, null).otherwise(UDF) filter. Such function works well when I test in spark shell. But running as a scala jar in spark-submit with yarn cluster mode, it raised NullPointerException which points to the UDF function. If I remove the when().otherwsie() condition, but put null check inside the UDF, the function works without issue in spark-submit.",,apachespark,cloud_fan,jingxuan001,maropu,mgaido,mn-mikke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 17 15:00:06 UTC 2018,,,,,,,,,,"0|i3ta8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 12:31;mn-mikke;It seems that Spark is not able resolve nullability for nested types correctly.

{{val rows = new util.ArrayList[Row]()}}
 {{rows.add(Row(true, (""1"", 1)))}}
 {{rows.add(Row(false, (null, 2)))}}
 {{val schema = StructType(Seq(}}
 {{StructField(""cond"", BooleanType, false),}}
 {{StructField(""s"", StructType(Seq(}}
 {{StructField(""val1"", StringType, true),}}
 {{StructField(""val2"", IntegerType, false)}}
 {{)))}}
 {{))}}

{{val df = spark.createDataFrame(rows, schema)}}

{{df.select(when('cond, expr(""struct('x' as val1, 10 as val2)"")).otherwise('s) as ""result"").printSchema()}}

Result:

{{root}}
 {{|-- result: struct (nullable = true)}}
 {{| |-- val1: string (nullable = *{color:#ff0000}false{color}*)}}
 {{| |-- val2: integer (nullable = false)}}

 

I will take a look at the problem.

 ;;;","29/Jun/18 14:40;mn-mikke;[~jingxuan001] Please could you provide a short example demonstrating the bug? I would like to double check that my patch fixes exactly your problam. Thanks.;;;","01/Jul/18 14:07;apachespark;User 'mn-mikke' has created a pull request for this issue:
https://github.com/apache/spark/pull/21687;;;","11/Jul/18 04:21;cloud_fan;Issue resolved by pull request 21687
[https://github.com/apache/spark/pull/21687];;;","11/Jul/18 10:56;apachespark;User 'mn-mikke' has created a pull request for this issue:
https://github.com/apache/spark/pull/21747;;;","17/Jul/18 15:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR CRAN feasibility check server problem,SPARK-24152,13156657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,dongjoon,dongjoon,02/May/18 17:14,12/Dec/22 18:11,13/Jul/23 08:45,03/Nov/19 04:38,2.4.0,,,,,,,,,2.4.5,3.0.0,,,,SparkR,Tests,,,0,,,,"PR builder and master branch test fails with the following SparkR error with unknown reason. The following is an error message from that.

{code}
* this is package 'SparkR' version '2.4.0'
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) : 
  dims [product 24] do not match the length of object [0]
Execution halted
{code}

*PR BUILDER*
- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/90039/
- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/89983/
- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/89998/

*MASTER BRANCH*
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.6/4458/ (Fail with no failures)

This is critical because we already start to merge the PR by ignoring this **known unkonwn** SparkR failure.
- https://github.com/apache/spark/pull/21175",,dongjoon,felixcheung,josephkb,kiszk,mgaido,seancxmao,shivaram,vanzin,viirya,yucai,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,SPARK-25923,SPARK-29732,,SPARK-22812,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 03 04:38:38 UTC 2019,,,,,,,,,,"0|i3t9ov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/18 17:17;dongjoon;cc [~shivaram], [~felixcheung], [~yanboliang];;;","03/May/18 00:57;gurwls223;cc [~viirya] too;;;","03/May/18 02:56;viirya;Thanks [~hyukjin.kwon] for pinging me. I found a problem in CRAN PACKAGES.in file. Seems it causes the R test failure again. Already emailed to cran sysadmin for help.

 

 ;;;","03/May/18 04:29;shivaram;Unfortunately I dont have time to look at this till Friday. Do we know if the problem is in SparkR or from some other package ?;;;","03/May/18 04:37;gurwls223;From Liang-Chi's comment and given previous discussion and resolution - [https://github.com/apache/spark/pull/20005|https://github.com/apache/spark/pull/20005] (SPARK-22812) seems it's a problem from R's. We (mainly he) investigated this problem there and he solved that by asking / reporting the problem to R dev. I think it's outside of Spark and we could wait for the response.

BTW, I think this is quite critical since it blocks all other PRs.;;;","03/May/18 04:41;shivaram;If this is blocking all PRs I think its fine to temporarily remove the CRAN check from Jenkins – We'll just need to be extra careful while merging SparkR PRs for a short period of time.;;;","03/May/18 04:44;gurwls223;For the past issue, it was fixed within only a couple of hours (after his action to CRAN admin). Will take an action if it takes a longer while.;;;","03/May/18 05:45;viirya;CRAN sysadmin replied me it should be fixed now. I can't access laptop so
don't confirm it. Maybe someone can confirm it by checking if Jenkins R
tests pass now.

Thanks.


;;;","03/May/18 05:58;gurwls223;Thanks [~viirya], I retriggered one build. Will resolve this once it gets passed.;;;","03/May/18 06:13;gurwls223;FYI [~smilegator] and [~cloud_fan];;;","03/May/18 06:14;felixcheung;Is this still a problem?;;;","03/May/18 06:21;gurwls223;Seems fixed just now. I found one build passed - https://github.com/apache/spark/pull/21190#issuecomment-386198706 but let me check other ones before resolving this .. ;;;","03/May/18 06:25;felixcheung;ok good.

in the event this reoccurs persistently,

option 1:
 * since we have NO_TESTS, we could remove --as-cran from this line [https://github.com/apache/spark/blob/master/R/check-cran.sh#L54] (temporarily)

option 2:
 - we could set 

_R_CHECK_CRAN_INCOMING_ to ""FALSE"" in the environment to disable this check, 

check_CRAN_incoming()

(see [http://mtweb.cs.ucl.ac.uk/mus/bin/install_R/R-3.1.1/src/library/tools/R/check.R]

 

 

 ;;;","03/May/18 06:28;felixcheung;(I updated the bug title - it's not really flaky..);;;","03/May/18 07:09;viirya;I think it is fixed now. It works in local. But better to check Jenkins test results too.;;;","03/May/18 11:08;viirya;Can be resolved now as I saw Jenkins test passed.;;;","07/May/18 21:49;josephkb;Thank you all!;;;","06/Jul/18 17:00;gurwls223;Reopening this since I see the same symptom again:

{code}
* this is package 'SparkR' version '2.4.0'
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) : 
  dims [product 24] do not match the length of object [0]
Execution halted
Loading required package: methods
{code}

[~viirya], mind if I ask to take a look again?

If the same thing happens again, I think we should better upgrade our R version to upper version (at least 3.4.0 seems having a fix to handle this), or take an action to prevent this since it's third time.;;;","06/Jul/18 20:55;felixcheung;I'm pretty sure it's a problem on the server side though

;;;","06/Jul/18 23:30;viirya;I've noticed it too and already asked CRAN sysadmin for fixing it. I think it is fixed now.;;;","10/Aug/18 07:20;gurwls223;Reopening this since the same thing happens again

{code}
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) : 
  dims [product 26] do not match the length of object [0]
Execution halted
{code}

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/94536/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/94538/console

Let me try to take an action for a permanent solution within few days (maybe with [~felixcheung], [~viirya] and [~shivaram]).

[~viirya], would you please mind if I ask to tell this one again to CRAN sysadmin?;;;","10/Aug/18 16:03;viirya;Sorry just see this. I will ask CRAN sysadmin again. ;;;","10/Aug/18 16:52;viirya;I checked locally. Seems fine, I don't see the error now. Wait for re-triggered tests to verify.;;;","10/Aug/18 21:20;viirya;I found retriggered test still failed. I found out the issue and sent email to CRAN for help.;;;","11/Aug/18 05:59;viirya;CRAN sysadmin replied they fixed it now. Looks good locally. Will trigger test to verify.

Note: verified. It is fixed now.;;;","12/Dec/18 17:37;mgaido;[~viirya] [~hyukjin.kwon] I am seeing this again constantly:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/100023/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/100018/console

Any idea? Thanks.;;;","12/Dec/18 17:43;dongjoon;[~felixcheung], [~shivaram] .

Can we split out CRAN checking into another Jenkins job? This seems to block us too frequently.
{code:java}
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) : 
  dims [product 24] do not match the length of object [0]
Execution halted{code};;;","13/Dec/18 00:17;gurwls223;This will be permanently prevented after Spark 3.0 release anyway http://apache-spark-developers-list.1001551.n3.nabble.com/discuss-SparkR-CRAN-feasibility-check-server-problem-td25605.html. Once we upgrade our R version in Jenkins, we can get rid of this malformed responses from server side.

For the current status, [~viirya] is taking an action by asking to CRAN sysadmins. It will be fixed soon I guess. If not, I am going to forward the dev mailing list to CRAN side. ;;;","13/Dec/18 02:53;gurwls223;Adding [~vanzin]. R test failure was by this, FYI.;;;","13/Dec/18 07:18;viirya;Just got reply from CRAN admin. It should be fixed now.;;;","03/Jul/19 01:19;viirya;I noticed that this issue happens now again. Contacted CRAN admin and asked for help. Will update when they reply.;;;","03/Jul/19 01:30;gurwls223;Thanks for followup!;;;","03/Jul/19 05:18;viirya;Received reply that is cleaned up. So I think it is fine now.;;;","03/Jul/19 05:32;dongjoon;Thank you so much!;;;","21/Jul/19 07:28;viirya;This CRAN issue is happening now, again. Emailed to CRAN admins for help. Will update after they reply.
;;;","21/Jul/19 07:31;gurwls223;Thanks. Let's target do drop lower R versions than 3.4 in Spark 3.1 to prevent this test failure permanently.;;;","21/Jul/19 14:20;viirya;Ok. I think it was fixed.;;;","21/Jul/19 21:15;dongjoon;Thank you again, [~viirya]!;;;","03/Nov/19 02:46;dongjoon;Sorry guys. This seems to fail again~;;;","03/Nov/19 03:33;viirya;Looking into it.;;;","03/Nov/19 03:44;viirya;Sent a request for help to CRAN sysadmin. Let's wait for a fix.;;;","03/Nov/19 04:16;dongjoon;Always thank you, [~viirya]. However, it seems that it's time for us to minimize this kind of outage. If we have a separate Jenkins job to check CRAN, only that job will be affected and we can have a plenty of time to repair.;;;","03/Nov/19 04:31;gurwls223;Yeah, so we deprecated R prior to 3.4. R 3.4 can handle this malformed response. Once we upgrade R to 3.4, we wont face this issue anymore.;;;","03/Nov/19 04:38;dongjoon;Issue resolved by pull request 26375
[https://github.com/apache/spark/pull/26375];;;"
"CURRENT_DATE, CURRENT_TIMESTAMP incorrectly resolved as column names when caseSensitive is enabled",SPARK-24151,13156646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jamesthomp,jamesthomp,jamesthomp,02/May/18 16:00,18/Sep/18 06:20,13/Jul/23 08:45,18/Sep/18 06:20,2.2.1,2.3.0,,,,,,,,2.4.0,,,,,SQL,,,,1,,,,"After this change: https://issues.apache.org/jira/browse/SPARK-22333

Running SQL such as ""CURRENT_TIMESTAMP"" can fail spark.sql.caseSensitive has been enabled:
{code:java}
org.apache.spark.sql.AnalysisException: cannot resolve '`CURRENT_TIMESTAMP`' given input columns: [col1]{code}
This is due to the fact that the analyzer incorrectly uses a case sensitive resolver to resolve the function. I will submit a PR with a fix + test for this.

 ",,apachespark,dongjoon,jamesthomp,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22333,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 17 11:01:04 UTC 2018,,,,,,,,,,"0|i3t9mf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/18 16:09;apachespark;User 'jamesthomp' has created a pull request for this issue:
https://github.com/apache/spark/pull/21217;;;","03/May/18 05:40;dongjoon;Thank you for reporting and fixing this, [~jamesthomp].
I also checked that this is a regression at Apache Spark 2.2.1 as you reported.;;;","17/Sep/18 11:01;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/22440;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"filter empty blocks when convert mapstatus to (blockId, size) pair",SPARK-24143,13156509,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,02/May/18 05:39,07/May/18 06:19,13/Jul/23 08:45,07/May/18 06:19,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"In current code(MapOutputTracker.convertMapStatuses), mapstatus are converted to (blockId, size) pair for all blocks -- no matter the block is empty or not, which result in OOM when there are lots of consecutive empty blocks, especially when adaptive execution is enabled.

(blockId, size) pair is only used in ShuffleBlockFetcherIterator to control shuffle-read and only non-empty block request is sent. Can we just filter out the empty blocks in  MapOutputTracker.convertMapStatuses and save memory?",,apachespark,cloud_fan,jinxing6042@126.com,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 07 06:19:12 UTC 2018,,,,,,,,,,"0|i3t8rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/18 05:49;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/21212;;;","07/May/18 06:19;cloud_fan;Issue resolved by pull request 21212
[https://github.com/apache/spark/pull/21212];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug in CoarseGrainedSchedulerBackend.killExecutors,SPARK-24141,13156488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Ngone51,Ngone51,Ngone51,02/May/18 01:57,09/May/18 22:45,13/Jul/23 08:45,09/May/18 22:45,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"In method _CoarseGrainedSchedulerBackend.killExecutors()_, *numPendingExecutors* should add 

*executorsToKill.size* rather than *knownExecutors.size* if we do not adjust target number of executors.

 ",,apachespark,Ngone51,riza,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Wed May 09 22:45:12 UTC 2018,,,,,,,,,,"0|i3t8nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/18 02:02;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/21209;;;","09/May/18 22:45;vanzin;Issue resolved by pull request 21209
[https://github.com/apache/spark/pull/21209];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[K8s] Mount temporary directories in emptydir volumes,SPARK-24137,13156364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mcheah,mcheah,mcheah,01/May/18 17:00,17/May/20 18:26,13/Jul/23 08:45,10/May/18 18:38,2.3.0,,,,,,,,,2.4.0,,,,,Kubernetes,Spark Core,,,0,,,,"Currently the Spark local directories do not get any volumes and volume mounts, which means we're writing Spark shuffle and cache contents to the file system mounted by Docker. This can be terribly inefficient. We should use emptydir volumes for these directories instead for significant performance improvements.",,apachespark,dvogelbacher,foxish,liyinan926,mcheah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 18:38:48 UTC 2018,,,,,,,,,,"0|i3t7vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/May/18 17:01;mcheah;[~foxish] [~liyinan926] for SA.

We actually did this on our prototype fork, but looks like it got lost when we implemented this stuff in mainline. https://github.com/apache-spark-on-k8s/spark/pull/522;;;","01/May/18 17:06;mcheah;Benchmark results on this subject are here: https://github.com/apache-spark-on-k8s/spark/pull/486#issuecomment-335635260;;;","01/May/18 17:36;liyinan926;Yeah, {{LocalDirectoryMountConfigurationStep}} was missed in the upstream PRs. We probably should try to get it into 2.3.1.;;;","02/May/18 05:53;foxish;SGTM. Let's try and share as much code as possible with https://issues.apache.org/jira/browse/SPARK-23529.;;;","04/May/18 21:44;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/21238;;;","10/May/18 18:38;foxish;Issue resolved by pull request 21238
[https://github.com/apache/spark/pull/21238];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading Parquet files containing large strings can fail with java.lang.ArrayIndexOutOfBoundsException,SPARK-24133,13156311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,01/May/18 08:23,03/May/18 14:53,13/Jul/23 08:45,02/May/18 19:59,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"ColumnVectors store string data in one big byte array. Since the array size is capped at just under Integer.MAX_VALUE, a single ColumnVector cannot store more than 2GB of string data.

However, since the Parquet files commonly contain large blobs stored as strings, and ColumnVectors by default carry 4096 values, it's entirely possible to go past that limit.

In such cases a negative capacity is requested from WritableColumnVector.reserve(). The call succeeds (requested capacity is smaller than already allocated), and consequently  java.lang.ArrayIndexOutOfBoundsException is thrown when the reader actually attempts to put the data into the array.

This behavior is hard to troubleshoot for the users. Spark should instead check for negative requested capacity in WritableColumnVector.reserve() and throw more informative error, instructing the user to tweak ColumnarBatch size.",,ala.luszczak,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 11:17:06 UTC 2018,,,,,,,,,,"0|i3t7k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/May/18 09:42;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/21206;;;","03/May/18 11:17;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/21227;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a flaky test `DateTimeUtilsSuite.monthsBetween`,SPARK-24123,13156173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,dongjoon,dongjoon,30/Apr/18 18:15,02/May/18 20:50,13/Jul/23 08:45,02/May/18 20:50,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"**MASTER BRANCH**
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.6/4810/testReport/org.apache.spark.sql.catalyst.util/DateTimeUtilsSuite/monthsBetween/

{code}
Error Message

3.949596773820191 did not equal 3.9495967741935485

Stacktrace

      org.scalatest.exceptions.TestFailedException: 3.949596773820191 did not equal 3.9495967741935485
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
      at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
      at org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite$$anonfun$25.apply(DateTimeUtilsSuite.scala:495)
      at org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite$$anonfun$25.apply(DateTimeUtilsSuite.scala:488)
{code}",,apachespark,dongjoon,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 19:18:06 UTC 2018,,,,,,,,,,"0|i3t6pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/18 19:18;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21196;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid calling UGI loginUserFromKeytab in ThriftServer,SPARK-24110,13155635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,27/Apr/18 07:04,03/May/18 01:28,13/Jul/23 08:45,03/May/18 01:28,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Spark ThriftServer will call UGI.loginUserFromKeytab twice in initialization. This is unnecessary and will cause various potential problems, like Hadoop IPC failure after 7 days, or RM failover issue and so on.

So here we need to remove all the unnecessary login logics and make sure UGI in the context never be created again.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 01:28:52 UTC 2018,,,,,,,,,,"0|i3t3en:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/18 07:46;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21178;;;","03/May/18 01:28;jerryshao;Issue resolved by pull request 21178
[https://github.com/apache/spark/pull/21178];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChunkedByteBuffer.writeFully method has not reset the limit value,SPARK-24107,13155589,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,csbliss,csbliss,csbliss,27/Apr/18 03:03,17/May/20 18:21,13/Jul/23 08:45,02/May/18 14:52,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Block Manager,Input/Output,Spark Core,,0,correctness,,,"ChunkedByteBuffer.writeFully method has not reset the limit value. When 
chunks larger than bufferWriteChunkSize, such as 80*1024*1024 larger than
config.BUFFER_WRITE_CHUNK_SIZE(64 * 1024 * 1024)，only while once, will lost 16*1024*1024 byte",,apachespark,csbliss,joshrosen,longcao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21527,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 06:45:20 UTC 2018,,,,,,,,,,"0|i3t36n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/18 03:21;apachespark;User 'manbuyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21175;;;","15/May/18 02:29;joshrosen;This bug was originally introduced in SPARK-21527 so I think that this problem only impacts Spark 2.3.0+.

This is a critical correctness issue and should be tagged as such, so I've added the {{correctness}} label and have promoted this to a BLOCKER.;;;","15/May/18 04:33;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21327;;;","15/May/18 06:45;joshrosen;To work around this bug on unpatched / unhotfixed Spark 2.3.x releases, you can set the following Spark configuration at SparkContext creation time:

{{spark.buffer.write.chunkSize 2147483647}}

This effectively undoes the effects of SPARK-21527.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLAppStatusListener overwrites metrics onDriverAccumUpdates instead of updating them,SPARK-24104,13155469,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,26/Apr/18 17:14,27/Apr/18 21:14,13/Jul/23 08:45,27/Apr/18 21:14,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"SqlAppStatusListener does 
{code}
exec.driverAccumUpdates = accumUpdates.toMap
update(exec)
{code}
in onDriverAccumUpdates.
But postDriverMetricUpdates is called multiple time per query, e.g. from each FileSourceScanExec and BroadcastExchangeExec.

If the update does not really update it in the KV store (depending on liveUpdatePeriodNs), the previously posted metrics are lost.",,apachespark,juliuszsompolski,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 27 21:14:57 UTC 2018,,,,,,,,,,"0|i3t2gf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/18 17:59;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/21171;;;","27/Apr/18 21:14;vanzin;Issue resolved by pull request 21171
[https://github.com/apache/spark/pull/21171];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scalar subquery error,SPARK-24085,13155112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,abaturin,abaturin,25/Apr/18 13:18,27/Apr/18 18:44,13/Jul/23 08:45,27/Apr/18 18:44,2.3.0,2.3.1,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Error
{noformat}
SQL Error: java.lang.UnsupportedOperationException: Cannot evaluate expression: scalar-subquery{noformat}
Then query a partitioed table based on a parquet file then filter by partition column by scalar subquery.

Query to reproduce:
{code:sql}
CREATE TABLE test_prc_bug (
id_value string
)
partitioned by (id_type string)
location '/tmp/test_prc_bug'
stored as parquet;

insert into test_prc_bug values ('1','a');
insert into test_prc_bug values ('2','a');
insert into test_prc_bug values ('3','b');
insert into test_prc_bug values ('4','b');


select * from test_prc_bug
where id_type = (select 'b');
{code}
If table in ORC format it works fine",,abaturin,apachespark,dkbiswal,kevinyu98,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 26 22:20:05 UTC 2018,,,,,,,,,,"0|i3t09b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/18 19:07;dkbiswal;Working on a fix for this. My current thinking is to not consider subquery expressions in the partition pruning process.;;;","26/Apr/18 22:20;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/21174;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
very bad performance when shuffle.partition = 8192,SPARK-24076,13155007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,25/Apr/18 02:36,06/Mar/19 04:04,13/Jul/23 08:45,08/May/18 09:35,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,We see very bad performance when shuffle.partition = 8192 on some cases.,,apachespark,maropu,yucai,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/18 06:29;yucai;image-2018-04-25-14-29-39-958.png;https://issues.apache.org/jira/secure/attachment/12920586/image-2018-04-25-14-29-39-958.png","25/Apr/18 02:43;yucai;p1.png;https://issues.apache.org/jira/secure/attachment/12920562/p1.png","25/Apr/18 02:43;yucai;p2.png;https://issues.apache.org/jira/secure/attachment/12920563/p2.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 25 07:14:05 UTC 2018,,,,,,,,,,"0|i3szlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/18 02:45;yucai;shuffle.partition = 8192

!p1.png!

shuffle.partition = 8000

!p2.png!;;;","25/Apr/18 06:27;yucai;The query example:

{code:sql}
insert overwrite table target_xxx
SELECT
 item_id,
 auct_end_dt
FROM
 (select cast(item_id as double) as item_id, auct_end_dt from source_xxx
GROUP BY
 item_id,
 auct_end_dt
{code}
;;;","25/Apr/18 06:31;yucai;Root cause: very bad hash conflict in hashaggregate.

!image-2018-04-25-14-29-39-958.png!

 ;;;","25/Apr/18 06:56;yucai;1. When shuffle.partition = 8192, tuples in the same partition follows the connection like below:

hash(tuple x) = hash(tuple y) + n * 8192

2. In the next HashAggregate stage, tuples from the same partition need put into a 16K BytesToBytesMap (unsafeRowAggBuffer).

Here, the HashAggregate uses the same hash algorithm and seed as shuffle, it leads to all tuples will be hashed to only 2 different places actually. That's why hash conflict happens.;;;","25/Apr/18 07:14;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/21149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV schema inferring doesn't work for compressed files,SPARK-24068,13154814,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,24/Apr/18 14:08,12/Dec/22 18:10,13/Jul/23 08:45,09/May/18 00:33,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Here is a simple csv file compressed by lzo
{code}
$ cat ./test.csv
col1,col2
a,1
$ lzop ./test.csv
$ ls
test.csv     test.csv.lzo
{code}

Reading test.csv.lzo with LZO codec (see https://github.com/twitter/hadoop-lzo, for example):
{code:scala}
scala> val ds = spark.read.option(""header"", true).option(""inferSchema"", true).option(""io.compression.codecs"", ""com.hadoop.compression.lzo.LzopCodec"").csv(""/Users/maximgekk/tmp/issue/test.csv.lzo"")
ds: org.apache.spark.sql.DataFrame = [�LZO?: string]

scala> ds.printSchema
root
 |-- �LZO: string (nullable = true)


scala> ds.show
+-----+
|�LZO|
+-----+
|    a|
+-----+
{code}
but the file can be read if the schema is specified:
{code}
scala> import org.apache.spark.sql.types._
scala> val schema = new StructType().add(""col1"", StringType).add(""col2"", IntegerType)
scala> val ds = spark.read.schema(schema).option(""header"", true).option(""io.compression.codecs"", ""com.hadoop.compression.lzo.LzopCodec"").csv(""test.csv.lzo"")
scala> ds.show
+----+----+
|col1|col2|
+----+----+
|   a|   1|
+----+----+
{code}

Just in case, schema inferring works for the original uncompressed file:
{code:scala}
scala> spark.read.option(""header"", true).option(""inferSchema"", true).csv(""test.csv"").printSchema
root
 |-- col1: string (nullable = true)
 |-- col2: integer (nullable = true)
{code}",,apachespark,maxgekk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 13:00:07 UTC 2018,,,,,,,,,,"0|i3syf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/18 01:11;gurwls223;Hm, [~maxgekk], btw is this specific to CSV (not, for example JSON)?;;;","26/Apr/18 19:44;maxgekk;The same issue exists in JSON datasource. [~hyukjin.kwon] Do we need a separate ticket for that?;;;","27/Apr/18 00:51;gurwls223;I roughly assume the fix will be small, similar or the same? I think it's fine to describe both issues here.;;;","27/Apr/18 15:55;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21182;;;","09/May/18 00:33;gurwls223;Issue resolved by pull request 21182
[https://github.com/apache/spark/pull/21182];;;","10/May/18 13:00;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21292;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport SPARK-17147 to 2.3 (Spark Streaming Kafka 0.10 Consumer Can't Handle Non-consecutive Offsets (i.e. Log Compaction)),SPARK-24067,13154777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,jhereth,jhereth,24/Apr/18 11:54,14/May/18 18:52,13/Jul/23 08:45,11/May/18 18:41,2.3.0,,,,,,,,,2.3.1,,,,,DStreams,,,,0,,,,"SPARK-17147 fixes a problem with non-consecutive Kafka Offsets. The  [PR w|https://github.com/apache/spark/pull/20572]as merged to {{master}}. This should be backported to 2.3.

 

Original Description from SPARK-17147 :

 

When Kafka does log compaction offsets often end up with gaps, meaning the next requested offset will be frequently not be offset+1. The logic in KafkaRDD & CachedKafkaConsumer has a baked in assumption that the next offset will always be just an increment of 1 above the previous offset.

I have worked around this problem by changing CachedKafkaConsumer to use the returned record's offset, from:
 {{nextOffset = offset + 1}}
 to:
 {{nextOffset = record.offset + 1}}

and changed KafkaRDD from:
 {{requestOffset += 1}}
 to:
 {{requestOffset = r.offset() + 1}}

(I also had to change some assert logic in CachedKafkaConsumer).

There's a strong possibility that I have misconstrued how to use the streaming kafka consumer, and I'm happy to close this out if that's the case. If, however, it is supposed to support non-consecutive offsets (e.g. due to log compaction) I am also happy to contribute a PR.",,apachespark,hakim.acharifi,jhereth,koeninger,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17147,,,,,,,,,,SPARK-23685,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 14 18:52:33 UTC 2018,,,,,,,,,,"0|i3sy6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/18 14:24;koeninger;Given the response on the dev list about criteria for backporting, I think this is in a grey area.

In one sense, it's a new option.  But there have been reports of non-consecutive offsets even in normal non-compacted topics, which totally break jobs, so this could be seen as a critical bug.

I'd lean towards merging it to the 2.3 branch, but because I'm the one who wrote the code, I'm not comfortable making that call on my own. 

[~srowen] you merged the original PR, do you want to weigh in?;;;","25/Apr/18 14:51;srowen;It seems like a clear bug fix. Granted it's not trivial, but it is the kind of thing that certainly _can_ be backported. I think it's OK, as I don't see that it changes API or behavior that was already correct. The fact that there's a flag controlling it is a safety valve. I wouldn't think of that as a new feature.;;;","25/Apr/18 16:35;koeninger;The original PR [https://github.com/apache/spark/pull/20572] merges cleanly against 2.3, are you ok with merging it?;;;","02/May/18 11:01;jhereth;It would be great if this fix could go into 2.3.1. Any progress on that?;;;","11/May/18 15:07;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/21300;;;","11/May/18 18:41;koeninger;Issue resolved by pull request 21300
[https://github.com/apache/spark/pull/21300];;;","14/May/18 17:53;zsxwing;[~srowen] this sounds more like a new feature to me. The PR itself has a feature flag and it's supposed to support a Kafka feature that we didn't support before.;;;","14/May/18 18:03;koeninger;[~zsxwing] even in situations where users weren't enabling compaction, kafka is producing non-consecutive offsets, meaning this is a bug that prevents previously working jobs from being able to complete.;;;","14/May/18 18:52;zsxwing;Okey. Since this is turned off by default, I'm okey to backport it as it doesn't affect existing jobs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASL encryption cannot be worked in ThriftServer,SPARK-24062,13154720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,24/Apr/18 07:59,26/Apr/18 05:30,13/Jul/23 08:45,26/Apr/18 05:30,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,SQL,,,0,,,,"Spark thrift server will throw an exception when SASL encryption is used.

 
{noformat}
18/04/16 14:36:46 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 8384069538832556183
java.lang.IllegalArgumentException: A secret key must be specified via the spark.authenticate.secret config
at org.apache.spark.SecurityManager$$anonfun$getSecretKey$4.apply(SecurityManager.scala:510)
at org.apache.spark.SecurityManager$$anonfun$getSecretKey$4.apply(SecurityManager.scala:510)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.SecurityManager.getSecretKey(SecurityManager.scala:509)
at org.apache.spark.SecurityManager.getSecretKey(SecurityManager.scala:551)
at org.apache.spark.network.sasl.SparkSaslServer$DigestCallbackHandler.handle(SparkSaslServer.java:166)
at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:589)
at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)
at org.apache.spark.network.sasl.SparkSaslServer.response(SparkSaslServer.java:119)
at org.apache.spark.network.sasl.SaslRpcHandler.receive(SaslRpcHandler.java:103)
at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:187)
at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111){noformat}
To investigate it, the issue is:

Spark on Yarn stores SASL secret in current UGI's credentials, this credentials will be distributed to AM and executors, so that executors and drive share the same secret to communicate. But STS/Hive library code will refresh the current UGI by UGI's loginFromKeytab(), this will create a new UGI in the current context with empty tokens and secret keys, so secret key is lost in the current context's UGI, that's why Spark driver throws secret key not found exception.

In Spark 2.2 code, Spark also stores this secret key in {{SecurityManager}}'s class variable, so even UGI is refreshed, the secret is still existed in the object, so STS with SASL can still be worked in Spark 2.2. But in Spark 2.3, we always search key from current UGI, which makes it fail to work in Spark 2.3.

To fix this issue, there're two possible solutions:

1. Fix in STS/Hive library, when a new UGI is refreshed, copy the secret key from original UGI to the new one. The difficulty is that some codes to refresh the UGI is existed in Hive library, which makes us hard to change the code.
2. Roll back the logics in SecurityManager to match Spark 2.2, so that this issue can be fixed.

2nd solution seems a simple one. So I will propose a PR with 2nd solution.",,apachespark,jerryshao,riza,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 26 05:30:12 UTC 2018,,,,,,,,,,"0|i3sxu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/18 08:32;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21138;;;","26/Apr/18 05:30;jerryshao;Issue resolved by pull request 21138
[https://github.com/apache/spark/pull/21138|https://github.com/apache/spark/pull/21138];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SS]TypedFilter is not supported in Continuous Processing,SPARK-24061,13154694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanlin-Lynn,yanlin-Lynn,yanlin-Lynn,24/Apr/18 04:33,01/May/18 08:30,13/Jul/23 08:45,01/May/18 08:29,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"using filter with filter function in continuous processing application ,cause error",,apachespark,yanlin-Lynn,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/18 04:33;yanlin-Lynn;TypedFilter_error.png;https://issues.apache.org/jira/secure/attachment/12920402/TypedFilter_error.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 01 08:29:01 UTC 2018,,,,,,,,,,"0|i3sxof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/18 04:40;apachespark;User 'yanlin-Lynn' has created a pull request for this issue:
https://github.com/apache/spark/pull/21136;;;","01/May/18 08:29;zsxwing;Issue resolved by pull request 21136
[https://github.com/apache/spark/pull/21136];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make consumer creation lazy in Kafka source for Structured streaming,SPARK-24056,13154617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tdas,tdas,tdas,23/Apr/18 21:05,13/Jul/18 05:27,13/Jul/23 08:45,24/Apr/18 21:34,2.4.0,,,,,,,,,2.4.0,,,,,Structured Streaming,,,,0,,,,"Currently, the driver side of the Kafka source (i.e. KafkaMicroBatchReader) eagerly creates a consumer as soon as the Kafk aMicroBatchReader is created. However, we create dummy KafkaMicroBatchReader to get the schema and immediately stop it. Its better to make the consumer creation lazy, it will be created on the first attempt to fetch offsets using the KafkaOffsetReader.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 24 21:34:58 UTC 2018,,,,,,,,,,"0|i3sx7b:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"23/Apr/18 22:01;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/21134;;;","24/Apr/18 21:34;tdas;Issue resolved by pull request 21134
[https://github.com/apache/spark/pull/21134];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingQuery does not calculate input / processing rates in some cases,SPARK-24050,13154374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,23/Apr/18 05:39,13/Jul/18 05:27,13/Jul/23 08:45,25/Apr/18 19:22,2.1.0,2.1.1,2.1.2,2.2.0,2.2.1,2.3.0,,,,2.4.0,,,,,Structured Streaming,,,,0,,,,"In some streaming queries, the input and processing rates are not calculated at all (shows up as zero) because MicroBatchExecution fails to associated metrics from the executed plan of a trigger with the sources in the logical plan of the trigger. The way this executed-plan-leaf-to-logical-source attribution works is as follows. With V1 sources, there was no way to identify which execution plan leaves were generated by a streaming source. So did a best-effort attempt to match logical and execution plan leaves when the number of leaves were same. In cases where the number of leaves is different, we just give up and report zero rates. An example where this may happen is as follows.
{code}
val cachedStaticDF = someStaticDF.union(anotherStaticDF).cache()
val streamingInputDF = ...

val query = streamingInputDF.join(cachedStaticDF).writeStream....
{code}

In this case, the {{cachedStaticDF}} has multiple logical leaves, but in the trigger's execution plan it only has leaf because a cached subplan is represented as a single InMemoryTableScanExec leaf. This leads to a mismatch in the number of leaves causing the input rates to be computed as zero. 

With DataSourceV2, all inputs are represented in the executed plan using {{DataSourceV2ScanExec}}, each of which has a reference to the associated logical {{DataSource}} and {{DataSourceReader}}. So its easy to associate the metrics to the original streaming sources. So the solution is to take advantage of the presence of DataSourceV2 whenever possible.

",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 25 19:22:09 UTC 2018,,,,,,,,,,"0|i3svpb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"23/Apr/18 06:24;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/21126;;;","25/Apr/18 19:22;tdas;Issue resolved by pull request 21126
[https://github.com/apache/spark/pull/21126];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InterpretedPredicate.eval fails if expression tree contains Nondeterministic expressions,SPARK-24043,13154287,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,22/Apr/18 03:46,07/May/18 15:54,13/Jul/23 08:45,07/May/18 15:54,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"When whole-stage codegen and predicate codegen both fail, FilterExec falls back to using InterpretedPredicate. If the predicate's expression contains any non-deterministic expressions, the evaluation throws an error:
{noformat}
scala> val df = Seq((1)).toDF(""a"")
df: org.apache.spark.sql.DataFrame = [a: int]

scala> df.filter('a > 0).show // this works fine
2018-04-21 20:39:26 WARN  FilterExec:66 - Codegen disabled for this expression:
 (value#1 > 0)
+---+
|  a|
+---+
|  1|
+---+

scala> df.filter('a > rand(7)).show // this will throw an error
2018-04-21 20:39:40 WARN  FilterExec:66 - Codegen disabled for this expression:
 (cast(value#1 as double) > rand(7))
2018-04-21 20:39:40 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.IllegalArgumentException: requirement failed: Nondeterministic expression org.apache.spark.sql.catalyst.expressions.Rand should be initialized before eval.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.catalyst.expressions.Nondeterministic$class.eval(Expression.scala:326)
	at org.apache.spark.sql.catalyst.expressions.RDG.eval(randomExpressions.scala:34)
{noformat}
This is because no code initializes the Nondeterministic expressions before eval is called on them.

This is a low impact issue, since it would require both whole-stage codegen and predicate codegen to fail before FilterExec would fall back to using InterpretedPredicate.",,apachespark,bersprockets,kiszk,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 24 19:31:04 UTC 2018,,,,,,,,,,"0|i3sv5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/18 06:20;maropu;I tried this in the master and v2.3 though, the issue didn't happen there. Do I miss any precondition?;;;","24/Apr/18 04:47;bersprockets;[~maropu]

> Do I miss any precondition?

For this bug to materialize in spark-shell, Spark SQL needs to be interpreted mode (whole-stage codegen and predicate codegen are shut off).

I ran some of the DataFrame and Dataset test suites in interpreted mode and this bug popped out (during the run for the test ""handle nondeterministic expressions correctly for set operations""). To put Spark SQL in interpreted mode, I manually shut off whole-stage codegen and predicate codegen. It was still off when I did the above spark-shell demo.

Outside of manually tweaking Spark, it's difficult to get predicate codegen to fail (It's easy to get whole-stage codegen to fall back – just supply more than 300 columns in your query. Predicate codegen is more resilient). That's why this is a low impact bug. However, at some point we might want to test interpreted mode.

I will make a PR, but it's no emergency.

To see the bug in action with Spark as-is, add these test cases to PredicateSuite. The first should succeed (no Nondeterministic expressions). The second will fail with an exception (""Nondeterministic expression org.apache.spark.sql.catalyst.expressions.Rand should be initialized before eval""):
{code:java}
  test(""Interpreted Predicate should work without nondeterministic expressions"") {
    val interpreted = InterpretedPredicate.create(LessThan(Literal(0.2), Literal(1.0)))
    interpreted.initialize(0)
    assert(interpreted.eval(new UnsafeRow()))
  }

  test(""Interpreted Predicate should initialize nondeterministic expressions"") {
    val interpreted = InterpretedPredicate.create(LessThan(Rand(7), Literal(1.0)))
    interpreted.initialize(0)
    assert(interpreted.eval(new UnsafeRow()))
  }
{code};;;","24/Apr/18 05:42;maropu;I tried this with codegen=off;
{code:java}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_31)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sql(""SET spark.sql.codegen.wholeStage=false"")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> Seq((1)).toDF(""a"").filter('a > rand(7)).show 
+---+
|  a|
+---+
|  1|
+---+
{code};;;","24/Apr/18 06:12;bersprockets;You're half-way there. When whole-stage codegen is off (and only then), FilterExec requests code generation and compilation for the predicate. See FilterExec.doExecute (which calls SparkPlan.newPredicate, which attempts to generate code for the predicate).

I couldn't find a configuration setting to turn off predicate codegen.;;;","24/Apr/18 07:37;maropu;Aha, I gotcha. ya, we currently have no configuration for the expression tree.;;;","24/Apr/18 19:31;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/21144;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LAG Window function broken in Spark 2.3,SPARK-24033,13154013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,emlyn,emlyn,20/Apr/18 12:18,21/Apr/18 17:46,13/Jul/23 08:45,21/Apr/18 17:46,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"The {{LAG}} window function appears to be broken in Spark 2.3.0, always failing with an AnalysisException. Interestingly, {{LEAD}} is not affected, so it can be worked around by negating the lag and using lead instead.

Reproduction (run in {{spark-shell}}):
{code:java}
import org.apache.spark.sql.expressions.Window
val ds = Seq((1,1),(1,2),(1,3),(2,1),(2,2)).toDF(""n"", ""i"")
// The following works:
ds.withColumn(""m"", lead(""i"", -1).over(Window.partitionBy(""n"").orderBy(""i"").rowsBetween(-1, -1))).show
// The following (equivalent) fails:
ds.withColumn(""m"", lag(""i"", 1).over(Window.partitionBy(""n"").orderBy(""i"").rowsBetween(-1, -1))).show
{code}

Here is the stacktrace:
{quote}
org.apache.spark.sql.AnalysisException: Window Frame specifiedwindowframe(RowFrame, -1, -1) must match the required frame specifiedwindowframe(RowFrame, -1, -1);
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:41)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame$$anonfun$apply$31$$anonfun$applyOrElse$10.applyOrElse(Analyzer.scala:2034)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame$$anonfun$apply$31$$anonfun$applyOrElse$10.applyOrElse(Analyzer.scala:2030)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:85)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:76)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame$$anonfun$apply$31.applyOrElse(Analyzer.scala:2030)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame$$anonfun$apply$31.applyOrElse(Analyzer.scala:2029)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame$.apply(Analyzer.scala:2029)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame$.apply(Analyzer.scala:2028)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
  at scala.collection.immutable.List.foldLeft(List.scala:84)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:123)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:117)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:102)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3295)
  at org.apache.spark.sql.Dataset.select(Dataset.scala:1307)
  at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2192)
  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2159)
  ... 49 elided
{quote}",,apachespark,emlyn,irinatruong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 20 17:50:05 UTC 2018,,,,,,,,,,"0|i3sthb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/18 17:50;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21115;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: SparkContextSuite,SPARK-24022,13153629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,19/Apr/18 09:16,19/Apr/18 22:07,13/Jul/23 08:45,19/Apr/18 22:07,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,Tests,,,0,,,,A flaky pattern found and fixed in SPARK-23775 but similar things exist in SparkContextSuite.,,apachespark,gsomogyi,riza,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 19 22:07:00 UTC 2018,,,,,,,,,,"0|i3sr5z:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"19/Apr/18 09:17;gsomogyi;Working on this.;;;","19/Apr/18 09:39;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/21105;;;","19/Apr/18 22:07;vanzin;Issue resolved by pull request 21105
[https://github.com/apache/spark/pull/21105];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug in BlacklistTracker's updateBlacklistForFetchFailure,SPARK-24021,13153565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,19/Apr/18 02:41,19/Apr/18 14:01,13/Jul/23 08:45,19/Apr/18 14:01,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,easyfix,,,"There's a miswrite in BlacklistTracker's updateBlacklistForFetchFailure:

 
{code:java}
val blacklistedExecsOnNode =
    nodeToBlacklistedExecs.getOrElseUpdate(exec, HashSet[String]())
blacklistedExecsOnNode += exec{code}
 

where first *exec* should be *host*.",,apachespark,irashid,Ngone51,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Apr 19 14:01:09 UTC 2018,,,,,,,,,,"0|i3sqrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/18 02:46;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/21104;;;","19/Apr/18 14:01;irashid;Issue resolved by pull request 21104
[https://github.com/apache/spark/pull/21104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-without-hadoop package fails to create or read parquet files with snappy compression,SPARK-24018,13153439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,jeanfrancisroy,jeanfrancisroy,18/Apr/18 18:45,10/Dec/18 16:44,13/Jul/23 08:45,10/Dec/18 16:44,2.3.0,,,,,,,,,2.3.2,,,,,Deploy,,,,2,,,,"On a brand-new installation of Spark 2.3.0 with a user-provided hadoop-2.8.3, Spark fails to read or write dataframes in parquet format with snappy compression.

This is due to an incompatibility between the snappy-java version that is required by parquet (parquet is provided in Spark jars but snappy isn't) and the version that is available from hadoop-2.8.3.

 

Steps to reproduce:
 * Download and extract hadoop-2.8.3
 * Download and extract spark-2.3.0-without-hadoop
 * export JAVA_HOME, HADOOP_HOME, SPARK_HOME, PATH
 * Following instructions from [https://spark.apache.org/docs/latest/hadoop-provided.html], set SPARK_DIST_CLASSPATH=$(hadoop classpath) in spark-env.sh
 * Start a spark-shell, enter the following:

 
{code:java}
import spark.implicits._
val df = List(1, 2, 3, 4).toDF
df.write
  .format(""parquet"")
  .option(""compression"", ""snappy"")
  .mode(""overwrite"")
  .save(""test.parquet"")
{code}
 

 

This fails with the following:
{noformat}
java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
    at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
    at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:316)
    at org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)
    at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)
    at org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)
    at org.apache.parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:112)
    at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:93)
    at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:150)
    at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:238)
    at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:121)
    at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:167)
    at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)
    at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:405)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:396)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748){noformat}
 

  Downloading snappy-java-1.1.2.6.jar and placing it in Sparks's jar folder solves the issue.",,dlazerka,jeanfrancisroy,pclay,yumwang,yuryn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18646,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 10 16:44:26 UTC 2018,,,,,,,,,,"0|i3spzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 08:47;yuryn;Hi, I have got exactly same problem with spark-2.3.1 without hadoop and I have spent nice morning till I have found this issue. It would be fine to fix this issue.;;;","06/Jul/18 22:26;pclay;I believe this is limited to spark-shell and was caused by SPARK-18646. Reverting it seems to fix the issue for me.

I don't know if there is a simple solution that fixes both this and the user classpath issue from that.;;;","07/Jul/18 18:06;jeanfrancisroy;I don't think this is only related to the Spark shell: in my case I don't use any user classpath, snappy-1.1.2.6 is just nowhere to be found in Spark's or Hadoop's ClassPath. I first got this issue using spark-submit. The parquet lib version provided by Spark is incompatible with snappy-1.0.4.1 found in Hadoop's classpath.

[~pclay] did you start spark-shell with any arguments? Maybe snappy is shaded in one of the JARs you have in your classpath?;;;","09/Jul/18 20:49;pclay;I believe we are both partially correct in that a fix (with Spark 2.3.0) does require snappy-java-1.1.2, and it was caused by SPARK-18646. The native library loader of Snappy 1.0.4 [uses a self-described hack|https://github.com/xerial/snappy-java/blob/snappy-java-1.0.4/src/main/java/org/xerial/snappy/SnappyLoader.java#L175] to inject the loader onto the root class loader. The hack was [later removed|https://github.com/xerial/snappy-java/commit/06f007a08#diff-a1c8fc77f8] in 1.1.2, which allows the non-inheriting class loader to pick it up.

 

I believed this only affects spark-shell, because neither pyspark (the REPL and spark-submit) nor
{code:java}
./bin/spark-submit --class org.apache.spark.examples.sql.SQLDataSourceExample examples/jars/spark-examples_2.11-2.3.0.jar{code}
have this issue. What repro did you have without spark-shell?

 

I don't believe this is related to Parquet versioning because this also throws:
{code:java}
scala> import org.xerial.snappy.Snappy 
import org.xerial.snappy.Snappy 

scala> sc.parallelize(Seq(""foo"")).map(Snappy.compress).collect 
2018-07-09 13:44:14 ERROR Executor:91 - Exception in task 11.0 in stage 0.0 (TID 11) 
java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
...{code}
 

In answer to your last question I did not pass any arguments to spark-shell. All I did to repro was
{code:java}
export SPARK_DIST_CLASSPATH=$(~/Downloads/hadoop-2.8.3/bin/hadoop classpath)
~/Downloads/spark-2.3.0-bin-without-hadoop/bin/spark-shell{code}
 

 ;;;","10/Jul/18 00:03;jeanfrancisroy;Oh, indeed you are right! I was mistaken when I thought that I had the error using spark-submit, I just verified again and it works, thanks for the explanation!;;;","22/Sep/18 11:51;yumwang;It may be fixed by [SPARK-24927|https://issues.apache.org/jira/browse/SPARK-24927].
;;;","10/Dec/18 16:44;jeanfrancisroy;I confirm the fix that appeared in Spark 2.3.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApproximatePercentile grinds to a halt on sorted input.,SPARK-24013,13153343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,juliuszsompolski,juliuszsompolski,18/Apr/18 13:12,02/May/18 19:00,13/Jul/23 08:45,02/May/18 19:00,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Running
{code}
sql(""select approx_percentile(rid, array(0.1)) from (select rand() as rid from range(10000000))"").collect()
{code}
takes 7 seconds, while
{code}
sql(""select approx_percentile(id, array(0.1)) from range(10000000)"").collect()
{code}
grinds to a halt - processes the first million rows quickly, and then slows down to a few thousands rows / second (4m rows processed after 20 minutes).

Thread dumps show that it spends time in QuantileSummary.compress.
Seems it hits some edge case inefficiency when dealing with sorted data?",,apachespark,juliuszsompolski,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/18 12:50;juliuszsompolski;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12920281/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 23 20:53:05 UTC 2018,,,,,,,,,,"0|i3spef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/18 13:15;juliuszsompolski;This hits when trying to create histogram statistics (SPARK-21975) on columns like monotonically increasing id - histograms cannot be created in reasonable time.;;;","23/Apr/18 12:29;mgaido;I cannot reproduce on current master. For me it was very fast the second query too.;;;","23/Apr/18 12:51;juliuszsompolski;Hi [~mgaido]
I tested again on current master (afbdf427302aba858f95205ecef7667f412b2a6a) and I reproduce it:
 !screenshot-1.png! 

Maybe you need to bump up 10000000 to something higher when running on a bigger cluster that splits the range into more tasks?
For me it grinds to a halt after about 2500000 per task.;;;","23/Apr/18 16:09;mgaido;[~juliuszsompolski] I have been able to reproduce with 10000000. Probably SPARK-17439 is related. The problem is that the compress method is called too many times in this condition. The fix is easy, I'll submit a patch soon, but I am not so familiar with this algorithm and the real root cause of the problem, so I have to study it a bit in order to check if there are other problems causing the performance issue.;;;","23/Apr/18 20:53;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21133;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union of map and other compatible column,SPARK-24012,13153322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liutang123,liutang123,liutang123,18/Apr/18 11:17,25/Apr/18 11:01,13/Jul/23 08:45,25/Apr/18 10:12,2.1.0,2.2.1,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Union of map and other compatible column result in unresolved operator 'Union; exception

Reproduction

spark-sql>select map(1,2), 'str' union all select map(1,2,3,null), 1

Output:

Error in query: unresolved operator 'Union;;
'Union
:- Project [map(1, 2) AS map(1, 2)#106, str AS str#107]
: +- OneRowRelation$
+- Project [map(1, cast(2 as int), 3, cast(null as int)) AS map(1, CAST(2 AS INT), 3, CAST(NULL AS INT))#109, 1 AS 1#108]
 +- OneRowRelation$

 ",,apachespark,cloud_fan,liutang123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 25 11:01:05 UTC 2018,,,,,,,,,,"0|i3sp9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/18 14:44;apachespark;User 'liutang123' has created a pull request for this issue:
https://github.com/apache/spark/pull/21100;;;","25/Apr/18 10:12;cloud_fan;Issue resolved by pull request 21100
[https://github.com/apache/spark/pull/21100];;;","25/Apr/18 11:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21154;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark2.3.0 INSERT OVERWRITE LOCAL DIRECTORY '/home/spark/aaaaab' ,SPARK-24009,13153267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xiaoxinniux,xiaoxinniux,18/Apr/18 07:25,28/May/19 07:23,13/Jul/23 08:45,28/May/19 07:23,2.3.0,,,,,,,,,,,,,,SQL,,,,0,,,,"local mode  spark execute ""INSERT OVERWRITE LOCAL DIRECTORY "" successfully.

on yarn spark execute ""INSERT OVERWRITE LOCAL DIRECTORY "" failed, not permission problem also 

 

1.spark-sql -e ""INSERT OVERWRITE LOCAL DIRECTORY '/home/spark/aaaaab'row format delimited FIELDS TERMINATED BY '\t' STORED AS TEXTFILE select * from default.dim_date""  write local directory successful

2.spark-sql  --master yarn -e ""INSERT OVERWRITE DIRECTORY 'aaaaab'row format delimited FIELDS TERMINATED BY '\t' STORED AS TEXTFILE select * from default.dim_date""  write hdfs successful

3.spark-sql --master yarn -e ""INSERT OVERWRITE LOCAL DIRECTORY '/home/spark/aaaaab'row format delimited FIELDS TERMINATED BY '\t' STORED AS TEXTFILE select * from default.dim_date""  on yarn write local directory failed

 

 

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create [file:/home/spark/aaaaab/.hive-staging_hive_2018-04-18_14-14-37_208_1244164279218288723-1/-ext-10000/_temporary/0/_temporary/attempt_20180418141439_0000_m_000000_0|file://home/spark/aaaaab/.hive-staging_hive_2018-04-18_14-14-37_208_1244164279218288723-1/-ext-10000/_temporary/0/_temporary/attempt_20180418141439_0000_m_000000_0] (exists=false, cwd=[file:/data/hadoop/tmp/nm-local-dir/usercache/spark/appcache/application_1523246226712_0403/container_1523246226712_0403_01_000002|file://data/hadoop/tmp/nm-local-dir/usercache/spark/appcache/application_1523246226712_0403/container_1523246226712_0403_01_000002])
 at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:249)
 at org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:123)
 at org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:103)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
 at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
 ... 8 more
 Caused by: java.io.IOException: Mkdirs failed to create [file:/home/spark/aaaaab/.hive-staging_hive_2018-04-18_14-14-37_208_1244164279218288723-1/-ext-10000/_temporary/0/_temporary/attempt_20180418141439_0000_m_000000_0|file://home/spark/aaaaab/.hive-staging_hive_2018-04-18_14-14-37_208_1244164279218288723-1/-ext-10000/_temporary/0/_temporary/attempt_20180418141439_0000_m_000000_0] (exists=false, cwd=[file:/data/hadoop/tmp/nm-local-dir/usercache/spark/appcache/application_1523246226712_0403/container_1523246226712_0403_01_000002|file://data/hadoop/tmp/nm-local-dir/usercache/spark/appcache/application_1523246226712_0403/container_1523246226712_0403_01_000002])
 at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:447)
 at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:801)
 at org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:80)
 at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:261)
 at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:246)

 ",,ant_nebula,Chopinxb,guanzhe,mgaido,xiaoxinniux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26596,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 11 02:14:56 UTC 2019,,,,,,,,,,"0|i3soxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/18 09:09;guanzhe;Try to config *hive.exec.scratchdir* and *hive.exec.stagingdir* in hive-site.xml. Make sure your current user has the right to write into the configured directory.;;;","19/Apr/18 10:37;xiaoxinniux;thanks your answer, but inoperant. local mode spark execute ""INSERT OVERWRITE LOCAL DIRECTORY "" successfully.

on yarn spark execute ""INSERT OVERWRITE LOCAL DIRECTORY "" failed, not permission problem also ;;;","23/Apr/18 12:27;mgaido;I have not been able to reproduce.;;;","27/Aug/18 10:17;Chopinxb;any progress here?  i met the same error 

sql: INSERT OVERWRITE LOCAL DIRECTORY '/search/odin/test' row format delimited FIELDS TERMINATED BY '\t' select vrid, query, url, loc_city from custom.common_wap_vr where logdate >= '2018073000' and logdate <= '2018073023' and vrid = '11000801' group by vrid,query, loc_city,url; 

spark command is : spark-sql --master yarn --deploy-mode client -f test.sql

*hive.exec.scratchdir* : /user/hive/datadir-tmp

*hive.exec.stagingdir* : /user/hive/datadir-tmp

18/08/27 17:16:21 ERROR util.Utils: Aborting task org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create file:/user/hive/datadir-tmp_hive_2018-08-27_17-14-45_908_2829491226961893146-1/-ext-10000/_temporary/0/_temporary/attempt_20180827171619_0002_m_000000_0 (exists=false, cwd=file:/search/hadoop09/yarn_local/usercache/ultraman/appcache/application_1535079600137_133521/container_e09_1535079600137_133521_01_000051) at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:249) at org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:123) at org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:103) at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367) at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414) at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Mkdirs failed to create file:/user/hive/datadir-tmp_hive_2018-08-27_17-14-45_908_2829491226961893146-1/-ext-10000/_temporary/0/_temporary/attempt_20180827171619_0002_m_000000_0 (exists=false, cwd=file:/search/hadoop09/yarn_local/usercache/ultraman/appcache/application_1535079600137_133521/container_e09_1535079600137_133521_01_000051) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:447) at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:818) at org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:80) at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:261) at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:246);;;","11/Jan/19 02:14;ant_nebula;Because on yarn client mode, ""INSERT OVERWRITE LOCAL DIRECTORY"" does not write back to dirver node,

but on the yarn nodemanager which the last task run, and you does not has the pemission to create directory '/home/spark/aaaaab'.

Of cause, It is not reasonable. It should write back to driver.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EqualNullSafe for FloatType and DoubleType might generate a wrong result by codegen.,SPARK-24007,13153245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,18/Apr/18 05:11,18/Apr/18 15:24,13/Jul/23 08:45,18/Apr/18 15:24,2.0.2,2.1.2,2.2.1,2.3.0,,,,,,2.2.2,2.3.1,2.4.0,,,SQL,,,,0,correctness,,,"{{EqualNullSafe}} for {{FloatType}} and {{DoubleType}} might generate a wrong result by codegen.

{noformat}
scala> val df = Seq((Some(-1.0d), None), (None, Some(-1.0d))).toDF()
df: org.apache.spark.sql.DataFrame = [_1: double, _2: double]

scala> df.show()
+----+----+
|  _1|  _2|
+----+----+
|-1.0|null|
|null|-1.0|
+----+----+


scala> df.filter(""_1 <=> _2"").show()
+----+----+
|  _1|  _2|
+----+----+
|-1.0|null|
|null|-1.0|
+----+----+
{noformat}

The result should be empty but the result remains two rows.
",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 06:02:05 UTC 2018,,,,,,,,,,"0|i3sosn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/18 06:02;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21094;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task not serializable caused by org.apache.parquet.io.api.Binary$ByteBufferBackedBinary.getBytes,SPARK-24002,13153047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,17/Apr/18 15:42,17/May/18 14:19,13/Jul/23 08:45,24/Apr/18 05:05,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Having two queries one is a 1000-line SQL query and a 3000-line SQL query. Need to run at least one hour with a heavy write workload to reproduce once. 

{code}
Py4JJavaError: An error occurred while calling o153.sql.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:223)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:189)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
	at org.apache.spark.sql.Dataset$$anonfun$59.apply(Dataset.scala:3021)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:89)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:127)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3020)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:190)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:646)
	at sun.reflect.GeneratedMethodAccessor153.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
	at py4j.Gateway.invoke(Gateway.java:293)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:226)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Exception thrown in Future.get: 
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:190)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:267)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doConsume(BroadcastNestedLoopJoinExec.scala:530)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:69)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)
	at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:144)
	...
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	... 23 more
Caused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Task not serializable
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:179)
	... 276 more
Caused by: org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:340)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:330)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:156)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2380)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:371)
	at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:417)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$3.apply(SparkPlan.scala:152)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:149)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:118)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:89)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:125)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:116)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:116)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$3.apply(SparkPlan.scala:152)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:149)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:118)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:271)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:181)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:414)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:123)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$3.apply(SparkPlan.scala:152)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:149)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:118)
	at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:61)
	at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:70)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:264)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1$$anonfun$call$1.apply(BroadcastExchangeExec.scala:93)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1$$anonfun$call$1.apply(BroadcastExchangeExec.scala:81)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:150)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.call(BroadcastExchangeExec.scala:80)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.call(BroadcastExchangeExec.scala:76)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.nio.BufferUnderflowException
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:151)
	at java.nio.ByteBuffer.get(ByteBuffer.java:715)
	at org.apache.parquet.io.api.Binary$ByteBufferBackedBinary.getBytes(Binary.java:405)
	at org.apache.parquet.io.api.Binary$ByteBufferBackedBinary.getBytesUnsafe(Binary.java:414)
	at org.apache.parquet.io.api.Binary$ByteBufferBackedBinary.writeObject(Binary.java:484)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1128)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
{code}",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 17 06:19:04 UTC 2018,,,,,,,,,,"0|i3snkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/18 15:55;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21086;;;","24/Apr/18 05:05;dongjoon;This is resolved via [https://github.com/apache/spark/pull/21086] .;;;","17/May/18 06:19;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21351;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configurable max number of buckets,SPARK-23997,13152847,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ferdonline,ferdonline,ferdonline,16/Apr/18 21:56,29/Aug/18 15:55,13/Jul/23 08:45,28/Aug/18 17:32,2.2.1,2.3.0,,,,,,,,2.4.0,,,,,Input/Output,SQL,,,1,,,,"When exporting data as a table the user can choose to split data in buckets by choosing the columns and the number of buckets. Currently there is a hard-coded limit of 99'999 buckets.

However, for heavy workloads this limit might be too restrictive, a situation that will eventually become more common as workloads grow.

As per the comments in SPARK-19618 this limit could be made configurable.",,apachespark,ferdonline,matz-e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 29 15:55:07 UTC 2018,,,,,,,,,,"0|i3smcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/18 16:48;apachespark;User 'ferdonline' has created a pull request for this issue:
https://github.com/apache/spark/pull/21087;;;","13/Jun/18 23:34;ferdonline;cc [~cloud_fan] [~tejasp]

This a pretty straightforward patch that has been sitting for 2 months... Could you please check?

Thanks;;;","19/Jul/18 08:13;matz-e;Is there any issue with having this limit configurable? We save large amounts of data in tables to avoid unnecessary shuffles, and often get hit by this bucket limitation.;;;","29/Aug/18 15:55;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/22269;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data loss when allocateBlocksToBatch,SPARK-23991,13152693,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,kevin09fjw,kevin09fjw,16/Apr/18 11:30,29/May/18 12:12,13/Jul/23 08:45,29/May/18 12:12,2.2.0,,,,,,,,,2.3.1,2.4.0,,,,DStreams,Input/Output,,,0,,,,"with checkpoint and WAL enabled, driver will write the allocation of blocks to batch into hdfs. however, if it fails as following, the blocks of this batch cannot be computed by the DAG. Because the blocks have been dequeued from the receivedBlockQueue and get lost.
{panel:title=error log}
18/04/15 11:11:25 WARN ReceivedBlockTracker: Exception thrown while writing record: BatchAllocationEvent(1523765480000 ms,AllocatedBlocks(Map(0 -> ArrayBuffer()))) to the WriteAheadLog. org.apache.spark.SparkException: Exception thrown in awaitResult: at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194) at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:83) at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234) at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.allocateBlocksToBatch(ReceivedBlockTracker.scala:118) at org.apache.spark.streaming.scheduler.ReceiverTracker.allocateBlocksToBatch(ReceiverTracker.scala:213) at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248) at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247) at scala.util.Try$.apply(Try.scala:192) at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247) at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183) at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89) at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) Caused by: java.util.concurrent.TimeoutException: Futures timed out after [5000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190) ... 12 more 18/04/15 11:11:25 INFO ReceivedBlockTracker: Possibly processed batch 1523765480000 ms needs to be processed again in WAL recovery{panel}
the concerning codes are showed below:
{code}
  /**
   * Allocate all unallocated blocks to the given batch.
   * This event will get written to the write ahead log (if enabled).
   */
  def allocateBlocksToBatch(batchTime: Time): Unit = synchronized {
    if (lastAllocatedBatchTime == null || batchTime > lastAllocatedBatchTime) {
      val streamIdToBlocks = streamIds.map { streamId =>
          (streamId, getReceivedBlockQueue(streamId).dequeueAll(x => true))
      }.toMap
      val allocatedBlocks = AllocatedBlocks(streamIdToBlocks)
      if (writeToLog(BatchAllocationEvent(batchTime, allocatedBlocks))) {
        timeToAllocatedBlocks.put(batchTime, allocatedBlocks)
        lastAllocatedBatchTime = batchTime
      } else {
        logInfo(s""Possibly processed batch $batchTime needs to be processed again in WAL recovery"")
      }
    } else {
      // This situation occurs when:
      // 1. WAL is ended with BatchAllocationEvent, but without BatchCleanupEvent,
      // possibly processed batch job or half-processed batch job need to be processed again,
      // so the batchTime will be equal to lastAllocatedBatchTime.
      // 2. Slow checkpointing makes recovered batch time older than WAL recovered
      // lastAllocatedBatchTime.
      // This situation will only occurs in recovery time.
      logInfo(s""Possibly processed batch $batchTime needs to be processed again in WAL recovery"")
    }
  }

{code}",spark 2.11,apachespark,gsomogyi,jerryshao,kevin09fjw,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 12:12:31 UTC 2018,,,,,,,,,,"0|i3slen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/18 13:27;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/21430;;;","29/May/18 12:12;jerryshao;Issue resolved by pull request 21430
[https://github.com/apache/spark/pull/21430];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using `SortShuffleWriter`, the data will be overwritten",SPARK-23989,13152648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cloud_fan,10110346,10110346,16/Apr/18 08:48,19/Apr/18 15:57,13/Jul/23 08:45,19/Apr/18 15:57,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,,,,"{color:#333333}When using `SortShuffleWriter`, we only insert  '{color}{color:#cc7832}AnyRef{color}{color:#333333}' into '{color}PartitionedAppendOnlyMap{color:#333333}' or '{color}PartitionedPairBuffer{color:#333333}'.{color}

{color:#333333}For this function:{color}

{color:#cc7832}override def {color}{color:#ffc66d}write{color}(records: {color:#4e807d}Iterator{color}[Product2[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}]])

the value of 'records' is `UnsafeRow`, so  the value will be overwritten

{color:#333333} {color}",,10110346,apachespark,cloud_fan,jerryshao,mgaido,riza,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 14:46:05 UTC 2018,,,,,,,,,,"0|i3sl4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/18 08:57;10110346;[~joshrosen] [~cloud_fan];;;","16/Apr/18 09:58;cloud_fan;Can you be more specific about what the problem is?;;;","16/Apr/18 10:18;10110346;For {color:#333333}`SortShuffleWriter`{color},  `records: {color:#4e807d}Iterator{color}[Product2[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}]]` is key-value pair, but the value is 'UnsafeRow' type.

For example ,we insert the first record  {color:#333333}into `PartitionedPairBuffer`, we only save the  'AnyRef{color}',   but the {color:#333333} 'AnyRef{color}'  of  next  record(only value, not key)  is same as the first record  , so the first record  is overwritten.;;;","16/Apr/18 12:50;cloud_fan;do you have an end-to-end case to show this bug? IIRC we always copy the unsafe row before sending it to something like `SortShuffleWriter`.;;;","17/Apr/18 01:50;10110346;If we make 'BypassMergeSortShuffleHandle' and 'SerializedShuffleHandle' disable, a lot of unit tests in 'DataFrameAggregateSuite.scala' will fail;;;","18/Apr/18 05:02;jerryshao;Please provide a reproducible case.

Did you reuse the object in your code? I think in the Spark side we already handled such case.;;;","18/Apr/18 06:21;10110346;1.  Make 'BypassMergeSortShuffleHandle' and 'SerializedShuffleHandle' disable

{color:#cc7832}override def {color}{color:#ffc66d}registerShuffle{color}[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}{color:#cc7832}, {color}{color:#4e807d}C{color}](
 shuffleId: {color:#cc7832}Int,{color} numMaps: {color:#cc7832}Int,{color} dependency: ShuffleDependency[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}{color:#cc7832}, {color}{color:#4e807d}C{color}]): ShuffleHandle = {
 {color:#cc7832}if {color}(SortShuffleWriter.shouldBypassMergeSort(conf{color:#cc7832}, {color}dependency){color:#14892c} && false {color}) {
 {color:#808080}// If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don't{color}{color:#808080} // need map-side aggregation, then write numPartitions files directly and just concatenate{color}{color:#808080} // them at the end. This avoids doing serialization and deserialization twice to merge{color}{color:#808080} // together the spilled files, which would happen with the normal code path. The downside is{color}{color:#808080} // having multiple files open at a time and thus more memory allocated to buffers.{color} {color:#cc7832}new {color}BypassMergeSortShuffleHandle[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}](
 shuffleId{color:#cc7832}, {color}numMaps{color:#cc7832}, {color}dependency.asInstanceOf[ShuffleDependency[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}{color:#cc7832}, {color}{color:#4e807d}V{color}]])
 } {color:#cc7832}else if {color}(SortShuffleManager.canUseSerializedShuffle(dependency) {color:#14892c}&& false{color}) {
 {color:#808080}// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:{color} {color:#cc7832}new {color}SerializedShuffleHandle[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}](
 shuffleId{color:#cc7832}, {color}numMaps{color:#cc7832}, {color}dependency.asInstanceOf[ShuffleDependency[{color:#4e807d}K{color}{color:#cc7832}, {color}{color:#4e807d}V{color}{color:#cc7832}, {color}{color:#4e807d}V{color}]])
 } {color:#cc7832}else {color}{
 {color:#808080}// Otherwise, buffer map outputs in a deserialized form:{color} {color:#cc7832}new {color}BaseShuffleHandle(shuffleId{color:#cc7832}, {color}numMaps{color:#cc7832}, {color}dependency)
 }
 }

 

2. Run this unit test in 'DataFrameAggregateSuite.scala'

test({color:#6a8759}""SPARK-21580 ints in aggregation expressions are taken as group-by ordinal.""{color})

3.  I have been debugging in IDEA, grab this information:

{{ _buffer = \{PartitionedPairBuffer@9817}_ }}
 {{ _capacity = 64_}}
 {{ _curSize = 2_}}
 {{ _data = {Object[128]@9832}_ }}
 {{  _0 = \{Tuple2@9834} ""(3,3)""_}}
 {{  {color:#14892c}_1 = \{UnsafeRow@9835} ""[0,2,2]""_{color}}}
 {{  _2 = \{Tuple2@9841} ""(4,4)""_}}
 {{  _{color:#14892c}3 = \{UnsafeRow@9835} ""[0,2,2]""{color}_}}

 

 ;;;","18/Apr/18 06:27;jerryshao;I've no idea what are you trying to express.

What specific issue are you seeing? I don't think you can comment out some codes and say ""hey, this UT is failed""...;;;","18/Apr/18 06:43;10110346;{color:#9876aa}I think '{color:#333333}SortShuffleWriter{color}' should adapt to any  shufflewrite scene{color};;;","18/Apr/18 06:45;10110346;We assume that: numPartitions > {color:#9876aa}MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE{color};;;","18/Apr/18 06:56;cloud_fan;You have to provide an end-to-end use case to convince us this is a bug(e.g. a SQL/DataFrame query). If you fork Spark, change some code and break something, it is not a bug.;;;","18/Apr/18 09:20;10110346;test({color:#6a8759}""groupBy""{color}) {
 {color:#808080} spark.conf.set(""spark.sql.shuffle.partitions"", 16777217){color}

{color:#cc7832}val {color}df1 = {color:#9876aa}Seq{color}(({color:#6a8759}""a""{color}{color:#cc7832}, {color}{color:#6897bb}1{color}{color:#cc7832}, {color}{color:#6897bb}0{color}{color:#cc7832}, {color}{color:#6a8759}""b""{color}){color:#cc7832}, {color}({color:#6a8759}""b""{color}{color:#cc7832}, {color}{color:#6897bb}2{color}{color:#cc7832}, {color}{color:#6897bb}4{color}{color:#cc7832}, {color}{color:#6a8759}""c""{color}){color:#cc7832}, {color}({color:#6a8759}""a""{color}{color:#cc7832}, {color}{color:#6897bb}2{color}{color:#cc7832}, {color}{color:#6897bb}3{color}{color:#cc7832}, {color}{color:#6a8759}""d""{color}))
 .toDF({color:#6a8759}""key""{color}{color:#cc7832}, {color}{color:#6a8759}""value1""{color}{color:#cc7832}, {color}{color:#6a8759}""value2""{color}{color:#cc7832}, {color}{color:#6a8759}""rest""{color})

checkAnswer(
 df1.groupBy({color:#6a8759}""key""{color}).min({color:#6a8759}""value2""{color}){color:#cc7832},{color} {color:#9876aa}Seq{color}(Row({color:#6a8759}""a""{color}{color:#cc7832}, {color}{color:#6897bb}0{color}){color:#cc7832}, {color}Row({color:#6a8759}""b""{color}{color:#cc7832}, {color}{color:#6897bb}4{color}))
 )
 }

Because the number of partitions is too large, it will run for a long time.

The number of partitions is so large that the purpose is to go `SortShuffleWriter`

 ;;;","18/Apr/18 13:44;cloud_fan;it goes to `SortShuffleWriter` and then? We get the correct result, don't we?;;;","18/Apr/18 14:12;cloud_fan;OK now I see the problem, `ShuffleExchangeExec.needToCopyObjectsBeforeShuffle` doesn't catch all the cases, so we may produce wrong result.;;;","18/Apr/18 14:46;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21101;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompileException when using too many avg aggregation after joining,SPARK-23986,13152532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,RustedBones,RustedBones,15/Apr/18 08:42,24/May/19 21:28,13/Jul/23 08:45,17/Apr/18 16:38,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Considering the following code:
{code:java}
    val df1: DataFrame = sparkSession.sparkContext
      .makeRDD(Seq((0, 1, 2, 3, 4, 5, 6)))
      .toDF(""key"", ""col1"", ""col2"", ""col3"", ""col4"", ""col5"", ""col6"")

    val df2: DataFrame = sparkSession.sparkContext
      .makeRDD(Seq((0, ""val1"", ""val2"")))
      .toDF(""key"", ""dummy1"", ""dummy2"")

    val agg = df1
      .join(df2, df1(""key"") === df2(""key""), ""leftouter"")
      .groupBy(df1(""key""))
      .agg(
        avg(""col2"").as(""avg2""),
        avg(""col3"").as(""avg3""),
        avg(""col4"").as(""avg4""),
        avg(""col1"").as(""avg1""),
        avg(""col5"").as(""avg5""),
        avg(""col6"").as(""avg6"")
      )

    val head = agg.take(1)
{code}
This logs the following exception:
{code:java}
ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 467, Column 28: Redefinition of parameter ""agg_expr_11""
{code}
I am not a spark expert but after investigation, I realized that the generated {{doConsume}} method is responsible of the exception.

Indeed, {{avg}} calls several times {{org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction}}. The 1st time with the 'avg' Expr and a second time for the base aggregation Expr (count and sum).

The problem comes from the generation of parameters in CodeGenerator:
{code:java}
  /**
   * Returns a term name that is unique within this instance of a `CodegenContext`.
   */
  def freshName(name: String): String = synchronized {
    val fullName = if (freshNamePrefix == """") {
      name
    } else {
      s""${freshNamePrefix}_$name""
    }
    if (freshNameIds.contains(fullName)) {
      val id = freshNameIds(fullName)
      freshNameIds(fullName) = id + 1
      s""$fullName$id""
    } else {
      freshNameIds += fullName -> 1
      fullName
    }
  }
{code}
The {{freshNameIds}} already contains {{agg_expr_[1..6]}} from the 1st call.
 The second call is made with {{agg_expr_[1..12]}} and generates the following names:
 {{agg_expr_[11|21|31|41|51|61|11|12]}}. We then have a parameter name conflicts in the generated code: {{agg_expr_11.}}

Appending the 'id' in s""$fullName$id"" to generate unique term name is source of conflict. Maybe simply using undersoce can solve this issue : $fullName_$id""",,apachespark,cloud_fan,dzanozin,kiszk,mgaido,pedromorfeu,RustedBones,sdangi1,shendley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/18 11:41;RustedBones;spark-generated.java;https://issues.apache.org/jira/secure/attachment/12919207/spark-generated.java",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 24 21:27:49 UTC 2019,,,,,,,,,,"0|i3skev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/18 19:33;kiszk;Thank for reporting an issue with deep dive.

When I run this repro with the latest master, it works well without an exception. When I checked the generated code, I cannot find variables {{agg_expr_[21|31|41|51|61]}}. I will check it with branch-2.3 tomorrow.
Would it be possible to attach the log file of the generated code?;;;","16/Apr/18 00:35;kiszk;While I also checked it with branch-2.3, it works well without any exception.;;;","16/Apr/18 11:41;RustedBones;I tested on Spark v2.3.0
I attached the generated code: [^spark-generated.java] . Here is the faulty  line (467):
{code:java}
private void agg_doConsume1(int agg_expr_01, double agg_expr_11, boolean agg_exprIsNull_1, long agg_expr_21, boolean agg_exprIsNull_2, double agg_expr_31, boolean agg_exprIsNull_3, long agg_expr_41, boolean agg_exprIsNull_4, double agg_expr_51, boolean agg_exprIsNull_5, long agg_expr_61, boolean agg_exprIsNull_6, double agg_expr_7, boolean agg_exprIsNull_7, long agg_expr_8, boolean agg_exprIsNull_8, double agg_expr_9, boolean agg_exprIsNull_9, long agg_expr_10, boolean agg_exprIsNull_10, double agg_expr_11, boolean agg_exprIsNull_11, long agg_expr_12, boolean agg_exprIsNull_12) throws java.io.IOException
{code}
Maybe a precision: the code does not throw, it just logs an error. I also checked the computed average values, everything seems correct.;;;","16/Apr/18 12:57;mgaido;[~RustedBones] I was able to reproduce. Yes, I do agree with you in all your analysis and also with your proposal of solution. I am submitting a patch. Thanks for reporting this.;;;","16/Apr/18 13:11;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21080;;;","16/Apr/18 19:26;RustedBones;Thx [~mgaido]. I didn't have time to setup the environment to submit the pull request this weekend :);;;","17/Apr/18 16:38;cloud_fan;Issue resolved by pull request 21080
[https://github.com/apache/spark/pull/21080];;;","10/Sep/18 19:46;dzanozin;Spark 2.3.1 still generates methods with duplicate parameter names. I've just got this method (which obviously failed with the following exception: ""{{ERROR CodeGenerator:91 - failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 686, Column 28: Redefinition of parameter ""agg_expr_21""}}"" ):
{code}
/* 686 */
private void agg_doConsume1(byte agg_expr_01, boolean agg_exprIsNull_01,
                            short agg_expr_11, boolean agg_exprIsNull_11,
                            short agg_expr_21, boolean agg_exprIsNull_21,
                            int agg_expr_31, boolean agg_exprIsNull_31,
                            int agg_expr_41, boolean agg_exprIsNull_41,
                            int agg_expr_51, boolean agg_exprIsNull_51,
                            UTF8String agg_expr_61, boolean agg_exprIsNull_61,
                            byte agg_expr_71, boolean agg_exprIsNull_71,
                            long agg_expr_81, boolean agg_exprIsNull_81,
                            double agg_expr_91, boolean agg_exprIsNull_91,
                            long agg_expr_101, boolean agg_exprIsNull_101,
                            double agg_expr_111, boolean agg_exprIsNull_111,
                            long agg_expr_121, boolean agg_exprIsNull_121,
                            int agg_expr_131, boolean agg_exprIsNull_131,
                            long agg_expr_141, boolean agg_exprIsNull_141,
                            int agg_expr_151, boolean agg_exprIsNull_151,
                            boolean agg_expr_161, boolean agg_exprIsNull_161,
                            long agg_expr_171,
                            byte agg_expr_18, boolean agg_exprIsNull_18,
                            boolean agg_expr_19, boolean agg_exprIsNull_19,
                            byte agg_expr_20, boolean agg_exprIsNull_20,
                            boolean agg_expr_21, boolean agg_exprIsNull_21,
                            short agg_expr_22, boolean agg_exprIsNull_22,
                            int agg_expr_23, boolean agg_exprIsNull_23) throws java.io.IOException {
{code};;;","11/Sep/18 08:19;mgaido;[~dzanozin] thanks for reporting this, may you please provide a reproducer for that? Thanks. Anyway, it seems that this patch is not applied according to the code you have pasted here (after the patch, we should have {{agg_doConsume_1}} as function name).;;;","11/Sep/18 09:11;dzanozin;Well, it's a bit hard to provide the exact code we use when this failure occurs, many aggregated attributes with several custom UDFs. The same code works fine with a different dataframe which differs from the first one by having 1 string attribute instead of 4 int ones (among other 22 attributes). Here is the DF schema which fails:
{noformat}
scala> df.printSchema()
root
 |-- attr1: timestamp (nullable = true)
 |-- attr2: string (nullable = true)
 |-- attr3: integer (nullable = true)
 |-- attr4: integer (nullable = true)
 |-- attr5: string (nullable = true)
 |-- attr6: integer (nullable = true)
 |-- attr7: byte (nullable = true)
 |-- attr8: integer (nullable = true)
 |-- attr9: timestamp (nullable = true)
 |-- attr10: string (nullable = true)
 |-- attr11: byte (nullable = true)
 |-- attr12: short (nullable = true)
 |-- attr13: short (nullable = true)
 |-- attr14: integer (nullable = true)
 |-- attr15: integer (nullable = true)
 |-- attr16: short (nullable = true)
 |-- attr17: integer (nullable = true)
 |-- attr18: string (nullable = true)
 |-- attr19: byte (nullable = true)
 |-- attr20: byte (nullable = true)
 |-- attr21: integer (nullable = true)
 |-- attr22: date (nullable = true)
{noformat}
if we replace attr12-attr15 (which represent a unique record key and are part of a group by statement) with a single string attr (a primary key of another object type) the code works fine.

Information about the execution environment:
{noformat}
spark-2.3.1-bin-without-hadoop
hadoop-2.8.4
Scala 2.11.8
Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162
{noformat};;;","11/Sep/18 09:21;mgaido;[~dzanozin] can you then please try the reproducer above reported in the JIRA in your env? I just tried it on 2.3.1 and it works for me. At least we can be sure that this is a different issue. Without a reproducer is nearly impossible to work on this, because - as I mentioned you - you seem to miss the patch, otherwise {{agg_doConsume1}} should have been {{agg_doConsume_1}}.;;;","11/Sep/18 10:32;dzanozin;Please accept my apologies. I had to spend more time on this investigation. That was a problem with one specific machine configuration where we run the tests and which had a mess with old and new spark/hadoop versions.
Runs fine on other machines and works as expected on the test machine after cleaning up Spark 2.3.0 debris.
Now generates the method signature as expected:
{code}
private void agg_doConsume_1(byte agg_expr_0_1, boolean agg_exprIsNull_0_1,
                             short agg_expr_1_1, boolean agg_exprIsNull_1_1,
...
{code}
Thank you for your time and sorry again!;;;","05/Mar/19 14:15;pedromorfeu;-Guys, is there a workaround for the folks that can't upgrade Spark version? Thanks.-

Here's my workaround for, say, 10 aggregation operations:
 # dataframe1 = aggregations 1 to 5
 # dataframe2 = aggregations 6 to 10
 # dataframe1.join(dataframe2);;;","09/May/19 19:57;sdangi1;[~pedromorfeu] I tried the workaround you mentioned above, but still encountered this issue (my code is below).

Since I don't have access to Spark 2.3.1, is there another workaround I can try with Spark 2.3.0?
{code:java}
val sumColNames = List[String](...)  // list of 25 Strings
val sumCols: List[Column] = sumColNames.map(name => sum(col(name)))

/** code that causes error */
val output = input
.groupBy(groupByColNames map col: _*)
.agg(sumCols.head, sumCols.tail: _*)

/** workaround I tried */
val middleIdx = sumCols.length / 2
val sumColsFirstHalf = sumCols.slice(0, middleIdx)
val sumColsSecondHalf = sumCols.slice(middleIdx, sumCols.length)

val grouped = input.groupBy(groupByCols)
val data1 = grouped.agg(sumColsFirstHalf.head, sumColsFirstHalf.tail: _*)
val data2 = grouped.agg(sumColsSecondHalf.head, sumColsSecondHalf.tail: _*)
val output = data1.join(data2, groupByColNames)
{code};;;","10/May/19 06:52;pedromorfeu;[~sdangi1], how many operations do you have in each aggregation? If it's more than 5, I would suggest to slice it even more.;;;","24/May/19 21:27;sdangi1;[~pedromorfeu] I had 25 columns in my sumCols list – once I split it up into 5 groups of 5, it worked!  Thank you very much for your workaround and suggestion.

As a reference for anyone else who wants to try this – here is the workaround code that worked for me:
{code:java}
val sumColNames = List[String](...)  // list of 25 column names
val sumCols: List[Column] = sumColNames.map(name => sum(col(name)))

val grouped = input.groupBy(groupByColNames map col: _*)

/** only do 5 agg operations at a time, collect the results into a list,
  * and then use reduce to join them all together into one dataframe
  */
val step = 5
val output = List.range(0, sumCols.length, step)
  .map(idx => {
    val cols = sumCols.slice(idx, idx + step)
    grouped.agg(cols.head, cols.tail: _*)
  })
  .reduce((df1, df2) => df1.join(df2, groupByColNames)){code}
 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add commit protocol binding to Hadoop 3.1 PathOutputCommitter mechanism,SPARK-23977,13152273,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,13/Apr/18 12:39,16/Feb/22 15:28,13/Jul/23 08:45,15/Aug/19 17:16,2.4.0,,,,,,,,,3.0.0,,,,,SQL,,,,2,,,,"Hadoop 3.1 adds a mechanism for job-specific and store-specific committers (MAPREDUCE-6823, MAPREDUCE-6956), and one key implementation, S3A committers, HADOOP-13786

These committers deliver high-performance output of MR and spark jobs to S3, and offer the key semantics which Spark depends on: no visible output until job commit, a failure of a task at an stage, including partway through task commit, can be handled by executing and committing another task attempt. 

In contrast, the FileOutputFormat commit algorithms on S3 have issues:

* Awful performance because files are copied by rename
* FileOutputFormat v1: weak task commit failure recovery semantics as the (v1) expectation: ""directory renames are atomic"" doesn't hold.
* S3 metadata eventual consistency can cause rename to miss files or fail entirely (SPARK-15849)

Note also that FileOutputFormat ""v2"" commit algorithm doesn't offer any of the commit semantics w.r.t observability of or recovery from task commit failure, on any filesystem.

The S3A committers address these by way of uploading all data to the destination through multipart uploads, uploads which are only completed in job commit.

The new {{PathOutputCommitter}} factory mechanism allows applications to work with the S3A committers and any other, by adding a plugin mechanism into the MRv2 FileOutputFormat class, where it job config and filesystem configuration options can dynamically choose the output committer.

Spark can use these with some binding classes to 

# Add a subclass of {{HadoopMapReduceCommitProtocol}} which uses the MRv2 classes and {{PathOutputCommitterFactory}} to create the committers.
# Add a {{BindingParquetOutputCommitter extends ParquetOutputCommitter}}
to wire up Parquet output even when code requires the committer to be a subclass of {{ParquetOutputCommitter}}

This patch builds on SPARK-23807 for setting up the dependencies.",,apachespark,cloud_fan,csun,danzhi,ddgirard,gumartinm,ianoc,itayb,java8964,jonathak,jonathan.bender,lucacanali,martha.solarte,seb.arzt,stevel@apache.org,vanzin,wjensen,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-15421,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 16 15:28:11 UTC 2022,,,,,,,,,,"0|i3sitb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/18 13:17;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/21066;;;","07/May/18 01:34;wjensen;[~stevel@apache.org] is the intention of this ticket to incorporate the Hadoop libraries within Spark itself, ie. no Hadoop dependency?
Trying to understand whether this is a viable solution for Spark on Kubernetes writing to S3;;;","07/May/18 13:38;stevel@apache.org;It will need the hadoop-aws module and deoendencies as that is where the core code is. This patch just does the binding to the InsertIntoHadoopFS relation (move to Hadoop MRv2 FileOutputFormat & expect the new superclass, PathOutputCommtter, rather than always a FileOutputcommitter, and for Parquet, something similar with a ParquetOutputCommitter.

+its only in Hadoop 3.1, though you can backport to branch-2, especially if you are prepared to bump up the minimum java version to 8 in that branch.

t should work on k8s, given it works standalone. All it needs is an endpoint supporting the multipart upload operation of S3, which includes some non-AWS object stores.

 Look @ the HADOOP-13786 work and the paper [a zero rename committer|https://github.com/steveloughran/zero-rename-committer/releases/download/tag_draft_003/a_zero_rename_committer.pdf]. 

And there's some integration tests downstream in https://github.com/hortonworks-spark/cloud-integration . I can help set you up to run those, if you email me directly. Essentially: you need to choose which stores to test against from: s3, openstack, azure, and configure them

Note that of the two variant committers, ""staging"" and ""magic"", the magic one needs a consistent S3 endpoint, which you only get on AWS S3 with an external services, usually dynamo DB based (S3mper, EMR consisent S3, S3Guard). The staging one needs enough local HDD to buffer the output of all active tasks, but doesn't need that consistency for its own query. You will need a plan for chaining together work though, which is inevitably one of ""consistency layer"" or ""wait long enough between writer and reader that you expect the metadata to be consistent""

Finally, if you are using spark to write directly to S3 today, without any consistency layer, then your commit algorithm had better not be mimicing directory rename by list + copy + delete. You need this code for safe as well as performant committing of work to S3.

;;;","10/Sep/18 13:58;cloud_fan;I'm removing the target version, since we are not going to merge it to 2.4;;;","15/Aug/19 17:16;vanzin;Issue resolved by pull request 24970
[https://github.com/apache/spark/pull/24970];;;","25/Mar/21 21:39;danzhi;[~stevel@apache.org] How to workaround following exception during the execution of ""INSERT OVERWRITE"" in spark.sql (spark 3.1.1 with hadoop 3.2)?
{quote}  if (dynamicPartitionOverwrite) {

    // until there's explicit extensions to the PathOutputCommitProtocols

    // to support the spark mechanism, it's left to the individual committer

    // choice to handle partitioning.

    throw new IOException(PathOutputCommitProtocol.UNSUPPORTED)

  }
{quote};;;","26/Mar/21 12:29;stevel@apache.org;use the partitioned committer and configure it to do the right thing when updating partitions (merge, delete everything already there, fail);;;","30/Mar/21 21:14;danzhi;[~stevel@apache.org] Thanks for the info. Below are the related (key, value) we used:
 # spark.hadoop.fs.s3a.committer.name — partitioned
 # spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a — org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
 # spark.sql.sources.commitProtocolClass — org.apache.spark.internal.io.cloud.PathOutputCommitProtocol
 # spark.sql.parquet.output.committer.class — org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter

3 & 4 appear to be necessary to ensure S3A committers being used by Spark for parquet outputs, except that ""INSERT OVERWRITE"" is blocked by the dynamicPartitionOverwrite exception. It will be helpful and appreciated if you can patiently elaborate on the proper way to ""use the partitioned committer and configure it to do the right thing ..."" in Spark. For example:
 * PathOutputCommitProtocol appears to be needed but its constructor fails with the exception.
 * S3A committers honor ""fs.s3a.committer.staging.conflict-mode"" which needs to be ""replace"" for ""INSERT OVERWRITE"" but ""append"" for ""INSERT INTO"". So it is spark.sql query specific. How to make spark.sql automatically set the right value?
 * Does above require code change in Spark or there is a configuration-only way?;;;","03/Apr/21 16:27;stevel@apache.org;the spark settings don't make it down from sql; you can do more at the RDD API level.

The problem is that the spark partition insert logic all relies on renaming which has the O(data) performance penalty as well as the other commit correctness issues. Yes, something to do the pushdown could be done, or with the multipart APIs of HADOOP-13186 give Spark a standard API to implement a zero rename committer in its own code.

However, focus is on things like Iceberg and Delta lake, which offer more in terms of : 
* atomic job commit
* avoid the performance and cost issues of relying on directory tree scan as a way to identify source files.; the performance issues of doing all IO down a single shard of S3 storage.
I do not disagree with the direction of that work; we have to view the S3A committers (and IBM's Stocator + AWS EMR spark committers) as the final attempts to maintain that ""it's just a directory tree"" model into a cloud world where directories don't always exist, and listing them is measurable in hundreds of milliseconds. ;;;","01/Nov/21 10:33;gumartinm;Thank you very much [~stevel@apache.org] for your explanations.

I am experiencing the same problems as [~danzhi] and your comments helped me a lot.

Sad magic committer does not work with dynamic partition overwrite because it has an amazing performance when writing loads of JSON partitioned data.;;;","02/Nov/21 10:06;stevel@apache.org;[~gumartinm] can I draw your attention to Apache Iceberg?

meanwhile
MAPREDUCE-7341 adds a high performance targeting abfs and gcs; all tree scanning is in task commit, which is atomic; job commit aggressively parallelised and optimized for stores whose listStatusIterator calls are incremental with prefetching: we can start processing at the first page of task manifests found in a listing well the second Page is still being retrieved. Also in there: rate limiting, IO Statistics Collection.

HADOOP-17833 I will pick up some of that work, including incremental loading and rate limiting. And if we can keep reads and writes below the S3 IOPS limits, we should be able to avoid situations where we have to start sleeping and re-trying.

HADOOP-17981 is my homework this week -emergency work to deal with a rare but current failure in abfs under heavy load.
;;;","16/Feb/22 15:28;itayb;Hi all,

 

I follow the [recommendations|https://spark.apache.org/docs/latest/cloud-integration.html#parquet-io-settings] and getting the following warning:
{code:java}
2022-02-16 15:22:03.292 WARN FlowThread0 ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level {code}
What would be the recommended value for `parquet.summary.metadata.level ` ? 
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTF8String.concat() or ByteArray.concat() may allocate shorter structure.,SPARK-23976,13152265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,13/Apr/18 11:57,19/Apr/18 16:08,13/Jul/23 08:45,19/Apr/18 16:08,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"When the three inputs has `0x7FFF_FF00`, `0x7FFF_FF00`, and `0xE00`, the current algorithm allocate the result structure with 0x1000 length due to integer sum overflow.

We should detect overflow.
",,apachespark,kiszk,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 13 12:07:04 UTC 2018,,,,,,,,,,"0|i3sirj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/18 12:07;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21064;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Clustering to take Arrays of Double as input features,SPARK-23975,13152183,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lu.DB,lu.DB,lu.DB,13/Apr/18 05:15,08/May/18 03:09,13/Jul/23 08:45,08/May/18 03:09,2.3.0,,,,,,,,,2.4.0,,,,,ML,,,,0,,,,Clustering algorithms should accept Arrays in addition to Vectors as input features. The python interface should also be changed so that it would make PySpark a lot easier to use. ,,apachespark,josephkb,lu.DB,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 17:20:05 UTC 2018,,,,,,,,,,"0|i3sian:",9223372036854775807,,,,,josephkb,,,,,,,,,,,,,,,,,,"16/Apr/18 18:01;apachespark;User 'ludatabricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/21081;;;","24/Apr/18 16:26;josephkb;I merged https://github.com/apache/spark/pull/21081 for KMeans, and [~lu.DB] will follow up for the other algs.;;;","30/Apr/18 17:20;apachespark;User 'ludatabricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/21195;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not leak Spark sessions across test suites,SPARK-23971,13152065,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,12/Apr/18 19:46,02/May/18 19:02,13/Jul/23 08:45,13/Apr/18 05:31,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,Tests,,,0,,,,Many suites currently leak Spark sessions (sometimes with stopped SparkContexts) via the thread-local active Spark session and default Spark session. We should attempt to clean these up and detect when this happens to improve the reproducibility of tests.,,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 19:47:06 UTC 2018,,,,,,,,,,"0|i3shkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/18 19:48;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/21058;;;","30/Apr/18 19:47;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21197;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark toLocalIterator throws an exception,SPARK-23961,13151638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bryanc,FlamingMike,FlamingMike,11/Apr/18 12:38,12/Dec/22 18:10,13/Jul/23 08:45,07/May/19 21:48,2.0.2,2.1.2,2.2.1,2.3.0,,,,,,3.0.0,,,,,PySpark,,,,0,DataFrame,pyspark,,"Given a dataframe and use toLocalIterator. If we do not consume all records, it will throw: 
{quote}ERROR PythonRDD: Error while sending iterator
 java.net.SocketException: Connection reset by peer: socket write error
 at java.net.SocketOutputStream.socketWrite0(Native Method)
 at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
 at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
 at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
 at java.io.DataOutputStream.write(DataOutputStream.java:107)
 at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
 at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:497)
 at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:509)
 at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:509)
 at scala.collection.Iterator$class.foreach(Iterator.scala:893)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
 at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)
 at org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:705)
 at org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply(PythonRDD.scala:705)
 at org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply(PythonRDD.scala:705)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
 at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:706)
{quote}
 

To reproduce, here is a simple pyspark shell script that show the error:
{quote}import itertools
 df = spark.read.parquet(""large parquet folder"").cache()
print(df.count())
 b = df.toLocalIterator()
 print(len(list(itertools.islice(b, 20))))
 b = None # Make the iterator goes out of scope.  Throws here.
{quote}
 

Observations:
 * Consuming all records do not throw.  Taking only a subset of the partitions create the error.
 * In another experiment, doing the same on a regular RDD works if we cache/materialize it. If we do not cache the RDD, it throws similarly.
 * It works in scala shell

 ",,bryanc,dongjoon,FlamingMike,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25733,,,,,,,,,,SPARK-27659,SPARK-27548,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 07 21:48:45 UTC 2019,,,,,,,,,,"0|i3sexz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/18 01:20;gurwls223;FWIW, I met this issue a while ago too (and I gave up with using this at that time and forget to debug it ahead).;;;","23/Apr/18 21:25;dongjoon;Yep. I'm also able to observe this for all Spark 2.X (2.0 ~ 2.3). `toLocalIterator` is introduced at Spark 2.0.;;;","05/Mar/19 20:00;bryanc;I could also reproduce with a nearly identical error using the following

{code}
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand, udf
from pyspark.sql.types import *

spark = SparkSession\
        .builder\
        .appName(""toLocalIterator_Test"")\
        .getOrCreate()

df = spark.range(1 << 16).select(rand())

it = df.toLocalIterator()

print(next(it))
it = None

time.sleep(5)
spark.stop()
{code}

I think there are a couple issues with the way this is currently working. When toLocalIterator is called in Python, the Scala side also creates a local iterator which immediately starts a loop to consume the entire iterator and write it all to Python without any synchronization with the Python iterator. Blocking the write operation only happens when the socket receive buffer is full.  Small examples work fine if the data all fits in the read buffer, but the above code fails because the writing becomes blocked, then the Python iterator stops reading and closes the connection, which the Scala side sees as an error.  I can work on a fix for this.;;;","07/May/19 21:48;bryanc;Issue resolved by pull request 24070
[https://github.com/apache/spark/pull/24070];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in parameter name 'rawPredicition',SPARK-23955,13151476,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,JohnHBauer,JohnHBauer,10/Apr/18 19:41,12/Dec/22 18:10,13/Jul/23 08:45,12/Apr/18 02:35,2.3.0,,,,,,,,,,,,,,PySpark,,,,0,,,,"classifier.py MultilayerPerceptronClassifier.__init__ API call had typo rawPredicition instead of rawPrediction

also present in doc",,apachespark,JohnHBauer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://issues.apache.org/jira/browse/SPARK-21856,,,,,,,,,,9223372036854775807,,,Thu Apr 12 02:35:34 UTC 2018,,,,,,,,,,"0|i3sdxz:",9223372036854775807,,,,,JohnHBauer,,,,,,,,,,,,,,,,,,"11/Apr/18 02:14;gurwls223;Fixing a typo doesn't need a JIRA. Let's avoid this next time.;;;","11/Apr/18 20:44;apachespark;User 'codeforfun15' has created a pull request for this issue:
https://github.com/apache/spark/pull/21046;;;","12/Apr/18 02:35;gurwls223;Fixed in https://github.com/apache/spark/pull/21030;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use java classed in ExprValue and simplify a bunch of stuff,SPARK-23951,13151354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,10/Apr/18 11:38,11/Apr/18 12:12,13/Jul/23 08:45,11/Apr/18 12:12,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 11 12:12:01 UTC 2018,,,,,,,,,,"0|i3sd6v:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"10/Apr/18 14:37;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/21026;;;","11/Apr/18 12:12;cloud_fan;Issue resolved by pull request 21026
[https://github.com/apache/spark/pull/21026];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos task failed on specific spark app name,SPARK-23941,13150977,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tiboun,tiboun,tiboun,09/Apr/18 09:07,13/Dec/18 00:40,13/Jul/23 08:45,01/May/18 15:29,2.2.1,2.3.0,,,,,,,,2.2.2,2.3.1,2.4.0,,,Mesos,Spark Submit,,,0,,,,"It seems to be a bug related to spark's MesosClusterDispatcher. In order to reproduce the bug, you need to have mesos and mesos dispatcher running.

I'm currently running mesos 1.5 and spark 2.3.0 (tried with 2.2.1 as well).

If you launch the following program:

 
{code:java}
spark-submit --master mesos://127.0.1.1:7077 --deploy-mode cluster --class org.apache.spark.examples.SparkPi --name ""my favorite task (myId = 123-456)"" /home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar 100
{code}
, then the task fails with the following output :

 
{code:java}
I0409 11:00:35.360352 22726 fetcher.cpp:551] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/tiboun"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""cache"":false,""extract"":true,""value"":""\/home\/tiboun\/tools\/spark\/examples\/jars\/spark-examples_2.11-2.3.0.jar""}}],""sandbox_directory"":""\/var\/lib\/mesos\/slaves\/0262246c-14a3-4408-9b74-5e3b65dc1344-S0\/frameworks\/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014\/executors\/driver-20180409110035-0004\/runs\/8ac20902-74e1-45c4-9ab6-c52a79940189"",""user"":""tiboun""}
I0409 11:00:35.363119 22726 fetcher.cpp:450] Fetching URI '/home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar'
I0409 11:00:35.363143 22726 fetcher.cpp:291] Fetching directly into the sandbox directory
I0409 11:00:35.363168 22726 fetcher.cpp:225] Fetching URI '/home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar'
W0409 11:00:35.366839 22726 fetcher.cpp:330] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: /home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar
I0409 11:00:35.366873 22726 fetcher.cpp:603] Fetched '/home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar' to '/var/lib/mesos/slaves/0262246c-14a3-4408-9b74-5e3b65dc1344-S0/frameworks/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014/executors/driver-20180409110035-0004/runs/8ac20902-74e1-45c4-9ab6-c52a79940189/spark-examples_2.11-2.3.0.jar'
I0409 11:00:35.366878 22726 fetcher.cpp:608] Successfully fetched all URIs into '/var/lib/mesos/slaves/0262246c-14a3-4408-9b74-5e3b65dc1344-S0/frameworks/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014/executors/driver-20180409110035-0004/runs/8ac20902-74e1-45c4-9ab6-c52a79940189'
I0409 11:00:35.438725 22733 exec.cpp:162] Version: 1.5.0
I0409 11:00:35.440770 22734 exec.cpp:236] Executor registered on agent 0262246c-14a3-4408-9b74-5e3b65dc1344-S0
I0409 11:00:35.441388 22733 executor.cpp:171] Received SUBSCRIBED event
I0409 11:00:35.441586 22733 executor.cpp:175] Subscribed executor on tiboun-Dell-Precision-M3800
I0409 11:00:35.441643 22733 executor.cpp:171] Received LAUNCH event
I0409 11:00:35.441767 22733 executor.cpp:638] Starting task driver-20180409110035-0004
I0409 11:00:35.445050 22733 executor.cpp:478] Running '/usr/libexec/mesos/mesos-containerizer launch <POSSIBLY-SENSITIVE-DATA>'
I0409 11:00:35.445770 22733 executor.cpp:651] Forked command at 22743
sh: 1: Syntax error: ""("" unexpected
I0409 11:00:35.538661 22736 executor.cpp:938] Command exited with status 2 (pid: 22743)
I0409 11:00:36.541016 22739 process.cpp:887] Failed to accept socket: future discarded
{code}
If you remove the parentheses, you get the following result:

 
{code:java}
I0409 11:03:02.023701 23085 fetcher.cpp:551] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/tiboun"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""cache"":false,""extract"":true,""value"":""\/home\/tiboun\/tools\/spark\/examples\/jars\/spark-examples_2.11-2.3.0.jar""}}],""sandbox_directory"":""\/var\/lib\/mesos\/slaves\/0262246c-14a3-4408-9b74-5e3b65dc1344-S0\/frameworks\/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014\/executors\/driver-20180409110301-0006\/runs\/f887c0ab-b48f-4382-850c-383c1c944269"",""user"":""tiboun""}
I0409 11:03:02.028268 23085 fetcher.cpp:450] Fetching URI '/home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar'
I0409 11:03:02.028302 23085 fetcher.cpp:291] Fetching directly into the sandbox directory
I0409 11:03:02.028336 23085 fetcher.cpp:225] Fetching URI '/home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar'
W0409 11:03:02.031209 23085 fetcher.cpp:330] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: /home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar
I0409 11:03:02.031250 23085 fetcher.cpp:603] Fetched '/home/tiboun/tools/spark/examples/jars/spark-examples_2.11-2.3.0.jar' to '/var/lib/mesos/slaves/0262246c-14a3-4408-9b74-5e3b65dc1344-S0/frameworks/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014/executors/driver-20180409110301-0006/runs/f887c0ab-b48f-4382-850c-383c1c944269/spark-examples_2.11-2.3.0.jar'
I0409 11:03:02.031258 23085 fetcher.cpp:608] Successfully fetched all URIs into '/var/lib/mesos/slaves/0262246c-14a3-4408-9b74-5e3b65dc1344-S0/frameworks/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014/executors/driver-20180409110301-0006/runs/f887c0ab-b48f-4382-850c-383c1c944269'
I0409 11:03:02.090797 23095 exec.cpp:162] Version: 1.5.0
I0409 11:03:02.095283 23092 exec.cpp:236] Executor registered on agent 0262246c-14a3-4408-9b74-5e3b65dc1344-S0
I0409 11:03:02.096693 23095 executor.cpp:171] Received SUBSCRIBED event
I0409 11:03:02.097040 23095 executor.cpp:175] Subscribed executor on tiboun-Dell-Precision-M3800
I0409 11:03:02.097141 23095 executor.cpp:171] Received LAUNCH event
I0409 11:03:02.097357 23095 executor.cpp:638] Starting task driver-20180409110301-0006
I0409 11:03:02.101521 23095 executor.cpp:478] Running '/usr/libexec/mesos/mesos-containerizer launch <POSSIBLY-SENSITIVE-DATA>'
I0409 11:03:02.102332 23095 executor.cpp:651] Forked command at 23100
Error: Cannot load main class from JAR file:/var/lib/mesos/slaves/0262246c-14a3-4408-9b74-5e3b65dc1344-S0/frameworks/edff1a6f-38c6-46e0-a3c1-62a8fbfc2b5d-0014/executors/driver-20180409110301-0006/runs/f887c0ab-b48f-4382-850c-383c1c944269/favorite
Run with --help for usage help or --verbose for debug output
I0409 11:03:02.792325 23090 executor.cpp:938] Command exited with status 1 (pid: 23100)
I0409 11:03:03.794505 23098 process.cpp:887] Failed to accept socket: future discarded
{code}
Interesting things is that mesos try to find main class on a file called ""favorite"" which is part of the task name.

 

I've tried to launch spark-shell with the same name and it works fine. Task name's get driver's name and add a sequence after it.

 ","OS: Ubuntu 16.0.4

Spark: 2.3.0

Mesos: 1.5.0",apachespark,tiboun,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24380,SPARK-23464,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 01 15:29:11 UTC 2018,,,,,,,,,,"0|i3savb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/18 21:42;apachespark;User 'tiboun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21014;;;","01/May/18 15:29;vanzin;Issue resolved by pull request 21014
[https://github.com/apache/spark/pull/21014];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify code generation for Add/Subtract with CalendarIntervals,SPARK-23898,13150860,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,08/Apr/18 19:21,10/Apr/18 04:50,13/Jul/23 08:45,10/Apr/18 04:50,2.3.0,,,,,,,,,,,,,,SQL,,,,0,,,,,,apachespark,hvanhovell,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 08 19:40:04 UTC 2018,,,,,,,,,,"0|i3sa5b:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"08/Apr/18 19:40;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/21005;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible overflow in long = int * int,SPARK-23893,13150771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,07/Apr/18 14:11,08/Apr/18 18:40,13/Jul/23 08:45,08/Apr/18 18:40,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,SQL,,,0,,,,"To perform `int * int` and then to cast to `long` may cause overflow if the MSB of the multiplication result is `1`. In other words, the result would be negative due to sign extension.",,apachespark,kiszk,maropu,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 07 14:25:04 UTC 2018,,,,,,,,,,"0|i3s9lj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/18 14:25;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/21002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
speculative task should not run on a given host where another attempt is already running on,SPARK-23888,13150641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,06/Apr/18 15:55,23/Apr/18 20:38,13/Jul/23 08:45,23/Apr/18 20:38,2.3.0,,,,,,,,,2.4.0,,,,,Scheduler,Spark Core,,,0,speculation,,," 

There's a bug in:
{code:java}
/** Check whether a task is currently running an attempt on a given host */
 private def hasAttemptOnHost(taskIndex: Int, host: String): Boolean = {
   taskAttempts(taskIndex).exists(_.host == host)
 }
{code}
This will ignore hosts which have finished attempts, so we should check whether the attempt is currently running on the given host. 

And it is possible for a speculative task to run on a host where another attempt failed here before.

Assume we have only two machines: host1, host2.  We first run task0.0 on host1. Then, due to  a long time waiting for task0.0, we launch a speculative task0.1 on host2. And, task0.1 finally failed on host1, but it can not re-run since there's already  a copy running on host2. After another long time, we launch a new  speculative task0.2. And, now, we can run task0.2 on host1 again, since there's no more running attempt on host1.

******

After discussion in the PR, we simply make the comment be consistent the method's behavior. See details in PR#20998.

 ",,apachespark,irashid,Ngone51,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 23 20:38:17 UTC 2018,,,,,,,,,,"0|i3s8sn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/18 16:03;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/20998;;;","23/Apr/18 20:38;irashid;Issue resolved by pull request 20998
[https://github.com/apache/spark/pull/20998];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is UTF8StringSuite.writeToOutputStreamUnderflow() supported?,SPARK-23882,13150530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,06/Apr/18 07:47,06/Apr/18 16:49,13/Jul/23 08:45,06/Apr/18 16:49,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"The unit test {{UTF8StringSuite.writeToOutputStreamUnderflow()}} accesses metadata of an Java byte array objected where {{Platform.BYTE_ARRAY_OFFSET}} reserves.
Is this test valid? Is this test necessary for Spark implementation?",,apachespark,kiszk,riza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 06 08:13:05 UTC 2018,,,,,,,,,,"0|i3s83z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/18 08:13;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20995;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scala.MatchError in literals.sql.out ,SPARK-23868,13150078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,smilegator,smilegator,04/Apr/18 17:06,04/Apr/18 17:07,13/Jul/23 08:45,04/Apr/18 17:07,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"To fix scala.MatchError in literals.sql.out, https://github.com/apache/spark/pull/20872 added an entry for CalendarIntervalType in QueryExecution.toHiveStructString.
",,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-04-04 17:06:57.0,,,,,,,,,,"0|i3s5cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able to load file from Spark Dataframes,SPARK-23865,13149990,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,renjithgk,renjithgk,04/Apr/18 12:48,05/Apr/18 17:14,13/Jul/23 08:45,05/Apr/18 17:14,2.3.0,,,,,,,,,,,,,,Examples,,,,0,newbie,,,"Hello,

I am in the phase of learning Spark as part of it trying examples. I am using the following lines of code as below for my file named df.scala:

import org.apache.spark.sql.SparkSession
 val spark = SparkSession.builder().getOrCreate()
 val df = spark.read.csv(""CitiGroup2006_2008"")
 df.Head(5)

In my Scala Terminal:

scala> :load df.scala

Loading df.scala...
 import org.apache.spark.sql.SparkSession
 spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4756e5cc
 org.apache.spark.sql.AnalysisException: Path does not exist: [file:/C:/Spark/MyPrograms/Scala_and_Spark_Bootcamp_master/SparkD|file:///C:/Spark/MyPrograms/Scala_and_Spark_Bootcamp_master/SparkD]
 ataFrames/CitiGroup2006_2008;
 at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGl
 obPathIfNecessary(DataSource.scala:715)
 at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)
 at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
 at scala.collection.immutable.List.flatMap(List.scala:344)

at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:594)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:473)
 ... 72 elided
 <console>:25: error: not found: value df
 df.Head(5)
 ^

all environment variables are set and pointed. Is this a version issue of Spark 2.3.0 or should i degrade the version if so please let me know which version is stable to do my practicals",Executed in Atom Editor.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Apr 05 17:14:35 UTC 2018,,,,,,,,,,"0|i3s4t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/18 17:14;renjithgk;Import package was missing, added.

import org.apache.spark.sql.DataFrame;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark ExpressionEncoder should support java enum type in scala,SPARK-23862,13149895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,shengzhixia,shengzhixia,04/Apr/18 04:27,22/Dec/20 17:56,13/Jul/23 08:45,22/Dec/20 17:56,2.3.0,,,,,,,,,3.2.0,,,,,SQL,,,,0,,,,"In SPARK-21255, spark upstream adds support for creating encoders for java enum types, but the support is only added to Java API(for enum working within Java Beans). Since the java enum can come from third-party java library, we have use case that requires 
1. using java enum types as field of scala case class
2. using java enum as the type T in Dataset[T]

Spark ExpressionEncoder already supports ser/de many java types in ScalaReflection, so we propose to add support for java enum as well, as a follow up of SPARK-21255.",,apachespark,dongjoon,shengzhixia,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31453,,,,,,,SPARK-21255,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 17:56:12 UTC 2020,,,,,,,,,,"0|i3s47z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/18 05:00;apachespark;User 'fangshil' has created a pull request for this issue:
https://github.com/apache/spark/pull/20974;;;","18/Dec/20 23:45;xkrogen;I'm going to take up this work. Will submit new PR soon.;;;","21/Dec/20 17:52;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/30877;;;","22/Dec/20 17:56;dongjoon;Issue resolved by pull request 30877
[https://github.com/apache/spark/pull/30877];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In mesos cluster mode spark submit requires the keytab to be available on the local file system.,SPARK-23857,13149758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,skonto,skonto,skonto,03/Apr/18 14:49,20/May/18 23:15,13/Jul/23 08:45,20/May/18 23:15,2.3.0,,,,,,,,,2.4.0,,,,,Mesos,,,,0,,,,"Users could submit their jobs from an external to the cluster host which may not have the required keytab locally (also discussed here).  

Moreover, in cluster mode it does not make much sense to reference a local resource unless this is uploaded/stored somewhere in the cluster. For yarn HDFS is used, on mesos and certainly on DC/OS right now the secret store is used for storing secrets and consequently keytabs. There is a check [here|https://github.com/apache/spark/blob/7cf9fab33457ccc9b2d548f15dd5700d5e8d08ef/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L387] that makes spark submit difficult to use in such deployment scenarios.

On DC/OS the workaround is to directly submit to the mesos dispatcher rest api by passing the spark.yarn.tab property pointing to a path within the driver's container where the keytab will be mounted after its fetched from the secret store, at container's launch time. Target is to allow spark submit be flexible enough for mesos in cluster mode, as DC/OS users often want to deploy using that.",,apachespark,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 20 23:15:29 UTC 2018,,,,,,,,,,"0|i3s3dz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/18 15:02;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/20967;;;","20/May/18 23:15;srowen;Issue resolved by pull request 20967
[https://github.com/apache/spark/pull/20967];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip doctests which require hive support built in PySpark,SPARK-23853,13149577,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,holden,holden,02/Apr/18 20:42,12/Dec/22 18:10,13/Jul/23 08:45,01/May/18 01:08,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,PySpark,SQL,,,0,,,,"As we do with detecting if various libraries are installed if there is no support built in we should skip the tests which require hive.

e.g. the readwrite doctest.",,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 01 01:08:22 UTC 2018,,,,,,,,,,"0|i3s2af:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/18 11:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21141;;;","01/May/18 01:08;gurwls223;Issue resolved by pull request 21141
[https://github.com/apache/spark/pull/21141];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet MR bug can lead to incorrect SQL results,SPARK-23852,13149576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,henryr,henryr,02/Apr/18 20:41,20/Sep/18 17:51,13/Jul/23 08:45,10/May/18 02:56,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,correctness,,,"Parquet MR 1.9.0 and 1.8.2 both have a bug, PARQUET-1217, that means that pushing certain predicates to Parquet scanners can return fewer results than they should.

The bug triggers in Spark when:
 * The Parquet file being scanner has stats for the null count, but not the max or min on the column with the predicate (Apache Impala writes files like this).
 * The vectorized Parquet reader path is not taken, and the parquet-mr reader is used.
 * A suitable <, <=, > or >= predicate is pushed down to Parquet.

The bug is that the parquet-mr interprets the max and min of a row-group's column as 0 in the absence of stats. So {{col > 0}} will filter all results, even if some are > 0.

There is no upstream release of Parquet that contains the fix for PARQUET-1217, although a 1.10 release is planned.

The least impactful workaround is to set the Parquet configuration {{parquet.filter.stats.enabled}} to {{false}}.",,apachespark,dongjoon,emaynard1121,henryr,rxin,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PARQUET-1217,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 14 21:25:05 UTC 2018,,,,,,,,,,"0|i3s2a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/18 05:10;rxin;Does turning the flag parquet.filter.stats.enabled off also turning off row group level skipping?

 ;;;","03/Apr/18 05:27;henryr;Partly, but not completely. If the column is dictionary encoded, the filters are applied to the dictionary as well at the row-group level. See [RowGroupFilter|https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java#L92].;;;","12/Apr/18 20:53;henryr;[Here's a branch|https://github.com/henryr/spark/tree/spark-23852] with a test that fails if PARQUET-1217 is not fixed.;;;","24/Apr/18 15:01;emaynard1121;{color:#333333}>There is no upstream release of Parquet that contains the fix for {color}PARQUET-1217{color:#333333}, although a 1.10 release is planned.{color}

PARQUET-1217 seems to have been merged into Parquet 1.8.3 today.;;;","24/Apr/18 21:42;henryr;Yes it has - the Parquet community are going to do a 1.8.3 release, mostly just for us for this issue. Parquet 1.10 has already been released, and includes this fix. Upgrading Spark trunk to that version is the subject of SPARK-23972.;;;","09/May/18 19:10;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/21284;;;","11/May/18 19:06;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/21302;;;","14/May/18 21:25;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/21323;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"We should not redact username|user|url from UI by default",SPARK-23850,13149549,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,tgraves,tgraves,02/Apr/18 18:47,09/Aug/19 07:23,13/Jul/23 08:45,18/May/18 23:25,2.2.1,,,,,,,,,2.2.2,2.3.1,2.4.0,,,Web UI,,,,0,,,,"SPARK-22479 was filed to not print the log jdbc credentials, but in there they also added  the username and url to be redacted.  I'm not sure why these were added and to me by default these do not have security concerns.  It makes it more usable by default to be able to see these things.  Users with high security concerns can simply add them in their configs.

Also on yarn just redacting url doesn't secure anything because if you go to the environment ui page you see all sorts of paths and really its just confusing that some of its redacted and other parts aren't.  If this was specifically for jdbc I think it needs to be just applied there and not broadly.

If we remove these we need to test what the jdbc driver is going to log from SPARK-22479.",,apachespark,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23567,,,,,,,SPARK-28675,,,,SPARK-22479,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 23:25:52 UTC 2018,,,,,,,,,,"0|i3s247:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/18 18:50;tgraves;ping [~ash211@gmail.com] [~onursatici] [~LI,Xiao] [~jiangxb1987] [~cloud_fan]  who were in code review, is there more background on why these were added?;;;","23/Apr/18 23:37;vanzin;Unless I hear back from the people CC'ed above, I'm going to revert the offending part of the original change, which is this:

{noformat}
-      .createWithDefault(""(?i)secret|password"".r) 
+      .createWithDefault(""(?i)secret|password|url|user|username"".r)
{noformat}

I'm a little inclined to keep redacting the URL since at least Oracle allows the password to be defined in the URL; it would be useful to see the rest of the URL for debugging purposes, but that would require extra code.

But redacting the user name is pretty silly; if you look at the env page with this change, you see a lot of things like this:

{noformat}
user.home	*********(redacted)
user.timezone	*********(redacted)
user.country	*********(redacted)
{noformat}

Which is kinda pointless.

;;;","23/Apr/18 23:47;tgraves;the url seems somewhat silly to me to, look at the environment page on yarn, at least in our environment it has redacted in a bunch of places that don't make sense.  If its an issue with the thriftserver and certain urls we should fix those separately.;;;","24/Apr/18 00:12;vanzin;Yeah, things like {{java.vendor.url}} end up redacted too...

But there's the issue of JDBC drivers allowing passwords in their URL. Though they generally allow the password to be provided separately in a properties object too, which is preferable.

Also, it turns out other parts of SQL use a different way of redacting things added in SPARK-22791. The way I read that, the code added to {{SaveIntoDataSourceCommand}} is now redundant, since paths that print plans will be automatically redacted by the code in {{QueryExecution}}.

That change added a separate config that defaults to the value of the config in core. If we change that to be a separate config instead of falling back to the code config, we could have different defaults (leaving the URL alone on the core side), but that changes behavior slightly.;;;","24/Apr/18 00:13;vanzin;Anyway, I'll take a stab at cleaning this up tomorrow if I find the time.;;;","25/Apr/18 17:37;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21158;;;","18/May/18 19:05;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21365;;;","18/May/18 23:25;vanzin;Issue resolved by pull request 21365
[https://github.com/apache/spark/pull/21365];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When Dataset.as converts column from nullable to non-nullable type, null Doubles are converted silently to -1",SPARK-23835,13149220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,josephkb,josephkb,30/Mar/18 18:22,17/Apr/18 13:48,13/Jul/23 08:45,17/Apr/18 13:48,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"I constructed a DataFrame with a nullable java.lang.Double column (and an extra Double column).  I then converted it to a Dataset using ```as[(Double, Double)]```.  When the Dataset is shown, it has a null.  When it is collected and printed, the null is silently converted to a -1.

Code snippet to reproduce this:
{code}
val localSpark = spark
import localSpark.implicits._
val df = Seq[(java.lang.Double, Double)](
  (1.0, 2.0),
  (3.0, 4.0),
  (Double.NaN, 5.0),
  (null, 6.0)
).toDF(""a"", ""b"")
df.show()  // OUTPUT 1: has null

df.printSchema()
val data = df.as[(Double, Double)]
data.show()  // OUTPUT 2: has null
data.collect().foreach(println)  // OUTPUT 3: has -1
{code}

OUTPUT 1 and 2:
{code}
+----+---+
|   a|  b|
+----+---+
| 1.0|2.0|
| 3.0|4.0|
| NaN|5.0|
|null|6.0|
+----+---+
{code}

OUTPUT 3:
{code}
(1.0,2.0)
(3.0,4.0)
(NaN,5.0)
(-1.0,6.0)
{code}
",,apachespark,josephkb,marmbrus,mgaido,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 04 15:20:04 UTC 2018,,,,,,,,,,"0|i3s033:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/18 18:27;marmbrus;/cc [~cloud_fan];;;","31/Mar/18 14:36;viirya;What is the better behavior it should have?;;;","02/Apr/18 10:01;mgaido;Actually this is not the first time we see this. Previously, we said that it was a user error, since if the data is a nullable Double, you should convert it using {{.as[Option[Double]]}}.

Anyway, enforcing this would mean avoiding the conversion of a nullable value to Dobule/Int/etc. (throwing an exception during analysis); but this can break existing users' applications (where maybe null are not present). Or we can eventually asserting there there is no null if we try to convert to primitive type (better than the previous I think).;;;","02/Apr/18 19:54;marmbrus;I believe the correct semantics are to throw a {{NullPointerException}} if the users tries to deserialize a null value into a type that doesn't support nulls.  See [this test case for an example|https://github.com/apache/spark/blob/b2edc30db1dcc6102687d20c158a2700965fdf51/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala#L1428-L1443]. We must just be missing the assertion in this case.;;;","04/Apr/18 15:20;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20976;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: LauncherServerSuite.testAppHandleDisconnect,SPARK-23834,13149209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,30/Mar/18 17:27,02/Apr/18 21:35,13/Jul/23 08:45,02/Apr/18 21:35,2.4.0,,,,,,,,,2.4.0,,,,,Tests,,,,0,,,,"https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.6/4349/testReport/junit/org.apache.spark.launcher/LauncherServerSuite/testAppHandleDisconnect/

{noformat}
Error Message
java.lang.IllegalStateException: Failed check after 99 tries: Expected error but message went through..
Stacktrace
sbt.ForkMain$ForkError: java.lang.IllegalStateException: Failed check after 99 tries: Expected error but message went through..
	at org.apache.spark.launcher.BaseSuite.eventually(BaseSuite.java:85)
	at org.apache.spark.launcher.LauncherServerSuite.waitForError(LauncherServerSuite.java:224)
	at org.apache.spark.launcher.LauncherServerSuite.testAppHandleDisconnect(LauncherServerSuite.java:200)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{noformat}
",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 02 21:35:29 UTC 2018,,,,,,,,,,"0|i3s00n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/18 20:09;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20950;;;","02/Apr/18 21:35;vanzin;Issue resolved by pull request 20950
[https://github.com/apache/spark/pull/20950];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-sql-kafka source in spark 2.3 causes reading stream failure frequently,SPARK-23829,13149097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,bethunebtj,bethunebtj,30/Mar/18 05:38,16/Jul/20 10:47,13/Jul/23 08:45,05/Dec/18 08:35,2.3.0,,,,,,,,,2.4.0,,,,,Structured Streaming,,,,1,,,,"In spark 2.3 , it provides a source ""spark-sql-kafka-0-10_2.11"".

 

When I wanted to read from my kafka-0.10.2.1 cluster, it throws out an error ""*java.util.concurrent.TimeoutException: Cannot fetch record xxxx for offset in 12000 milliseconds*""  frequently , and the job thus failed.

 

I searched on google & stackoverflow for a while, and found many other people who got this excption too, and nobody gave an answer.

 

I debuged the source code, found nothing, but I guess it's because the dependency spark-sql-kafka-0-10_2.11 is using.

 
{code:java}
<dependency>
 <groupId>org.apache.spark</groupId>
 <artifactId>spark-sql-kafka-0-10_2.11</artifactId>
 <version>2.3.0</version>
 <exclusions>
 <exclusion>
 <artifactId>kafka-clients</artifactId>
 <groupId>org.apache.kafka</groupId>
 </exclusion>
 </exclusions>
</dependency>
<dependency>
 <groupId>org.apache.kafka</groupId>
 <artifactId>kafka-clients</artifactId>
 <version>0.10.2.1</version>
</dependency>{code}
I excluded it from maven ,and added another version , rerun the code , and now it works.

 

I guess something is wrong on kafka-clients0.10.0.1 working with kafka0.10.2.1, or more kafka versions. 

 

Hope for an explanation.

Here is the error stack.
{code:java}
[ERROR] 2018-03-30 13:34:11,404 [stream execution thread for [id = 83076cf1-4bf0-4c82-a0b3-23d8432f5964, runId = b3e18aa6-358f-43f6-a077-e34db0822df6]] org.apache.spark.sql.execution.streaming.MicroBatchExecution logError - Query [id = 83076cf1-4bf0-4c82-a0b3-23d8432f5964, runId = b3e18aa6-358f-43f6-a077-e34db0822df6] terminated with error
org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 1 times, most recent failure: Lost task 6.0 in stage 0.0 (TID 6, localhost, executor driver): java.util.concurrent.TimeoutException: Cannot fetch record for offset 6481521 in 120000 milliseconds
at org.apache.spark.sql.kafka010.CachedKafkaConsumer.org$apache$spark$sql$kafka010$CachedKafkaConsumer$$fetchData(CachedKafkaConsumer.scala:230)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer$$anonfun$get$1.apply(CachedKafkaConsumer.scala:122)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer$$anonfun$get$1.apply(CachedKafkaConsumer.scala:106)
at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer.runUninterruptiblyIfPossible(CachedKafkaConsumer.scala:68)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:106)
at org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(KafkaSourceRDD.scala:157)
at org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(KafkaSourceRDD.scala:148)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:107)
at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:105)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)
at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:929)
at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:927)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:927)
at org.apache.spark.sql.execution.streaming.ForeachSink.addBatch(ForeachSink.scala:49)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3$$anonfun$apply$16.apply(MicroBatchExecution.scala:477)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3.apply(MicroBatchExecution.scala:475)
at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:474)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:133)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:121)
at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:117)
at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
Caused by: java.util.concurrent.TimeoutException: Cannot fetch record for offset 6481521 in 120000 milliseconds
at org.apache.spark.sql.kafka010.CachedKafkaConsumer.org$apache$spark$sql$kafka010$CachedKafkaConsumer$$fetchData(CachedKafkaConsumer.scala:230)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer$$anonfun$get$1.apply(CachedKafkaConsumer.scala:122)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer$$anonfun$get$1.apply(CachedKafkaConsumer.scala:106)
at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer.runUninterruptiblyIfPossible(CachedKafkaConsumer.scala:68)
at org.apache.spark.sql.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:106)
at org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(KafkaSourceRDD.scala:157)
at org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(KafkaSourceRDD.scala:148)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:107)
at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:105)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
{code}",,_paddy_,BdLearner,bethunebtj,collin-scangarella,Denys Tyshetskyy,dongjoon,gsomogyi,hster,leo.zhi,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,SPARK-31460,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 16 10:47:21 UTC 2020,,,,,,,,,,"0|i3rzbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/18 14:21;gsomogyi;In 2.4 it's fixed as it's using 2.0.0. I think an upgrade will solve this issue (if description about versions is correct).;;;","04/Dec/18 23:09;collin-scangarella;Confirmed, upgrading to spark 2.4 and kafka 2.0 resolves this issue.;;;","05/Dec/18 08:30;gsomogyi;[~collin-scangarella] thanks for your efforts!;;;","16/Aug/19 03:35;leo.zhi;{color:#14892c}I am using 2.4.0-chd6.3.0, and got this error again.{color}

Logical Plan:
 TypedFilter <function1>, interface org.apache.spark.sql.Row, [StructField(personId,LongType,true), StructField(code,LongType,true), StructField(eventTime,LongType,true), StructField(processTime,LongType,true)], createexternalrow(personId#35L, code#36L, eventTime#33L, processTime#34L, StructField(personId,LongType,true), StructField(code,LongType,true), StructField(eventTime,LongType,true), StructField(processTime,LongType,true))
 +- Project [json#30.personId AS personId#35L, json#30.code AS code#36L, unix_timestamp(to_utc_timestamp(cast(json#30.eventTime as timestamp), GMT-8), yyyy-MM-dd HH:mm:ss, Some(Asia/Shanghai)) AS eventTime#33L, unix_timestamp(timestamp#12, yyyy-MM-dd HH:mm:ss, Some(Asia/Shanghai)) AS processTime#34L|#33L, unix_timestamp(timestamp#12, yyyy-MM-dd HH:mm:ss, Some(Asia/Shanghai)) AS processTime#34L]
 +- Project [jsontostructs(StructField(code,LongType,true), StructField(eventTime,StringType,true), StructField(personId,LongType,true), cast(value#8 as string), Some(Asia/Shanghai)) AS json#30, timestamp#12|#8 as string), Some(Asia/Shanghai)) AS json#30, timestamp#12]
 +- StreamingExecutionRelation KafkaV2[Subscribe[capp-events]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13|#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]

at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
 Caused by: org.apache.spark.SparkException: Writing job aborted.
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
 at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
 at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
 at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
 at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
 at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)
 at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2782);;;","06/Sep/19 19:53;gsomogyi;[~leo.zhi] Just seen your comment. From the stack you've provided I'm not able to tell whether it's the same issue or not. Please open a new jira and attach full stacktrace and logs. If you ping me on the jira I can take a look next week.;;;","28/Jan/20 01:53;Denys Tyshetskyy;Hi [~gsomogyi], I am experiencing the same issue leo.zhi mentioned in his comment. Was wondering if there has been any progress on it/ticket number to follow?

We are using *2.4.0-cdh6.1.1*

 

In our case it starts with:

 
 Lost task 0.0 in stage 5965.0 (TID 15060, ..., executor 1): java.util.concurrent.TimeoutException: Cannot fetch record for offset 813529 in 2048 milliseconds
 at org.apache.spark.sql.kafka010.InternalKafkaConsumer.fetchData(KafkaDataConsumer.scala:488)
 at org.apache.spark.sql.kafka010.InternalKafkaConsumer.org$apache$spark$sql$kafka010$InternalKafkaConsumer$$fetchRecord(KafkaDataConsumer.scala:371)
 at org.apache.spark.sql.kafka010.InternalKafkaConsumer$$anonfun$get$1.apply(KafkaDataConsumer.scala:251)
 at org.apache.spark.sql.kafka010.InternalKafkaConsumer$$anonfun$get$1.apply(KafkaDataConsumer.scala:234)
 at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
 at org.apache.spark.sql.kafka010.InternalKafkaConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:209)
 at org.apache.spark.sql.kafka010.InternalKafkaConsumer.get(KafkaDataConsumer.scala:234)
 at org.apache.spark.sql.kafka010.KafkaDataConsumer$class.get(KafkaDataConsumer.scala:64)
 at org.apache.spark.sql.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.get(KafkaDataConsumer.scala:500)
 at org.apache.spark.sql.kafka010.KafkaMicroBatchInputPartitionReader.next(KafkaMicroBatchReader.scala:337)
 at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:49)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)
 at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
 at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
 at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
 at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
 at org.apache.spark.scheduler.Task.run(Task.scala:121)
 at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
  
 And then:
  
 Logical plan:
 ...
 Caused by: org.apache.spark.SparkException: Writing job aborted.
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
 at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
 at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
 at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
 at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
 at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)
 at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2782)
 at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2782)
 at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)
 at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
 at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)
 at org.apache.spark.sql.Dataset.collect(Dataset.scala:2782)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$17.apply(MicroBatchExecution.scala:537)
 at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:532)
 at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
 at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:531)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
 at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
 at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
 at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
 at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
 at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
  ;;;","28/Jan/20 09:05;gsomogyi;Cannot fetch record means Spark initiated the fetch but was timing out. First I would take a look at what happened on the Kafka side (since Kafka didn't respond in time). If you still think it's a Spark issue I would like to reproduce it with vanilla Spark + create a new jira with logs.;;;","04/Feb/20 09:46;BdLearner;[~gsomogyi]  me new to kafka and spark , what do you mean spark-vanila version here ?;;;","04/Feb/20 12:46;gsomogyi;[~BdLearner] I mean upstream Spark.;;;","07/Jul/20 07:01;_paddy_;[~gsomogyi] Isn't the fix backported to spark 2.3?
 We are restricted to Spark 2.3 due to HDP and facing this issue often. Tried excluding and implicitly adding kafka-clients 2.1.1 (our kafka version), but it doesn't seem to help.

Is there a way around on Spark 2.3?;;;","07/Jul/20 08:53;gsomogyi;This is not backported to 2.3.
Here is the list of Kafka changes relevant from Spark point of view: https://gist.github.com/gaborgsomogyi/3476c32d69ff2087ed5d7d031653c7a9
Kafka client can be hacked around manually but I discourage because it's risky.

I understand the upgrade constraints but I highly encourage to make further efforts to jump to 2.4 because that's not the only severe issue which 2.3 contains!
;;;","09/Jul/20 08:48;_paddy_;Thanks for the update [~gsomogyi]

The investigation doesn't seem to mention the root cause, Do we know why this happens and Is there some steps to reproduce this problem consistently?

I was able to replicate it consistently with transactional commits. When end offset = N, Spark 2.3 fails with a TimeoutException if (N-1)^th^ offset was a transaction-commit offset and there are no more messages after that.;;;","16/Jul/20 10:47;gsomogyi;Since this is race issue there is no consistent way to reproduce. Please reproduce it with our latest release (3.x) and open a Jira with the driver and executor logs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingJoinExec should ensure that input data is partitioned into specific number of partitions,SPARK-23827,13149035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,29/Mar/18 22:31,13/Jul/18 05:28,13/Jul/23 08:45,30/Mar/18 23:49,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Structured Streaming,,,,0,,,,"Currently, the requiredChildDistribution does not specify the partitions. This can cause the weird corner cases where the child's distribution is `SinglePartition` which satisfies the required distribution of `ClusterDistribution(no-num-partition-requirement)`, thus eliminating the shuffle needed to repartition input data into the required number of partitions (i.e. same as state stores). That can lead to ""file not found"" errors on the state store delta files as the micro-batch-with-no-shuffle will not run certain tasks and therefore not generate the expected state store delta files.",,apachespark,tdas,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 30 23:49:39 UTC 2018,,,,,,,,,,"0|i3ryy7:",9223372036854775807,,,,,,,,,,,,,2.3.1,2.4.0,3.0.0,,,,,,,,"29/Mar/18 23:26;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20941;;;","30/Mar/18 23:49;tdas;Issue resolved by pull request 20941
[https://github.com/apache/spark/pull/20941];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[K8s] Spark pods should request memory + memoryOverhead as resources,SPARK-23825,13149022,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dvogelbacher,dvogelbacher,29/Mar/18 21:59,17/May/20 18:23,13/Jul/23 08:45,02/Apr/18 19:02,2.3.0,,,,,,,,,2.4.0,,,,,Kubernetes,Spark Core,,,0,,,,"We currently request  {{spark.[driver,executor].memory}} as memory from Kubernetes (e.g., [here|https://github.com/apache/spark/blob/master/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/steps/BasicDriverConfigurationStep.scala#L95]).
The limit is set to {{spark.[driver,executor].memory + spark.kubernetes.[driver,executor].memoryOverhead}}.
This seems to be using Kubernetes wrong. 
[How Pods with resource limits are run|https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-limits-are-run], states:

{noformat}
If a Container exceeds its memory request, it is likely that its Pod will be evicted whenever the node runs out of memory.
{noformat}
Thus, if a the  spark driver/executor uses {{memory + memoryOverhead}} memory, it can be evicted. While an executor might get restarted (but it would still be very bad performance-wise), the driver would be hard to recover.

I think spark should be able to run with the requested (and, thus, guaranteed) resources from Kubernetes without being in danger of termination without needing to rely on optional available resources.

Thus, we shoud request {{memory + memoryOverhead}} memory from Kubernetes (and this should also be the limit).",,apachespark,dvogelbacher,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 30 00:18:05 UTC 2018,,,,,,,,,,"0|i3ryvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/18 22:05;dvogelbacher;Will make a PR shortly, cc [~mcheah];;;","29/Mar/18 23:51;dvogelbacher;addressed by https://github.com/apache/spark/pull/20943;;;","30/Mar/18 00:18;apachespark;User 'dvogelbacher' has created a pull request for this issue:
https://github.com/apache/spark/pull/20943;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResolveReferences loses correct origin,SPARK-23823,13148960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,qzhzm173227,qzhzm173227,qzhzm173227,29/Mar/18 17:47,06/Apr/18 03:07,13/Jul/23 08:45,06/Apr/18 03:07,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Introduced in [https://github.com/apache/spark/pull/19585]

ResolveReferences stopped doing transfromsUp after this change and Attributes sometimes lose its correct origin",,apachespark,kiszk,maropu,qzhzm173227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 02 17:07:05 UTC 2018,,,,,,,,,,"0|i3ryhj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/18 17:52;apachespark;User 'JiahuiJiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20939;;;","02/Apr/18 17:07;apachespark;User 'JiahuiJiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20961;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchFailedException when killing speculative task,SPARK-23816,13148812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,xchen12138,xchen12138,29/Mar/18 08:46,15/Jul/19 19:30,13/Jul/23 08:45,09/Apr/18 18:35,2.2.0,,,,,,,,,2.2.2,2.3.1,2.4.0,,,SQL,,,,0,speculation,,,"When spark trying to kill speculative tasks because of another attempt has already success, sometimes the task throws ""org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer"" and the whole stage will fail.

Other active stages will also fail with error ""org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle"" Then I checked the log in failed executor, there is not error like ""MetadataFetchFailedException"". So they just failed with no error.
{code:java}
18/03/26 23:12:09 INFO Executor: Executor is trying to kill task 2879.1 in stage 4.0 (TID 13023), reason: another attempt succeeded
18/03/26 23:12:09 ERROR ShuffleBlockFetcherIterator: Failed to create input stream from local block
java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/hadoop02/yarn/local/usercache/pp_risk_grs_datamart_batch/appcache/application_1521504416249_116088/blockmgr-754a22fd-e8d6-4478-bcf8-f1d95f07f4a2/0c/shuffle_24_10_0.data, offset=263687568, length=87231}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:114)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:401)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:104)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:103)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:164)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.io.InputStream.skip(InputStream.java:224)
	at org.spark_project.guava.io.ByteStreams.skipFully(ByteStreams.java:755)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:100)
	... 28 more

{code}
 

And in Spark UI, I found the only failed task is 13023, which means killing speculative task make whole stage fails.
||[Index|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Index&task.pageSize=100]||[ID|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=ID&task.pageSize=100]||[Attempt|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Attempt&task.pageSize=100]||[Status ▴|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Status&task.desc=true&task.pageSize=100]||[Locality Level|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Locality+Level&task.pageSize=100]||[Executor ID / Host|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Executor+ID+%2F+Host&task.pageSize=100]||[Launch Time|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Launch+Time&task.pageSize=100]||[Duration|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Duration&task.pageSize=100]||[GC Time|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=GC+Time&task.pageSize=100]||[Shuffle Read Size / Records|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Shuffle+Read+Size+%2F+Records&task.pageSize=100]||[Write Time|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Write+Time&task.pageSize=100]||[Shuffle Write Size / Records|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Shuffle+Write+Size+%2F+Records&task.pageSize=100]||[Errors|http://lvshdc2en0012.lvs.paypal.com:8088/proxy/application_1521504416249_116088/stages/stage?id=4&attempt=0&task.sort=Errors&task.pageSize=100]||
|2879|13023|1 (speculative)|FAILED|PROCESS_LOCAL|33 / lvshdc2dn2202.lvs.****.com
 [stdout|http://lvshdc2dn2202.lvs.paypalinc.com:8042/node/containerlogs/container_e39_1521504416249_116088_01_000131/pp_risk_grs_datamart_batch/stdout?start=-4096]
 [stderr|http://lvshdc2dn2202.lvs.paypalinc.com:8042/node/containerlogs/container_e39_1521504416249_116088_01_000131/pp_risk_grs_datamart_batch/stderr?start=-4096]|2018/03/26 23:12:09| | |/| |/|FetchFailed(BlockManagerId(33, lvshdc2dn2202.lvs.paypalinc.com, 33481, None), shuffleId=24, mapId=10, reduceId=2879, message=|",,apachespark,irashid,nadavw,roczei,tgraves,vanzin,xchen12138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28340,SPARK-19276,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 09 18:35:36 UTC 2018,,,,,,,,,,"0|i3rxkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/18 21:28;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/20987;;;","09/Apr/18 18:35;vanzin;Issue resolved by pull request 20987
[https://github.com/apache/spark/pull/20987];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark writer dynamic partition overwrite mode fails to write output on multi level partition,SPARK-23815,13148791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shengzhixia,shengzhixia,shengzhixia,29/Mar/18 05:16,13/Apr/18 05:59,13/Jul/23 08:45,13/Apr/18 05:48,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,,,,"Spark introduced new writer mode to overwrite only related partitions in SPARK-20236. While we are using this feature in our production cluster, we found a bug when writing multi-level partitions on HDFS.

A simple test case to reproduce this issue:
 val df = Seq((""1"",""2"",""3"")).toDF(""col1"", ""col2"",""col3"")
 df.write.partitionBy(""col1"",""col2"").mode(""overwrite"").save(""/my/hdfs/location"")

If HDFS location ""/my/hdfs/location"" does not exist, there will be no output.

This seems to be caused by the job commit change in SPARK-20236 in HadoopMapReduceCommitProtocol.

In the commit job process, the output has been written into staging dir /my/hdfs/location/.spark-staging.xxx/col1=1/col2=2, and then the code calls fs.rename to rename /my/hdfs/location/.spark-staging.xxx/col1=1/col2=2 to /my/hdfs/location/col1=1/col2=2. However, in our case the operation will fail on HDFS because /my/hdfs/location/col1=1 does not exists. HDFS rename can not create directory for more than one level. 

This does not happen in unit test covered with SPARK-20236 with local file system.

We are proposing a fix. When cleaning current partition dir /my/hdfs/location/col1=1/col2=2 before the rename op, if the delete op fails (because /my/hdfs/location/col1=1/col2=2 may not exist), we call mkdirs op to create the parent dir /my/hdfs/location/col1=1 (if the parent dir does not exist) so the following rename op can succeed.

 

Reference: In official hdfs documentation([https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/filesystem.html),] the rename operation has preconditions: 

{code}

{{dest}} must be root, or have a parent that exists

{code}
 ",,apachespark,cloud_fan,jonasamrich,shengzhixia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 13 05:48:23 UTC 2018,,,,,,,,,,"0|i3rxfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/18 05:21;apachespark;User 'fangshil' has created a pull request for this issue:
https://github.com/apache/spark/pull/20931;;;","13/Apr/18 05:48;cloud_fan;Issue resolved by pull request 20931
[https://github.com/apache/spark/pull/20931];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Active SparkSession should be set by getOrCreate,SPARK-23809,13148733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ekhliang,ekhliang,ekhliang,28/Mar/18 22:34,08/Apr/18 04:20,13/Jul/23 08:45,08/Apr/18 04:20,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Currently, the active spark session is set inconsistently (e.g., in createDataFrame, prior to query execution). Many places in spark also incorrectly query active session when they should be calling activeSession.getOrElse(defaultSession).

The semantics here can be cleaned up if we also set the active session when the default session is set.",,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 04 00:31:05 UTC 2018,,,,,,,,,,"0|i3rx3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/18 22:36;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/20927;;;","04/Apr/18 00:31;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/20971;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test spark sessions should set default session,SPARK-23808,13148715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,joseph.torres,joseph.torres,28/Mar/18 21:24,30/Mar/18 04:38,13/Jul/23 08:45,30/Mar/18 04:38,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"SparkSession.getOrCreate() ensures that the session it returns is set as a default. Test code (TestSparkSession and TestHiveSparkSession) shortcuts around this method, and thus a default is never set. We need to set it.",,apachespark,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 28 21:26:05 UTC 2018,,,,,,,,,,"0|i3rwzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/18 21:26;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20926;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broadcast. unpersist can cause fatal exception when used with dynamic allocation,SPARK-23806,13148614,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,28/Mar/18 15:51,29/Mar/18 08:38,13/Jul/23 08:45,29/Mar/18 08:38,2.2.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,,,,"Very similar to https://issues.apache.org/jira/browse/SPARK-22618 . But this could also apply to Broadcast.unpersist.

 

2018-03-24 05:29:17,836 [Spark Context Cleaner] ERROR org.apache.spark.ContextCleaner - Error cleaning broadcast 85710 org.apache.spark.SparkException: Exception thrown in awaitResult: at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:152) at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:306) at org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45) at org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:60) at org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:238) at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$1.apply(ContextCleaner.scala:194) at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$1.apply(ContextCleaner.scala:185) at scala.Option.foreach(Option.scala:257) at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:185) at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1286) at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:178) at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:73) Caused by: java.io.IOException: Failed to send RPC 7228115282075984867 to /10.10.10.10:53804: java.nio.channels.ClosedChannelException at org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237) at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507) at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481) at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420) at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122) at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:852) at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:738) at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1251) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:733) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:725) at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:35) at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1062) at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1116) at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1051) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:446) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) at java.lang.Thread.run(Thread.java:745) Caused by: java.nio.channels.ClosedChannelException",,apachespark,cloud_fan,glenn.strycker@gmail.com,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22618,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 08:38:42 UTC 2018,,,,,,,,,,"0|i3rwcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/18 18:18;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/20924;;;","29/Mar/18 08:38;cloud_fan;Issue resolved by pull request 20924
[https://github.com/apache/spark/pull/20924];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PropagateEmptyRelation can leave query plan in unresolved state,SPARK-23802,13148257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,robert3005,robert3005,robert3005,27/Mar/18 15:52,04/Apr/18 00:27,13/Jul/23 08:45,04/Apr/18 00:27,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Since [https://github.com/apache/spark/pull/19825] PropagateEmptyRelation has been taught to handle more cases it can cause the optimized query plan to be unresolved.

Simple repro is to run following through the optimizer
{code:java}
LocalRelation.fromExternalRows(Seq('a.int), data = Seq(Row(1))) .join(LocalRelation('a.int, 'b.int), UsingJoin(FullOuter, ""a"" :: Nil), None){code}
Which results in
{code:java}
Project [coalesce(a#0, null) AS a#7, null AS b#6]
+- LocalRelation [a#0]{code}
This then fails type check on coalesce expression since `a` and null have different type.

 

Simple, targeted fix is to change PropagateEmptyRelation to add casts around nulls. More comprehensive fix would be to run type coercion at the end of optimization so it can fix cases like those. Alternatively the type checking code could treat NullType as equal to any other type and not fail the type check in the first place.",,apachespark,robert3005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 27 15:55:05 UTC 2018,,,,,,,,,,"0|i3ru5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/18 15:55;apachespark;User 'robert3005' has created a pull request for this issue:
https://github.com/apache/spark/pull/20914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[CBO] FilterEstimation.evaluateInSet produces devision by zero in a case of empty table with analyzed statistics,SPARK-23799,13148232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mshtelma,mshtelma,mshtelma,27/Mar/18 13:51,17/May/20 17:58,13/Jul/23 08:45,22/Apr/18 06:36,2.2.1,2.3.0,,,,,,,,2.4.0,,,,,Optimizer,SQL,,,0,,,,"Spark 2.2.1 and 2.3.0 can produce NumberFormatException (see below) during the analysis of the queries, which are using previously analyzed hive tables. 

The NumberFormatException occurs because in [FilterEstimation.scala on lines 50 and 52|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/FilterEstimation.scala?utf8=%E2%9C%93#L50-L52] the method calculateFilterSelectivity returns NaN, which is caused by devision by zero. This leads to NumberFormatException during conversion from Double to BigDecimal. 

NaN is caused by devision by zero in evaluateInSet method. 

Exception:

java.lang.NumberFormatException

at java.math.BigDecimal.<init>(BigDecimal.java:494)

at java.math.BigDecimal.<init>(BigDecimal.java:824)

at scala.math.BigDecimal$.decimal(BigDecimal.scala:52)

at scala.math.BigDecimal$.decimal(BigDecimal.scala:55)

at scala.math.BigDecimal$.double2bigDecimal(BigDecimal.scala:343)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation.estimate(FilterEstimation.scala:52)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.BasicStatsPlanVisitor$.visitFilter(BasicStatsPlanVisitor.scala:43)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.BasicStatsPlanVisitor$.visitFilter(BasicStatsPlanVisitor.scala:25)

at org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor$class.visit(LogicalPlanVisitor.scala:30)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.BasicStatsPlanVisitor$.visit(BasicStatsPlanVisitor.scala:25)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply(LogicalPlanStats.scala:35)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply(LogicalPlanStats.scala:33)

at scala.Option.getOrElse(Option.scala:121)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$class.stats(LogicalPlanStats.scala:33)

at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:30)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils$$anonfun$rowCountsExist$1.apply(EstimationUtils.scala:32)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils$$anonfun$rowCountsExist$1.apply(EstimationUtils.scala:32)

at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)

at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43)

at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.EstimationUtils$.rowCountsExist(EstimationUtils.scala:32)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.ProjectEstimation$.estimate(ProjectEstimation.scala:27)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.BasicStatsPlanVisitor$.visitProject(BasicStatsPlanVisitor.scala:63)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.BasicStatsPlanVisitor$.visitProject(BasicStatsPlanVisitor.scala:25)

at org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor$class.visit(LogicalPlanVisitor.scala:37)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.BasicStatsPlanVisitor$.visit(BasicStatsPlanVisitor.scala:25)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply(LogicalPlanStats.scala:35)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$$anonfun$stats$1.apply(LogicalPlanStats.scala:33)

at scala.Option.getOrElse(Option.scala:121)

at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats$class.stats(LogicalPlanStats.scala:33)

at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:30)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$$anonfun$2.apply(CostBasedJoinReorder.scala:64)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$$anonfun$2.apply(CostBasedJoinReorder.scala:64)

at scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:83)

at scala.collection.immutable.List.forall(List.scala:84)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$.org$apache$spark$sql$catalyst$optimizer$CostBasedJoinReorder$$reorder(CostBasedJoinReorder.scala:64)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$$anonfun$1.applyOrElse(CostBasedJoinReorder.scala:46)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$$anonfun$1.applyOrElse(CostBasedJoinReorder.scala:43)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)

at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)

at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

at scala.collection.immutable.List.foreach(List.scala:392)

at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)

at scala.collection.immutable.List.map(List.scala:296)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)

at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

at scala.collection.immutable.List.foreach(List.scala:392)

at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)

at scala.collection.immutable.List.map(List.scala:296)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$.apply(CostBasedJoinReorder.scala:43)

at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$.apply(CostBasedJoinReorder.scala:35)

at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)

at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)

at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)

at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)

at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)

at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)

at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)

at scala.collection.immutable.List.foreach(List.scala:392)

at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)",,apachespark,maropu,mshtelma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 25 04:01:05 UTC 2018,,,,,,,,,,"0|i3rtzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/18 13:54;apachespark;User 'mshtelma' has created a pull request for this issue:
https://github.com/apache/spark/pull/20913;;;","12/Apr/18 12:08;apachespark;User 'mshtelma' has created a pull request for this issue:
https://github.com/apache/spark/pull/21052;;;","25/Apr/18 04:01;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21147;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUID() should be stateful,SPARK-23794,13147924,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,hvanhovell,hvanhovell,26/Mar/18 12:05,27/Mar/18 12:50,13/Jul/23 08:45,27/Mar/18 12:50,2.4.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,The UUID() expression is stateful and should implement the Stateful trait instead of the Nondeterministic trait.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23599,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 27 09:17:05 UTC 2018,,,,,,,,,,"0|i3rs4f:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"27/Mar/18 09:17;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20912;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in StreamingQuerySuite,SPARK-23788,13147639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joseph.torres,joseph.torres,joseph.torres,24/Mar/18 02:13,25/Mar/18 01:29,13/Jul/23 08:45,25/Mar/18 01:22,2.4.0,,,,,,,,,2.2.2,2.3.1,2.4.0,,,Structured Streaming,,,,0,,,,"The serializability test uses the same MemoryStream instance for 3 different queries. If any of those queries ask it to commit before the others have run, the rest will see empty dataframes. This can fail the test if q3 is affected.",,apachespark,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 24 02:16:04 UTC 2018,,,,,,,,,,"0|i3rqdj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/18 02:16;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkSubmitSuite::""download remote resource if it is not supported by yarn"" fails on Hadoop 2.9",SPARK-23787,13147612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,23/Mar/18 22:26,26/Mar/18 06:01,13/Jul/23 08:45,26/Mar/18 06:01,2.4.0,,,,,,,,,2.4.0,,,,,Spark Core,Tests,,,0,,,,"{noformat}
[info] - download list of files to local (10 milliseconds)
[info]   ""http:///work/apache/spark/target/tmp/spark-a17bc160-641b-41e1-95be-a2e31b175e09/testJar3393247632492201277.jar"" did not start with substring ""file:"" (SparkSubmitSuite.scala:1022)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:340)
[info]   at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668)
[info]   at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6704)
[info]   at org.apache.spark.deploy.SparkSubmitSuite.org$apache$spark$deploy$SparkSubmitSuite$$testRemoteResources(SparkSubmitSuite.scala:1022)
[info]   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$18.apply$mcV$sp(SparkSubmitSuite.scala:962)
[info]   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$18.apply(SparkSubmitSuite.scala:962)
[info]   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$18.apply(SparkSubmitSuite.scala:962)
{noformat}

That's because Hadoop 2.9 supports http as a file system, and the test expects the Hadoop libraries not to. I also found a couple of other bugs in the test (although the code itself for the feature is fine).",,apachespark,jerryshao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 26 06:01:42 UTC 2018,,,,,,,,,,"0|i3rq7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 22:35;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20895;;;","26/Mar/18 06:01;jerryshao;Issue resolved by pull request 20895
[https://github.com/apache/spark/pull/20895];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV schema validation - column names are not checked,SPARK-23786,13147596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,23/Mar/18 21:02,16/Aug/18 15:49,13/Jul/23 08:45,04/Jun/18 05:02,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,1,,,,"Here is a csv file contains two columns of the same type:
{code}
$cat marina.csv
depth, temperature
10.2, 9.0
5.5, 12.3
{code}

If we define the schema with correct types but wrong column names (reversed order):
{code:scala}
val schema = new StructType().add(""temperature"", DoubleType).add(""depth"", DoubleType)
{code}

Spark reads the csv file without any errors:
{code:scala}
val ds = spark.read.schema(schema).option(""header"", ""true"").csv(""marina.csv"")
ds.show
{code}
and outputs wrong result:
{code}
+-----------+-----+
|temperature|depth|
+-----------+-----+
|       10.2|  9.0|
|        5.5| 12.3|
+-----------+-----+
{code}
The correct behavior would be either output error or read columns according its names in the schema.",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,SPARK-25134,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 21:15:05 UTC 2018,,,,,,,,,,"0|i3rq3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 21:15;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LauncherBackend doesn't check state of connection before setting state,SPARK-23785,13147580,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stakiar,stakiar,stakiar,23/Mar/18 20:12,30/Mar/18 17:26,13/Jul/23 08:45,29/Mar/18 17:23,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,Found in HIVE-18533 while trying to integration with the {{InProcessLauncher}}. {{LauncherBackend}} doesn't check the state of its connection to the {{LauncherServer}} before trying to run {{setState}} - which sends a {{SetState}} message on the connection.,,apachespark,stakiar,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-18533,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 17:23:56 UTC 2018,,,,,,,,,,"0|i3rq0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 20:27;apachespark;User 'sahilTakiar' has created a pull request for this issue:
https://github.com/apache/spark/pull/20893;;;","23/Mar/18 20:47;stakiar;[~vanzin] opened a PR that just checks if {{isConnected}} is true before writing to the connection, but not sure this will fix the issue from HIVE-18533. Looking through the code it seems like this only gets set when the {{LauncherBackend}} closes the connection, but doesn't detect when the {{LauncherServer}} closes the connection. Unless, I'm missing something.;;;","23/Mar/18 21:42;vanzin;This is a little trickier than just the checks you have in the PR.

The check that is triggering in Hive is on the {{LauncherBackend}} side. So it has somehow already been closed, and a {{setState}} call happens. That can happen if there are two calls to {{LocalSchedulerBackend.stop}}, which can happen if someone with a launcher handle calls {{stop()}} on the handle. But the code should be safe against that and just ignore subsequent calls.

The race you describe also exists; it's not what the exception in the Hive bug shows, though.

So perhaps it's better to do a few different things:

- add the checks in your PR
- in LauncherBackend.BackendConnection, set ""isDisconnected"" before calling super.close()
- in that same class, override the ""send()"" method to ignore ""SocketException"", to handle the second race.
;;;","24/Mar/18 02:17;stakiar;Updated the PR.
{quote} in LauncherBackend.BackendConnection, set ""isDisconnected"" before calling super.close()
{quote}
Done
{quote} The race you describe also exists; it's not what the exception in the Hive bug shows, though.
{quote}
I tried to write a test to replicate this issue, but it seems its already handled in {{LauncherConnection}}, if the {{SparkAppHandle}} calls {{disconnect}} the client connection automatically gets closed because it gets an {{EOFException}}, which triggers {{close()}} - the logic is in {{LauncherConnection#run}}.

So I guess its already handled? I added my test case to the PR, should be useful since it covers what happens when {{SparkAppHandle#disconnect}} is called.;;;","29/Mar/18 17:23;vanzin;Issue resolved by pull request 20893
[https://github.com/apache/spark/pull/20893];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to use googleVis library with new SparkR,SPARK-23780,13147511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,idzikovsky,idzikovsky,23/Mar/18 15:55,22/May/18 19:25,13/Jul/23 08:45,15/May/18 02:26,2.2.1,2.3.0,,,,,,,,2.3.1,2.4.0,,,,SparkR,,,,0,regression,,,"I've tried to use googleVis library with Spark 2.2.1, and faced with problem.

Steps to reproduce:
# Install R with googleVis library.
# Run SparkR:
{code}
sparkR --master yarn --deploy-mode client
{code}
# Run code that uses googleVis:
{code}
library(googleVis)
df=data.frame(country=c(""US"", ""GB"", ""BR""), 
              val1=c(10,13,14), 
              val2=c(23,12,32))
Bar <- gvisBarChart(df)
cat(""%html "", Bar$html$chart)
{code}

Than I got following error message:
{code}
Error : .onLoad failed in loadNamespace() for 'googleVis', details:
  call: rematchDefinition(definition, fdef, mnames, fnames, signature)
  error: methods can add arguments to the generic 'toJSON' only if '...' is an argument to the generic
Error : package or namespace load failed for 'googleVis'
{code}

But expected result is to get some HTML code output, as it was with Spark 2.1.0.
",,achan,apachespark,asukhenko,felixcheung,idzikovsky,sarjeet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 02:27:06 UTC 2018,,,,,,,,,,"0|i3rpl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/18 06:45;felixcheung;hmm, I think the cause of this is the incompatibility of the method signature of toJSON;;;","24/Mar/18 06:49;felixcheung;here

[https://github.com/mages/googleVis/blob/master/R/zzz.R#L39]

 or here

[https://github.com/jeroen/jsonlite/blob/master/R/toJSON.R#L2] ;;;","25/Mar/18 07:09;felixcheung;though there are other methods

 

[https://www.rforge.net/doc/packages/JSON/toJSON.html]

 ;;;","04/May/18 07:25;achan;I have the same issue.  I trying Spark 2.3.0.  I tried installing the mages/googleVis and still had the same problem.  Had to fall back to Spark 2.1.7.;;;","04/May/18 07:53;idzikovsky;[~achan]

You can still use googleVis with Spark 2.3. To do this you will need to detach SparkR package before importing googleVis, and then reimport it again:
{code:java}
detach(""package:SparkR"")
library(googleVis)
suppressPackageStartupMessages(library(SparkR))

df=data.frame(country=c(""US"", ""GB"", ""BR""), 
val1=c(10,13,14), 
val2=c(23,12,32))
Bar <- gvisBarChart(df)
print(Bar, tag = 'chart'){code};;;","04/May/18 17:14;achan;Thanks Ivan.  I'll give it a try.;;;","08/May/18 05:59;felixcheung;I suppose if you load googleVis first and then SparkR it would have the same effect as Ivan's steps?;;;","13/May/18 21:29;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/21315;;;","15/May/18 02:27;felixcheung;[~vanzin] - FYI - if we have another RC this can be pulled into 2.3.1.

(I guess I just missed your cut of RC1);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.emptyRDD confuses SparkContext.union,SPARK-23778,13147468,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mgaido,ragazzojp,ragazzojp,23/Mar/18 13:18,20/Jun/18 05:29,13/Jul/23 08:45,20/Jun/18 05:29,1.3.0,2.3.0,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"SparkContext.emptyRDD is an unpartitioned RDD. Clearly it's empty so whether it's partitioned or not should be just a academic debate. Unfortunately it doesn't seem to be like this and the issue has side effects.

Namely, it confuses the RDD union.

When there are N classic RDDs partitioned the same way, the union is implemented with the optimized PartitionerAwareUnionRDD, that retains the common partitioner in the result. If one of the N RDDs happens to be an emptyRDD, as it doesn't have a partitioner, the union is implemented by just appending all the partitions of the N RDDs, dropping the partitioner. But there's no need for this, as the emptyRDD contains no elements. This results in further unneeded shuffles once the result of the union is used.

See for example:

{{val p = new HashPartitioner(3)}}
{{val a = context.parallelize(List(10, 20, 30, 40, 50)).keyBy(_ / 10).partitionBy(p)}}
{{val b1 = a.mapValues(_ + 1)}}
{{val b2 = a.mapValues(_ - 1)}}
{{val e = context.emptyRDD[(Int, Int)]}}
{{val x = context.union(a, b1, b2, e)}}
{{val y = x.reduceByKey(_ + _)}}
{{assert(x.partitioner.contains(p))}}
{{y.collect()}}

The assert fails. Disabling it, it's possible to see that reduceByKey introduced a shuffles, although all the input RDDs are already partitioned the same way, but the emptyRDD.

Forcing a partitioner on the emptyRDD:

{{val e = context.emptyRDD[(Int, Int)].partitionBy(p)}}

solves the problem with the assert and doesn't introduce the unneeded extra stage and shuffle.

Union implementation should be changed to ignore the partitioner of emptyRDDs and consider those as _partitioned in a way compatible with any partitioner_, basically ignoring them.

Present since 1.3 at least.",,apachespark,cloud_fan,mgaido,ragazzojp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 13:19;ragazzojp;as_it_should_be.png;https://issues.apache.org/jira/secure/attachment/12915899/as_it_should_be.png","23/Mar/18 13:18;ragazzojp;partitioner_lost_and_unneeded_extra_stage.png;https://issues.apache.org/jira/secure/attachment/12915898/partitioner_lost_and_unneeded_extra_stage.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 20 05:29:28 UTC 2018,,,,,,,,,,"0|i3rpbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/18 14:47;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/21333;;;","20/Jun/18 05:29;cloud_fan;Issue resolved by pull request 21333
[https://github.com/apache/spark/pull/21333];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: DataFrameRangeSuite,SPARK-23775,13147347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,23/Mar/18 02:27,07/May/18 06:48,13/Jul/23 08:45,07/May/18 06:48,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,Tests,,,0,,,,"DataFrameRangeSuite.test(""Cancelling stage in a query with Range."") stays sometimes in an infinite loop and times out the build.

I presume the original intention of this test is to start a job with range and just cancel it.
The submitted job has 2 stages but I think the author tried to cancel the first stage with ID 0 which is not the case here:

{code:java}
eventually(timeout(10.seconds), interval(1.millis)) {
  assert(DataFrameRangeSuite.stageToKill > 0)
}
{code}

All in all if the first stage is slower than 10 seconds it throws TestFailedDueToTimeoutException and cancelStage will be never ever called.


- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/4780/",,apachespark,cloud_fan,gsomogyi,irashid,kiszk,vanzin,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 05:23;gsomogyi;filtered.log;https://issues.apache.org/jira/secure/attachment/12915817/filtered.log","09/Apr/18 13:32;gsomogyi;filtered_more_logs.log;https://issues.apache.org/jira/secure/attachment/12918154/filtered_more_logs.log",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 07 06:48:53 UTC 2018,,,,,,,,,,"0|i3rokn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 02:28;gsomogyi;Working on it.;;;","23/Mar/18 02:47;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20888;;;","18/Apr/18 23:38;vanzin;Issue resolved by pull request 20888
[https://github.com/apache/spark/pull/20888];;;","02/May/18 10:00;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/21214;;;","07/May/18 06:48;cloud_fan;Issue resolved by pull request 21214
[https://github.com/apache/spark/pull/21214];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodegenContext.withSubExprEliminationExprs should save/restore CSE state correctly,SPARK-23760,13146728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,21/Mar/18 05:01,22/Mar/18 04:22,13/Jul/23 08:45,22/Mar/18 04:22,2.2.0,2.2.1,2.3.0,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"There's a bug in {{CodegenContext.withSubExprEliminationExprs()}} that makes it effectively always clear the subexpression elimination state after it's called.

The original intent of this function was that it should save the old state, set the given new state as current and perform codegen (invoke {{Expression.genCode()}}), and at the end restore the subexpression elimination state back to the old state. This ticket tracks a fix to actually implement the original intent.",,apachespark,cloud_fan,kiszk,maropu,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 22 04:22:21 UTC 2018,,,,,,,,,,"0|i3rkrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/18 05:07;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/20870;;;","22/Mar/18 04:22;cloud_fan;Issue resolved by pull request 20870
[https://github.com/apache/spark/pull/20870];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to bind Spark UI to specific host name / IP,SPARK-23759,13146684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falbani,falbani,falbani,20/Mar/18 22:33,23/Mar/18 17:38,13/Jul/23 08:45,23/Mar/18 17:37,2.2.0,,,,,,,,,2.2.2,2.3.1,2.4.0,,,Spark Core,Web UI,,,0,,,,"Ideally, exporting SPARK_LOCAL_IP=<private-interface-ip-address> in spark2 environment should allow Spark2 History server to bind to private interface however this is not working in spark 2.2.0

 

Spark2 history server still listens on 0.0.0.0
{code:java}
[root@sparknode1 ~]# netstat -tulapn|grep 18081
tcp        0      0 0.0.0.0:18081               0.0.0.0:*                   LISTEN      21313/java
tcp        0      0 172.26.104.151:39126        172.26.104.151:18081        TIME_WAIT   -
{code}
On earlier versions this change was working fine:
{code:java}
[root@dwphive1 ~]# netstat -tulapn|grep 18081
tcp        0      0 172.26.113.55:18081         0.0.0.0:*                   LISTEN      2565/java
{code}
 

This issue not only affects SHS but also Spark UI in general

 ",,apachespark,falbani,h_o,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 17:37:18 UTC 2018,,,,,,,,,,"0|i3rki7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/18 00:49;apachespark;User 'felixalbani' has created a pull request for this issue:
https://github.com/apache/spark/pull/20867;;;","22/Mar/18 19:27;apachespark;User 'felixalbani' has created a pull request for this issue:
https://github.com/apache/spark/pull/20883;;;","23/Mar/18 17:37;vanzin;Issue resolved by pull request 20883
[https://github.com/apache/spark/pull/20883];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StopIterator exception in Python UDF results in partial result,SPARK-23754,13146640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,edorigatti,icexelloss,icexelloss,20/Mar/18 19:09,12/Dec/22 18:11,13/Jul/23 08:45,30/May/18 10:14,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,PySpark,,,,0,,,,"Reproduce:
{code:java}
df = spark.range(0, 1000)
from pyspark.sql.functions import udf

def foo(x):
    raise StopIteration()

df.withColumn('v', udf(foo)).show()

# Results
# +---+---+
# | id|  v|
# +---+---+
# +---+---+{code}

I think the task should fail in this case",,apachespark,bryanc,icexelloss,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24034,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 09:48:05 UTC 2018,,,,,,,,,,"0|i3rk8n:",9223372036854775807,,,,,,,,,,,,,2.3.1,,,,,,,,,,"21/May/18 18:38;apachespark;User 'e-dorigatti' has created a pull request for this issue:
https://github.com/apache/spark/pull/21383;;;","24/May/18 22:17;vanzin;Adding target version so it actually shows up in searches...;;;","30/May/18 10:14;gurwls223;Fixed in https://github.com/apache/spark/pull/21383

Backporting is needed.;;;","30/May/18 10:23;apachespark;User 'e-dorigatti' has created a pull request for this issue:
https://github.com/apache/spark/pull/21461;;;","30/May/18 14:53;apachespark;User 'e-dorigatti' has created a pull request for this issue:
https://github.com/apache/spark/pull/21463;;;","31/May/18 12:34;apachespark;User 'e-dorigatti' has created a pull request for this issue:
https://github.com/apache/spark/pull/21467;;;","01/Jun/18 01:39;gurwls223;For clarification, this is _fixed_. We are just trying to clean up a bit which shouldn't be a blocker.;;;","12/Jun/18 09:48;apachespark;User 'e-dorigatti' has created a pull request for this issue:
https://github.com/apache/spark/pull/21538;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the directories of the “hive.downloaded.resources.dir” when HiveThriftServer2 stopped,SPARK-23745,13146489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,zuo.tingbing9,zuo.tingbing9,20/Mar/18 08:58,16/Mar/21 02:41,13/Jul/23 08:45,16/Mar/21 02:41,2.3.0,,,,,,,,,3.2.0,,,,,SQL,,,,0,,,,"!2018-03-20_164832.png!  

when start the HiveThriftServer2, we create some directories for hive.downloaded.resources.dir, but when stop the HiveThriftServer2 we do not remove these directories. The directories could accumulate a lot.",linux,apachespark,yumwang,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/18 09:07;zuo.tingbing9;2018-03-20_164832.png;https://issues.apache.org/jira/secure/attachment/12915285/2018-03-20_164832.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 02:41:39 UTC 2021,,,,,,,,,,"0|i3rjb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/18 10:01;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20864;;;","15/Mar/21 02:14;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31833;;;","15/Mar/21 02:15;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31833;;;","16/Mar/21 02:41;yumwang;Issue resolved by pull request 31833
https://github.com/apache/spark/pull/31833;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IsolatedClientLoader.isSharedClass returns an unindented result against `slf4j` keyword,SPARK-23743,13146426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jongyoul,jongyoul,jongyoul,20/Mar/18 01:39,12/Dec/22 18:10,13/Jul/23 08:45,30/Mar/18 06:09,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"{{isSharedClass}} returns if some classes can/should be shared or not. It checks if the classes names have some keywords or start with some names. Following the logic, it can occur unintended behaviors when a custom package has {{slf4j}} inside the package or class name. As I guess, the first intention seems to figure out the class containing {{org.slf4j}}. It would be better to change the comparison logic to {{name.startsWith(""org.slf4j"")}}",,apachespark,jerryshao,jongyoul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 30 06:09:18 UTC 2018,,,,,,,,,,"0|i3rix3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/18 01:40;jongyoul;Please assign this issue to me.;;;","20/Mar/18 01:52;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/20860;;;","20/Mar/18 03:34;gurwls223;(let's avoid to set a target version which is usually set by a committer);;;","20/Mar/18 03:35;jongyoul;Understood. Thanks for fixing it.;;;","30/Mar/18 06:09;jerryshao;Issue resolved by pull request 20860
[https://github.com/apache/spark/pull/20860];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidSchemaException While Saving ALSModel,SPARK-23734,13146096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,spoon,spoon,19/Mar/18 02:31,07/Dec/18 23:30,13/Jul/23 08:45,07/Dec/18 23:30,2.3.0,,,,,,,,,2.3.1,,,,,ML,,,,0,ALS,parquet,persistence,"After fitting an ALSModel, get following error while saving the model:

Caused by: org.apache.parquet.schema.InvalidSchemaException: A group type can not be empty. Parquet does not support empty group without leaves. Empty group: spark_schema

Exactly the same code ran ok on 2.2.1.

Same issue also occurs on other ALSModels we have.
h2. *To reproduce*

Get ALSExample: [https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala] and add the following line to save the model right before ""spark.stop"".
{quote}   model.write.overwrite().save(""SparkExampleALSModel"") 
{quote}
h2. Stack Trace
Exception in thread ""main"" java.lang.ExceptionInInitializerError
at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$setSchema$2.apply(ParquetWriteSupport.scala:444)
at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$setSchema$2.apply(ParquetWriteSupport.scala:444)
at scala.collection.immutable.List.foreach(List.scala:392)
at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:444)
at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.prepareWrite(ParquetFileFormat.scala:112)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:140)
at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)
at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)
at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)
at org.apache.spark.ml.recommendation.ALSModel$ALSModelWriter.saveImpl(ALS.scala:510)
at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)
at com.vitalmove.model.ALSExample$.main(ALSExample.scala:83)
at com.vitalmove.model.ALSExample.main(ALSExample.scala)
Caused by: org.apache.parquet.schema.InvalidSchemaException: A group type can not be empty. Parquet does not support empty group without leaves. Empty group: spark_schema
at org.apache.parquet.schema.GroupType.<init>(GroupType.java:92)
at org.apache.parquet.schema.GroupType.<init>(GroupType.java:48)
at org.apache.parquet.schema.MessageType.<init>(MessageType.java:50)
at org.apache.parquet.schema.Types$MessageTypeBuilder.named(Types.java:1256)
at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.<init>(ParquetSchemaConverter.scala:567)
at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.<clinit>(ParquetSchemaConverter.scala)
 ","macOS 10.13.2

Scala 2.11.8

Spark 2.3.0  v2.3.0-rc5 (Feb 22 2018)",mgaido,spoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 07 23:29:39 UTC 2018,,,,,,,,,,"0|i3rgvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/18 07:25;viirya;I use the latest master branch and can't reproduce the reported issue.;;;","11/Apr/18 16:56;spoon;[~viirya] Thank you for checking into this. I added the Spark release details where this is reproducible - v2.3.0-rc5 released on Feb 22, 2018.

And will verify that it is fixed in the next release.;;;","07/Dec/18 23:29;spoon;Just confirmed the problem is fixed in Spark 2.3.1. The test environment uses Scala 2.11.11. And there are no other dependency. I will close the case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link to scala source code in Spark Scala api Scaladoc,SPARK-23732,13146087,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,vanzin,yogeshtewari,yogeshtewari,19/Mar/18 00:11,12/Dec/22 18:11,13/Jul/23 08:45,12/Jun/18 01:32,2.3.0,2.3.1,,,,,,,,2.1.3,2.2.2,2.3.2,2.4.0,,Build,Documentation,Project Infra,,0,build,documentation,scaladocs,"Scala source code link in Spark api scaladoc is broken.

Turns out instead of the relative path to the scala files the ""€\{FILE_PATH}.scala"" expression in [https://github.com/apache/spark/blob/master/project/SparkBuild.scala] is generating the absolute path from the developers computer. In this case, if I try to access the source link on [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.Accumulable], it tries to take me to [https://github.com/apache/spark/tree/v2.3.0/Users/sameera/dev/spark/core/src/main/scala/org/apache/spark/Accumulable.scala]

where ""/Users/sameera/dev/spark"" portion of the URL is coming from the developers macos home folder.

There seems to be no change in the code responsible for generating this path during the build in /project/SparkBuild.scala :

Line # 252:
{code:java}
scalacOptions in Compile ++= Seq(
s""-target:jvm-${scalacJVMVersion.value}"",
""-sourcepath"", (baseDirectory in ThisBuild).value.getAbsolutePath // Required for relative source links in scaladoc
),
{code}
Line # 726
{code:java}
// Use GitHub repository for Scaladoc source links
unidocSourceBase := s""https://github.com/apache/spark/tree/v${version.value}"",

scalacOptions in (ScalaUnidoc, unidoc) ++= Seq(
""-groups"", // Group similar methods together based on the @group annotation.
""-skip-packages"", ""org.apache.hadoop""
) ++ (
// Add links to sources when generating Scaladoc for a non-snapshot release
if (!isSnapshot.value) {
Opts.doc.sourceUrl(unidocSourceBase.value + ""€{FILE_PATH}.scala"")
} else {
Seq()
}
){code}
 

It seems more like a developers dev environment issue.

I was successfully able to reproduce this in my dev environment. Environment details attached. 

 ","{code:java}
~/spark/docs$ cat /etc/*release*
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.4 LTS""
NAME=""Ubuntu""
VERSION=""16.04.4 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.4 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
{code}
Using spark packaged sbt.

Other versions:
{code:java}
~/spark/docs$ ruby -v 
ruby 2.3.1p112 (2016-04-26) [x86_64-linux-gnu] 
~/spark/docs$ gem -v 
2.5.2.1 
~/spark/docs$ jekyll -v 
jekyll 3.7.3  
~/spark/docs$ java -version 
java version ""1.8.0_112"" Java(TM) SE Runtime Environment (build 1.8.0_112-b15) Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)
{code}",apachespark,sameerag,Simon_poortman@icloud.com,yogeshtewari,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23737,SPARK-23733,SPARK-23946,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 01:32:53 UTC 2018,,,,,,,,,,"0|i3rgtz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/18 02:08;yogeshtewari;Adding sourcepath to the code seems to fix the issue.
{code:java}
scalacOptions in (ScalaUnidoc, unidoc) ++= Seq(
 ""-groups"", // Group similar methods together based on the @group annotation.
 ""-skip-packages"", ""org.apache.hadoop"",
 ""-sourcepath"", (baseDirectory in ThisBuild).value.getAbsolutePath // Required for relative source links in scaladoc
) ++ ({code}
 ;;;","10/Apr/18 01:18;gurwls223;Let's leave SPARK-23733 resolved as a duplicate. Seems the root cause the same.;;;","10/Apr/18 01:59;yogeshtewari;SPARK-23732 and SPARK-23733 are similar in nature. Will leave it to your better judgement. (y)

Cheers.;;;","09/Jun/18 18:01;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/21521;;;","12/Jun/18 01:32;gurwls223;Issue resolved by pull request 21521
[https://github.com/apache/spark/pull/21521];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSourceScanExec throws NullPointerException in subexpression elimination,SPARK-23731,13146060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,jlaskowski,jlaskowski,18/Mar/18 17:10,12/Dec/22 17:35,13/Jul/23 08:45,20/Jul/18 13:02,2.2.1,2.3.0,2.3.1,,,,,,,2.3.2,2.4.0,,,,SQL,,,,0,,,,"While working with a SQL with many {{CASE WHEN}} and {{ScalarSubqueries}} I faced the following exception (in Spark 2.3.0):
{code:java}
Caused by: java.lang.NullPointerException
  at org.apache.spark.sql.execution.FileSourceScanExec.<init>(DataSourceScanExec.scala:167)
  at org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:502)
  at org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:158)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:210)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:224)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:224)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:224)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:210)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:224)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:224)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:224)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:210)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:224)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:224)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:224)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:210)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.sameResult(QueryPlan.scala:257)
  at org.apache.spark.sql.execution.ScalarSubquery.semanticEquals(subquery.scala:58)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$Expr.equals(EquivalentExpressions.scala:36)
  at scala.collection.mutable.HashTable$class.elemEquals(HashTable.scala:358)
  at scala.collection.mutable.HashMap.elemEquals(HashMap.scala:40)
  at scala.collection.mutable.HashTable$class.scala$collection$mutable$HashTable$$findEntry0(HashTable.scala:136)
  at scala.collection.mutable.HashTable$class.findEntry(HashTable.scala:132)
  at scala.collection.mutable.HashMap.findEntry(HashMap.scala:40)
  at scala.collection.mutable.HashMap.get(HashMap.scala:70)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:54)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:95)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$$anonfun$addExprTree$1.apply(EquivalentExpressions.scala:96)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:96)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:1168)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:1168)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionElimination(CodeGenerator.scala:1168)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1218)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.createCode(GenerateUnsafeProjection.scala:310)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:373)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.generate(GenerateUnsafeProjection.scala:362)
  at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)
  at org.apache.spark.sql.execution.ProjectExec$$anonfun$9.apply(basicPhysicalOperators.scala:71)
  at org.apache.spark.sql.execution.ProjectExec$$anonfun$9.apply(basicPhysicalOperators.scala:70)
  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$apply$24.apply(RDD.scala:818)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:109)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748){code}
I've got the query and the parquet files that lead to the NPE, but cannot disclose it due to NDA. I've been trying to replicate it with different queries, but failed, and after over 3 days of exploring subexpression elimination I gave up.

The NPE technically is due to [@transient relation: HadoopFsRelation|https://github.com/apache/spark/blob/branch-2.3/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala?utf8=%E2%9C%93#L159] that is not re-created on executors, but don't know why this {{@transient}} is required in the first place and more importantly how to write a test to reproduce it.

FYI Disabling subexpression elimination with {{spark.sql.subexpressionElimination.enabled}} Spark internal configuration property helps.",,apachespark,cloud_fan,jlaskowski,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 13:02:20 UTC 2018,,,,,,,,,,"0|i3rgnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/18 17:17;apachespark;User 'jaceklaskowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/20855;;;","18/Mar/18 17:27;apachespark;User 'jaceklaskowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/20856;;;","19/Jul/18 07:52;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21815;;;","20/Jul/18 13:02;cloud_fan;Issue resolved by pull request 21815
[https://github.com/apache/spark/pull/21815];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Glob resolution breaks remote naming of files/archives,SPARK-23729,13146021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,misutoth,misutoth,misutoth,18/Mar/18 05:59,27/Apr/18 18:30,13/Jul/23 08:45,22/Mar/18 00:14,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Submit,,,,0,regression,,,"Given one uses {{spark-submit}} with either of the {{\-\-archives}} or the {{\-\-files}} parameters, in case the file name actually contains glob patterns, the rename part ({{...#nameAs}}) of the filename will eventually be ignored.

Thinking over the resolution cases, if the resolution results in multiple files, it does not make sense to send all of them under the same remote name. So this should then result in an error.",,apachespark,misutoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24113,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 18 06:15:04 UTC 2018,,,,,,,,,,"0|i3rgfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/18 06:00;misutoth;Already working on this. Will submit a PR shortly.;;;","18/Mar/18 06:15;apachespark;User 'misutoth' has created a pull request for this issue:
https://github.com/apache/spark/pull/20853;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML test with expected exceptions testing streaming fails on 2.3,SPARK-23728,13146019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,18/Mar/18 04:52,19/Mar/18 17:42,13/Jul/23 08:45,19/Mar/18 17:42,2.3.0,,,,,,,,,2.3.1,,,,,ML,Tests,,,0,,,,The testTransformerByInterceptingException fails to catch the expected message as on 2.3 during streaming if an exception happens within an ml feature then feature generated message is not at the direct caused by exception but even one level deeper. ,,apachespark,attilapiros,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22915,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 19 17:42:39 UTC 2018,,,,,,,,,,"0|i3rgev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/18 04:52;attilapiros;I am soon creating a PR.;;;","18/Mar/18 05:09;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/20852;;;","19/Mar/18 17:42;josephkb;Issue resolved by pull request 20852
[https://github.com/apache/spark/pull/20852];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark code contains numerous undefined names in Python 3,SPARK-23698,13145517,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cclauss,cclauss,cclauss,15/Mar/18 22:35,12/Dec/22 18:10,13/Jul/23 08:45,04/Jul/18 01:44,2.3.0,,,,,,,,,2.4.0,,,,,PySpark,,,,0,,,,"flake8 testing of https://github.com/apache/spark on Python 3.6.3

$ *flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics*

./dev/merge_spark_pr.py:98:14: F821 undefined name 'raw_input'
    result = raw_input(""\n%s (y/n): "" % prompt)
             ^
./dev/merge_spark_pr.py:136:22: F821 undefined name 'raw_input'
    primary_author = raw_input(
                     ^
./dev/merge_spark_pr.py:186:16: F821 undefined name 'raw_input'
    pick_ref = raw_input(""Enter a branch name [%s]: "" % default_branch)
               ^
./dev/merge_spark_pr.py:233:15: F821 undefined name 'raw_input'
    jira_id = raw_input(""Enter a JIRA id [%s]: "" % default_jira_id)
              ^
./dev/merge_spark_pr.py:278:20: F821 undefined name 'raw_input'
    fix_versions = raw_input(""Enter comma-separated fix version(s) [%s]: "" % default_fix_versions)
                   ^
./dev/merge_spark_pr.py:317:28: F821 undefined name 'raw_input'
            raw_assignee = raw_input(
                           ^
./dev/merge_spark_pr.py:430:14: F821 undefined name 'raw_input'
    pr_num = raw_input(""Which pull request would you like to merge? (e.g. 34): "")
             ^
./dev/merge_spark_pr.py:442:18: F821 undefined name 'raw_input'
        result = raw_input(""Would you like to use the modified title? (y/n): "")
                 ^
./dev/merge_spark_pr.py:493:11: F821 undefined name 'raw_input'
    while raw_input(""\n%s (y/n): "" % pick_prompt).lower() == ""y"":
          ^
./dev/create-release/releaseutils.py:58:16: F821 undefined name 'raw_input'
    response = raw_input(""%s [y/n]: "" % msg)
               ^
./dev/create-release/releaseutils.py:152:38: F821 undefined name 'unicode'
        author = unidecode.unidecode(unicode(author, ""UTF-8"")).strip()
                                     ^
./python/setup.py:37:11: F821 undefined name '__version__'
VERSION = __version__
          ^
./python/pyspark/cloudpickle.py:275:18: F821 undefined name 'buffer'
        dispatch[buffer] = save_buffer
                 ^
./python/pyspark/cloudpickle.py:807:18: F821 undefined name 'file'
        dispatch[file] = save_file
                 ^
./python/pyspark/sql/conf.py:61:61: F821 undefined name 'unicode'
        if not isinstance(obj, str) and not isinstance(obj, unicode):
                                                            ^
./python/pyspark/sql/streaming.py:25:21: F821 undefined name 'long'
    intlike = (int, long)
                    ^
./python/pyspark/streaming/dstream.py:405:35: F821 undefined name 'long'
        return self._sc._jvm.Time(long(timestamp * 1000))
                                  ^
./sql/hive/src/test/resources/data/scripts/dumpdata_script.py:21:10: F821 undefined name 'xrange'
for i in xrange(50):
         ^
./sql/hive/src/test/resources/data/scripts/dumpdata_script.py:22:14: F821 undefined name 'xrange'
    for j in xrange(5):
             ^
./sql/hive/src/test/resources/data/scripts/dumpdata_script.py:23:18: F821 undefined name 'xrange'
        for k in xrange(20022):
                 ^
20    F821 undefined name 'raw_input'
20",,apachespark,bryanc,cclauss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 26 09:42:04 UTC 2018,,,,,,,,,,"0|i3rdb3:",9223372036854775807,,,,,holden.karau@gmail.com,,,,,,,,2.4.0,,,,,,,,,,"15/Mar/18 23:18;apachespark;User 'cclauss' has created a pull request for this issue:
https://github.com/apache/spark/pull/20838;;;","03/Jul/18 07:19;apachespark;User 'cclauss' has created a pull request for this issue:
https://github.com/apache/spark/pull/21702;;;","04/Jul/18 01:44;gurwls223;Issue resolved by pull request 21702
[https://github.com/apache/spark/pull/21702];;;","02/Aug/18 05:14;apachespark;User 'cclauss' has created a pull request for this issue:
https://github.com/apache/spark/pull/21959;;;","02/Aug/18 05:30;apachespark;User 'cclauss' has created a pull request for this issue:
https://github.com/apache/spark/pull/21960;;;","22/Aug/18 17:20;bryanc;Followup resolved by pull request 20838
https://github.com/apache/spark/pull/20838
;;;","24/Aug/18 09:38;apachespark;User 'cclauss' has created a pull request for this issue:
https://github.com/apache/spark/pull/22214;;;","26/Aug/18 09:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/22235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accumulators of Spark 1.x no longer work with Spark 2.x,SPARK-23697,13145496,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,szhemzhitsky,szhemzhitsky,15/Mar/18 21:53,04/May/18 11:23,13/Jul/23 08:45,04/May/18 11:23,2.2.0,2.2.1,2.3.0,,,,,,,2.0.3,2.1.3,2.2.2,2.3.1,2.4.0,Spark Core,,,,2,,,,"I've noticed that accumulators of Spark 1.x no longer work with Spark 2.x failing with
{code:java}
java.lang.AssertionError: assertion failed: copyAndReset must return a zero value copy{code}

 It happens while serializing an accumulator [here|https://github.com/apache/spark/blob/4f5bad615b47d743b8932aea1071652293981604/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L165]
{code:java}
val copyAcc = copyAndReset()
assert(copyAcc.isZero, ""copyAndReset must return a zero value copy""){code}
... although copyAndReset returns zero-value copy for sure, just consider the accumulator below
{code:java}
val concatParam = new AccumulatorParam[jl.StringBuilder] {
  override def zero(initialValue: jl.StringBuilder): jl.StringBuilder = new jl.StringBuilder()
  override def addInPlace(r1: jl.StringBuilder, r2: jl.StringBuilder): jl.StringBuilder = r1.append(r2)
}{code}
So, Spark treats zero value as non-zero due to how [isZero|https://github.com/apache/spark/blob/4f5bad615b47d743b8932aea1071652293981604/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L489] is implemented in LegacyAccumulatorWrapper.
{code:java}
override def isZero: Boolean = _value == param.zero(initialValue){code}
All this means that the values to be accumulated must implement equals and hashCode, otherwise isZero is very likely to always return false.

So I'm wondering whether the assertion 
{code:java}
assert(copyAcc.isZero, ""copyAndReset must return a zero value copy""){code}
is really necessary and whether it can be safely removed from there?

If not - is it ok to just override writeReplace for LegacyAccumulatorWrapper to prevent such failures?","Spark 2.2.0
Scala 2.11",apachespark,cloud_fan,szhemzhitsky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 04 11:23:35 UTC 2018,,,,,,,,,,"0|i3rd6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 13:49;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21229;;;","04/May/18 11:23;cloud_fan;Issue resolved by pull request 21229
[https://github.com/apache/spark/pull/21229];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"entrypoint.sh does not accept arbitrary UIDs, returning as an error",SPARK-23680,13145047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rmartine,rmartine,rmartine,14/Mar/18 14:41,17/May/20 18:23,13/Jul/23 08:45,16/Mar/18 17:44,2.3.0,,,,,,,,,2.4.0,,,,,Kubernetes,Spark Core,,,1,easyfix,,,"Openshift supports running pods using arbitrary UIDs ([https://docs.openshift.com/container-platform/3.7/creating_images/guidelines.html#openshift-specific-guidelines)]  to improve security. Although entrypoint.sh was developed to cover this feature, the script is returning an error[1].

The issue is that the script uses getent to find the passwd entry of the current UID, and if the entry is not found it creates an entry in /etc/passwd. According to the getent man page:
{code:java}
EXIT STATUS
       One of the following exit values can be returned by getent:
          0         Command completed successfully.
          1         Missing arguments, or database unknown.
          2         One or more supplied key could not be found in the database.
          3         Enumeration not supported on this database.
{code}
And since the script begin with a ""set -ex"" command, which means it turns on debug and breaks the script if the command pipelines returns an exit code other than 0.--

Having that said, this line below must be changed to remove the ""-e"" flag from set command:

https://github.com/apache/spark/blob/v2.3.0/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L20

 

 [1]https://github.com/apache/spark/blob/v2.3.0/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L25-L34",OpenShift,apachespark,eje,felixcheung,foxish,rmartine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Apr 04 08:50:36 UTC 2018,,,,,,,,,,"0|i3raev:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"14/Mar/18 14:53;eje;[~rmartine] thanks for catching this! It will impact platforms running w/ anonymous uid such as OpenShift.;;;","14/Mar/18 15:43;apachespark;User 'rimolive' has created a pull request for this issue:
https://github.com/apache/spark/pull/20822;;;","16/Mar/18 17:44;eje;merged to master;;;","16/Mar/18 17:56;eje;commit workflow indicates to set the Assignee, however I cannot edit that field;;;","02/Apr/18 19:04;foxish;[~felixcheung] helped me get the right permissions in JIRA to edit that field.;;;","04/Apr/18 08:49;felixcheung;try now. i also had to add rmartine - seemed like this was his first contribution, congrats!;;;","04/Apr/18 08:50;felixcheung;also, please check fixed version and target version when resolving issue. in this case this is merged into master only, so only going to 2.4 and not 2.3.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
uiWebUrl show inproper URL when running on YARN,SPARK-23679,13145038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,maver1ck,maver1ck,14/Mar/18 14:07,17/May/20 18:14,13/Jul/23 08:45,28/Aug/18 17:34,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,Web UI,YARN,,0,,,,"uiWebUrl returns local url.
Using it will cause HTTP ERROR 500
{code}
Problem accessing /. Reason:

    Server Error
Caused by:
javax.servlet.ServletException: Could not determine the proxy server for redirection
	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:205)
	at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:145)
	at org.spark_project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:461)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:524)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:319)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:253)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:95)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:745)
{code}

We should give address to yarn proxy instead.",,apachespark,jerryshao,maver1ck,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 28 17:34:13 UTC 2018,,,,,,,,,,"0|i3racv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/18 06:29;jerryshao;Let me fix this issue. This is mainly a RM HA introduced issue, Spark on Yarn's current AmIpFilter cannot work well with RM HA scenario.;;;","21/Aug/18 04:03;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/22164;;;","28/Aug/18 17:34;vanzin;Issue resolved by pull request 22164
[https://github.com/apache/spark/pull/22164];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHS is ignoring number of replay threads,SPARK-23671,13144803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,vanzin,vanzin,13/Mar/18 18:03,16/Mar/18 00:12,13/Jul/23 08:45,16/Mar/18 00:12,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,,,,I mistakenly flipped a condition in a previous change and the SHS is now basically doing single-threaded parsing of event logs.,,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 16 00:12:52 UTC 2018,,,,,,,,,,"0|i3r8wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/18 18:07;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20814;;;","16/Mar/18 00:12;vanzin;Issue resolved by pull request 20814
[https://github.com/apache/spark/pull/20814];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak of SparkPlanGraphWrapper in sparkUI,SPARK-23670,13144753,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,myroslav.lisniak,myroslav.lisniak,myroslav.lisniak,13/Mar/18 14:46,08/Sep/18 13:16,13/Jul/23 08:45,16/Mar/18 00:21,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"Memory leak on driver for a long time running application. We have application using structured streaming and running 48 hours. But driver fails with out of memory after 25 hours. After investigating heap dump we found that most of the memory was occupied with a lot of *SparkPlanGraphWrapper* objects inside *InMemoryStore*.

 Application was run with option: 
--driver-memory 4G",,apachespark,myroslav.lisniak,spektom,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/18 20:20;spektom;Screen Shot 2018-09-06 at 23.19.56.png;https://issues.apache.org/jira/secure/attachment/12938704/Screen+Shot+2018-09-06+at+23.19.56.png","13/Mar/18 14:50;myroslav.lisniak;heap.png;https://issues.apache.org/jira/secure/attachment/12914286/heap.png","06/Sep/18 11:06;spektom;heapdump_OOM.png;https://issues.apache.org/jira/secure/attachment/12938638/heapdump_OOM.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 08 13:16:01 UTC 2018,,,,,,,,,,"0|i3r8lr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/18 15:26;apachespark;User 'myroslavlisniak' has created a pull request for this issue:
https://github.com/apache/spark/pull/20813;;;","16/Mar/18 00:21;vanzin;Issue resolved by pull request 20813
[https://github.com/apache/spark/pull/20813];;;","06/Sep/18 11:10;spektom;The bug still happens in Apache Spark 2.3.1:

!heapdump_OOM.png|width=1595,height=966!

 

 

These char[] objects occupy 88% of driver memory.

 ;;;","06/Sep/18 17:39;vanzin;How many instances of the following classes do you have in your dump:

- {{SQLExecutionUIData}}
- {{SparkPlanGraphWrapper}} 
- {{SparkPlanGraphNodeWrapper}}

With the fix I'd expect the first two to be capped at 1000 (unless you changed the config), but the latter may still be high if you have complicated queries. That could explain high memory usage.
;;;","06/Sep/18 19:28;spektom;Not many: 7, 7 and 49.;;;","06/Sep/18 20:09;vanzin;Do you mind listing the {{SparkPlanGraphNodeWrapper}} and {{SparkPlanGraphWrapper}} instances in your heap dump and checking their retained size?

From the (truncated) screenshot you posted it just looks like the plan descriptions are huge. This seems like a separate issue; could be a feature request to truncate large plans in the UI.;;;","06/Sep/18 20:21;spektom;!Screen Shot 2018-09-06 at 23.19.56.png|width=756,height=340!;;;","06/Sep/18 20:24;vanzin;Yep, looks like just huge plans. Do you mind opening a separate bug? That's not really a memory leak, but probably good to have some control over how much data is kept here if the plans can get that large.;;;","08/Sep/18 13:16;spektom;Reported: SPARK-25380;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Undeterministic column name with UDFs,SPARK-23666,13144685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,darabos,darabos,13/Mar/18 11:11,21/Mar/18 06:20,13/Jul/23 08:45,21/Mar/18 06:20,2.2.0,2.3.0,,,,,,,,2.4.0,,,,,SQL,,,,1,,,,"When you access structure fields in Spark SQL, the auto-generated result column name includes an internal ID.
{code:java}
scala> import spark.implicits._
scala> Seq(((1, 2), 3)).toDF(""a"", ""b"").createOrReplaceTempView(""x"")
scala> spark.udf.register(""f"", (a: Int) => a)
scala> spark.sql(""select f(a._1) from x"").show
+---------------------+
|UDF:f(a._1 AS _1#148)|
+---------------------+
|                    1|
+---------------------+
{code}
This ID ({{#148}}) is only included for UDFs.
{code:java}
scala> spark.sql(""select factorial(a._1) from x"").show
+-----------------------+
|factorial(a._1 AS `_1`)|
+-----------------------+
|                      1|
+-----------------------+
{code}
The internal ID is different on every invocation. The problem this causes for us is that the schema of the SQL output is never the same:
{code:java}
scala> spark.sql(""select f(a._1) from x"").schema ==
       spark.sql(""select f(a._1) from x"").schema
Boolean = false
{code}
We rely on similar schema checks when reloading persisted data.",,apachespark,darabos,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 15 00:34:04 UTC 2018,,,,,,,,,,"0|i3r86n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/18 11:13;darabos;I've looked at the code and both {{ScalaUDF.scala}} and {{mathExpressions.scala}} just call {{toString}} on an {{Expression}} child. I don't see why the ID is added in one case and not the other...;;;","14/Mar/18 23:58;maropu;This is just a bug, so I'll make a pr later.;;;","15/Mar/18 00:34;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20827;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn throws exception in cluster mode when the application is small,SPARK-23660,13144584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,13/Mar/18 01:59,17/May/20 18:14,13/Jul/23 08:45,20/Mar/18 01:02,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,YARN,,,0,,,,"Yarn throws the following exception in cluster mode when the application is really small:
{code:java}
18/03/07 23:34:22 WARN netty.NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7c974942 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1eea9d2d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
18/03/07 23:34:22 ERROR yarn.ApplicationMaster: Uncaught exception: 
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.deploy.yarn.YarnAllocator.<init>(YarnAllocator.scala:102)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:77)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:450)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:493)
	at org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$runImpl(ApplicationMaster.scala:345)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$2.apply$mcV$sp(ApplicationMaster.scala:260)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$2.apply(ApplicationMaster.scala:260)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$2.apply(ApplicationMaster.scala:260)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$5.run(ApplicationMaster.scala:810)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.spark.deploy.yarn.ApplicationMaster.doAsUser(ApplicationMaster.scala:809)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:259)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:834)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:158)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:135)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:229)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:523)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:91)
	... 17 more
18/03/07 23:34:22 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 13, (reason: Uncaught exception: org.apache.spark.SparkException: Exception thrown in awaitResult: )
{code}
Example application:
{code:java}
object ExampleApp {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(""ExampleApp"")
    val sc = new SparkContext(conf)
    try {
      // Do nothing
    } finally {
      sc.stop()
    }
  }
{code}",,apachespark,gsomogyi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 20 01:02:39 UTC 2018,,,,,,,,,,"0|i3r7k7:",9223372036854775807,,,,,,,,,,,,,2.3.1,,,,,,,,,,"13/Mar/18 02:00;gsomogyi;I'm working on it.;;;","13/Mar/18 04:46;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20807;;;","20/Mar/18 01:02;vanzin;Issue resolved by pull request 20807
[https://github.com/apache/spark/pull/20807];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InProcessAppHandle uses the wrong class in getLogger,SPARK-23658,13144542,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stakiar,stakiar,stakiar,12/Mar/18 22:19,16/Mar/18 00:05,13/Jul/23 08:45,16/Mar/18 00:05,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Submit,,,,0,,,,"{{InProcessAppHandle}} uses {{ChildProcAppHandle}} as the class in {{getLogger}}, it should just use its own name.",,apachespark,stakiar,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-18533,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 16 00:05:08 UTC 2018,,,,,,,,,,"0|i3r7av:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/18 22:21;vanzin;I already fixed {{AbstractAppHandle}} but missed {{InProcessAppHandle}}.;;;","13/Mar/18 18:27;apachespark;User 'sahilTakiar' has created a pull request for this issue:
https://github.com/apache/spark/pull/20815;;;","16/Mar/18 00:05;vanzin;Issue resolved by pull request 20815
[https://github.com/apache/spark/pull/20815];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV schema inferring fails on some UTF-8 chars,SPARK-23649,13144174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,maxgekk,maxgekk,11/Mar/18 17:24,31/May/18 03:42,13/Jul/23 08:45,31/May/18 03:42,2.3.0,,,,,,,,,2.2.2,2.3.1,2.4.0,,,SQL,,,,0,,,,"Schema inferring of CSV files fails if the file contains a char starts from *0xFF.* 
{code:java}
spark.read.option(""header"", ""true"").csv(""utf8xFF.csv"")
{code}
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 63
  at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
  at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
{code}
Here is content of the file:
{code:java}
hexdump -C ~/tmp/utf8xFF.csv
00000000  63 68 61 6e 6e 65 6c 2c  63 6f 64 65 0d 0a 55 6e  |channel,code..Un|
00000010  69 74 65 64 2c 31 32 33  0d 0a 41 42 47 55 4e ff  |ited,123..ABGUN.|
00000020  2c 34 35 36 0d                                    |,456.|
00000025
{code}
Schema inferring doesn't fail in multiline mode:
{code}
spark.read.option(""header"", ""true"").option(""multiline"", ""true"").csv(""utf8xFF.csv"")
{code}
{code:java}
+-------+-----+
|channel|code
+-------+-----+
| United| 123
| ABGUN�| 456
+-------+-----+
{code}
and Spark is able to read the csv file if the schema is specified:
{code}
import org.apache.spark.sql.types._
val schema = new StructType().add(""channel"", StringType).add(""code"", StringType)
spark.read.option(""header"", ""true"").schema(schema).csv(""utf8xFF.csv"").show
{code}
{code:java}
+-------+----+
|channel|code|
+-------+----+
| United| 123|
| ABGUN�| 456|
+-------+----+
{code}",,apachespark,maropu,maxgekk,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/18 17:25;maxgekk;utf8xFF.csv;https://issues.apache.org/jira/secure/attachment/12913943/utf8xFF.csv",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 30 23:01:31 UTC 2018,,,,,,,,,,"0|i3r5gn:",9223372036854775807,,,,,hvanhovell,,,,,,,,,,,,,,,,,,"11/Mar/18 17:33;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20796;;;","30/May/18 23:01;zsxwing;[~cloud_fan] looks like this is fixed?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XORShiftRandom.hashSeed allocates unnecessary memory,SPARK-23643,13144051,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,10/Mar/18 15:03,08/Jun/20 17:36,13/Jul/23 08:45,23/Mar/19 16:26,2.3.0,,,,,,,,,3.0.0,,,,,Spark Core,,,,0,,,,The hashSeed method allocates 64 bytes buffer and puts only 8 bytes of the seed parameter into it. Other bytes are always zero and could be easily excluded from hash calculation.,,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 01 01:52:23 UTC 2019,,,,,,,,,,"0|i3r4pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/18 15:11;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20793;;;","23/Mar/19 16:26;srowen;Issue resolved by pull request 20793
[https://github.com/apache/spark/pull/20793];;;","01/Nov/19 01:50;dongjoon;I added `release-note` label because this changes the seed and expected result.
cc [~jiangxb1987] and [~smilegator];;;","01/Nov/19 01:52;dongjoon;This causes the UI test result difference between Apache Spark 3.0 and 2.4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop config may override spark config,SPARK-23640,13143866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,09/Mar/18 11:02,30/Mar/18 21:09,13/Jul/23 08:45,30/Mar/18 21:09,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"More details:
https://github.com/apache/spark/pull/20785#issue-173982312",,apachespark,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 30 21:09:39 UTC 2018,,,,,,,,,,"0|i3r3kf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/18 11:30;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/20785;;;","30/Mar/18 21:09;vanzin;Issue resolved by pull request 20785
[https://github.com/apache/spark/pull/20785];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL CLI fails talk to Kerberized metastore when use proxy user,SPARK-23639,13143858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,09/Mar/18 10:23,20/Sep/18 17:39,13/Jul/23 08:45,29/Mar/18 17:47,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"In SparkSQLCLI, SessionState generates before SparkContext instantiating. When we use --proxy-user to impersonate,  it's unable to initializing a metastore client to talk to the secured metastore for no kerberos ticket.",,apachespark,Qin Yao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24349,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 17:47:05 UTC 2018,,,,,,,,,,"0|i3r3in:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/18 10:29;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/20784;;;","29/Mar/18 17:47;vanzin;Issue resolved by pull request 20784
[https://github.com/apache/spark/pull/20784];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn might allocate more resource if a same executor is killed multiple times.,SPARK-23637,13143806,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,09/Mar/18 05:48,17/May/20 18:13,13/Jul/23 08:45,04/Apr/18 22:52,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,YARN,,,0,,,,"*{{YarnAllocator}}* uses *{{numExecutorsRunning}}* to track the number of running executor. *{{numExecutorsRunning}}* is used to check if there're executors missing and need to allocate more.

 In current code, *{{numExecutorsRunning}}* can be negative when driver asks to kill a same idle executor multiple times.",,apachespark,jinxing6042@126.com,junping_du,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 04 22:52:01 UTC 2018,,,,,,,,,,"0|i3r373:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/18 05:55;jinxing6042@126.com;PR here: https://github.com/apache/spark/pull/20781;;;","09/Mar/18 05:56;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/20781;;;","04/Apr/18 22:52;vanzin;Issue resolved by pull request 20781
[https://github.com/apache/spark/pull/20781];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[SPARK 2.2] | Kafka Consumer | KafkaUtils.createRDD throws Exception - java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access",SPARK-23636,13143777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mcdeepak,mcdeepak,09/Mar/18 03:12,07/Jan/19 12:43,13/Jul/23 08:45,07/Jan/19 12:43,2.1.1,2.2.0,,,,,,,,2.4.0,,,,,DStreams,,,,1,performance,,,"h2.  
h2. Summary

 

While using the KafkaUtils.createRDD API - we receive below listed error, specifically when 1 executor connects to 1 kafka topic-partition, but with more than 1 core & fetches an Array(OffsetRanges)

 

_I've tagged this issue to ""Structured Streaming"" - as I could not find a more appropriate component_ 

 
----
h2. Error Faced
{noformat}
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access{noformat}
 Stack Trace
{noformat}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 1.0 failed 4 times, most recent failure: Lost task 5.3 in stage 1.0 (TID 17, host, executor 16): java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1629)
at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1528)
at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1508)
at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.close(CachedKafkaConsumer.scala:59)
at org.apache.spark.streaming.kafka010.CachedKafkaConsumer$.remove(CachedKafkaConsumer.scala:185)
at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.<init>(KafkaRDD.scala:204)
at org.apache.spark.streaming.kafka010.KafkaRDD.compute(KafkaRDD.scala:181)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323){noformat}
 
----
h2. Config Used to simulate the error

A session with : 
 * Executors - 1
 * Cores - 2 or More
 * Kafka Topic - has only 1 partition
 * While fetching - More than one Array of Offset Range , Example 

{noformat}
Array(OffsetRange(""kafka_topic"",0,608954201,608954202),
OffsetRange(""kafka_topic"",0,608954202,608954203)
){noformat}
 
----
h2. Was this approach working before?

 

This was working in spark 1.6.2

However, from spark 2.1 onwards - the approach throws exception

 
----
h2. Why are we fetching from kafka as mentioned above.

 

This gives us the capability to establish a connection to Kafka Broker for every spark executor's core, thus each core can fetch/process its own set of messages based on the specified (offset ranges).

 

 
----
h2. Sample Code

 
{quote}scala snippet - on versions spark 2.2.0 or 2.1.0

// Bunch of imports

import kafka.serializer.\{DefaultDecoder, StringDecoder}
 import org.apache.avro.generic.GenericRecord
 import org.apache.kafka.clients.consumer.ConsumerRecord
 import org.apache.kafka.common.serialization._
 import org.apache.spark.rdd.RDD
 import org.apache.spark.sql.\{DataFrame, Row, SQLContext}
 import org.apache.spark.sql.Row
 import org.apache.spark.sql.hive.HiveContext
 import org.apache.spark.sql.types.\{StringType, StructField, StructType}
 import org.apache.spark.streaming.kafka010._
 import org.apache.spark.streaming.kafka010.KafkaUtils._
{quote}
{quote}// This forces two connections - from a single executor - to topic-partition <kafka_topic-0>.

// And with 2 cores assigned to 1 executor : each core has a task - pulling respective offsets : OffsetRange(""kafka_topic"",0,1,2) & OffsetRange(""kafka_topic"",0,2,3)

val parallelizedRanges = Array(OffsetRange(""kafka_topic"",0,1,2), // Fetching sample 2 records 
 OffsetRange(""kafka_topic"",0,2,3) // Fetching sample 2 records 
 )

 

// Initiate kafka properties

val kafkaParams1: java.util.Map[String, Object] = new java.util.HashMap()

// kafkaParams1.put(""key"",""val"") add all the parameters such as broker, topic.... Not listing every property here.

 

// Create RDD

val rDDConsumerRec: RDD[ConsumerRecord[String, String]] =
 createRDD[String, String](sparkContext
 , kafkaParams1, parallelizedRanges, LocationStrategies.PreferConsistent)

 

// Map Function

val data: RDD[Row] = rDDConsumerRec.map \{ x => Row(x.topic().toString, x.partition().toString, x.offset().toString, x.timestamp().toString, x.value() ) }

 

// Create a DataFrame

val df = sqlContext.createDataFrame(data, StructType(
 Seq(
 StructField(""topic"", StringType),
 StructField(""partition"", StringType),
 StructField(""offset"", StringType),
 StructField(""timestamp"", StringType),
 StructField(""value"", BinaryType)
 )))

 

df.show() //  You will see the error reported.
{quote}
 
----
 
h2. Similar Issue reported earlier, but on a different API

 

A similar issue reported for DirectStream is 

https://issues.apache.org/jira/browse/SPARK-19185

 

 

 
----
h2. What is the impact - if a fix is not available for this problem?

 

 

We have a lot of Spark Applications that are running in production, making parallel connections to the 1 topic-partition from each spark-executor: so parallelism is directly proportional to the num-cores in each executor.

With spark 2.1 onwards : we are not allowed to make concurrent connections from 1 executor to 1 topic-partition. Only workaround is to start our applications with executor-cores = 1, with dynamic resource allocation enabled.

With above configuration - for every offset range we ask kafka - a new executor is spawned to run the fetch task.

Downside of Workaround -

Above approach is not allowing us to leverage more than 1 spark-core per spark-executor.

And asking for an executor - for each offset range - is costly : in terms of scheduling and allocation.

 

 

 ",,gsomogyi,mcdeepak,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 12:43:10 UTC 2019,,,,,,,,,,"0|i3r30n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/18 05:26;yuzhihong@gmail.com;It seems in KafkaDataConsumer#close :
{code}
 def close(): Unit = consumer.close()
{code}
The code should catch ConcurrentModificationException and try closing the consumer again.;;;","12/Dec/18 08:49;gsomogyi;[~mcdeepak] It should be resolved in SPARK-19185 with 2.4.0 (even if it's different API). Can you re-test it please?;;;","07/Jan/19 12:43;gsomogyi;Please reopen if problem re-appears.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark executor env variable is overwritten by same name AM env variable,SPARK-23635,13143762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,09/Mar/18 02:21,17/May/20 18:14,13/Jul/23 08:45,16/Mar/18 08:23,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,YARN,,,0,,,,"In the current Spark on YARN code, AM always will copy and overwrite its env variables to executors, so we cannot set different values for executors.

To reproduce issue, user could start spark-shell like:
{code:java}
./bin/spark-shell --master yarn-client --conf spark.executorEnv.SPARK_ABC=executor_val --conf  spark.yarn.appMasterEnv.SPARK_ABC=am_val

{code}
Then check executor env variables by
{code:java}
sc.parallelize(1 to 1).flatMap \{ i => sys.env.toSeq }.collect.foreach(println)

{code}
You will always get {{am_val}} instead of {{executor_val}}. So we should not let AM to overwrite specifically set executor env variables.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 16 08:23:06 UTC 2018,,,,,,,,,,"0|i3r2xb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/18 03:34;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/20799;;;","16/Mar/18 08:23;jerryshao;Issue resolved by pull request 20799
[https://github.com/apache/spark/pull/20799];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-on-YARN missing user customizations of hadoop config,SPARK-23630,13143652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,08/Mar/18 19:16,17/May/20 18:13,13/Jul/23 08:45,09/Mar/18 18:37,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,YARN,,,0,,,,"In my change to fix SPARK-22372, I removed some code that allowed user customizations of hadoop configs to take effect. Notably, these lines in YarnSparkHadoopUtil:

{noformat}
 override def newConfiguration(conf: SparkConf): Configuration = {
   val hadoopConf = new YarnConfiguration(super.newConfiguration(conf))
   hadoopConf.addResource(Client.SPARK_HADOOP_CONF_FILE)
   hadoopConf
 }
{noformat}

The {{addResource}} call is now missing so user configs are ignored (only the NM config is used).",,apachespark,gsomogyi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 09 18:37:13 UTC 2018,,,,,,,,,,"0|i3r28v:",9223372036854775807,,,,,,,,,,,,,2.3.1,,,,,,,,,,"08/Mar/18 21:34;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20776;;;","09/Mar/18 18:37;vanzin;Issue resolved by pull request 20776
[https://github.com/apache/spark/pull/20776];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 DAGScheduler blocked due to JobSubmitted event,SPARK-23626,13143452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,ajithshetty,ajithshetty,08/Mar/18 06:28,14/Oct/21 21:44,13/Jul/23 08:45,14/Oct/21 21:44,2.2.1,2.3.3,2.4.3,3.0.0,,,,,,3.0.4,3.1.3,3.2.1,3.3.0,,Scheduler,Spark Core,,,0,,,,"DAGScheduler becomes a bottleneck in cluster when multiple JobSubmitted events has to be processed as DAGSchedulerEventProcessLoop is single threaded and it will block other tasks in queue like TaskCompletion.

The JobSubmitted event is time consuming depending on the nature of the job (Example: calculating parent stage dependencies, shuffle dependencies, partitions) and thus it blocks all the events to be processed.

 

I see multiple JIRA referring to this behavior

https://issues.apache.org/jira/browse/SPARK-2647

https://issues.apache.org/jira/browse/SPARK-4961

 

Similarly in my cluster some jobs partition calculation is time consuming (Similar to stack at SPARK-2647) hence it slows down the spark DAGSchedulerEventProcessLoop which results in user jobs to slowdown, even if its tasks are finished within seconds, as TaskCompletion Events are processed at a slower rate due to blockage.",,ajithshetty,apachespark,irashid,joshrosen,pgaref,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2647,SPARK-4961,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 14 21:44:30 UTC 2021,,,,,,,,,,"0|i3r10f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/18 06:45;apachespark;User 'AjithShetty2489' has created a pull request for this issue:
https://github.com/apache/spark/pull/20770;;;","08/Mar/18 06:48;ajithshetty;Cc [~rxin@databricks.com] , [~shivaram] [~blue_impala_48d6];;;","21/May/19 07:12;ajithshetty;Resolution in progress;;;","16/Jan/20 09:39;ajithshetty;Old PR was closed due to inactivity. Reopening with a new PR and hence to conclude;;;","12/Oct/21 23:37;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/34265;;;","14/Oct/21 21:44;joshrosen;Fixed in https://github.com/apache/spark/pull/34265;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid concurrent use of cached KafkaConsumer in CachedKafkaConsumer (kafka-0-10-sql),SPARK-23623,13143419,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,08/Mar/18 02:23,24/Aug/18 22:35,13/Jul/23 08:45,16/Mar/18 18:11,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Structured Streaming,,,,0,,,,"CacheKafkaConsumer in the project `kafka-0-10-sql` is designed to maintain a pool of KafkaConsumers that can be reused. However, it was built with the assumption there will be only one task using trying to read the same Kafka TopicPartition at the same time. Hence, the cache was keyed by the TopicPartition a consumer is supposed to read. And any cases where this assumption may not be true, we have SparkPlan flag to disable the use of a cache. So it was up to the planner to correctly identify when it was not safe to use the cache and set the flag accordingly. 

Fundamentally, this is the wrong way to approach the problem. It is HARD for a high-level planner to reason about the low-level execution model, whether there will be multiple tasks in the same query trying to read the same partition. Case in point, 2.3.0 introduced stream-stream joins, and you can build a streaming self-join query on Kafka. It's pretty non-trivial to figure out how this leads to two tasks reading the same partition twice, possibly concurrently. And due to the non-triviality, it is hard to figure this out in the planner and set the flag to avoid the cache / consumer pool. And this can inadvertently lead to {{ConcurrentModificationException}} ,or worse, silent reading of incorrect data.

Here is a better way to design this. The planner shouldnt have to understand these low-level optimizations. Rather the consumer pool should be smart enough avoid concurrent use of a cached consumer. Currently, it tries to do so but incorrectly (the flag {{inuse}} is not checked when returning a cached consumer, see [this|[https://github.com/apache/spark/blob/master/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/CachedKafkaConsumer.scala#L403]).] If there is another request for the same partition as a currently in-use consumer, the pool should automatically return a fresh consumer that should be closed when the task is done.

 

 

 

 

 

 

 ",,apachespark,gsomogyi,tdas,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20919,,,,,,SPARK-24987,,,,SPARK-23714,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 16 23:11:05 UTC 2018,,,,,,,,,,"0|i3r0t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/18 02:41;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20767;;;","16/Mar/18 23:11;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20848;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split thread dump lines by using the br tag,SPARK-23620,13143356,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,07/Mar/18 20:56,08/Mar/18 09:52,13/Jul/23 08:45,08/Mar/18 09:52,2.3.0,,,,,,,,,2.4.0,,,,,Web UI,,,,0,,,,The '\n' line separator should be replaced by the <br> tag in the generated html of thread dump UI to guarantee that each class name is on separate line. There are some cases when the html are proxied and the '\n' could be replaced by another whitespaces (see the screenshot - https://drive.google.com/file/d/18t6yf-jnr072b-hPq4LhMZeRrw1PjpT5/view?usp=sharing).,,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 07 21:09:05 UTC 2018,,,,,,,,,,"0|i3r0f3:",9223372036854775807,,,,,zsxwing,,,,,,,,,,,,,,,,,,"07/Mar/18 21:09;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20762;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker-image-tool.sh Fails While Building Image,SPARK-23618,13143104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ninadingole,ninadingole,07/Mar/18 06:03,17/May/20 18:24,13/Jul/23 08:45,12/Mar/18 18:37,2.3.0,,,,,,,,,2.3.2,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,"I am trying to build kubernetes image for version 2.3.0, using 
{code:java}
./bin/docker-image-tool.sh -r ninadingole/spark-docker -t v2.3.0 build
{code}
giving me an issue for docker build 

error:
{code:java}
""docker build"" requires exactly 1 argument.
See 'docker build --help'.
Usage: docker build [OPTIONS] PATH | URL | - [flags]
Build an image from a Dockerfile
{code}
 

Executing the command within the spark distribution directory. Please let me know what's the issue.

 ",,apachespark,felixcheung,foxish,jooseong,ninadingole,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 14 07:10:14 UTC 2018,,,,,,,,,,"0|i3qyv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/18 05:49;apachespark;User 'jooseong' has created a pull request for this issue:
https://github.com/apache/spark/pull/20791;;;","11/Mar/18 05:55;jooseong;In function build, ""local BUILD_ARGS"" effectively creates an array of one element where the first and only element is an empty string, so ""${BUILD_ARGS[@]}"" expands to """" and passes an extra argument to docker.;;;","12/Mar/18 18:41;foxish;[~felixcheung], just merged this PR. But I'm unable to add an assignee to the JIRA. Am I missing some permissions?;;;","13/Mar/18 16:30;felixcheung;[~foxish] - Jira has a different user role system, I've added you to the right role now, could you try again?

 ;;;","13/Mar/18 18:19;foxish;Thanks Felix. The mechanism appears to work, but I'm unable to assign the PR author in the JIRA. ;;;","14/Mar/18 07:10;felixcheung;I think this is because the user isn't in the user role list.

you can add him to Contributors here https://issues.apache.org/jira/plugins/servlet/project-config/SPARK/roles;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union produces incorrect results when caching is used,SPARK-23614,13142980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mhornbech,mhornbech,06/Mar/18 21:37,18/May/18 10:10,13/Jul/23 08:45,23/Mar/18 04:23,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,correctness,,,"We just upgraded from 2.2 to 2.3 and our test suite caught this error:
{code:java}
case class TestData(x: Int, y: Int, z: Int)

val frame = session.createDataset(Seq(TestData(1, 2, 3), TestData(4, 5, 6))).cache()
val group1 = frame.groupBy(""x"").agg(min(col(""y"")) as ""value"")
val group2 = frame.groupBy(""x"").agg(min(col(""z"")) as ""value"")
group1.union(group2).show()
// +---+-----+
// | x|value|
// +---+-----+
// | 1| 2|
// | 4| 5|
// | 1| 2|
// | 4| 5|
// +---+-----+
group2.union(group1).show()
// +---+-----+
// | x|value|
// +---+-----+
// | 1| 3|
// | 4| 6|
// | 1| 3|
// | 4| 6|
// +---+-----+
{code}
The error disappears if the first data frame is not cached or if the two group by's use separate copies. I'm not sure exactly what happens on the insides of Spark, but errors that produce incorrect results rather than exceptions always concerns me.",,apachespark,clembou,cloud_fan,maropu,mhornbech,yujhe.li,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 10:10:09 UTC 2018,,,,,,,,,,"0|i3qy3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/18 06:43;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20831;;;","23/Mar/18 04:23;cloud_fan;Issue resolved by pull request 20831
[https://github.com/apache/spark/pull/20831];;;","18/May/18 09:53;yujhe.li;Is this bug happening only when 1) cached dataframe 2) aggregation?;;;","18/May/18 10:10;mhornbech;In the example provided caching is required to produce the bug, and I'm pretty sure aggregation is required as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHS needs synchronization between attachSparkUI and detachSparkUI functions,SPARK-23608,13142735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhouyejoe,zhouyejoe,zhouyejoe,06/Mar/18 02:15,16/Mar/18 00:17,13/Jul/23 08:45,16/Mar/18 00:16,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,Web UI,,,0,,,,"We continuously hit an issue with SHS after it runs for a while and have some REST API calls to it. SHS suddenly shows an empty home page with 0 application. It is caused by the unexpected JSON data returned from rest call ""api/v1/applications?limit=8000"". This REST call returns the home page html codes instead of list of application summary. Some other REST call which asks for application detailed information also returns home page html codes. But there are still some working REST calls. We have to restart SHS to solve the issue.

We attached remote debugger to the problematic process and checked the attached jetty handlers tree in the web server. We found that the jetty handler added by ""attachHandler(ApiRootResource.getServletHandler(this))"" is not in the tree as well as some other handlers. Without the root resource servlet handler, SHS will not work correctly serving both UI and REST calls. SHS will directly return the HistoryServerPage html to user as it cannot find handlers to handle the request.

Spark History Server has to attachSparkUI in order to serve user requests. The application SparkUI getting attached when the application details data gets loaded into Guava Cache. While attaching SparkUI, SHS will add attach all jetty handlers into the current web service. But while the data gets cleared out from Guava Cache, SHS will detach all the application's SparkUI jetty handlers. Due to the asynchronous feature in Guava Cache, the clear out from cache is not synchronized with loading into cache. The actual clear out in Guava Cache which triggers detachSparkUI might be detaching the handlers while the attachSparkUI is attaching jetty handlers.

After adding synchronization between attachSparkUI and detachSparkUI in history server, this issue never happens again.",,apachespark,vanzin,zhouyejoe,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 16 00:16:33 UTC 2018,,,,,,,,,,"0|i3qwl3:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"06/Mar/18 02:16;zhouyejoe;I will post a pull request for this minor change.

[~vanzin] [~zsxwing] Any comments? Thanks.;;;","06/Mar/18 02:29;zhouyejoe;Pull Request: https://github.com/apache/spark/pull/20744;;;","06/Mar/18 02:30;apachespark;User 'zhouyejoe' has created a pull request for this issue:
https://github.com/apache/spark/pull/20744;;;","16/Mar/18 00:16;vanzin;Issue resolved by pull request 20744
[https://github.com/apache/spark/pull/20744];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PrintToStderr should behave the same in interpreted mode,SPARK-23602,13142565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mgaido,hvanhovell,hvanhovell,05/Mar/18 15:31,08/Mar/18 21:03,13/Jul/23 08:45,08/Mar/18 21:03,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,The {{PrintToStderr}} behaves differently for the interpreted and code generated code paths. We should fix this.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 14:57:05 UTC 2018,,,,,,,,,,"0|i3qvjb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"08/Mar/18 14:57;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The UUID() expression is too non-deterministic,SPARK-23599,13142529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,hvanhovell,hvanhovell,05/Mar/18 13:27,30/Dec/21 18:50,13/Jul/23 08:45,22/Mar/18 18:58,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"The current {{Uuid()}} expression uses {{java.util.UUID.randomUUID}} for UUID generation. There are a couple of major problems with this:
- It is non-deterministic across task retries. This breaks Spark's processing model, and this will to very hard to trace bugs, like non-deterministic shuffles, duplicates and missing rows.
- It uses a single secure random for UUID generation. This uses a single JVM wide lock, and this can lead to lock contention and other performance problems.

We should move to something that is deterministic between retries. This can be done by using seeded PRNGs for which we set the seed during planning. It is important here to use a PRNG that provides enough entropy for creating a proper UUID.",,apachespark,hvanhovell,maropu,mgaido,stubartmess,viirya,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23794,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 30 18:50:24 UTC 2021,,,,,,,,,,"0|i3qvbb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"14/Mar/18 05:48;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20817;;;","19/Mar/18 08:42;hvanhovell;PR 1 out of 2 has been merged.;;;","20/Mar/18 03:43;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20861;;;","26/Mar/18 09:25;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20903;;;","30/Dec/21 18:50;stubartmess;We have encountered this problem with Spark 3.1.2, resulting in duplicate values in a situation where a spark executor died. As suggested in the description, this error was hard to track down and difficult to replicate. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WholeStageCodegen can lead to IllegalAccessError  calling append for HashAggregateExec,SPARK-23598,13142525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,dvogelbacher,dvogelbacher,05/Mar/18 13:20,27/Mar/18 04:08,13/Jul/23 08:45,13/Mar/18 22:05,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,,,,"Got the following stacktrace for a large QueryPlan using WholeStageCodeGen:
{noformat}
java.lang.IllegalAccessError: tried to access method org.apache.spark.sql.execution.BufferedRowIterator.append(Lorg/apache/spark/sql/catalyst/InternalRow;)V from class org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7$agg_NestedClass
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7$agg_NestedClass.agg_doAggregateWithKeysOutput$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345){noformat}
After disabling codegen, everything works.

The root cause seems to be that we are trying to call the protected _append_ method of [BufferedRowIterator|https://github.com/apache/spark/blob/master/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java#L68] from an inner-class of a sub-class that is loaded by a different class-loader (after codegen compilation).

[https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-5.html#jvms-5.4.4] states that a protected method _R_ can be accessed only if one of the following two conditions is fulfilled:
 # R is protected and is declared in a class C, and D is either a subclass of C or C itself. Furthermore, if R is not static, then the symbolic reference to R must contain a symbolic reference to a class T, such that T is either a subclass of D, a superclass of D, or D itself.
 # R is either protected or has default access (that is, neither public nor protected nor private), and is declared by a class in the same run-time package as D.

2.) doesn't apply as we have loaded the class with a different class loader (and are in a different package) and 1.) doesn't apply because we are apparently trying to call the method from an inner class of a subclass of _BufferedRowIterator_.

Looking at the Code path of _WholeStageCodeGen_, the following happens:
 # In [WholeStageCodeGen|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala#L527], we create the subclass of _BufferedRowIterator_, along with a _processNext_ method for processing the output of the child plan.
 # In the child, which is a [HashAggregateExec|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala#L517], we create the method which shows up at the top of the stack trace (called _doAggregateWithKeysOutput_ )
 # We add this method to the compiled code invoking _addNewFunction_ of [CodeGenerator|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L460]
In the generated function body we call the _append_ method.|

Now, the _addNewFunction_ method states that:
{noformat}
If the code for the `OuterClass` grows too large, the function will be inlined into a new private, inner class
{noformat}
This indeed seems to happen: the _doAggregateWithKeysOutput_ method is put into a new private inner class. Thus, it doesn't have access to the protected _append_ method anymore but still tries to call it, which results in the _IllegalAccessError._ 

Possible fixes:
 * Pass in the _inlineToOuterClass_ flag when invoking the _addNewFunction_
 * Make the _append_ method public
 * Re-declare the _append_ method in the generated subclass (just invoking _super_). This way, inner classes should have access to it.

 ",,apachespark,dongjoon,dvogelbacher,hvanhovell,kiszk,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 27 04:08:50 UTC 2018,,,,,,,,,,"0|i3qvaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/18 13:53;hvanhovell;[~dvogelbacher] Do you have some code which we can use to reproduce this?;;;","05/Mar/18 15:45;mgaido;thanks for reporting this. Actually the one which you designed as a possible fix isn't really an option, because it would mean basically inlining everything to the outer class, which causes other problems (namely the Java limit for constant pool entries). Redeclaring it seems a bit hacky to me (and it causes an extra function call for each row...). I'd go for making the method public, if no better option comes out.;;;","07/Mar/18 16:10;dvogelbacher;[~hvanhovell] unfortunately, I can't extract any code for reproducing.
It should be possible to come up with code, by making a large enough query (one that adds many methods in the code gen stage) and contains a HashAggregateExec node.

Or to make it even easier, one could compile spark with a lowered `final val GENERATED_CLASS_SIZE_THRESHOLD` in [CodeGenerator title|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L1170], to force the generation of the private inner class.;;;","07/Mar/18 16:38;kiszk;I also think that the easiest way is to make {{append}} public.;;;","08/Mar/18 15:11;mgaido;[~dvogelbacher] the parameter you are talking about is taken in account only when we split expressions and it is not done in HashAggregateExec. I haven't been able to reproduce this. If you can provide a sample to reproduce this, it would be very helpful.;;;","08/Mar/18 18:38;dvogelbacher;[~mgaido] {{HashAggregateExec}} calls {{addNewFunction}}, which calls {{addNewFunctionInternal}} which uses that flag and checks if the current size is bigger than {{GENERATED_CLASS_SIZE_THRESHOLD}} ([see|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L478])

I just compiled develop with {{GENERATED_CLASS_SIZE_THRESHOLD}} set to -1 and was able to reproduce (cc [~hvanhovell]) . I applied the following diff before compiling:

{noformat}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala
index 793824b0b0..7fad817d89 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala
@@ -1167,7 +1167,7 @@ object CodeGenerator extends Logging {
   // limit, 65,536. We cannot know how many constants will be inserted for a class, so we use a
   // threshold of 1000k bytes to determine when a function should be inlined to a private, inner
   // class.
-  final val GENERATED_CLASS_SIZE_THRESHOLD = 1000000
+  final val GENERATED_CLASS_SIZE_THRESHOLD = -1

   // This is the threshold for the number of global variables, whose types are primitive type or
   // complex type (e.g. more than one-dimensional array), that will be placed at the outer class
(END)
{noformat}

Then, I executed a simple groupBy-Aggregate in the spark-shell and got the same error:

{noformat}
➜  spark git:(master) ✗ ./bin/spark-shell
18/03/08 18:30:24 WARN Utils: Your hostname, dvogelbac resolves to a loopback address: 127.0.0.1; using 10.111.11.111 instead (on interface en0)
18/03/08 18:30:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/03/08 18:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://10.111.11.111:4040
Spark context available as 'sc' (master = local[*], app id = local-1520533829643).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.0-SNAPSHOT
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.conf.set(""spark.sql.codegen.wholeStage"", true)

scala> val df_pet_age = Seq(
     |   (8, ""bat""),
     |   (5, ""bat""),
     |   (15, ""bat""),
     |   (30, ""mouse""),
     |   (15, ""mouse""),
     |   (23, ""mouse""),
     |   (8, ""horse""),
     |   (-5, ""horse"")
     | ).toDF(""age"", ""name"")
df_pet_age: org.apache.spark.sql.DataFrame = [age: int, name: string]

scala> df_pet_age.groupBy(""name"").avg(""age"").show()
18/03/08 18:31:20 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.IllegalAccessError: tried to access method org.apache.spark.sql.execution.BufferedRowIterator.stopEarly()Z from class org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1$agg_NestedClass1
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1$agg_NestedClass1.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:616)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 18:31:20 ERROR Executor: Exception in task 5.0 in stage 0.0 (TID 5)
{noformat}

It is actually failing trying to invoke a different method in {{BufferedRowIterator}} now ({{stopEarly}}), but it is the same problem. That method is also declared {{protected}} and can't be accessed.



;;;","08/Mar/18 19:46;kiszk;Thanks, I confirmed that I can reproduce this issue with the master.

Generated code is here
{code:java}
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg;
/* 010 */   private boolean agg_bufIsNull;
/* 011 */   private double agg_bufValue;
/* 012 */   private boolean agg_bufIsNull1;
/* 013 */   private long agg_bufValue1;
/* 014 */   private agg_FastHashMap agg_fastHashMap;
/* 015 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> agg_fastHashMapIter;
/* 016 */   private org.apache.spark.unsafe.KVIterator agg_mapIter;
/* 017 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap;
/* 018 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter;
/* 019 */   private scala.collection.Iterator inputadapter_input;
/* 020 */   private boolean agg_agg_isNull11;
/* 021 */   private boolean agg_agg_isNull25;
/* 022 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder[] agg_mutableStateArray1 = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder[2];
/* 023 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 024 */   private UnsafeRow[] agg_mutableStateArray = new UnsafeRow[2];
/* 025 */
/* 026 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 027 */     this.references = references;
/* 028 */   }
/* 029 */
/* 030 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 031 */     partitionIndex = index;
/* 032 */     this.inputs = inputs;
/* 033 */
/* 034 */     agg_fastHashMap = new agg_FastHashMap(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());
/* 035 */     agg_hashMap = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 036 */     inputadapter_input = inputs[0];
/* 037 */     agg_mutableStateArray[0] = new UnsafeRow(1);
/* 038 */     agg_mutableStateArray1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_mutableStateArray[0], 32);
/* 039 */     agg_mutableStateArray2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(agg_mutableStateArray1[0], 1);
/* 040 */     agg_mutableStateArray[1] = new UnsafeRow(3);
/* 041 */     agg_mutableStateArray1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_mutableStateArray[1], 32);
/* 042 */     agg_mutableStateArray2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(agg_mutableStateArray1[1], 3);
/* 043 */
/* 044 */   }
/* 045 */
/* 046 */   public class agg_FastHashMap {
/* 047 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;
/* 048 */     private int[] buckets;
/* 049 */     private int capacity = 1 << 16;
/* 050 */     private double loadFactor = 0.5;
/* 051 */     private int numBuckets = (int) (capacity / loadFactor);
/* 052 */     private int maxSteps = 2;
/* 053 */     private int numRows = 0;
/* 054 */     private org.apache.spark.sql.types.StructType keySchema = new org.apache.spark.sql.types.StructType().add(((java.lang.String) references[1] /* keyName */), org.apache.spark.sql.types.DataTypes.StringType);
/* 055 */     private org.apache.spark.sql.types.StructType valueSchema = new org.apache.spark.sql.types.StructType().add(((java.lang.String) references[2] /* keyName */), org.apache.spark.sql.types.DataTypes.DoubleType)
/* 056 */     .add(((java.lang.String) references[3] /* keyName */), org.apache.spark.sql.types.DataTypes.LongType);
/* 057 */     private Object emptyVBase;
/* 058 */     private long emptyVOff;
/* 059 */     private int emptyVLen;
/* 060 */     private boolean isBatchFull = false;
/* 061 */
/* 062 */     public agg_FastHashMap(
/* 063 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,
/* 064 */       InternalRow emptyAggregationBuffer) {
/* 065 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch
/* 066 */       .allocate(keySchema, valueSchema, taskMemoryManager, capacity);
/* 067 */
/* 068 */       final UnsafeProjection valueProjection = UnsafeProjection.create(valueSchema);
/* 069 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();
/* 070 */
/* 071 */       emptyVBase = emptyBuffer;
/* 072 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;
/* 073 */       emptyVLen = emptyBuffer.length;
/* 074 */
/* 075 */       buckets = new int[numBuckets];
/* 076 */       java.util.Arrays.fill(buckets, -1);
/* 077 */     }
/* 078 */
/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String agg_key) {
/* 080 */       long h = hash(agg_key);
/* 081 */       int step = 0;
/* 082 */       int idx = (int) h & (numBuckets - 1);
/* 083 */       while (step < maxSteps) {
/* 084 */         // Return bucket index if it's either an empty slot or already contains the key
/* 085 */         if (buckets[idx] == -1) {
/* 086 */           if (numRows < capacity && !isBatchFull) {
/* 087 */             // creating the unsafe for new entry
/* 088 */             UnsafeRow agg_result = new UnsafeRow(1);
/* 089 */             org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder agg_holder
/* 090 */             = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_result,
/* 091 */               32);
/* 092 */             org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter
/* 093 */             = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(
/* 094 */               agg_holder,
/* 095 */               1);
/* 096 */             agg_holder.reset(); //TODO: investigate if reset or zeroout are actually needed
/* 097 */             agg_rowWriter.zeroOutNullBytes();
/* 098 */             agg_rowWriter.write(0, agg_key);
/* 099 */             agg_result.setTotalSize(agg_holder.totalSize());
/* 100 */             Object kbase = agg_result.getBaseObject();
/* 101 */             long koff = agg_result.getBaseOffset();
/* 102 */             int klen = agg_result.getSizeInBytes();
/* 103 */
/* 104 */             UnsafeRow vRow
/* 105 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);
/* 106 */             if (vRow == null) {
/* 107 */               isBatchFull = true;
/* 108 */             } else {
/* 109 */               buckets[idx] = numRows++;
/* 110 */             }
/* 111 */             return vRow;
/* 112 */           } else {
/* 113 */             // No more space
/* 114 */             return null;
/* 115 */           }
/* 116 */         } else if (equals(idx, agg_key)) {
/* 117 */           return batch.getValueRow(buckets[idx]);
/* 118 */         }
/* 119 */         idx = (idx + 1) & (numBuckets - 1);
/* 120 */         step++;
/* 121 */       }
/* 122 */       // Didn't find it
/* 123 */       return null;
/* 124 */     }
/* 125 */
/* 126 */     private boolean equals(int idx, UTF8String agg_key) {
/* 127 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);
/* 128 */       return (row.getUTF8String(0).equals(agg_key));
/* 129 */     }
/* 130 */
/* 131 */     private long hash(UTF8String agg_key) {
/* 132 */       long agg_hash = 0;
/* 133 */
/* 134 */       int agg_result = 0;
/* 135 */       byte[] agg_bytes = agg_key.getBytes();
/* 136 */       for (int i = 0; i < agg_bytes.length; i++) {
/* 137 */         int agg_hash1 = agg_bytes[i];
/* 138 */         agg_result = (agg_result ^ (0x9e3779b9)) + agg_hash1 + (agg_result << 6) + (agg_result >>> 2);
/* 139 */       }
/* 140 */
/* 141 */       agg_hash = (agg_hash ^ (0x9e3779b9)) + agg_result + (agg_hash << 6) + (agg_hash >>> 2);
/* 142 */
/* 143 */       return agg_hash;
/* 144 */     }
/* 145 */
/* 146 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {
/* 147 */       return batch.rowIterator();
/* 148 */     }
/* 149 */
/* 150 */     public void close() {
/* 151 */       batch.close();
/* 152 */     }
/* 153 */
/* 154 */   }
/* 155 */
/* 156 */   protected void processNext() throws java.io.IOException {
/* 157 */     if (!agg_initAgg) {
/* 158 */       agg_initAgg = true;
/* 159 */       long wholestagecodegen_beforeAgg = System.nanoTime();
/* 160 */       agg_nestedClassInstance1.agg_doAggregateWithKeys();
/* 161 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg) / 1000000);
/* 162 */     }
/* 163 */
/* 164 */     // output the result
/* 165 */
/* 166 */     while (agg_fastHashMapIter.next()) {
/* 167 */       UnsafeRow agg_aggKey = (UnsafeRow) agg_fastHashMapIter.getKey();
/* 168 */       UnsafeRow agg_aggBuffer = (UnsafeRow) agg_fastHashMapIter.getValue();
/* 169 */       wholestagecodegen_nestedClassInstance.agg_doAggregateWithKeysOutput(agg_aggKey, agg_aggBuffer);
/* 170 */
/* 171 */       if (shouldStop()) return;
/* 172 */     }
/* 173 */     agg_fastHashMap.close();
/* 174 */
/* 175 */     while (agg_mapIter.next()) {
/* 176 */       UnsafeRow agg_aggKey = (UnsafeRow) agg_mapIter.getKey();
/* 177 */       UnsafeRow agg_aggBuffer = (UnsafeRow) agg_mapIter.getValue();
/* 178 */       wholestagecodegen_nestedClassInstance.agg_doAggregateWithKeysOutput(agg_aggKey, agg_aggBuffer);
/* 179 */
/* 180 */       if (shouldStop()) return;
/* 181 */     }
/* 182 */
/* 183 */     agg_mapIter.close();
/* 184 */     if (agg_sorter == null) {
/* 185 */       agg_hashMap.free();
/* 186 */     }
/* 187 */   }
/* 188 */
/* 189 */   private wholestagecodegen_NestedClass wholestagecodegen_nestedClassInstance = new wholestagecodegen_NestedClass();
/* 190 */   private agg_NestedClass1 agg_nestedClassInstance1 = new agg_NestedClass1();
/* 191 */   private agg_NestedClass agg_nestedClassInstance = new agg_NestedClass();
/* 192 */
/* 193 */   private class agg_NestedClass1 {
/* 194 */     private void agg_doAggregateWithKeys() throws java.io.IOException {
/* 195 */       while (inputadapter_input.hasNext() && !stopEarly()) {
/* 196 */         InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 197 */         int inputadapter_value = inputadapter_row.getInt(0);
/* 198 */         boolean inputadapter_isNull1 = inputadapter_row.isNullAt(1);
/* 199 */         UTF8String inputadapter_value1 = inputadapter_isNull1 ?
/* 200 */         null : (inputadapter_row.getUTF8String(1));
/* 201 */
/* 202 */         agg_nestedClassInstance.agg_doConsume(inputadapter_row, inputadapter_value, inputadapter_value1, inputadapter_isNull1);
/* 203 */         if (shouldStop()) return;
/* 204 */       }
/* 205 */
/* 206 */       agg_fastHashMapIter = agg_fastHashMap.rowIterator();
/* 207 */       agg_mapIter = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap, agg_sorter, ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* avgHashProbe */));
/* 208 */
/* 209 */     }
/* 210 */
/* 211 */   }
/* 212 */
/* 213 */   private class wholestagecodegen_NestedClass {
/* 214 */     private void agg_doAggregateWithKeysOutput(UnsafeRow agg_keyTerm, UnsafeRow agg_bufferTerm)
/* 215 */     throws java.io.IOException {
/* 216 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);
/* 217 */
/* 218 */       boolean agg_isNull35 = agg_keyTerm.isNullAt(0);
/* 219 */       UTF8String agg_value37 = agg_isNull35 ?
/* 220 */       null : (agg_keyTerm.getUTF8String(0));
/* 221 */       boolean agg_isNull36 = agg_bufferTerm.isNullAt(0);
/* 222 */       double agg_value38 = agg_isNull36 ?
/* 223 */       -1.0 : (agg_bufferTerm.getDouble(0));
/* 224 */       boolean agg_isNull37 = agg_bufferTerm.isNullAt(1);
/* 225 */       long agg_value39 = agg_isNull37 ?
/* 226 */       -1L : (agg_bufferTerm.getLong(1));
/* 227 */
/* 228 */       agg_mutableStateArray1[1].reset();
/* 229 */
/* 230 */       agg_mutableStateArray2[1].zeroOutNullBytes();
/* 231 */
/* 232 */       if (agg_isNull35) {
/* 233 */         agg_mutableStateArray2[1].setNullAt(0);
/* 234 */       } else {
/* 235 */         agg_mutableStateArray2[1].write(0, agg_value37);
/* 236 */       }
/* 237 */
/* 238 */       if (agg_isNull36) {
/* 239 */         agg_mutableStateArray2[1].setNullAt(1);
/* 240 */       } else {
/* 241 */         agg_mutableStateArray2[1].write(1, agg_value38);
/* 242 */       }
/* 243 */
/* 244 */       if (agg_isNull37) {
/* 245 */         agg_mutableStateArray2[1].setNullAt(2);
/* 246 */       } else {
/* 247 */         agg_mutableStateArray2[1].write(2, agg_value39);
/* 248 */       }
/* 249 */       agg_mutableStateArray[1].setTotalSize(agg_mutableStateArray1[1].totalSize());
/* 250 */       append(agg_mutableStateArray[1]);
/* 251 */
/* 252 */     }
/* 253 */
/* 254 */   }
/* 255 */
/* 256 */   private class agg_NestedClass {
/* 257 */     private void agg_doConsume(InternalRow inputadapter_row, int agg_expr_0, UTF8String agg_expr_1, boolean agg_exprIsNull_1) throws java.io.IOException {
/* 258 */       UnsafeRow agg_unsafeRowAggBuffer = null;
/* 259 */       UnsafeRow agg_fastAggBuffer = null;
/* 260 */
/* 261 */       if (true) {
/* 262 */         if (!agg_exprIsNull_1) {
/* 263 */           agg_fastAggBuffer = agg_fastHashMap.findOrInsert(
/* 264 */             agg_expr_1);
/* 265 */         }
/* 266 */       }
/* 267 */       // Cannot find the key in fast hash map, try regular hash map.
/* 268 */       if (agg_fastAggBuffer == null) {
/* 269 */         // generate grouping key
/* 270 */         agg_mutableStateArray1[0].reset();
/* 271 */
/* 272 */         agg_mutableStateArray2[0].zeroOutNullBytes();
/* 273 */
/* 274 */         if (agg_exprIsNull_1) {
/* 275 */           agg_mutableStateArray2[0].setNullAt(0);
/* 276 */         } else {
/* 277 */           agg_mutableStateArray2[0].write(0, agg_expr_1);
/* 278 */         }
/* 279 */         agg_mutableStateArray[0].setTotalSize(agg_mutableStateArray1[0].totalSize());
/* 280 */         int agg_value7 = 42;
/* 281 */
/* 282 */         if (!agg_exprIsNull_1) {
/* 283 */           agg_value7 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_1.getBaseObject(), agg_expr_1.getBaseOffset(), agg_expr_1.numBytes(), agg_value7);
/* 284 */         }
/* 285 */         if (true) {
/* 286 */           // try to get the buffer from hash map
/* 287 */           agg_unsafeRowAggBuffer =
/* 288 */           agg_hashMap.getAggregationBufferFromUnsafeRow(agg_mutableStateArray[0], agg_value7);
/* 289 */         }
/* 290 */         // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 291 */         // aggregation after processing all input rows.
/* 292 */         if (agg_unsafeRowAggBuffer == null) {
/* 293 */           if (agg_sorter == null) {
/* 294 */             agg_sorter = agg_hashMap.destructAndCreateExternalSorter();
/* 295 */           } else {
/* 296 */             agg_sorter.merge(agg_hashMap.destructAndCreateExternalSorter());
/* 297 */           }
/* 298 */
/* 299 */           // the hash map had be spilled, it should have enough memory now,
/* 300 */           // try to allocate buffer again.
/* 301 */           agg_unsafeRowAggBuffer = agg_hashMap.getAggregationBufferFromUnsafeRow(
/* 302 */             agg_mutableStateArray[0], agg_value7);
/* 303 */           if (agg_unsafeRowAggBuffer == null) {
/* 304 */             // failed to allocate the first page
/* 305 */             throw new OutOfMemoryError(""No enough memory for aggregation"");
/* 306 */           }
/* 307 */         }
/* 308 */
/* 309 */       }
/* 310 */
/* 311 */       if (agg_fastAggBuffer != null) {
/* 312 */         // common sub-expressions
/* 313 */         boolean agg_isNull21 = false;
/* 314 */         long agg_value23 = -1L;
/* 315 */         if (!false) {
/* 316 */           agg_value23 = (long) agg_expr_0;
/* 317 */         }
/* 318 */         // evaluate aggregate function
/* 319 */         boolean agg_isNull23 = true;
/* 320 */         double agg_value25 = -1.0;
/* 321 */
/* 322 */         boolean agg_isNull24 = agg_fastAggBuffer.isNullAt(0);
/* 323 */         double agg_value26 = agg_isNull24 ?
/* 324 */         -1.0 : (agg_fastAggBuffer.getDouble(0));
/* 325 */         if (!agg_isNull24) {
/* 326 */           agg_agg_isNull25 = true;
/* 327 */           double agg_value27 = -1.0;
/* 328 */           do {
/* 329 */             boolean agg_isNull26 = agg_isNull21;
/* 330 */             double agg_value28 = -1.0;
/* 331 */             if (!agg_isNull21) {
/* 332 */               agg_value28 = (double) agg_value23;
/* 333 */             }
/* 334 */             if (!agg_isNull26) {
/* 335 */               agg_agg_isNull25 = false;
/* 336 */               agg_value27 = agg_value28;
/* 337 */               continue;
/* 338 */             }
/* 339 */
/* 340 */             boolean agg_isNull27 = false;
/* 341 */             double agg_value29 = -1.0;
/* 342 */             if (!false) {
/* 343 */               agg_value29 = (double) 0;
/* 344 */             }
/* 345 */             if (!agg_isNull27) {
/* 346 */               agg_agg_isNull25 = false;
/* 347 */               agg_value27 = agg_value29;
/* 348 */               continue;
/* 349 */             }
/* 350 */
/* 351 */           } while (false);
/* 352 */
/* 353 */           agg_isNull23 = false; // resultCode could change nullability.
/* 354 */           agg_value25 = agg_value26 + agg_value27;
/* 355 */
/* 356 */         }
/* 357 */         boolean agg_isNull29 = false;
/* 358 */         long agg_value31 = -1L;
/* 359 */         if (!false && agg_isNull21) {
/* 360 */           boolean agg_isNull31 = agg_fastAggBuffer.isNullAt(1);
/* 361 */           long agg_value33 = agg_isNull31 ?
/* 362 */           -1L : (agg_fastAggBuffer.getLong(1));
/* 363 */           agg_isNull29 = agg_isNull31;
/* 364 */           agg_value31 = agg_value33;
/* 365 */         } else {
/* 366 */           boolean agg_isNull32 = true;
/* 367 */           long agg_value34 = -1L;
/* 368 */
/* 369 */           boolean agg_isNull33 = agg_fastAggBuffer.isNullAt(1);
/* 370 */           long agg_value35 = agg_isNull33 ?
/* 371 */           -1L : (agg_fastAggBuffer.getLong(1));
/* 372 */           if (!agg_isNull33) {
/* 373 */             agg_isNull32 = false; // resultCode could change nullability.
/* 374 */             agg_value34 = agg_value35 + 1L;
/* 375 */
/* 376 */           }
/* 377 */           agg_isNull29 = agg_isNull32;
/* 378 */           agg_value31 = agg_value34;
/* 379 */         }
/* 380 */         // update fast row
/* 381 */         if (!agg_isNull23) {
/* 382 */           agg_fastAggBuffer.setDouble(0, agg_value25);
/* 383 */         } else {
/* 384 */           agg_fastAggBuffer.setNullAt(0);
/* 385 */         }
/* 386 */
/* 387 */         if (!agg_isNull29) {
/* 388 */           agg_fastAggBuffer.setLong(1, agg_value31);
/* 389 */         } else {
/* 390 */           agg_fastAggBuffer.setNullAt(1);
/* 391 */         }
/* 392 */       } else {
/* 393 */         // common sub-expressions
/* 394 */         boolean agg_isNull7 = false;
/* 395 */         long agg_value9 = -1L;
/* 396 */         if (!false) {
/* 397 */           agg_value9 = (long) agg_expr_0;
/* 398 */         }
/* 399 */         // evaluate aggregate function
/* 400 */         boolean agg_isNull9 = true;
/* 401 */         double agg_value11 = -1.0;
/* 402 */
/* 403 */         boolean agg_isNull10 = agg_unsafeRowAggBuffer.isNullAt(0);
/* 404 */         double agg_value12 = agg_isNull10 ?
/* 405 */         -1.0 : (agg_unsafeRowAggBuffer.getDouble(0));
/* 406 */         if (!agg_isNull10) {
/* 407 */           agg_agg_isNull11 = true;
/* 408 */           double agg_value13 = -1.0;
/* 409 */           do {
/* 410 */             boolean agg_isNull12 = agg_isNull7;
/* 411 */             double agg_value14 = -1.0;
/* 412 */             if (!agg_isNull7) {
/* 413 */               agg_value14 = (double) agg_value9;
/* 414 */             }
/* 415 */             if (!agg_isNull12) {
/* 416 */               agg_agg_isNull11 = false;
/* 417 */               agg_value13 = agg_value14;
/* 418 */               continue;
/* 419 */             }
/* 420 */
/* 421 */             boolean agg_isNull13 = false;
/* 422 */             double agg_value15 = -1.0;
/* 423 */             if (!false) {
/* 424 */               agg_value15 = (double) 0;
/* 425 */             }
/* 426 */             if (!agg_isNull13) {
/* 427 */               agg_agg_isNull11 = false;
/* 428 */               agg_value13 = agg_value15;
/* 429 */               continue;
/* 430 */             }
/* 431 */
/* 432 */           } while (false);
/* 433 */
/* 434 */           agg_isNull9 = false; // resultCode could change nullability.
/* 435 */           agg_value11 = agg_value12 + agg_value13;
/* 436 */
/* 437 */         }
/* 438 */         boolean agg_isNull15 = false;
/* 439 */         long agg_value17 = -1L;
/* 440 */         if (!false && agg_isNull7) {
/* 441 */           boolean agg_isNull17 = agg_unsafeRowAggBuffer.isNullAt(1);
/* 442 */           long agg_value19 = agg_isNull17 ?
/* 443 */           -1L : (agg_unsafeRowAggBuffer.getLong(1));
/* 444 */           agg_isNull15 = agg_isNull17;
/* 445 */           agg_value17 = agg_value19;
/* 446 */         } else {
/* 447 */           boolean agg_isNull18 = true;
/* 448 */           long agg_value20 = -1L;
/* 449 */
/* 450 */           boolean agg_isNull19 = agg_unsafeRowAggBuffer.isNullAt(1);
/* 451 */           long agg_value21 = agg_isNull19 ?
/* 452 */           -1L : (agg_unsafeRowAggBuffer.getLong(1));
/* 453 */           if (!agg_isNull19) {
/* 454 */             agg_isNull18 = false; // resultCode could change nullability.
/* 455 */             agg_value20 = agg_value21 + 1L;
/* 456 */
/* 457 */           }
/* 458 */           agg_isNull15 = agg_isNull18;
/* 459 */           agg_value17 = agg_value20;
/* 460 */         }
/* 461 */         // update unsafe row buffer
/* 462 */         if (!agg_isNull9) {
/* 463 */           agg_unsafeRowAggBuffer.setDouble(0, agg_value11);
/* 464 */         } else {
/* 465 */           agg_unsafeRowAggBuffer.setNullAt(0);
/* 466 */         }
/* 467 */
/* 468 */         if (!agg_isNull15) {
/* 469 */           agg_unsafeRowAggBuffer.setLong(1, agg_value17);
/* 470 */         } else {
/* 471 */           agg_unsafeRowAggBuffer.setNullAt(1);
/* 472 */         }
/* 473 */
/* 474 */       }
/* 475 */
/* 476 */     }
/* 477 */
/* 478 */   }
/* 479 */
/* 480 */ }
{code};;;","09/Mar/18 03:45;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20779;;;","24/Mar/18 23:34;dongjoon;Hi, [~hvanhovell] and [~kiszk].
Although this test case is failing in `branch-2.3` sometimes, I added `2.3.1` to `Fixed Versions` because the patch landed on `branch-2.3`.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.6/lastCompletedBuild/testReport/org.apache.spark.sql.execution/WholeStageCodegenSuite/SPARK_23598__Codegen_working_for_lots_of_aggregation_operations_without_runtime_errors/;;;","25/Mar/18 22:02;hvanhovell;[~dongjoon] thanks for doing this. As for the failing tests, I have backported the hotfix to branch-2.3.;;;","27/Mar/18 04:08;dongjoon;Thank you, [~hvanhovell] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinglePartition in data source V2 scan,SPARK-23574,13142247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,joseph.torres,joseph.torres,03/Mar/18 03:39,20/Mar/18 18:53,13/Jul/23 08:45,20/Mar/18 18:53,2.4.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,DataSourceV2ScanExec currently reports UnknownPartitioning whenever the reader doesn't mix in SupportsReportPartitioning. It can also report SinglePartition in the case where there's a single reader factory.,,apachespark,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 03 04:39:05 UTC 2018,,,,,,,,,,"0|i3qtkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/18 04:39;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Spark-2.3 in HiveExternalCatalogVersionsSuite,SPARK-23570,13142175,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,02/Mar/18 19:21,02/Mar/18 22:31,13/Jul/23 08:45,02/Mar/18 22:31,2.3.1,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,Add 2.3.0 to HiveExternalCatalogVersionsSuite,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 19:25:05 UTC 2018,,,,,,,,,,"0|i3qt4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/18 19:25;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pandas_udf does not work with type-annotated python functions,SPARK-23569,13142170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mstewart141,mstewart141,mstewart141,02/Mar/18 19:04,12/Dec/22 18:10,13/Jul/23 08:45,05/Mar/18 04:39,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,PySpark,,,,0,,,,"When invoked against a type annotated function pandas_udf raises:

`ValueError: Function has keyword-only parameters or annotations, use getfullargspec() API which can support them`

 

the deprecated `getargsspec` call occurs in `pyspark/sql/udf.py`
{code:java}
def _create_udf(f, returnType, evalType):

    if evalType in (PythonEvalType.SQL_SCALAR_PANDAS_UDF,
                    PythonEvalType.SQL_GROUPED_MAP_PANDAS_UDF):
        import inspect
        from pyspark.sql.utils import require_minimum_pyarrow_version

        require_minimum_pyarrow_version()
        argspec = inspect.getargspec(f)

        ...{code}
To reproduce: 
{code:java}
from pyspark.sql import SparkSession

from pyspark.sql.functions import pandas_udf, PandasUDFType, col, lit

spark = SparkSession.builder.getOrCreate()

df = spark.range(12).withColumn('b', col('id') * 2)

def ok(a,b): return a*b

df.withColumn('ok', pandas_udf(f=ok, returnType='bigint')('id','b')).show()  # no problems

import pandas as pd

def ok(a: pd.Series,b: pd.Series) -> pd.Series: return a*b

df.withColumn('ok', pandas_udf(f=ok, returnType='bigint')('id','b'))

 

---------------------------------------------------------------------------
ValueError Traceback (most recent call last)
<ipython-input-17-2e6ae67b15ee> in <module>()
----> 1 df.withColumn('ok', pandas_udf(f=ok, returnType='bigint')('id','b'))

/opt/miniconda/lib/python3.6/site-packages/pyspark/sql/functions.py in pandas_udf(f, returnType, functionType)
2277 return functools.partial(_create_udf, returnType=return_type, evalType=eval_type)
2278 else:
-> 2279 return _create_udf(f=f, returnType=return_type, evalType=eval_type)
2280
2281

/opt/miniconda/lib/python3.6/site-packages/pyspark/sql/udf.py in _create_udf(f, returnType, evalType)
44
45 require_minimum_pyarrow_version()
---> 46 argspec = inspect.getargspec(f)
47
48 if evalType == PythonEvalType.SQL_SCALAR_PANDAS_UDF and len(argspec.args) == 0 and \

/opt/miniconda/lib/python3.6/inspect.py in getargspec(func)
1043 getfullargspec(func)
1044 if kwonlyargs or ann:
-> 1045 raise ValueError(""Function has keyword-only parameters or annotations""
1046 "", use getfullargspec() API which can support them"")
1047 return ArgSpec(args, varargs, varkw, defaults)

ValueError: Function has keyword-only parameters or annotations, use getfullargspec() API which can support them
{code}","python 3.6 | pyspark 2.3.0 | Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_141 | Revision a0d7949896e70f427e7f3942ff340c9484ff0aab",apachespark,mstewart141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 05 04:39:21 UTC 2018,,,,,,,,,,"0|i3qt3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/18 09:34;gurwls223;I think it's because of deprecated {{inspect.getargspec}} in Python 3. Can we replace it to like {{inspect.signature}} in Python 3?;;;","03/Mar/18 09:35;gurwls223;If it's hard to add a test, manual tests will be fine enough but please add it in PR description. Would you be interested in submitting a PR?;;;","03/Mar/18 20:59;mstewart141;yes, i'll give it a go;;;","03/Mar/18 22:16;apachespark;User 'mstewart141' has created a pull request for this issue:
https://github.com/apache/spark/pull/20728;;;","05/Mar/18 04:39;gurwls223;Fixed in https://github.com/apache/spark/pull/20728;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude `hadoop-mapreduce-client-core` dependency from `orc-mapreduce`,SPARK-23551,13141865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,01/Mar/18 17:54,14/Apr/18 02:35,13/Jul/23 08:45,02/Mar/18 01:28,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Build,,,,0,,,,"This issue aims to prevent `orc-mapreduce` dependency from making IDEs and maven confused.

*BEFORE*
Please note that 2.6.4 at Spark Project SQL.
{code}
$ mvn dependency:tree -Phadoop-2.7 -Dincludes=org.apache.hadoop:hadoop-mapreduce-client-core
...
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Catalyst 2.4.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:3.0.2:tree (default-cli) @ spark-catalyst_2.11 ---
[INFO] org.apache.spark:spark-catalyst_2.11:jar:2.4.0-SNAPSHOT
[INFO] \- org.apache.spark:spark-core_2.11:jar:2.4.0-SNAPSHOT:compile
[INFO]    \- org.apache.hadoop:hadoop-client:jar:2.7.3:compile
[INFO]       \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3:compile
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project SQL 2.4.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:3.0.2:tree (default-cli) @ spark-sql_2.11 ---
[INFO] org.apache.spark:spark-sql_2.11:jar:2.4.0-SNAPSHOT
[INFO] \- org.apache.orc:orc-mapreduce:jar:nohive:1.4.3:compile
[INFO]    \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.6.4:compile
{code}

*AFTER*
{code}
$ mvn dependency:tree -Phadoop-2.7 -Dincludes=org.apache.hadoop:hadoop-mapreduce-client-core
...
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Catalyst 2.4.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:3.0.2:tree (default-cli) @ spark-catalyst_2.11 ---
[INFO] org.apache.spark:spark-catalyst_2.11:jar:2.4.0-SNAPSHOT
[INFO] \- org.apache.spark:spark-core_2.11:jar:2.4.0-SNAPSHOT:compile
[INFO]    \- org.apache.hadoop:hadoop-client:jar:2.7.3:compile
[INFO]       \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3:compile
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project SQL 2.4.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:3.0.2:tree (default-cli) @ spark-sql_2.11 ---
[INFO] org.apache.spark:spark-sql_2.11:jar:2.4.0-SNAPSHOT
[INFO] \- org.apache.spark:spark-core_2.11:jar:2.4.0-SNAPSHOT:compile
[INFO]    \- org.apache.hadoop:hadoop-client:jar:2.7.3:compile
[INFO]       \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3:compile
{code}",,apachespark,dongjoon,megaserg,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 14 02:35:27 UTC 2018,,,,,,,,,,"0|i3qr7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/18 18:02;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20704;;;","02/Mar/18 01:28;vanzin;Issue resolved by pull request 20704
[https://github.com/apache/spark/pull/20704];;;","14/Apr/18 02:35;megaserg;Thank you [~dongjoon]! This was also affecting our Spark job performance!

We're using \{{mapreduce.fileoutputcommitter.algorithm.version=2}} in our Spark job config, as recommended e.g. here: [http://spark.apache.org/docs/latest/cloud-integration.html]. We're using user-provided Hadoop 2.9.0.

However, since this 2.6.5 JAR was in spark/jars, it was given priority in the classpath over Hadoop-distributed 2.9.0 JAR. The 2.6.5 was silently ignoring the \{{mapreduce.fileoutputcommitter.algorithm.version}} setting and used the default, slow algorithm (I believe hadoop-mapreduce-client-core only had one, slow, algorithm until 2.7.0).

I believe this affects everyone who uses any mapreduce settings with Spark 2.3.0. Great job!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL unexpected behavior when comparing timestamp to date,SPARK-23549,13141815,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,djiangxu,djiangxu,01/Mar/18 14:53,02/Nov/18 02:52,13/Jul/23 08:45,25/Mar/18 23:40,1.6.3,2.0.2,2.1.2,2.2.1,2.3.0,,,,,2.4.0,,,,,SQL,,,,0,release_notes,,,"{code:java}
scala> spark.version

res1: String = 2.2.1

scala> spark.sql(""select cast('2017-03-01 00:00:00' as timestamp) between cast('2017-02-28' as date) and cast('2017-03-01' as date)"").show


+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

|((CAST(CAST(2017-03-01 00:00:00 AS TIMESTAMP) AS STRING) >= CAST(CAST(2017-02-28 AS DATE) AS STRING)) AND (CAST(CAST(2017-03-01 00:00:00 AS TIMESTAMP) AS STRING) <= CAST(CAST(2017-03-01 AS DATE) AS STRING)))|

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

|                                                                                                                                                                                                          false|

+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+{code}
As shown above, when a timestamp is compared to date in SparkSQL, both timestamp and date are downcast to string, and leading to unexpected result. If run the same SQL in presto/Athena, I got the expected result
{code:java}
select cast('2017-03-01 00:00:00' as timestamp) between cast('2017-02-28' as date) and cast('2017-03-01' as date)
 	_col0
1	true
{code}

Is this a bug for Spark or a feature?",,apachespark,cane,djiangxu,dongjoon,emlyn,kiszk,mgaido,Sathiya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 02:03:51 UTC 2018,,,,,,,,,,"0|i3qqwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/18 22:12;djiangxu;Tested in spark 2.3.0, same thing;;;","08/Mar/18 14:06;cane;I think this is a bug.Which may caused by rule below:(I have not run any test,just guessing)
{code:java}
case p @ BinaryComparison(left, right)
        if findCommonTypeForBinaryComparison(left.dataType, right.dataType).isDefined =>
        val commonType = findCommonTypeForBinaryComparison(left.dataType, right.dataType).get
        p.makeCopy(Array(castExpr(left, commonType), castExpr(right, commonType)))
{code}
findCommonTypeForBinaryComparison will return StringType:
{code:java}
case (DateType, TimestampType) => Some(StringType)
{code}
May be we can add a new rule for this case?



;;;","08/Mar/18 14:46;kiszk;I think that this is a problem in Spark.
My question is what should be the result for the following query?
select cast('2017-03-01 23:59:59' as timestamp) between cast('2017-02-28' as date) and cast('2017-03-01' as date);;;","08/Mar/18 14:53;djiangxu;[~kiszk], I expect your query to return false, as presto/Athena does.
A date in SQL is typically thought of equivalent to timestamp at 00:00:00;;;","08/Mar/18 15:26;kiszk;I see. Make sense. It would be good to cast `TimestampType` when we compare `DateType` with `TimestampType`.
 I will submit a PR.;;;","08/Mar/18 18:27;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20774;;;","08/Mar/18 20:24;dongjoon;Thank you for reporting and making a patch for this, [~djiangxu] and [~kiszk].
I checked Hive and saw that Hive 1.2~2.1 also had this issue and it's fixed at Hive 2.0.0.;;;","30/Apr/18 12:08;emlyn;Will this be included in Spark 2.3.1? It only says 2.4.0;;;","30/Apr/18 21:57;dongjoon;[~emlyn]. This issue changes the behavior and introduces new conf `spark.sql.hive.compareDateTimestampInTimestamp`. I don't think this will be included in Spark 2.3.1.;;;","21/Sep/18 02:03;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup the .pipeout file when the Hive Session closed,SPARK-23547,13141771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,01/Mar/18 12:02,13/Mar/18 18:32,13/Jul/23 08:45,13/Mar/18 18:32,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"  !2018-03-07_121010.png!

 

when the hive session closed, we should also cleanup the .pipeout file.

 ",,apachespark,cloud_fan,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/18 05:53;zuo.tingbing9;2018-03-07_121010.png;https://issues.apache.org/jira/secure/attachment/12913325/2018-03-07_121010.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 13 18:32:03 UTC 2018,,,,,,,,,,"0|i3qqnb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/18 12:21;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20702;;;","13/Mar/18 18:32;cloud_fan;Issue resolved by pull request 20702
[https://github.com/apache/spark/pull/20702];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE CHANGE COLUMN COMMENT doesn't work for external hive table,SPARK-23525,13141192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,pavlo.skliar,pavlo.skliar,27/Feb/18 11:13,02/Jan/19 23:43,13/Jul/23 08:45,07/Mar/18 21:53,2.2.0,2.3.0,,,,,,,,2.2.2,2.3.1,2.4.0,,,SQL,,,,0,,,,"{code:java}
print(spark.sql(""""""
SHOW CREATE TABLE test.trends
"""""").collect()[0].createtab_stmt)

/// OUTPUT
CREATE EXTERNAL TABLE `test`.`trends`(`id` string COMMENT '', `metric` string COMMENT '', `amount` bigint COMMENT '')
COMMENT ''
PARTITIONED BY (`date` string COMMENT '')
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
WITH SERDEPROPERTIES (
  'serialization.format' = '1'
)
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION 's3://xxxxx/xxxxx/xxxx'
TBLPROPERTIES (
  'transient_lastDdlTime' = '1519729384',
  'last_modified_time' = '1519645652',
  'last_modified_by' = 'pavlo',
  'last_castor_run_ts' = '1513561658.0'
)


spark.sql(""""""
DESCRIBE test.trends
"""""").collect()

// OUTPUT
[Row(col_name='id', data_type='string', comment=''),
 Row(col_name='metric', data_type='string', comment=''),
 Row(col_name='amount', data_type='bigint', comment=''),
 Row(col_name='date', data_type='string', comment=''),
 Row(col_name='# Partition Information', data_type='', comment=''),
 Row(col_name='# col_name', data_type='data_type', comment='comment'),
 Row(col_name='date', data_type='string', comment='')]


spark.sql(""""""alter table test.trends change column id id string comment 'unique identifier'"""""")


spark.sql(""""""
DESCRIBE test.trends
"""""").collect()

// OUTPUT
[Row(col_name='id', data_type='string', comment=''), Row(col_name='metric', data_type='string', comment=''), Row(col_name='amount', data_type='bigint', comment=''), Row(col_name='date', data_type='string', comment=''), Row(col_name='# Partition Information', data_type='', comment=''), Row(col_name='# col_name', data_type='data_type', comment='comment'), Row(col_name='date', data_type='string', comment='')]
{code}
The strange is that I've assigned comment to the id field from hive successfully, and it's visible in Hue UI, but it's still not visible in from spark, and any spark requests doesn't have effect on the comments.

 ",,apachespark,cloud_fan,jiangxb1987,pavlo.skliar,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 02:57:04 UTC 2018,,,,,,,,,,"0|i3qn33:",9223372036854775807,,,,,jiangxb1987,,,,,,,,,,,,,,,,,,"27/Feb/18 12:56;jiangxb1987;Thank you for reporting this. I believe the bug is caused by: https://github.com/apache/spark/blob/8077bb04f350fd35df83ef896135c0672dc3f7b0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala#L613;;;","27/Feb/18 13:06;jiangxb1987;I'm working on a fix for it, and will try to backport the fix to 2.2.;;;","27/Feb/18 13:12;pavlo.skliar;Unbelievably quick response as for an open-sourced project. so appreciated, thanks;;;","28/Feb/18 15:39;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20696;;;","07/Mar/18 21:53;cloud_fan;Issue resolved by pull request 20696
[https://github.com/apache/spark/pull/20696];;;","08/Mar/18 02:57;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Big local shuffle blocks should not be checked for corruption.,SPARK-23524,13141168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,27/Feb/18 09:55,08/Mar/18 04:09,13/Jul/23 08:45,08/Mar/18 04:09,2.2.1,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,0,,,,"In current code, all local blocks will be checked for corruption no matter it's big or not.  The reasons are as below:
 # Size in FetchResult for local block is set to be 0 ([https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L327)]
 # SPARK-4105 meant to only check the small blocks(size<maxBytesInFlight/3), but for reason 1, below check will be invalid. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L420",,apachespark,cloud_fan,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 04:09:44 UTC 2018,,,,,,,,,,"0|i3qmxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/18 10:04;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/20685;;;","08/Mar/18 04:09;cloud_fan;Issue resolved by pull request 20685
[https://github.com/apache/spark/pull/20685];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result caused by the rule OptimizeMetadataOnlyQuery,SPARK-23523,13141118,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,27/Feb/18 05:09,02/Mar/20 19:53,13/Jul/23 08:45,27/Feb/18 16:45,2.1.0,2.1.2,2.2.1,2.3.0,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,correctness,,,"{code:scala}
 val tablePath = new File(s""${path.getCanonicalPath}/cOl3=c/cOl1=a/cOl5=e"")
 Seq((""a"", ""b"", ""c"", ""d"", ""e"")).toDF(""cOl1"", ""cOl2"", ""cOl3"", ""cOl4"", ""cOl5"")
 .write.json(tablePath.getCanonicalPath)
 val df = spark.read.json(path.getCanonicalPath).select(""CoL1"", ""CoL5"", ""CoL3"").distinct()
 df.show()
{code}

This returns a wrong result 
{{[c,e,a]}}",,apachespark,guoxiaolongzte,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15752,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 07 23:49:05 UTC 2018,,,,,,,,,,"0|i3qmmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/18 05:25;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20684;;;","28/Feb/18 12:48;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20693;;;","07/Mar/18 23:49;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark should always use sys.exit rather than exit,SPARK-23522,13141109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,benjaminp,benjaminp,benjaminp,27/Feb/18 03:30,12/Dec/22 18:10,13/Jul/23 08:45,08/Mar/18 11:40,2.2.1,,,,,,,,,2.4.0,,,,,PySpark,,,,0,,,,"pyspark uses the builtin exit() function a lot. In certain Python environments exit is not is not defined as a builtin. E.g.,
{code}
$ python -c ""exit(4)""
$ python -S -c ""exit(4)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
NameError: name 'exit' is not defined
{code}
exit() as a builtin is really only intended for interactive use. Real code should always use sys.exit.",,apachespark,benjaminp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 11:40:28 UTC 2018,,,,,,,,,,"0|i3qmkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/18 03:33;apachespark;User 'benjaminp' has created a pull request for this issue:
https://github.com/apache/spark/pull/20682;;;","08/Mar/18 11:40;gurwls223;Fixed in https://github.com/apache/spark/pull/20682;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Create View Commands Fails with  The view output (col1,col1) contains duplicate column name",SPARK-23519,13141079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hem1891,tafranky@gmail.com,tafranky@gmail.com,27/Feb/18 00:06,08/Nov/19 06:41,13/Jul/23 08:45,28/Aug/19 04:11,2.2.1,,,,,,,,,2.4.5,3.0.0,,,,SQL,,,,0,,,,"1- create and populate a hive table  . I did this in a hive cli session .[ not that this matters ]

create table  atable (col1 int) ;

insert  into atable values (10 ) , (100)  ;

2. create a view from the table.  

[These actions were performed from a spark shell ]

spark.sql(""create view  default.aview  (int1 , int2 ) as select  col1 , col1 from atable "")
 java.lang.AssertionError: assertion failed: The view output (col1,col1) contains duplicate column name.
 at scala.Predef$.assert(Predef.scala:170)
 at org.apache.spark.sql.execution.command.ViewHelper$.generateViewProperties(views.scala:361)
 at org.apache.spark.sql.execution.command.CreateViewCommand.prepareTable(views.scala:236)
 at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:174)
 at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
 at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
 at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)",,cloud_fan,emaynard1121,hem1891,maropu,mgaido,RohitSindhu,shahid,tafranky@gmail.com,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/18 17:48;tafranky@gmail.com;image-2018-05-10-10-48-57-259.png;https://issues.apache.org/jira/secure/attachment/12922880/image-2018-05-10-10-48-57-259.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 01 05:56:41 UTC 2019,,,,,,,,,,"0|i3qmdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/18 07:08;tafranky@gmail.com;Any updates on this ?

Could someone assist with this?;;;","02/Apr/18 07:08;shahid;[~tafranky@gmail.com] You can try,

 spark.sql(""create view  default.aview  (int1 , int2 ) as select  col1 as a, col1 as b from atable "");;;","18/Apr/18 20:35;tafranky@gmail.com;thanks for the suggestion [~shahid]

The issue with your suggestion is that I dynamically generate the create view statement  ;Moreover   the select statement is kind of Opaque to me because it is provided by the customer. 

 

It would be nice is spark could fix such a simple  case.;;;","24/Apr/18 14:24;emaynard1121;Why is the fact that you dynamically generate the statement mean that you can't alias the columns in your select statement? You can generate aliases as well. This seems like a non-issue.;;;","10/May/18 06:37;maropu;I think typical databases can't use duplicate column names in views, e.g.,
{code:java}

postgres=# create table r(c1 int);
postgres=# create temporary view v as select c1, c1 from r;
ERROR: column ""c1"" specified more than once

{code};;;","10/May/18 17:49;tafranky@gmail.com;I do not agree with the 'typical database' claim . 

mysql , oracle  , hive support this  syntax. 

 

example

!image-2018-05-10-10-48-57-259.png!;;;","14/Aug/19 21:38;tafranky@gmail.com;Ok Spark Community 

I am sorry for being a pest about this , but I re-opening this Jira because I really believe that this should be addressed . 

Right now I do not have any way satisfying my  customer's requirement . 

My current use case is the following . 

My customer can provide any customer  Hive query . I am oblivious to the actually content of the query and parsing the query is not an option .

All I know if the number of fields projected from the customer query and the type of those fields . 

I do not know the name of the fields projected from the custom query.

What is currently  do with spark sql is run a  query of the form . 

Create view view_name ;;;","21/Aug/19 21:13;hem1891;View creation currently uses analyzed plan which is built using only the select part of command and it doesn't use aliases provided in query. I checked and found that before [PR-16613|https://github.com/apache/spark/pull/16613/files] this used to be handled by aliasing and using aliasedPlan instead of analyzedPlan.

I tried applying those changes but now, CheckAnalysis fails because of some other fix. I am working on this.

Hey [~jiangxb], [~cloud_fan] any thoughts on why it was removed in PR-16613?;;;","23/Aug/19 02:06;cloud_fan;I think this is a bug and should be fixed. cc [~viirya] do you have any clues about this bug?;;;","23/Aug/19 04:34;viirya;Thanks for pinging me.

I am going on a flight soon. If this is not urgent, I can look into it after today.;;;","23/Aug/19 06:11;hem1891;I have a fix for this. checkColumnNameDuplication is checking analyzed schema(id, id) whereas it should be checking aliased schema(int1, int2). I got it to work. I will run tests and submit a PR.;;;","23/Aug/19 21:49;hem1891;PR raised [https://github.com/apache/spark/pull/25570] Someone please review.;;;","26/Aug/19 14:07;viirya;I test with Hive 2.1. It doesn't support duplicate column names:
{code:java}
hive> create view test_view (c1, c2) as select c1, c1 from test;
FAILED: SemanticException [Error 10036]: Duplicate column name: c1
{code}
[~tafranky@gmail.com] you said Hive supports it, is newer versions of Hive supporting this?;;;","27/Aug/19 04:09;tafranky@gmail.com;[~viirya]

My mistake , i tested it with Oracle and MySql . I then assume that hive would honor the same . 

You are correct.  Hive does not appear to support this after all . 

 

I need to test this on newer versions of Hive. ;;;","28/Aug/19 04:11;cloud_fan;Issue resolved by pull request 25570
[https://github.com/apache/spark/pull/25570];;;","01/Sep/19 05:56;viirya;This was closed and then reopened and fixed. The label [bulk-closed|https://issues.apache.org/jira/issues/?jql=labels+%3D+bulk-closed] looks not correct. I remove it. Feel free to add it back if I misunderstand it.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make pyspark.util._exception_message produce the trace from Java side for Py4JJavaError,SPARK-23517,13140955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,26/Feb/18 15:47,12/Dec/22 18:11,13/Jul/23 08:45,28/Feb/18 15:44,2.3.0,,,,,,,,,2.3.1,,,,,PySpark,,,,0,,,,"Currently {{pyspark.util._exception_message}} doesn't show its trace and message from Py4JJavaError as below:

{code}
>>> from pyspark.util import _exception_message
>>> try:
...     sc._jvm.java.lang.String(None)
... except Exception as e:
...     pass
...
>>> e.message
''
{code}

This is actually a problem in some code paths we can expect this error. For example, see

{code}
from pyspark.sql.functions import udf
spark.conf.set(""spark.sql.execution.arrow.enabled"", True)
spark.range(1).select(udf(lambda x: [[]])()).toPandas()
{code}

{code}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 2009, in toPandas
    raise RuntimeError(""%s\n%s"" % (_exception_message(e), msg))
RuntimeError:
Note: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true. Please set it to false to disable this.
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 28 15:44:59 UTC 2018,,,,,,,,,,"0|i3qlmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/18 15:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/20680;;;","28/Feb/18 15:44;gurwls223;Issue resolved by pull request 20680
[https://github.com/apache/spark/pull/20680];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace spark.sparkContext.hadoopConfiguration by spark.sessionState.newHadoopConf(),SPARK-23514,13140772,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,smilegator,smilegator,25/Feb/18 07:55,02/Mar/18 17:22,13/Jul/23 08:45,28/Feb/18 16:45,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Check all the places where we directly use {{spark.sparkContext.hadoopConfiguration}}. Instead, in some scenarios, it makes more sense to call {{spark.sessionState.newHadoopConf()}} which blends in settings from SQLConf.
",,apachespark,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 17:22:05 UTC 2018,,,,,,,,,,"0|i3qkiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/18 07:56;smilegator;cc [~dongjoon] Do you want to make a try?;;;","26/Feb/18 15:29;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/20679;;;","02/Mar/18 17:22;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/20718;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
blockManagerIdCache in BlockManagerId may cause oom,SPARK-23508,13140706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cane,cane,cane,24/Feb/18 10:55,28/Feb/18 15:17,13/Jul/23 08:45,28/Feb/18 15:17,2.1.1,2.2.1,,,,,,,,2.2.2,2.3.1,2.4.0,,,Deploy,Spark Core,,,0,,,,"blockManagerIdCache in BlockManagerId will not remove old values which may cause oom
{code:java}
val blockManagerIdCache = new ConcurrentHashMap[BlockManagerId, BlockManagerId]()
{code}
Since whenever we apply a new BlockManagerId, it will put into this map.

below is an jmap:

!elepahnt-oom1.png!

!elephant-oom.png!",,apachespark,cane,cloud_fan,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/18 10:57;cane;elepahnt-oom1.png;https://issues.apache.org/jira/secure/attachment/12911897/elepahnt-oom1.png","24/Feb/18 10:57;cane;elephant-oom.png;https://issues.apache.org/jira/secure/attachment/12911898/elephant-oom.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 28 15:17:59 UTC 2018,,,,,,,,,,"0|i3qk47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/18 11:02;apachespark;User 'caneGuy' has created a pull request for this issue:
https://github.com/apache/spark/pull/20667;;;","28/Feb/18 15:17;cloud_fan;Issue resolved by pull request 20667
[https://github.com/apache/spark/pull/20667];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Locality of coalesced partitions can be severely skewed by the order of input partitions,SPARK-23496,13140512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,23/Feb/18 14:30,05/Mar/18 13:33,13/Jul/23 08:45,05/Mar/18 13:33,3.0.0,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"Example:

Consider RDD ""R"" with 100 partitions, half of which have locality preference ""hostA"" and half have ""hostB"".
 * Assume odd-numbered input partitions of R prefer ""hostA"" and even-numbered prefer ""hostB"". Then R.coalesce(50) will have 25 partitions with preference ""hostA"" and 25 with ""hostB"" (even distribution).
 * Assume partitions with index 0-49 of R prefer ""hostA"" and partitions with index 50-99 prefer ""hostB"". Then R.coalesce(50) will have 49 partitions with ""hostA"" and 1 with ""hostB"" (extremely skewed distribution).

 

The algorithm in {{DefaultPartitionCoalescer.setupGroups}} is responsible for picking preferred locations for coalesced partitions. It analyzes the preferred locations of input partitions. It starts by trying to create one partition for each unique location in the input. However, if the the requested number of coalesced partitions is higher that the number of unique locations, it has to pick duplicate locations.

Currently, the duplicate locations are picked by iterating over the input partitions in order, and copying their preferred locations to coalesced partitions. If the input partitions are clustered by location, this can result in severe skew.

Instead of iterating over the list of input partitions in order, we should pick them at random.",,ala.luszczak,apachespark,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 23 15:50:44 UTC 2018,,,,,,,,,,"0|i3qix3:",9223372036854775807,,,,,hvanhovell,,,,,,,,,,,,,,,,,,"23/Feb/18 14:46;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/20664;;;","23/Feb/18 14:52;mgaido;I read that the proposed solution is to use random numbers instead of iterating. This seems to me not as a solution but as a workaround, which doesn't solve the problem, but it makes it unlikely.

What about a solutions which does solve the problem, ie. we enforce an even distribution according to the incoming data distribution? I mean, what about creating a sort of reversed index with the preferred location as key and the partition as values and picking from each value list a ratio corresponding to the coalescing ratio?;;;","23/Feb/18 15:37;ala.luszczak;I agree that this solution is merely making the problem unlikely to occur, instead of really solving it.

But the code in {{DefaultPartitionsCoalescer.setGroups()}} ([see comment|https://github.com/ala/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala#L234-L240]) is deliberately written so that it's fast (O(n log n) with respect to number of coalesced partitions, which is assumed order of magnitude smaller than the number of input partitions), but not necessarily accurate. The same applies to other algorithms there.

Enforcing an even data distribution is not trivial. For example:
 * We merely look at the number of partitions, not on the number of rows in each of the partitions. There might be a severe skew across the partitions to begin with.
 * It's not clear how to treat partitions with multiple preferred location.
 * It's not clear if it's more important for every input location to find some matching coalesced partition, or if it's more important to keep the partition size even.
 * It's not clear how best to deal with a mix of partitions with and without locality preferences.

I think it's better to have a very simple fix that will work well vast majority of the time now, and maybe have a follow-up ticked for revisiting the design of {{DefaultPartitionCoalescer}} for later.;;;","23/Feb/18 15:50;mgaido;[~ala.luszczak] thanks for your answer. Honestly I don't see any value in a quick fix and revisiting it later. Since this won't come in Spark 2.3 and next release won't be out very soon, I think we have the time to design a final solution now. Anyway, I do agree with the objections you raised, ie. that there are so many factors to take in account that it is hard to define a method which works properly in all conditions. So, the proposed fix might be ok. But I would be happy to hear also opinions from other people in the community about this topic.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check storage.locationUri with existing table in CreateTable ,SPARK-23490,13140227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,22/Feb/18 15:34,08/Mar/18 05:53,13/Jul/23 08:45,23/Feb/18 05:51,2.3.1,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"For CreateTable with Append mode, we should check if `storage.locationUri` is the same with existing table in `PreprocessTableCreation`

In the current code, there is only a simple exception if the `storage.locationUri` is different with existing table:`org.apache.spark.sql.AnalysisException: Table or view not found:`

which can be improved.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 02:17:04 UTC 2018,,,,,,,,,,"0|i3qh5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/18 15:39;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20660;;;","08/Mar/18 02:17;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20766;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test: HiveExternalCatalogVersionsSuite,SPARK-23489,13140199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,mgaido,mgaido,22/Feb/18 14:16,15/Jul/18 21:04,13/Jul/23 08:45,03/May/18 07:17,2.2.1,2.3.0,2.4.0,,,,,,,2.2.2,2.3.1,2.4.0,,,SQL,Tests,,,0,,,,"I saw this error in an unrelated PR. It seems to me a bad configuration in the Jenkins node where the tests are run.

{code}
Error Message
java.io.IOException: Cannot run program ""./bin/spark-submit"" (in directory ""/tmp/test-spark/spark-2.2.1""): error=2, No such file or directory
Stacktrace
sbt.ForkMain$ForkError: java.io.IOException: Cannot run program ""./bin/spark-submit"" (in directory ""/tmp/test-spark/spark-2.2.1""): error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.sql.hive.SparkSubmitTestUtils$class.runSparkSubmit(SparkSubmitTestUtils.scala:73)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite.runSparkSubmit(HiveExternalCatalogVersionsSuite.scala:43)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite$$anonfun$beforeAll$1.apply(HiveExternalCatalogVersionsSuite.scala:176)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite$$anonfun$beforeAll$1.apply(HiveExternalCatalogVersionsSuite.scala:161)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite.beforeAll(HiveExternalCatalogVersionsSuite.scala:161)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:480)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:248)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 17 more
{code}

This is the link: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87615/testReport/.

*MASTER BRANCH*
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/4389

*BRANCH 2.3*
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.6/321/

*NOTE: This failure frequently looks as `Test Result (no failures)`*
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.6/4811/
",,apachespark,cloud_fan,dongjoon,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,SPARK-24813,,,SPARK-22654,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 19:42:05 UTC 2018,,,,,,,,,,"0|i3qgzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/18 05:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21210;;;","03/May/18 07:17;cloud_fan;Issue resolved by pull request 21210
[https://github.com/apache/spark/pull/21210];;;","03/May/18 19:42;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21232;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LookupFunctions should not check the same function name more than once,SPARK-23486,13140082,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kevinyu98,lian cheng,lian cheng,22/Feb/18 01:13,13/Jul/18 05:19,13/Jul/23 08:45,13/Jul/18 05:19,2.2.1,2.3.0,,,,,,,,2.4.0,,,,,SQL,,,,0,starter,,,"For a query invoking the same function multiple times, the current {{LookupFunctions}} rule performs a check for each invocation. For users using Hive metastore as external catalog, this issues unnecessary metastore accesses and can slow down the analysis phase quite a bit.",,apachespark,kevinyu98,LANDAIS Christophe,lian cheng,maropu,mgaido,vanzin,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19737,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 13 10:17:08 UTC 2018,,,,,,,,,,"0|i3qg9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/18 01:15;lian cheng;Please refer to [this comment|https://issues.apache.org/jira/browse/SPARK-19737?focusedCommentId=16371377&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16371377] for more details.;;;","23/Feb/18 03:05;kevinyu98;Hello [~lian cheng]: I think the easy way is to build a hash map around the LookupFunctions, if the function exists in the external catalog,  put into the hash map for the first time, next time when call the LookupFunctions, first check the hash map to avoid the metastore accesses, does this approach look ok to you? If you think it is ok, I can provide a pr for reviewing. Thanks.

Another approach is to cache the external catalog functions in the share state,  many queries can use, but it will be more involved to do the invalidation.  ;;;","11/Mar/18 07:04;apachespark;User 'kevinyu98' has created a pull request for this issue:
https://github.com/apache/spark/pull/20795;;;","17/May/18 18:41;vanzin;I removed the target versions since the PR seems stalled in review.;;;","13/Jun/18 10:17;LANDAIS Christophe;Hi,

 

What is the process to have the issue corrected and submitted ?

 

Thanks and BR,

Christophe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix possible race condition in KafkaContinuousReader,SPARK-23484,13140043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tdas,tdas,tdas,21/Feb/18 22:14,13/Jul/18 05:29,13/Jul/23 08:45,21/Feb/18 22:56,2.3.0,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,0,,,,"var `KafkaContinuousReader.knownPartitions` should be threadsafe as it is accessed from multiple threads - the query thread at the time of reader factory creation, and the epoch tracking thread at the time of `needsReconfiguration`.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 21 22:56:35 UTC 2018,,,,,,,,,,"0|i3qg0v:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"21/Feb/18 22:16;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20655;;;","21/Feb/18 22:56;tdas;Issue resolved by pull request 20655
[https://github.com/apache/spark/pull/20655];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The job page shows wrong stages when some of stages are evicted,SPARK-23481,13139983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,21/Feb/18 19:14,21/Feb/18 23:37,13/Jul/23 08:45,21/Feb/18 23:37,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Run ""bin/spark-shell --conf spark.ui.retainedJobs=10 --conf spark.ui.retainedStages=10"", type the following codes and click the job 19 page, it will show wrong stage ids:

{code}
val rdd = sc.parallelize(0 to 100, 100).repartition(10).cache()

(1 to 20).foreach { i =>
   rdd.repartition(10).count()
}
{code}

Please see the attached screenshots.",,apachespark,sameerag,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 19:15;zsxwing;Screen Shot 2018-02-21 at 12.39.46 AM.png;https://issues.apache.org/jira/secure/attachment/12911416/Screen+Shot+2018-02-21+at+12.39.46+AM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 21 23:37:49 UTC 2018,,,,,,,,,,"0|i3qfnj:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"21/Feb/18 19:34;vanzin;[~zsxwing] you assigned this to yourself so I suppose you were working on a patch?

Anyway, this seems to fix it:

{code}
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala b/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala
index efc2853..3990f9c 100644
--- a/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala
@@ -96,7 +96,7 @@ private[spark] class AppStatusStore(
 
   def lastStageAttempt(stageId: Int): v1.StageData = {
     val it = store.view(classOf[StageDataWrapper]).index(""stageId"").reverse().first(stageId)
-      .closeableIterator()
+      .last(stageId).closeableIterator()
     try {
       if (it.hasNext()) {
         it.next().info
{code}
;;;","21/Feb/18 19:35;zsxwing;[~vanzin] Yep. Here is the fix with a regression test: https://github.com/apache/spark/pull/20654;;;","21/Feb/18 19:38;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20654;;;","21/Feb/18 23:37;zsxwing;Issue resolved by pull request 20654
[https://github.com/apache/spark/pull/20654];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark will not start in local mode with authentication on,SPARK-23476,13139846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,21/Feb/18 09:22,22/Feb/18 20:08,13/Jul/23 08:45,22/Feb/18 20:08,2.3.0,,,,,,,,,2.4.0,,,,,Spark Core,Spark Shell,,,0,,,,"If spark is run with ""spark.authenticate=true"", then it will fail to start in local mode.
{noformat}
17/02/03 12:09:39 ERROR spark.SparkContext: Error initializing SparkContext.
java.lang.IllegalArgumentException: Error: a secret key must be specified via the spark.authenticate.secret config
	at org.apache.spark.SecurityManager.generateSecretKey(SecurityManager.scala:401)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:221)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:258)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:199)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:290)
...
{noformat}
It can be confusing when authentication is turned on by default in a cluster, and one tries to start spark in local mode for a simple test.

*Workaround*: If {{spark.authenticate=true}} is specified as a cluster wide config, then the following has to be added
{{--conf ""spark.authenticate=false"" --conf ""spark.shuffle.service.enabled=false"" --conf ""spark.dynamicAllocation.enabled=false"" --conf ""spark.network.crypto.enabled=false"" --conf ""spark.authenticate.enableSaslEncryption=false""}}
in the spark-submit command.",,apachespark,gsomogyi,mgaido,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 22 20:08:17 UTC 2018,,,,,,,,,,"0|i3qetj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 09:22;gsomogyi;I'm working on it.;;;","21/Feb/18 14:26;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20652;;;","22/Feb/18 20:08;vanzin;Issue resolved by pull request 20652
[https://github.com/apache/spark/pull/20652];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The ""stages"" page doesn't show any completed stages",SPARK-23475,13139839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,21/Feb/18 08:40,24/Feb/18 02:28,13/Jul/23 08:45,22/Feb/18 03:44,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Run ""bin/spark-shell --conf spark.ui.retainedJobs=10 --conf spark.ui.retainedStages=10"", type the following codes and click the ""stages"" page, it will not show completed stages:

{code}
val rdd = sc.parallelize(0 to 100, 100).repartition(10).cache()

(1 to 20).foreach { i =>
   rdd.repartition(10).count()
}
{code}

Please see the attached screenshots.",,apachespark,mgaido,sameerag,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 08:40;zsxwing;Screen Shot 2018-02-21 at 12.39.39 AM.png;https://issues.apache.org/jira/secure/attachment/12911354/Screen+Shot+2018-02-21+at+12.39.39+AM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 24 02:28:31 UTC 2018,,,,,,,,,,"0|i3qerz:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"21/Feb/18 08:41;zsxwing;cc [~vanzin];;;","21/Feb/18 12:45;mgaido;The reason of this behavior is that SKIPPED stages, which were previously shown in the PENDING table, are not shown anymore. This was introduced by SPARK-20648. I will submit a fix soon.;;;","21/Feb/18 13:03;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20651;;;","21/Feb/18 19:29;zsxwing;The job page issue is a separated issue. Created SPARK-23481 to track it instead.;;;","21/Feb/18 22:43;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20656;;;","22/Feb/18 19:02;vanzin;Marco's PR was also merged into master:
https://github.com/apache/spark/pull/20651;;;","23/Feb/18 09:33;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20662;;;","23/Feb/18 13:50;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20663;;;","24/Feb/18 02:28;vanzin;https://github.com/apache/spark/pull/20662 was merged to 2.3, but not in time for rc5, so probably will only be in 2.3.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.spark.ui.jobs.ApiHelper.lastStageNameAndDescription is too slow,SPARK-23470,13139746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,zsxwing,zsxwing,20/Feb/18 19:25,21/Feb/18 01:55,13/Jul/23 08:45,21/Feb/18 01:55,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"I was testing 2.3.0 RC3 and found that it's easy to hit ""read timeout"" when accessing All Jobs page. The stack dump says it was running ""org.apache.spark.ui.jobs.ApiHelper.lastStageNameAndDescription"".

{code}
""SparkUI-59"" #59 daemon prio=5 os_prio=0 tid=0x00007fc15b0a3000 nid=0x8dc runnable [0x00007fc0ce9f8000]
   java.lang.Thread.State: RUNNABLE
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.util.kvstore.KVTypeInfo$MethodAccessor.get(KVTypeInfo.java:154)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.compare(InMemoryStore.java:248)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.lambda$iterator$2(InMemoryStore.java:214)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView$$Lambda$36/1834982692.compare(Unknown Source)
	at java.util.TimSort.binarySort(TimSort.java:296)
	at java.util.TimSort.sort(TimSort.java:239)
	at java.util.Arrays.sort(Arrays.java:1512)
	at java.util.ArrayList.sort(ArrayList.java:1460)
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:387)
	at java.util.stream.Sink$ChainedReference.end(Sink.java:258)
	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:210)
	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161)
	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300)
	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.hasNext(InMemoryStore.java:278)
	at org.apache.spark.status.AppStatusStore.lastStageAttempt(AppStatusStore.scala:101)
	at org.apache.spark.ui.jobs.ApiHelper$$anonfun$38.apply(StagePage.scala:1014)
	at org.apache.spark.ui.jobs.ApiHelper$$anonfun$38.apply(StagePage.scala:1014)
	at org.apache.spark.status.AppStatusStore.asOption(AppStatusStore.scala:408)
	at org.apache.spark.ui.jobs.ApiHelper$.lastStageNameAndDescription(StagePage.scala:1014)
	at org.apache.spark.ui.jobs.JobDataSource.org$apache$spark$ui$jobs$JobDataSource$$jobRow(AllJobsPage.scala:434)
	at org.apache.spark.ui.jobs.JobDataSource$$anonfun$24.apply(AllJobsPage.scala:412)
	at org.apache.spark.ui.jobs.JobDataSource$$anonfun$24.apply(AllJobsPage.scala:412)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.ui.jobs.JobDataSource.<init>(AllJobsPage.scala:412)
	at org.apache.spark.ui.jobs.JobPagedTable.<init>(AllJobsPage.scala:504)
	at org.apache.spark.ui.jobs.AllJobsPage.jobsTable(AllJobsPage.scala:246)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:295)
	at org.apache.spark.ui.WebUI$$anonfun$3.apply(WebUI.scala:98)
	at org.apache.spark.ui.WebUI$$anonfun$3.apply(WebUI.scala:98)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
{code}

According to the heap dump, there are 954 JobDataWrapper and 54690 StageDataWrapper. It's obvious that the UI will be slow since we need to sort 54690 items for 954 jobs.
",,apachespark,dongjoon,mgaido,sameerag,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23051,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 21 01:55:36 UTC 2018,,,,,,,,,,"0|i3qe7b:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"20/Feb/18 19:27;zsxwing;[~vanzin] [~cloud_fan];;;","20/Feb/18 19:31;vanzin;It should be pretty easy to make {{lastStageNameAndDescription}} faster. It doesn't need the last attempt, just any attempt of the stage, since the name and description should be the same. So it could just call {{store.stageAttempt(stageId, 0)}} instead of {{lastStageAttempt}}.;;;","20/Feb/18 19:51;zsxwing;[~vanzin] could you make a PR to fix it? I can help you test the patch.;;;","20/Feb/18 19:54;vanzin;Sure, I'll take a look in the PM.;;;","20/Feb/18 21:55;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20644;;;","21/Feb/18 01:55;sameerag;Issue resolved by https://github.com/apache/spark/pull/20644;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashingTF should use corrected MurmurHash3 implementation,SPARK-23469,13139745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,josephkb,josephkb,20/Feb/18 19:17,08/Jun/20 17:37,13/Jul/23 08:45,02/Aug/19 15:53,2.4.0,,,,,,,,,3.0.0,,,,,ML,,,,0,,,,"[SPARK-23381] added a corrected MurmurHash3 implementation but left the old implementation alone.  In Spark 2.3 and earlier, HashingTF will use the old implementation.  (We should not backport a fix for HashingTF since it would be a major change of behavior.)  But we should correct HashingTF in Spark 2.4; this JIRA is for tracking this fix.
* Update HashingTF to use new implementation of MurmurHash3
* Ensure backwards compatibility for ML persistence by having HashingTF use the old MurmurHash3 when a model from Spark 2.3 or earlier is loaded.  We can add a Param to allow this.

Also, HashingTF still calls into the old spark.mllib.feature.HashingTF, so I recommend we first migrate the code to spark.ml: [SPARK-21748].  We can leave spark.mllib alone and just fix MurmurHash3 in spark.ml.",,huaxingao,josephkb,kiszk,mgaido,Teng Peng,viirya,wm624,,,,,,,,,,,,,,,,,,,,,SPARK-21748,,,,,,,,,,,,,,SPARK-23381,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"In Spark 3.0, the HashingTF Transformer uses a corrected implementation of the murmur3 hash function to hash elements to vectors. HashingTF fit with Spark 3.0 will map elements to different positions in vectors than in Spark 2. However, HashingTF created with Spark 2.x and loaded with Spark 3.0 will still use the previous hash function and will not change behavior.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 15:53:58 UTC 2019,,,,,,,,,,"0|i3qe73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/19 01:57;huaxingao;I will work on this jira once PR https://github.com/apache/spark/pull/25250 (migrate the implementation of HashingTF from MLlib to ML) is merged. ;;;","02/Aug/19 15:53;srowen;Issue resolved by pull request 25303
[https://github.com/apache/spark/pull/25303];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to authenticate with old shuffle service,SPARK-23468,13139742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,20/Feb/18 19:02,23/Feb/18 06:27,13/Jul/23 08:45,21/Feb/18 02:06,2.3.0,,,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,"I ran into this while testing a fix for SPARK-23361. Things seem to work fine with a 2.x shuffle service, but with a 1.6 shuffle service I get this error every once in a while:

{noformat}
org.apache.spark.SparkException: Unable to register with external shuffle server due to : java.lang.RuntimeException: javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response.
        at org.spark-project.guava.base.Throwables.propagate(Throwables.java:160)
        at org.apache.spark.network.sasl.SparkSaslServer.response(SparkSaslServer.java:121)
        at org.apache.spark.network.sasl.SaslRpcHandler.receive(SaslRpcHandler.java:101)
        at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:154)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
{noformat}

This is a regression in 2.3 and I have a fix for it as part of the fix for SPARK-23361. I'm filing this separately so that this particular fix is backported to 2.3.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 21 02:06:51 UTC 2018,,,,,,,,,,"0|i3qe6f:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"20/Feb/18 20:41;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20643;;;","21/Feb/18 02:06;vanzin;Issue resolved by pull request 20643
[https://github.com/apache/spark/pull/20643];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message in `StructType`,SPARK-23462,13139322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xiayunsun,smilegator,smilegator,18/Feb/18 19:18,12/Dec/22 18:10,13/Jul/23 08:45,12/Mar/18 13:14,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,starter,,,"The error message {{s""""""Field ""$name"" does not exist.""""""}} is thrown when looking up an unknown field in StructType. In the error message, we should also contain the information about which columns/fields exist in this struct. ",,apachespark,kiszk,smilegator,xiayunsun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 12 13:14:38 UTC 2018,,,,,,,,,,"0|i3qbl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 09:13;xiayunsun;I will take this. ;;;","21/Feb/18 09:37;xiayunsun;PR created: [https://github.com/apache/spark/pull/20649] ;;;","21/Feb/18 09:41;apachespark;User 'xysun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20649;;;","12/Mar/18 13:14;gurwls223;Issue resolved by pull request 20649
[https://github.com/apache/spark/pull/20649];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vignettes should include model predictions for some ML models,SPARK-23461,13139286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,felixcheung,felixcheung,18/Feb/18 08:59,11/Jul/18 06:19,13/Jul/23 08:45,11/Jul/18 06:19,2.2.1,2.3.0,,,,,,,,2.4.0,,,,,SparkR,,,,0,,,,"eg. 

Linear Support Vector Machine (SVM) Classifier
h4. Logistic Regression

Tree - GBT, RF, DecisionTree

(and ALS was disabled)

By doing something like {{head(select(gmmFitted, ""V1"", ""V2"", ""prediction""))}}",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 30 02:35:04 UTC 2018,,,,,,,,,,"0|i3qbd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/18 02:35;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21678;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message when unknown column is specified in partition columns,SPARK-23459,13139274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,smilegator,smilegator,17/Feb/18 23:05,24/Feb/18 00:31,13/Jul/23 08:45,24/Feb/18 00:31,2.3.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,starter,,,"{noformat}
  test(""save with an unknown partition column"") {
    withTempDir { dir =>
      val path = dir.getCanonicalPath
        Seq(1L -> ""a"").toDF(""i"", ""j"").write
          .format(""parquet"")
          .partitionBy(""unknownColumn"")
          .save(path)
    }
  }
{noformat}

We got the following error message:
{noformat}
Partition column unknownColumn not found in schema StructType(StructField(i,LongType,false), StructField(j,StringType,true));
{noformat}
We should not call toString, but catalogString in the function `partitionColumnsSchema` of `PartitioningUtils.scala`


",,apachespark,kiszk,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 21 19:24:05 UTC 2018,,,,,,,,,,"0|i3qbaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/18 17:25;kiszk;Have you started working for this? If not, I can take it.;;;","21/Feb/18 14:27;kiszk;I am working for this.;;;","21/Feb/18 19:24;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20653;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Flaky test: OrcQuerySuite,SPARK-23458,13139260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,mgaido,mgaido,17/Feb/18 17:33,25/Jan/19 17:15,13/Jul/23 08:45,01/Jan/19 06:20,2.4.0,,,,,,,,,2.4.1,3.0.0,,,,SQL,Tests,,,0,,,,"Sometimes we have UT failures with the following stacktrace:


{code:java}
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 15 times over 10.013962218000001 seconds. Last failure message: There are 1 possibly leaked file streams..
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:421)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:439)
	at org.apache.spark.sql.execution.datasources.orc.OrcTest.eventually(OrcTest.scala:45)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:308)
	at org.apache.spark.sql.execution.datasources.orc.OrcTest.eventually(OrcTest.scala:45)
	at org.apache.spark.sql.test.SharedSparkSession$class.afterEach(SharedSparkSession.scala:114)
	at org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite.afterEach(OrcQuerySuite.scala:583)
	at org.scalatest.BeforeAndAfterEach$$anonfun$1.apply$mcV$sp(BeforeAndAfterEach.scala:234)
	at org.scalatest.Status$$anonfun$withAfterEffect$1.apply(Status.scala:379)
	at org.scalatest.Status$$anonfun$withAfterEffect$1.apply(Status.scala:375)
	at org.scalatest.SucceededStatus$.whenCompleted(Status.scala:454)
	at org.scalatest.Status$class.withAfterEffect(Status.scala:375)
	at org.scalatest.SucceededStatus$.withAfterEffect(Status.scala:426)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:232)
	at org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite.runTest(OrcQuerySuite.scala:583)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:52)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:52)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:480)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: java.lang.IllegalStateException: There are 1 possibly leaked file streams.
	at org.apache.spark.DebugFilesystem$.assertNoOpenStreams(DebugFilesystem.scala:54)
	at org.apache.spark.sql.test.SharedSparkSession$$anonfun$afterEach$1.apply$mcV$sp(SharedSparkSession.scala:115)
	at org.apache.spark.sql.test.SharedSparkSession$$anonfun$afterEach$1.apply(SharedSparkSession.scala:115)
	at org.apache.spark.sql.test.SharedSparkSession$$anonfun$afterEach$1.apply(SharedSparkSession.scala:115)
	at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:395)
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:409)
	... 42 more
Caused by: sbt.ForkMain$ForkError: java.lang.Throwable: null
	at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)
	at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
	at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.open(RecordReaderUtils.java:173)
	at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:254)
	at org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:633)
	at org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.initialize(OrcColumnarBatchReader.java:140)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:197)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:161)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:106)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1834)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2063)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2063)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	... 3 more
{code}

Please see https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87486/testReport/org.apache.spark.sql.execution.datasources.orc/OrcQuerySuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/.


- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-sbt-hadoop-2.6/4455/testReport/junit/org.apache.spark.sql.execution.datasources.orc/OrcQuerySuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/
",AMPLab Jenkins,cloud_fan,dongjoon,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,SPARK-24138,,,,,,,SPARK-23505,SPARK-23390,,SPARK-26427,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 01 06:20:50 UTC 2019,,,,,,,,,,"0|i3qb7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/18 17:34;mgaido;cc [~dongjoon];;;","24/Feb/18 18:34;dongjoon;I updated the title bacause the reported URL is OrcQuerySuite and added a link to `ParquetQuerySuite` because OrcQuerySuite `Enabling/disabling ignoreCorruptFiles` comes from `ParquetQuerySuite`. I'm looking at the following three together.
- ParquetyQuerySuite
- OrcQuerySuite
- FileBasedDataSourceSuite;;;","02/May/18 00:27;dongjoon;We reduced the number of points of failures by SPARK-23457 and SPARK-23399. But, these are still reported. I'll reinvestigate the followings together.

- SPARK-23458 (ORC)
- SPARK-23505 (Parquet)
- SPARK-23390 (FileBasedDataSource for both ORC/Parquet)

BTW, these are hidden by another several flaky test failures; Hive test suites, DirectKakfaStreamSuite, ReceiverSuite, DateTimeUtilsSuite.monthsBetween. We had better resolve all of these before 2.3.1 if possible.


cc [~smilegator];;;","07/May/18 16:01;smilegator;Yeah. [~dongjoon] Please investigate why they still fail. 

After your fix, I still found HiveExternalCatalogVersionsSuite never pass in this test branch. Do you know the reason? 
https://spark-tests.appspot.com/jobs/spark-master-test-sbt-hadoop-2.7

;;;","10/May/18 17:00;dongjoon;Oh, I missed your ping here, [~smilegator]. According to the given log, the remaining flakiness of HiveExternalCatalogVersionsSuite seems to be `Py4JJavaError`. It's weird.
{code}
2018-05-07 23:14:55.233 - stderr> SLF4J: Class path contains multiple SLF4J bindings.
2018-05-07 23:14:55.233 - stderr> SLF4J: Found binding in [jar:file:/tmp/test-spark/spark-2.0.2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2018-05-07 23:14:55.233 - stderr> SLF4J: Found binding in [jar:file:/home/sparkivy/per-executor-caches/4/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2018-05-07 23:14:55.233 - stderr> SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2018-05-07 23:14:55.233 - stderr> SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2018-05-07 23:14:55.532 - stdout> 23:14:55.532 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-05-07 23:14:57.982 - stdout> 23:14:57.982 WARN DataNucleus.General: Plugin (Bundle) ""org.datanucleus"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/tmp/test-spark/spark-2.0.2/jars/datanucleus-core-3.2.10.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/home/sparkivy/per-executor-caches/4/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar.""
2018-05-07 23:14:57.988 - stdout> 23:14:57.988 WARN DataNucleus.General: Plugin (Bundle) ""org.datanucleus.api.jdo"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/tmp/test-spark/spark-2.0.2/jars/datanucleus-api-jdo-3.2.6.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/home/sparkivy/per-executor-caches/4/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar.""
2018-05-07 23:14:57.99 - stdout> 23:14:57.990 WARN DataNucleus.General: Plugin (Bundle) ""org.datanucleus.store.rdbms"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/home/sparkivy/per-executor-caches/4/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/tmp/test-spark/spark-2.0.2/jars/datanucleus-rdbms-3.2.9.jar.""
2018-05-07 23:15:17.844 - stdout> 23:15:17.843 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2018-05-07 23:15:18.152 - stdout> 23:15:18.152 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
2018-05-07 23:15:22.32 - stdout> Traceback (most recent call last):
2018-05-07 23:15:22.32 - stdout>   File ""/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/test8334480132298691726.py"", line 8, in <module>
2018-05-07 23:15:22.32 - stdout>     spark.sql(""create table data_source_tbl_{} using json as select 1 i"".format(version_index))
2018-05-07 23:15:22.32 - stdout>   File ""/tmp/test-spark/spark-2.0.2/python/lib/pyspark.zip/pyspark/sql/session.py"", line 543, in sql
2018-05-07 23:15:22.321 - stdout>   File ""/tmp/test-spark/spark-2.0.2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__
2018-05-07 23:15:22.321 - stdout>   File ""/tmp/test-spark/spark-2.0.2/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco
2018-05-07 23:15:22.321 - stdout>   File ""/tmp/test-spark/spark-2.0.2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value
2018-05-07 23:15:22.322 - stdout> py4j.protocol.Py4JJavaError: An error occurred while calling o28.sql.
2018-05-07 23:15:22.322 - stdout> : java.lang.ExceptionInInitializerError
2018-05-07 23:15:22.322 - stdout> 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
{code}

Previously, I've been monitoring [Spark QA Dashboard|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)].

Actually, `HiveExternalCatalogVersionSuite` passes in that branches. Only `spark-master-test-sbt-hadoop-2.7` branch dies for other reasons.

- 4439 Build timed out (after 275 minutes) during PySpark testing.
- 4438 terminated by signal 9
- 4437 Build timed out (after 275 minutes) during SparkR testing.
- 4436 Build timed out (after 275 minutes) during PySpark testing.
- 4435 PyPy test failures.
- 4434 terminated by signal 9
- 4433 Build timed out (after 275 minutes) during PySpark testing.

For these all recent tests, `HiveExternalCatalogVersionSuite` passes, [~smilegator].

Can we increase the timeout for the branch?;;;","21/Dec/18 21:49;dongjoon;SPARK-26427 will land ORC 1.5.4 to `master` branch with two ORC patches related to the reported `leaked file streams`. We can see the status of Jenkins after that.;;;","01/Jan/19 06:20;dongjoon;I'm closing this issue since this is resolved as a part of SPARK-26427 and I didn't hit this issue until now. Please feel free to reopen this if you see another instance of this failure.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register task completion listeners first for ParquetFileFormat,SPARK-23457,13139257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Feb/18 17:15,05/Mar/18 19:54,13/Jul/23 08:45,20/Feb/18 05:36,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"ParquetFileFormat leaks open files in some cases. This issue aims to register task completion listener first.

{code}
  test(""SPARK-23390 Register task completion listeners first in ParquetFileFormat"") {
    withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_BATCH_SIZE.key -> s""${Int.MaxValue}"") {
      withTempDir { dir =>
        val basePath = dir.getCanonicalPath
        Seq(0).toDF(""a"").write.format(""parquet"").save(new Path(basePath, ""first"").toString)
        Seq(1).toDF(""a"").write.format(""parquet"").save(new Path(basePath, ""second"").toString)
        val df = spark.read.parquet(
          new Path(basePath, ""first"").toString,
          new Path(basePath, ""second"").toString)
        val e = intercept[SparkException] {
          df.collect()
        }
        assert(e.getCause.isInstanceOf[OutOfMemoryError])
      }
    }
  }
{code}",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 04:37:05 UTC 2018,,,,,,,,,,"0|i3qb6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/18 17:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20619;;;","20/Feb/18 05:36;cloud_fan;Issue resolved by pull request 20619
[https://github.com/apache/spark/pull/20619];;;","02/Mar/18 04:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20714;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extra java options lose order in Docker context,SPARK-23449,13139039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,akorzhuev,akorzhuev,akorzhuev,16/Feb/18 14:21,17/May/20 18:23,13/Jul/23 08:45,26/Feb/18 18:29,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Kubernetes,Spark Core,,,0,,,,"`spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` when processed in `entrypoint.sh` does not preserve its ordering, which makes `-XX:+UnlockExperimentalVMOptions` unusable, as you have to pass it before any other experimental options.

 

Steps to reproduce:
 # Set `spark.driver.extraJavaOptions`, e.g. `-XX:+UnlockExperimentalVMOptions -XX:+UseG1GC -XX:+CMSClassUnloadingEnabled -XX:+UseCGroupMemoryLimitForHeap`
 # Submit application to k8s cluster.
 # Fetch logs and observe that on each run order of options is different and when `-XX:+UnlockExperimentalVMOptions` is not the first startup will fail.

 

Expected behaviour:
 # Order of `extraJavaOptions` should be preserved.

 

Cause:

`entrypoint.sh` fetches environment options with `env`, which doesn't guarantee ordering.
{code:java}
env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\(.*\)/\1/g' > /tmp/java_opts.txt{code}",Running Spark on K8S with supplied Docker image. Passing along extra java options.,akorzhuev,apachespark,dongjoon,vanzin,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 26 18:29:21 UTC 2018,,,,,,,,,,"0|i3q9u7:",9223372036854775807,,,,,vanzin,,,,,,,,,,,,,,,,,,"16/Feb/18 14:27;apachespark;User 'andrusha' has created a pull request for this issue:
https://github.com/apache/spark/pull/20628;;;","19/Feb/18 22:06;dongjoon;I removed the fixed version from this issue. We can update it when this merged into Apache Spark 2.3 RC if exists.;;;","26/Feb/18 18:29;vanzin;Issue resolved by pull request 20628
[https://github.com/apache/spark/pull/20628];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataframe returns wrong result when column don't respect datatype,SPARK-23448,13139034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,azaroui,azaroui,16/Feb/18 13:39,12/Dec/22 18:11,13/Jul/23 08:45,28/Feb/18 02:01,2.0.2,,,,,,,,,2.3.1,,,,,SQL,,,,0,,,,"I have the following json file that contains some noisy data(String instead of Array):

 
{code:java}
{""attr1"":""val1"",""attr2"":""[\""val2\""]""}
{""attr1"":""val1"",""attr2"":[""val2""]}
{code}
And i need to specify schema programatically like this:

 
{code:java}
implicit val spark = SparkSession
  .builder()
  .master(""local[*]"")
  .config(""spark.ui.enabled"", false)
  .config(""spark.sql.caseSensitive"", ""True"")
  .getOrCreate()
import spark.implicits._

val schema = StructType(
  Seq(StructField(""attr1"", StringType, true),
      StructField(""attr2"", ArrayType(StringType, true), true)))

spark.read.schema(schema).json(input).collect().foreach(println)
{code}
The result given by this code is:
{code:java}
[null,null]
[val1,WrappedArray(val2)]
{code}
Instead of putting null in corrupted column, all columns of the first message are null

 

 ",Local,apachespark,azaroui,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 28 02:01:36 UTC 2018,,,,,,,,,,"0|i3q9t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 09:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20648;;;","24/Feb/18 07:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20666;;;","24/Feb/18 09:18;viirya;In fact this is exactly the JSON parser's behavior, not a bug. We don't allow partial result for corrupted records. Except for the field configured by {{columnNameOfCorruptRecord}}, all fields will be set to {{null}}.;;;","28/Feb/18 02:01;gurwls223;Issue resolved by pull request 20666
[https://github.com/apache/spark/pull/20666];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DStreams could lose blocks with WAL enabled when driver crashes,SPARK-23438,13138823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gsomogyi,gsomogyi,gsomogyi,15/Feb/18 16:34,26/Feb/18 16:58,13/Jul/23 08:45,26/Feb/18 16:58,1.6.0,,,,,,,,,2.0.3,2.1.3,2.2.2,2.3.1,2.4.0,DStreams,,,,0,,,,"There is a race condition introduced in SPARK-11141 which could cause data loss.

This affects all versions since 1.6.0.

Problematic situation:
 # Start streaming job with 2 receivers with WAL enabled.
 # Receiver 1 receives a block and does the following

 ** Writes a BlockAdditionEvent into WAL
 ** Puts the block into it's received block queue with ID 1
 # Receiver 2 receives a block and does the following

 ** Writes a BlockAdditionEvent into WAL

 # Spark allocates all blocks from it's received block queue and writes AllocatedBlocks(IDs=(1)) into WAL
 # Driver crashes
 # New Driver recovers from WAL
 # Realise block with ID 2 never processed

 ",,apachespark,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 17:01:04 UTC 2018,,,,,,,,,,"0|i3q8if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/18 16:35;gsomogyi;I'm working on that.;;;","15/Feb/18 17:01;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20620;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Date column Inference in partition discovery,SPARK-23436,13138793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,apoorva.sareen@gmail.com,apoorva.sareen@gmail.com,15/Feb/18 15:16,09/Mar/18 12:42,13/Jul/23 08:45,20/Feb/18 05:57,2.2.1,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,,,,"If a Partition column appears to partial date/timestamp

    example : 2018-01-01-23 

where it is only truncated upto an hour then the data types of the partitioning columns are automatically inferred as date however, the values are loaded as null. 

Here is an example code to reproduce this behaviour

 

 
{code:java}
val data = Seq((""1"", ""2018-01"", ""2018-01-01-04"", ""test"")).toDF(""id"", ""date_month"", ""data_hour"", ""data"")  

data.write.partitionBy(""id"",""date_month"",""data_hour"").parquet(""output/test"")

val input = spark.read.parquet(""output/test"")  

input.printSchema()

input.show()


## Result ###

root

|-- data: string (nullable = true)

|-- id: integer (nullable = true)

|-- date_month: string (nullable = true)

|-- data_hour: date (nullable = true)



+----+---+----------+---------+

|data| id|date_month|data_hour|

+----+---+----------+---------+

|test|  1|   2018-01|     null|

+----+---+----------+---------+{code}
 ",,apachespark,apoorva.sareen@gmail.com,cloud_fan,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 00:02:04 UTC 2018,,,,,,,,,,"0|i3q8br:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/18 15:45;mgaido;Thanks for reporting this. This affects also current branch. I am working on it.;;;","15/Feb/18 17:14;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20621;;;","20/Feb/18 05:57;cloud_fan;Issue resolved by pull request 20621
[https://github.com/apache/spark/pull/20621];;;","08/Mar/18 00:02;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20764;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R tests should support latest testthat,SPARK-23435,13138733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,15/Feb/18 09:29,12/Dec/22 18:10,13/Jul/23 08:45,29/Jan/20 01:37,2.3.1,2.4.0,3.0.0,,,,,,,2.4.5,3.0.0,,,,SparkR,,,,0,,,,"To follow up on SPARK-22817, the latest version of testthat, 2.0.0 was released in Dec 2017, and its method has been changed.

In order for our tests to keep working, we need to detect that and call a different method.

Jenkins is running 1.0.1 though, we need to check if it is going to work.",,adrian555,felixcheung,phegstrom,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30645,,,,,,,SPARK-30611,,,,SPARK-30663,,,SPARK-30637,SPARK-30733,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 00:07:40 UTC 2020,,,,,,,,,,"0|i3q7yf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/18 20:21;felixcheung;Working on this. Debugging a problem.;;;","27/May/18 03:56;felixcheung;Work was stopped for a while. Hopefully I can come by in a few weeks.;;;","12/Jun/18 19:18;adrian555;I hit this since I just reinstalled R and bunch of stuff. It took me some time to get to locate this issue here. [~felixcheung], are you still working on this? If not, can I work on it? I can pull together the changes I have made and raise a PR.;;;","15/Jun/18 06:32;felixcheung;sorry, I did try but couldn't get it to work, but something like this?

 
{code:java}
# for testthat after 1.0.2 call test_dir as run_tests is removed.
if (packageVersion(""testthat"") >= ""2.0.0"") {
  test_pkg_env <- list2env(as.list(getNamespace(""SparkR""), all.names = TRUE),
  parent = parent.env(getNamespace(""SparkR"")))
  withr::local_options(list(topLevelEnvironment = test_pkg_env))
  test_dir(file.path(sparkRDir, ""pkg"", ""tests"", ""fulltests""),
    env = test_pkg_env,
    stop_on_failure = TRUE,
    stop_on_warning = FALSE)
} else {
  testthat:::run_tests(""SparkR"",
    file.path(sparkRDir, ""pkg"", ""tests"", ""fulltests""),
    NULL,
    ""summary"")
}

{code};;;","15/Jun/18 17:13;adrian555;yes, quite similar

 

```

sp <- getNamespace(""SparkR"")
 attach(sp)
 test_dir(file.path(sparkRDir, ""pkg"", ""tests"", ""fulltests""))

)

```

 ;;;","30/Aug/18 18:51;phegstrom;What is the status of this? My appveyor.yml has the lower version of testthat, but I'm still getting the original error.

Looks like it's because I followed http://spark.apache.org/docs/latest/building-spark.html#running-r-tests, but this command downloads the most recent testthat version.

Can someone change this until your PR goes through?;;;","24/Jan/20 19:20;shaneknapp;since we can't have different R environments for different spark branches, we should confirm that testthat 2.0.0 doesn't break the 2.4 branch before the jenkins workers are upgraded.;;;","29/Jan/20 01:37;gurwls223;Issue resolved by pull request 27359
[https://github.com/apache/spark/pull/27359];;;","04/Feb/20 23:09;shaneknapp;sadly (or not), i had to upgrade R to 3.5.2 on the centos (amp-jenkins-worker-\{2..6}, and the ubuntu workers (a-j-s-w-02, research-jenkins-*) have version 3.2.3.

worst case, i can downgrade R.;;;","05/Feb/20 00:07;gurwls223;I think that's no problem. Thanks Shane!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark should not warn `metadata directory` for a HDFS file path,SPARK-23434,13138702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Feb/18 05:11,05/Mar/18 22:29,13/Jul/23 08:45,21/Feb/18 00:03,2.0.2,2.1.2,2.2.1,2.3.0,,,,,,2.2.2,2.3.1,2.4.0,,,SQL,,,,0,,,,"In a kerberized cluster, when Spark reads a file path (e.g. `people.json`), it warns with a wrong error message during looking up `people.json/_spark_metadata`. The root cause of this istuation is the difference between `LocalFileSystem` and `DistributedFileSystem`. `LocalFileSystem.exists()` returns `false`, but `DistributedFileSystem.exists` raises Exception.

{code}
scala> spark.version
res0: String = 2.4.0-SNAPSHOT

scala> spark.read.json(""file:///usr/hdp/current/spark-client/examples/src/main/resources/people.json"").show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> spark.read.json(""hdfs:///tmp/people.json"")
18/02/15 05:00:48 WARN streaming.FileStreamSink: Error while looking for metadata directory.
18/02/15 05:00:48 WARN streaming.FileStreamSink: Error while looking for metadata directory.
res6: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
{code}

{code}
scala> spark.version
res0: String = 2.2.1

scala> spark.read.json(""hdfs:///tmp/people.json"").show
18/02/15 05:28:02 WARN FileStreamSink: Error while looking for metadata directory.
18/02/15 05:28:02 WARN FileStreamSink: Error while looking for metadata directory.
{code}

{code}
scala> spark.version
res0: String = 2.1.2

scala> spark.read.json(""hdfs:///tmp/people.json"").show
18/02/15 05:29:53 WARN DataSource: Error while looking for metadata directory.
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
{code}

{code}
scala> spark.version
res0: String = 2.0.2

scala> spark.read.json(""hdfs:///tmp/people.json"").show
18/02/15 05:25:24 WARN DataSource: Error while looking for metadata directory.
{code}",,apachespark,dongjoon,Tagar,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 04:41:04 UTC 2018,,,,,,,,,,"0|i3q7rj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/18 05:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20616;;;","21/Feb/18 00:03;zsxwing;Issue resolved by pull request 20616
[https://github.com/apache/spark/pull/20616];;;","02/Mar/18 04:28;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20713;;;","02/Mar/18 04:41;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20715;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalStateException: more than one active taskSet for stage,SPARK-23433,13138673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,zsxwing,zsxwing,15/Feb/18 01:14,15/Apr/19 12:35,13/Jul/23 08:45,03/May/18 16:01,2.2.1,,,,,,,,,2.2.2,2.3.1,2.4.0,,,Spark Core,,,,0,,,,"This following error thrown by DAGScheduler stopped the cluster:

{code}
18/02/11 13:22:27 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext
java.lang.IllegalStateException: more than one active taskSet for stage 7580621: 7580621.2,7580621.1
	at org.apache.spark.scheduler.TaskSchedulerImpl.submitTasks(TaskSchedulerImpl.scala:229)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1059)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$submitWaitingChildStages$6.apply(DAGScheduler.scala:900)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$submitWaitingChildStages$6.apply(DAGScheduler.scala:899)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.scheduler.DAGScheduler.submitWaitingChildStages(DAGScheduler.scala:899)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1427)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1929)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1880)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1868)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

{code}",,apachespark,feiwang,guoxiaolongzte,habren,irashid,mgaido,rajeshhadoop,roczei,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25250,,,SPARK-19868,SPARK-27065,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 03 16:01:33 UTC 2018,,,,,,,,,,"0|i3q7l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/18 23:35;zsxwing;{code}
18/02/11 13:22:20 INFO TaskSetManager: Finished task 17.0 in stage 7580621.1 (TID 65577139) in 303870 ms on 10.0.246.111 (executor 24) (18/19)
18/02/11 13:22:20 INFO DAGScheduler: ShuffleMapStage 7580621 (start at command-2841337:340) finished in 303.880 s
18/02/11 13:22:20 INFO DAGScheduler: Resubmitting ShuffleMapStage 7580621 (start at command-2841337:340) because some of its tasks had failed: 2, 15, 27, 28, 41
18/02/11 13:22:27 INFO DAGScheduler: Submitting ShuffleMapStage 7580621 (MapPartitionsRDD[2660062] at start at command-2841337:340), which has no missing parents
18/02/11 13:22:27 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 7580621 (MapPartitionsRDD[2660062] at start at command-2841337:340) (first 15 tasks are for partitions Vector(2, 15, 27, 28, 41))
18/02/11 13:22:27 INFO TaskSchedulerImpl: Adding task set 7580621.2 with 5 tasks
18/02/11 13:22:27 ERROR DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext
java.lang.IllegalStateException: more than one active taskSet for stage 7580621: 7580621.2,7580621.1
	at org.apache.spark.scheduler.TaskSchedulerImpl.submitTasks(TaskSchedulerImpl.scala:229)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1059)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$submitWaitingChildStages$6.apply(DAGScheduler.scala:900)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$submitWaitingChildStages$6.apply(DAGScheduler.scala:899)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.scheduler.DAGScheduler.submitWaitingChildStages(DAGScheduler.scala:899)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1427)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1929)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1880)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1868)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
18/02/11 13:22:27 INFO TaskSchedulerImpl: Cancelling stage 7580621
18/02/11 13:22:27 INFO TaskSchedulerImpl: Cancelling stage 7580621
18/02/11 13:22:27 INFO TaskSchedulerImpl: Stage 7580621 was cancelled
18/02/11 13:22:27 INFO DAGScheduler: ShuffleMapStage 7580621 (start at command-2841337:340) failed in 0.057 s due to Job aborted due to stage failure: Stage 7580621 cancelled
org.apache.spark.SparkException: Job aborted due to stage failure: Stage 7580621 cancelled
18/02/11 13:22:27 WARN TaskSetManager: Lost task 18.0 in stage 7580621.1 (TID 65577140, 10.0.144.170, executor 16): TaskKilled (Stage cancelled)
{code}

According to the above logs, I think the issue is in this line: https://github.com/apache/spark/blob/1dc2c1d5e85c5f404f470aeb44c1f3c22786bdea/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1281

""Task 18.0 in stage 7580621.0"" finished and updated ""shuffleStage.pendingPartitions"" when ""Task 18.0 in stage 7580621.1"" was still running. Hence, when 18 of 19 tasks finished in ""stage 7580621.1"", this condition (https://github.com/apache/spark/blob/1dc2c1d5e85c5f404f470aeb44c1f3c22786bdea/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1284) would be true and trigger ""stage 7580621.2"".

;;;","15/Feb/18 23:37;zsxwing;cc [~irashid];;;","16/Feb/18 17:57;irashid;yes I think you are right [~zsxwing].  Since a zombie taskset might still be running the same tasks as a the non-zombie one, when a zombie task finishes, it should be able to mark the non-zombie taskset as a zombie.  Or in this case, task 18.0 from 7580621.0 should be able to mark 7580621.1 as a zombie.

Are you working on this?;;;","16/Feb/18 17:59;irashid;actually, I realized its more general than just marking it as a zombie -- it should even be able to mark tasks as completed, so you don't have tasks submitted by later attempts when an earlier attempt says the output is ready.;;;","16/Feb/18 18:56;zsxwing;[~irashid] I'm busy with other stuff and not working on this. Your approach sounds good to me. Please go ahead if you have time to work on this.;;;","06/Mar/18 04:00;guoxiaolongzte;I also encountered the same problem, who can solve it?;;;","06/Mar/18 13:27;irashid;sorry it has taken me some time to get to this -- its a little more complex than I thought as currently tasksetmanagers for the same stage do not send any updates to each other, so I am trying to make sure I understand it well.  anyone else is welcome to take it up in the meantime.

[~guoxiaolongzte] -- are you sure you see the same issue, with the same sequence of events as described by Shixiong Zhu?  I want to make sure its not another instance of the same exception, but perhaps with a different underlying cause.;;;","23/Apr/18 20:34;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/21131;;;","03/May/18 16:01;irashid;fixed by https://github.com/apache/spark/pull/21131;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
load data for hdfs file path with wild card usage is not working properly,SPARK-23425,13138554,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,S71955,S71955,S71955,14/Feb/18 18:13,12/Dec/22 18:10,13/Jul/23 08:45,24/Aug/18 01:55,2.2.1,2.3.0,,,,,,,,2.4.0,,,,,SQL,,,,0,release-notes,,,"load data command  for loading data from non local  file paths by using wild card strings lke * are not working

eg:

""load data inpath 'hdfs://hacluster/user/ext*  into table t1""

Getting Analysis excepton while executing this query

!image-2018-02-14-23-41-39-923.png!",,apachespark,S71955,yutaochina,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25738,,,,,,,,,,,,"14/Feb/18 18:14;S71955;wildcard_issue.PNG;https://issues.apache.org/jira/secure/attachment/12910614/wildcard_issue.PNG",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,"Release notes:

Wildcard symbols {{*}} and {{?}} can now be used in SQL paths when loading data, e.g.:

LOAD DATA INPATH 'hdfs://hacluster/user/ext*'
LOAD DATA INPATH 'hdfs://hacluster/user/???/data'

Where these characters are used literally in paths, they must be escaped with a backslash.

Wildcards can be used in the folder level of a local File system in Load command from now.

 e.g. LOAD DATA LOCAL INPATH 'tmp/folder*/

Now onward normal Space convention can be used in folder/file names (e.g file Name.csv), Older versions space in folder/file names has been represented using '%20'(e.g. myFile%20Name).

#",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 11 18:46:49 UTC 2018,,,,,,,,,,"0|i3q6un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/18 18:16;S71955;I am working towards resolving this bug, please let me know for any suggestions or valuable feedback;;;","14/Feb/18 18:55;apachespark;User 'sujith71955' has created a pull request for this issue:
https://github.com/apache/spark/pull/20611;;;","24/Aug/18 01:55;gurwls223;Issue resolved by pull request 20611
[https://github.com/apache/spark/pull/20611];;;","10/Sep/18 17:50;zsxwing;Added ""release-note"" label.

Previously, when INPATH contains special characters (such as "" ""), the user has to manually escape them, e.g., use ""/a/b/foo%20bar"" rather than ""/a/b/foo bar"" because the former will throw ""URISyntaxException: Illegal character in path at index XX: /a/b/foo bar"".

After this patch, the above workaround will throw ""AnalysisException: LOAD DATA input path does not exist: /a/b/foo%20bar;"".

The root cause is we changed from ""new URI(user_specified_path)"" to ""new Path(user_specified_path)"". I believe this patch is indeed a bug fix but it's worth to highlight in the release note.;;;","11/Sep/18 18:46;apachespark;User 'sujith71955' has created a pull request for this issue:
https://github.com/apache/spark/pull/22396;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnShuffleIntegrationSuite failure when SPARK_PREPEND_CLASSES set to 1,SPARK-23422,13138484,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,14/Feb/18 12:48,23/Feb/18 06:34,13/Jul/23 08:45,15/Feb/18 11:53,2.3.0,,,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,"YarnShuffleIntegrationSuite fails when SPARK_PREPEND_CLASSES set to 1.

Normally mllib built before yarn module. When SPARK_PREPEND_CLASSES used mllib classes are on yarn test classpath.

Before 2.3 that did not cause issues. But 2.3 has SPARK-22450, which registered some mllib classes with the kryo serializer. Now it dies with the following error:
{code:java}
18/02/13 07:33:29 INFO SparkContext: Starting job: collect at YarnShuffleIntegrationSuite.scala:143
Exception in thread ""dag-scheduler-event-loop"" java.lang.NoClassDefFoundError: breeze/linalg/DenseMatrix
{code}",,apachespark,gsomogyi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 11:53:22 UTC 2018,,,,,,,,,,"0|i3q6f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/18 12:48;gsomogyi;I'm working on the solution.;;;","14/Feb/18 13:07;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20608;;;","15/Feb/18 11:53;vanzin;Issue resolved by pull request 20608
[https://github.com/apache/spark/pull/20608];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document the behavior change in SPARK-22356,SPARK-23421,13138427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,14/Feb/18 08:14,15/Feb/18 07:54,13/Jul/23 08:45,15/Feb/18 07:53,2.2.1,2.3.0,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,SPARK-22356 introduces a behavior change. We need to document it in the migration guide. Also update the HiveExternalCatalogVersionsSuite to verify it. ,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 08:17:07 UTC 2018,,,,,,,,,,"0|i3q62f:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"14/Feb/18 08:17;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data source v2 write path should re-throw interruption exceptions directly,SPARK-23419,13138391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,14/Feb/18 01:32,23/Feb/18 06:32,13/Jul/23 08:45,15/Feb/18 09:02,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 09:02:19 UTC 2018,,,,,,,,,,"0|i3q5uf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/18 02:22;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20605;;;","15/Feb/18 09:02;cloud_fan;Issue resolved by pull request 20605
[https://github.com/apache/spark/pull/20605];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark tests give wrong sbt instructions,SPARK-23417,13138323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,joseph.torres,joseph.torres,13/Feb/18 20:21,12/Dec/22 18:11,13/Jul/23 08:45,28/Feb/18 00:25,2.4.0,,,,,,,,,2.4.0,,,,,PySpark,,,,0,,,,"When running python/run-tests, the script indicates that I must run ""'build/sbt assembly/package streaming-kafka-0-8-assembly/assembly' or 'build/mvn -Pkafka-0-8 package'"". The sbt command fails:

 

[error] Expected ID character

[error] Not a valid command: streaming-kafka-0-8-assembly

[error] Expected project ID

[error] Expected configuration

[error] Expected ':' (if selecting a configuration)

[error] Expected key

[error] Not a valid key: streaming-kafka-0-8-assembly

[error] streaming-kafka-0-8-assembly/assembly

[error] ",,apachespark,bersprockets,joseph.torres,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 28 00:25:39 UTC 2018,,,,,,,,,,"0|i3q5fb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/18 23:50;bersprockets;This does the trick:
{noformat}
build/sbt -Pkafka-0-8 assembly/package streaming-kafka-0-8-assembly/assembly
{noformat}
There are also errant instructions for building a flume assembly jar. In that case the following works:
{noformat}
build/sbt -Pflume assembly/package streaming-flume-assembly/assembly
{noformat}
I can submit a PR to fix these messages.

By the way, the above is just for the pyspark-streaming tests. The pyspark-sql tests have similar build requirements (e.g., at least one test needs a build with Hive profiles. Also, udf.py needs /sql/core/target/scala-2.11/test-classes/test/org/apache/spark/sql/JavaStringLength.class to exist.). The pyspark-sql tests don't check for these requirements, they just throw exceptions. But I won't address that here.;;;","19/Feb/18 20:46;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/20638;;;","28/Feb/18 00:25;gurwls223;Issue resolved by pull request 20638
[https://github.com/apache/spark/pull/20638];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: KafkaSourceStressForDontFailOnDataLossSuite.stress test for failOnDataLoss=false,SPARK-23416,13138297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joseph.torres,joseph.torres,joseph.torres,13/Feb/18 19:08,13/Feb/19 04:03,13/Jul/23 08:45,24/May/18 00:22,2.4.0,,,,,,,,,2.3.4,2.4.0,,,,Structured Streaming,,,,2,,,,"I suspect this is a race condition latent in the DataSourceV2 write path, or at least the interaction of that write path with StreamTest.

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87241/testReport/org.apache.spark.sql.kafka010/KafkaSourceStressForDontFailOnDataLossSuite/stress_test_for_failOnDataLoss_false/]
h3. Error Message

org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 16b2a2b1-acdd-44ec-902f-531169193169, runId = 9567facb-e305-4554-8622-830519002edb] terminated with exception: Writing job aborted.
h3. Stacktrace

sbt.ForkMain$ForkError: org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 16b2a2b1-acdd-44ec-902f-531169193169, runId = 9567facb-e305-4554-8622-830519002edb] terminated with exception: Writing job aborted. at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295) at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189) Caused by: sbt.ForkMain$ForkError: org.apache.spark.SparkException: Writing job aborted. at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2.scala:108) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247) at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:294) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272) at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2722) at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2722) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252) at org.apache.spark.sql.Dataset.collect(Dataset.scala:2722) at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3$$anonfun$apply$15.apply(MicroBatchExecution.scala:488) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3.apply(MicroBatchExecution.scala:483) at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:482) at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:133) at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121) at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:121) at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58) at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:121) at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:117) at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279) ... 1 more Caused by: sbt.ForkMain$ForkError: java.lang.InterruptedException: null at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202) at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218) at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153) at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:222) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:633) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027) at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2.scala:79) ... 31 more",,apachespark,cloud_fan,dongjoon,joseph.torres,maropu,mgaido,tdas,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 19:39:35 UTC 2018,,,,,,,,,,"0|i3q59j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 19:24;dongjoon;Thank you for filing this!;;;","13/Feb/18 19:30;mgaido;I see this failing also with this stacktrace:


{code:java}
sbt.ForkMain$ForkError: org.apache.spark.sql.streaming.StreamingQueryException: Query memory [id = cca87cf7-0532-41af-b757-0948ec294c0c, runId = c1830af6-1715-4947-bd76-a1a63482280b] terminated with exception: null
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
Caused by: sbt.ForkMain$ForkError: java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runContinuous(ContinuousExecution.scala:271)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runActivatedStream(ContinuousExecution.scala:89)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
	... 1 more
{code}

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87401/testReport/org.apache.spark.sql.kafka010/KafkaContinuousSourceStressForDontFailOnDataLossSuite/stress_test_for_failOnDataLoss_false/;;;","13/Feb/18 19:47;joseph.torres;I think I see the problem.
 * StreamExecution.stop() works by interrupting the stream execution thread. This is not safe in general, and can throw any variety of exceptions.
 * StreamExecution.isInterruptedByStop() solves this problem by implementing a whitelist of exceptions which indicate the stop() happened.
 * The v2 write path adds calls to ThreadUtils.awaitResult(), which weren't in the V1 write path and (if the interrupt happens to fall in them) throw a new exception which isn't accounted for.

I'm going to write a PR to add another whitelist entry. This whole edifice is a bit fragile, but I don't have a good solution for that.;;;","13/Feb/18 19:53;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20602;;;","14/Feb/18 02:22;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20605;;;","15/Feb/18 09:02;cloud_fan;Issue resolved by pull request 20605
[https://github.com/apache/spark/pull/20605];;;","12/May/18 14:34;dongjoon;FYI.
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/90536  (branch-2.3)
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/342/
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/376/
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/347/;;;","19/May/18 17:56;dongjoon;Sorry. I'm reopening this due to the frequent failures;;;","19/May/18 18:52;joseph.torres;No problem. I've been working on this since last week.;;;","20/May/18 00:15;dongjoon;Thank you so much, [~joseph.torres].;;;","21/May/18 19:08;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/21384;;;","24/May/18 00:22;tdas;Issue resolved by pull request 21384
[https://github.com/apache/spark/pull/21384];;;","24/May/18 03:13;dongjoon;Thank you, [~joseph.torres] and [~tdas] .

Actually, recently failures are reported on `branch-2.3`. Can we have this fix on `branch-2.3` please?;;;","24/May/18 19:39;joseph.torres;Do you know how to drive that? I'm not sure what the process is.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferHolderSparkSubmitSuite is flaky,SPARK-23415,13138291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,dongjoon,dongjoon,13/Feb/18 19:02,24/Oct/18 18:12,13/Jul/23 08:45,09/Aug/18 12:32,2.3.0,,,,,,,,,2.4.0,,,,,SQL,Tests,,,0,,,,"The test suite fails due to 60-second timeout sometimes.
{code:java}
Error Message
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to failAfter did not complete within 60 seconds.
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to failAfter did not complete within 60 seconds.
{code}
 - [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87380/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.6/4206/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/4759/]
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/412/] (June 15th)",,apachespark,cloud_fan,dongjoon,kiszk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 09 12:32:14 UTC 2018,,,,,,,,,,"0|i3q587:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/18 19:55;kiszk;I am working for this.;;;","14/Feb/18 20:00;dongjoon;Thank you so much, [~kiszk]!;;;","15/Feb/18 17:02;kiszk;I realized that an issue in this test case. I think that this test case expects to try memory allocation in {{grow()}} method four times. However, since the allocated memory is reused, this test performed memory allocation only once with {{roundToWord(ARRAY_MAX / 2)}}. No memory allocations occur with other sizes.

If I updated this test to perform four memory allocations with {{4g}} heap size,  an exception occurs.;;;","19/Feb/18 09:12;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20636;;;","09/Aug/18 12:32;cloud_fan;Issue resolved by pull request 20636
[https://github.com/apache/spark/pull/20636];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting tasks by Host / Executor ID on the Stage page does not work,SPARK-23413,13138216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,attilapiros,attilapiros,attilapiros,13/Feb/18 15:50,15/Feb/18 20:59,13/Jul/23 08:45,15/Feb/18 20:04,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Sorting tasks by Host / Executor ID throws exceptions: 
{code}
java.lang.IllegalArgumentException: Invalid sort column: Executor ID at org.apache.spark.ui.jobs.ApiHelper$.indexName(StagePage.scala:1017) at org.apache.spark.ui.jobs.TaskDataSource.sliceData(StagePage.scala:694) at org.apache.spark.ui.PagedDataSource.pageData(PagedTable.scala:61) at org.apache.spark.ui.PagedTable$class.table(PagedTable.scala:96) at org.apache.spark.ui.jobs.TaskPagedTable.table(StagePage.scala:708) at org.apache.spark.ui.jobs.StagePage.liftedTree1$1(StagePage.scala:293) at org.apache.spark.ui.jobs.StagePage.render(StagePage.scala:282) at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82) at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82) at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
{code}

{code}
java.lang.IllegalArgumentException: Invalid sort column: Host at org.apache.spark.ui.jobs.ApiHelper$.indexName(StagePage.scala:1017) at org.apache.spark.ui.jobs.TaskDataSource.sliceData(StagePage.scala:694) at org.apache.spark.ui.PagedDataSource.pageData(PagedTable.scala:61) at org.apache.spark.ui.PagedTable$class.table(PagedTable.scala:96) at org.apache.spark.ui.jobs.TaskPagedTable.table(StagePage.scala:708) at org.apache.spark.ui.jobs.StagePage.liftedTree1$1(StagePage.scala:293) at org.apache.spark.ui.jobs.StagePage.render(StagePage.scala:282) at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82) at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82) at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90) at 
{code}

  !image-2018-02-13-16-50-32-600.png!",,ajbozarth,apachespark,attilapiros,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23430,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 20:59:16 UTC 2018,,,,,,,,,,"0|i3q4rj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 15:51;attilapiros;I am working on that.;;;","13/Feb/18 18:24;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/20601;;;","15/Feb/18 20:04;irashid;Issue resolved by pull request 20623
[https://github.com/apache/spark/pull/20623];;;","15/Feb/18 20:05;irashid;This was fixed by https://github.com/apache/spark/pull/20601 in master, and https://github.com/apache/spark/pull/20623 in branch-2.3 (because of a mistake I made while merging);;;","15/Feb/18 20:59;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/20623;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: StreamingOuterJoinSuite.left outer early state exclusion on right,SPARK-23408,13138167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tdas,vanzin,vanzin,13/Feb/18 13:16,13/Feb/19 04:35,13/Jul/23 08:45,23/Feb/18 20:42,2.4.0,,,,,,,,,2.3.4,2.4.0,,,,SQL,Tests,,,1,,,,"Seen on an unrelated PR.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87386/testReport/org.apache.spark.sql.streaming/StreamingOuterJoinSuite/left_outer_early_state_exclusion_on_right/

{noformat}
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 
Assert on query failed: Check total state rows = List(4), updated state rows = List(4): Array(1) did not equal List(4) incorrect updates rows
org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
	org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
	org.apache.spark.sql.streaming.StateStoreMetricsTest$$anonfun$assertNumStateRows$1.apply(StateStoreMetricsTest.scala:28)
	org.apache.spark.sql.streaming.StateStoreMetricsTest$$anonfun$assertNumStateRows$1.apply(StateStoreMetricsTest.scala:23)
	org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1$$anonfun$apply$14.apply$mcZ$sp(StreamTest.scala:568)
	org.apache.spark.sql.streaming.StreamTest$class.verify$1(StreamTest.scala:371)
	org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1.apply(StreamTest.scala:568)
	org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1.apply(StreamTest.scala:432)
	scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)


== Progress ==
   AddData to MemoryStream[value#19652]: 3,4,5
   AddData to MemoryStream[value#19662]: 1,2,3
   CheckLastBatch: [3,10,6,9]
=> AssertOnQuery(<condition>, Check total state rows = List(4), updated state rows = List(4))
   AddData to MemoryStream[value#19652]: 20
   AddData to MemoryStream[value#19662]: 21
   CheckLastBatch: 
   AddData to MemoryStream[value#19662]: 20
   CheckLastBatch: [20,30,40,60],[4,10,8,null],[5,10,10,null]

== Stream ==
Output Mode: Append
Stream state: {MemoryStream[value#19652]: 0,MemoryStream[value#19662]: 0}
Thread state: alive
Thread stack trace: java.lang.Thread.sleep(Native Method)
org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:152)
org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:120)
org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
{noformat}

No other failures in the history, though.",,apachespark,dongjoon,tdas,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24211,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 11 07:33:55 UTC 2019,,,,,,,,,,"0|i3q4gn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/18 23:49;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20646;;;","21/Feb/18 11:14;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20650;;;","23/Feb/18 20:42;tdas;Issue resolved by pull request 20650
[https://github.com/apache/spark/pull/20650];;;","11/Feb/19 07:33;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/23757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream-stream self joins does not work,SPARK-23406,13138115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,13/Feb/18 09:23,16/May/18 21:41,13/Jul/23 08:45,14/Feb/18 22:38,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Structured Streaming,,,,0,,,,"Currently stream-stream self join throws the following error
{code}
val df = spark.readStream.format(""rate"").option(""numRowsPerSecond"", ""1"").option(""numPartitions"", ""1"").load()
display(df.withColumn(""key"", $""value"" / 10).join(df.withColumn(""key"", $""value"" / 5), ""key""))
{code}
error:
{code}
Failure when resolving conflicting references in Join:
'Join UsingJoin(Inner,List(key))
:- Project [timestamp#850, value#851L, (cast(value#851L as double) / cast(10 as double)) AS key#855]
: +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7f1d2a68,rate,List(),None,List(),None,Map(numPartitions -> 1, numRowsPerSecond -> 1),None), rate, [timestamp#850, value#851L]
+- Project [timestamp#850, value#851L, (cast(value#851L as double) / cast(5 as double)) AS key#860]
 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7f1d2a68,rate,List(),None,List(),None,Map(numPartitions -> 1, numRowsPerSecond -> 1),None), rate, [timestamp#850, value#851L]

Conflicting attributes: timestamp#850,value#851L
;;
'Join UsingJoin(Inner,List(key))
:- Project [timestamp#850, value#851L, (cast(value#851L as double) / cast(10 as double)) AS key#855]
: +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7f1d2a68,rate,List(),None,List(),None,Map(numPartitions -> 1, numRowsPerSecond -> 1),None), rate, [timestamp#850, value#851L]
+- Project [timestamp#850, value#851L, (cast(value#851L as double) / cast(5 as double)) AS key#860]
 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7f1d2a68,rate,List(),None,List(),None,Map(numPartitions -> 1, numRowsPerSecond -> 1),None), rate, [timestamp#850, value#851L]

at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:101)
 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:378)
 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:98)
 at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:148)
 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:98)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:101)
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:71)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:73)
 at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3063)
 at org.apache.spark.sql.Dataset.join(Dataset.scala:787)
 at org.apache.spark.sql.Dataset.join(Dataset.scala:756)
 at org.apache.spark.sql.Dataset.join(Dataset.scala:731)
{code}",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23616,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 02:03:05 UTC 2018,,,,,,,,,,"0|i3q453:",9223372036854775807,,,,,,,,,,,,,2.3.1,2.4.0,,,,,,,,,"13/Feb/18 09:52;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20598;;;","14/Feb/18 22:38;tdas;Issue resolved by pull request 20598
[https://github.com/apache/spark/pull/20598];;;","07/Mar/18 02:12;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20755;;;","08/Mar/18 02:03;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20765;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The task will hang up when a small table left semi join a big table,SPARK-23405,13138111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,KaiXinXIaoLei,KaiXinXIaoLei,KaiXinXIaoLei,13/Feb/18 09:15,01/Mar/18 16:12,13/Jul/23 08:45,01/Mar/18 16:10,2.2.1,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"# I run a sql: `select ls.cs_order_number from ls left semi join catalog_sales cs on ls.cs_order_number = cs.cs_order_number`, The `ls` table is a small table ,and the number is one. The `catalog_sales` table is a big table,  and the number is 10 billion. The task will be hang up:

!taskhang up.png!

 And the sql page is :

!SQL.png!",,apachespark,cloud_fan,KaiXinXIaoLei,mgaido,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 09:16;KaiXinXIaoLei;SQL.png;https://issues.apache.org/jira/secure/attachment/12910362/SQL.png","13/Feb/18 09:16;KaiXinXIaoLei;taskhang up.png;https://issues.apache.org/jira/secure/attachment/12910363/taskhang+up.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 01 16:10:48 UTC 2018,,,,,,,,,,"0|i3q447:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 10:49;srowen;There's just not enough info here to even establish something 'hung' vs 'is taking a long time'. This isn't reproducible and there's not much context about your data. It is, however, huge. This isn't useful as a JIRA.;;;","13/Feb/18 11:23;yumwang;I think it's data skew, you should broadcast small table.;;;","23/Feb/18 01:26;KaiXinXIaoLei;[~q79969786] In the left semi join, the left table will not broadcast. And  there is many null value of 'cs_order_number' in the right table. I think the  null value should be filter in the logical plan.;;;","23/Feb/18 10:17;KaiXinXIaoLei;[~q79969786] And if i run 'select ls.cs_order_number from ls left semi join catalog_sales cs on ls.cs_order_number = cs.cs_order_number and cs.cs_order_number is not null', the job will success;;;","25/Feb/18 07:24;apachespark;User 'KaiXinXiaoLei' has created a pull request for this issue:
https://github.com/apache/spark/pull/20670;;;","25/Feb/18 08:16;KaiXinXIaoLei;i run `select ls.cs_order_number from ls left semi join catalog_sales cs on ls.cs_order_number = cs.cs_order_number`, the  Optimized Logical Plan is :

== Optimized Logical Plan ==
 Join LeftSemi, (cs_order_number#1 = cs_order_number#22)
 :- Project [cs_order_number#1|#1]
 : +- Filter isnotnull(cs_order_number#1)
 : +- MetastoreRelation 100t, ls
 +- Project [cs_order_number#22|#22]
 +-Relation[cs_sold_date_sk#5,cs_ext_sales_price#28,... 10 more fields] parquet

 

 

I think the Optimized Logical Plan should be:

== Optimized Logical Plan ==
 Join LeftSemi, (cs_order_number#1 = cs_order_number#22)
 :- Project [cs_order_number#1|#1]
 : +- Filter isnotnull(cs_order_number#1)
 : +- MetastoreRelation 100t, ls
 +- Project [cs_order_number#22|#22]
 {color:#ff0000}+- Filter isnotnull(cs_order_number#22){color}
 +- Relation[cs_sold_date_sk#5,,cs_ext_sales_price#28,... 10 more fields] parquet

 ;;;","01/Mar/18 16:10;cloud_fan;Issue resolved by pull request 20670
[https://github.com/apache/spark/pull/20670];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the extra constructors for ScalaUDF,SPARK-23400,13137993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,12/Feb/18 22:17,23/Feb/18 06:34,13/Jul/23 08:45,13/Feb/18 19:57,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The last few releases, we changed the interface of ScalaUDF. Unfortunately, some Spark Package (spark-deep-learning) are using our internal class `ScalaUDF`. In the release 2.3, we added new parameters into these class. The users hit the binary compatibility issues and got the exception:

> java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.expressions.ScalaUDF.&lt;init&gt;(Ljava/lang/Object;Lorg/apache/spark/sql/types/DataType;Lscala/collection/Seq;Lscala/collection/Seq;Lscala/Option;)V
",,apachespark,maropu,smilegator,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 13 19:57:17 UTC 2018,,,,,,,,,,"0|i3q3dz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 04:56;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20591;;;","13/Feb/18 19:57;zsxwing;Issue resolved by pull request 20591
[https://github.com/apache/spark/pull/20591];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register a task completion listener first for OrcColumnarBatchReader,SPARK-23399,13137975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Feb/18 21:45,23/Feb/18 06:34,13/Jul/23 08:45,14/Feb/18 02:57,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"This is related with SPARK-23390.

Currently, there was a opened file leak for OrcColumnarBatchReader.

{code}
[info] - Enabling/disabling ignoreMissingFiles using orc (648 milliseconds)
15:55:58.673 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 61.0 (TID 85, localhost, executor driver): TaskKilled (Stage cancelled)
15:55:58.674 WARN org.apache.spark.DebugFilesystem: Leaked filesystem connection created at:
java.lang.Throwable
	at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)
	at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.open(RecordReaderUtils.java:173)
	at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:254)
	at org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:633)
	at org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.initialize(OrcColumnarBatchReader.java:138)
{code}",,apachespark,cloud_fan,dongjoon,kiszk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,SPARK-23390,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 17 06:55:56 UTC 2018,,,,,,,,,,"0|i3q39z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/18 21:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20590;;;","14/Feb/18 02:57;cloud_fan;Issue resolved by pull request 20590
[https://github.com/apache/spark/pull/20590];;;","16/Feb/18 09:52;mgaido;I think we should reopen this, it is still happening: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87486/testReport/org.apache.spark.sql.execution.datasources.orc/OrcQuerySuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/;;;","17/Feb/18 06:55;dongjoon;[~mgaido]. I understand what is your intention, but please see the JIRA issue title.
It's not about `Fix OrcQuerySuite`. Why do you reopen this issue? Please proceed to file a new JIRA issue for that.

This JIRA issue handles the designed scope as described in the manual test case in the PR.
For the reported case, I'll investigate more. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Storage info's Cached Partitions doesn't consider the replications (but sc.getRDDStorageInfo does),SPARK-23394,13137820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,12/Feb/18 10:23,23/Feb/18 06:33,13/Jul/23 08:45,14/Feb/18 14:46,2.3.0,,,,,,,,,2.3.0,2.4.0,,,,Spark Core,,,,0,,,,"Start spark as:
{code:bash}
$ bin/spark-shell --master local-cluster[2,1,1024]
{code}


{code:scala}
scala> import org.apache.spark.storage.StorageLevel._
import org.apache.spark.storage.StorageLevel._

scala> sc.parallelize((1 to 100), 10).persist(MEMORY_AND_DISK_2).count
res0: Long = 100                                                                

scala> sc.getRDDStorageInfo(0).numCachedPartitions
res1: Int = 20
{code}

h2. Cached Partitions 

On the UI at the Storage tab Cached Partitions is 10:

 !Storage_Tab.png! .

h2. Full tab

Moreover the replicated partitions was also listed on the old 2.2.1 like:
 !Spark_2.2.1.png! 

But now it is like:
 !Spark_2.4.0-SNAPSHOT.png! 
",,apachespark,attilapiros,mgaido,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/18 12:50;attilapiros;Spark_2.2.1.png;https://issues.apache.org/jira/secure/attachment/12910204/Spark_2.2.1.png","12/Feb/18 12:50;attilapiros;Spark_2.4.0-SNAPSHOT.png;https://issues.apache.org/jira/secure/attachment/12910205/Spark_2.4.0-SNAPSHOT.png","12/Feb/18 12:51;attilapiros;Storage_Tab.png;https://issues.apache.org/jira/secure/attachment/12910206/Storage_Tab.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 14:46:40 UTC 2018,,,,,,,,,,"0|i3q2bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/18 11:04;mgaido;I think this is not an issue. `numCachedPartitions ` is 20 because each partition is cached twice (it has 2 replicas). I think it is not a bug, but maybe we can improve names/docs about it. But since `RDDInfo` is a developer API, I think it is not needed.;;;","12/Feb/18 13:36;vanzin;I talked to Attila offline, and to me it seems like the new UI is more correct. There are only 10 cached partitions, each one replicated to 2 executors; the table also reflects that (whereas   the old UI shows the same block twice). The only potential adjustment here would be to show the executor addresses instead of the executor IDs.

In the context of what lead us here (SPARK-20659 / https://github.com/apache/spark/pull/20546#discussion_r167070392), I think that we should fix the tests that rely on the old code returning the total count including replication, so that they work with the new code that returns more accurate information.;;;","12/Feb/18 19:45;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/20589;;;","14/Feb/18 14:46;vanzin;Issue resolved by pull request 20589
[https://github.com/apache/spark/pull/20589];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
It may lead to overflow for some integer multiplication ,SPARK-23391,13137763,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,12/Feb/18 03:24,12/Feb/18 14:53,13/Jul/23 08:45,12/Feb/18 14:53,2.3.0,,,,,,,,,2.2.2,2.3.0,,,,Spark Core,,,,0,,,,"In the {{getBlockData}},{{blockId.reduceId}} is the {{Int}} type, when it is greater than 2^28, {{blockId.reduceId*8}} will overflow.

In the _decompress0, len_ and  _unitSize are  {{Int}}_ type, so _len * unitSize_ may lead to  overflow

 

 ",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23358,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 12 14:53:05 UTC 2018,,,,,,,,,,"0|i3q1yv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/18 03:28;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/20581;;;","12/Feb/18 14:53;srowen;Issue resolved by pull request 20581
[https://github.com/apache/spark/pull/20581];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: FileBasedDataSourceSuite,SPARK-23390,13137759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,sameerag,sameerag,12/Feb/18 02:35,01/Jan/19 06:17,13/Jul/23 08:45,01/Jan/19 06:17,2.3.0,2.4.0,,,,,,,,3.0.0,,,,,SQL,,,,0,,,,"*RECENT HISTORY*

[http://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.FileBasedDataSourceSuite&test_name=%28It+is+not+a+test+it+is+a+sbt.testing.SuiteSelector%29]

 
----
We're seeing multiple failures in {{FileBasedDataSourceSuite}} in {{spark-branch-2.3-test-sbt-hadoop-2.7}}:
{code:java}
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 15 times over 10.012158059999999 seconds. Last failure message: There are 1 possibly leaked file streams..
{code}
Here's the full history: [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/189/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/history/]

From a very quick look, these failures seem to be correlated with [https://github.com/apache/spark/pull/20479] (cc [~dongjoon]) as evident from the following stack trace (full logs [here|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/189/console]):
{code:java}
[info] - Enabling/disabling ignoreMissingFiles using orc (648 milliseconds)
15:55:58.673 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 61.0 (TID 85, localhost, executor driver): TaskKilled (Stage cancelled)
15:55:58.674 WARN org.apache.spark.DebugFilesystem: Leaked filesystem connection created at:
java.lang.Throwable
	at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)
	at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.open(RecordReaderUtils.java:173)
	at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:254)
	at org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:633)
	at org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.initialize(OrcColumnarBatchReader.java:138)
{code}
Also, while this might be just a false correlation but the frequency of these test failures have increased considerably in [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/] after [https://github.com/apache/spark/pull/20562] (cc [~fengliu@databricks.com]) was merged.

The following is Parquet leakage.
{code:java}
Caused by: sbt.ForkMain$ForkError: java.lang.Throwable: null
	at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)
	at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:538)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:149)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:133)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:400)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:356)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:106)
{code}
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/322/] (May 3rd)
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/331/] (May 9th)
 - [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/90536] (May 11st)
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/342/] (May 16th)
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/347/] (May 19th)
 - [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/367/] (June 2nd)",,apachespark,dongjoon,kiszk,maropu,oae,sameerag,smilegator,viirya,zsxwing,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,SPARK-23606,,,,,,,SPARK-25688,SPARK-23399,,ORC-416,ORC-419,SPARK-23458,SPARK-23505,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 01 06:17:42 UTC 2019,,,,,,,,,,"0|i3q1xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/18 02:39;sameerag;I ran this test locally 50 times and it passed every time. Therefore, I'm currently not marking this as a release blocker as this could just be an artifact of our test environment (possibly due to the order in which tests are run).

Also, cc [~smilegator] [~cloud_fan];;;","12/Feb/18 04:30;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20584;;;","12/Feb/18 22:20;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20591;;;","15/Feb/18 16:44;dongjoon;I'm reopening this issue due to Parquet leakage is detected.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-sbt-hadoop-2.7/205/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

{code}
Caused by: sbt.ForkMain$ForkError: java.lang.Throwable: null
	at org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)
	at org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:538)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:149)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:133)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:400)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:356)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:106)
{code}
;;;","15/Feb/18 17:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20619;;;","19/Feb/18 22:02;dongjoon;Since this is not a regression for both Parquet(not a regression) and ORC (new ORC is disabled by default), I'll remove the target version from this to unblock RC4.
cc [~cloud_fan] and [~sameerag];;;","22/Feb/18 03:12;viirya;{{FileBasedDataSourceSuite}} seems still flaky.

 

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87603/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/]

 

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/87600/testReport/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/]

 ;;;","09/Oct/18 19:11;dongjoon;As reported in SPARK-25688 by [~smilegator] , all recent reported failures in 6 month occurs ORC data sources . I'll reinvestigate this further.;;;","09/Oct/18 20:12;zsxwing;I think the issue is probably in orc. Any exception throwing between  https://github.com/apache/orc/blob/b21b5ffcc1efcbd4aef337fa6faae4d25262f8f1/java/core/src/java/org/apache/orc/impl/RecordReaderImpl.java#L252 and https://github.com/apache/orc/blob/b21b5ffcc1efcbd4aef337fa6faae4d25262f8f1/java/core/src/java/org/apache/orc/impl/RecordReaderImpl.java#L273 will leak `dataReader`. For example, cancelling a Spark task may cause https://github.com/apache/orc/blob/b21b5ffcc1efcbd4aef337fa6faae4d25262f8f1/java/core/src/java/org/apache/orc/impl/RecordReaderImpl.java#L273 throw an exception.;;;","09/Oct/18 20:14;zsxwing;I didn't look at parquet. It may have a similar issue.;;;","10/Oct/18 09:45;dongjoon;Thank you, [~zsxwing]. It took some time, but ORC-416 is created as a first partial attempt. I'll try to proceed further.;;;","10/Oct/18 18:11;dongjoon;[~zsxwing]. ORC-416 is a first partial attempt to make ORC robust (as I mentioned previously.)

I think we had better have a more consistent test coverage for Spark task cancellation.

`FileBasedDataSourceSuite` happens to expose the issue for ORC and Parquet, but it's not consistent. Without a consistent reproducible environment, we cannot guarantee that ORC/Parquet has no issue for this.

So, I'm still looking at Spark task cancellation process, too. Please allow me more time.
;;;","10/Oct/18 18:43;zsxwing;[~dongjoon] when spark cancels a task, the task thread will get interrupted. I think this is what we need to test in Spark. I don't think Spark needs to have a great test coverage for codes inside third party libraries. It's unlikely we can reproduce this issue consistently without changing the third party libraries, since this will require to cancel a task when it's running some special codes in a third party library.;;;","10/Oct/18 19:19;dongjoon;Yep. Thank you for the guide, [~zsxwing].;;;","11/Oct/18 05:59;dongjoon;As a second approach, ORC-419 is created.;;;","18/Oct/18 00:14;dongjoon;[~zsxwing], [~cloud_fan], and [~smilegator].
Both ORC-416 and ORC-419 are fixed and will be released as next ORC release 1.5.4 and 1.6.0.
And, I'm still looking around both sides (ORC and Spark) for this issue.;;;","18/Oct/18 00:18;smilegator;Thanks!;;;","01/Jan/19 06:17;dongjoon;I'm closing this issue since this was resolved as a part of SPARK-26427 and I didn't hit this issues until now in our Jenkins environment. Please feel free to reopen this if there is another instance of this failure.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Parquet Binary DecimalType in VectorizedColumnReader,SPARK-23388,13137747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jamesthomp,jamesthomp,jamesthomp,11/Feb/18 22:42,05/Mar/18 06:54,13/Jul/23 08:45,12/Feb/18 19:37,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The following commit to spark removed support for decimal binary types: [https://github.com/apache/spark/commit/9c29c557635caf739fde942f53255273aac0d7b1#diff-7bdf5fd0ce0b1ccbf4ecf083611976e6R428]

As per the parquet spec, decimal can be used to annotate binary types, so support should be re-added: [https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#decimal]",,apachespark,cloud_fan,jamesthomp,robert3005,sameerag,Steven Rand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 13 17:45:00 UTC 2018,,,,,,,,,,"0|i3q1vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/18 22:57;apachespark;User 'jamesthomp' has created a pull request for this issue:
https://github.com/apache/spark/pull/20580;;;","13/Feb/18 08:42;cloud_fan;This is an interoperability problem: although Spark SQL always write out large precision decimal type as fixed-length-byte-array, Parquet spec also allow binary. In Spark 2.3 we may not be able to read parquet files written by other systems because of this bug.

cc [~sameerag] shall we include it in Spark 2.3.0?;;;","13/Feb/18 17:45;sameerag;yes, I agree;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport assertPandasEqual to branch-2.3.,SPARK-23387,13137699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,11/Feb/18 12:22,12/Dec/22 18:10,13/Jul/23 08:45,11/Feb/18 13:17,2.3.0,,,,,,,,,2.3.0,,,,,PySpark,SQL,Tests,,0,,,,"When backporting a pr with tests using {{assertPandasEqual}} from master to branch-2.3, the tests fail because {{assertPandasEqual}} doesn't exist in branch-2.3.
We should backport {{assertPandasEqual}} to branch-2.3 to avoid the failures.",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 11 13:17:18 UTC 2018,,,,,,,,,,"0|i3q1kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/18 12:38;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20577;;;","11/Feb/18 13:17;gurwls223;Issue resolved by pull request 20577
[https://github.com/apache/spark/pull/20577];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When it has no incomplete(completed) applications found, the last updated time is not formatted and client local time zone is not show in history server web ui.",SPARK-23384,13137679,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,guoxiaolongzte,guoxiaolongzte,guoxiaolongzte,11/Feb/18 06:36,23/Feb/18 06:33,13/Jul/23 08:45,13/Feb/18 12:24,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"When it has no incomplete(completed) applications found, the last updated time is not formatted and client local time zone is not show in history server web ui. It is a bug.

fix before: !1.png!

fix after:

!2.png!

 ",,apachespark,guoxiaolongzte,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/18 06:38;guoxiaolongzte;1.png;https://issues.apache.org/jira/secure/attachment/12910072/1.png","11/Feb/18 06:38;guoxiaolongzte;2.png;https://issues.apache.org/jira/secure/attachment/12910073/2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 11 14:51:11 UTC 2018,,,,,,,,,,"0|i3q1g7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/18 06:49;apachespark;User 'guoxiaolongzte' has created a pull request for this issue:
https://github.com/apache/spark/pull/20573;;;","11/Feb/18 14:51;srowen;Yes, seems like a clean fix for a small display problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bucketizer with multiple columns persistence bug,SPARK-23377,13137575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,bago.amirbekian,bago.amirbekian,09/Feb/18 23:11,20/Feb/18 20:50,13/Jul/23 08:45,15/Feb/18 19:24,2.3.0,,,,,,,,,2.3.0,2.4.0,,,,ML,,,,0,,,,"A Bucketizer with multiple input/output columns get ""inputCol"" set to the default value on write -> read which causes it to throw an error on transform. Here's an example.


{code:java}
import org.apache.spark.ml.feature._

val splits = Array(Double.NegativeInfinity, 0, 10, 100, Double.PositiveInfinity)
val bucketizer = new Bucketizer()
  .setSplitsArray(Array(splits, splits))
  .setInputCols(Array(""foo1"", ""foo2""))
  .setOutputCols(Array(""bar1"", ""bar2""))

val data = Seq((1.0, 2.0), (10.0, 100.0), (101.0, -1.0)).toDF(""foo1"", ""foo2"")
bucketizer.transform(data)

val path = ""/temp/bucketrizer-persist-test""
bucketizer.write.overwrite.save(path)
val bucketizerAfterRead = Bucketizer.read.load(path)
println(bucketizerAfterRead.isDefined(bucketizerAfterRead.outputCol))
// This line throws an error because ""outputCol"" is set
bucketizerAfterRead.transform(data)
{code}

And the trace:

{code:java}
java.lang.IllegalArgumentException: Bucketizer bucketizer_6f0acc3341f7 has the inputCols Param set for multi-column transform. The following Params are not applicable and should not be set: outputCol.
	at org.apache.spark.ml.param.ParamValidators$.checkExclusiveParams$1(params.scala:300)
	at org.apache.spark.ml.param.ParamValidators$.checkSingleVsMultiColumnParams(params.scala:314)
	at org.apache.spark.ml.feature.Bucketizer.transformSchema(Bucketizer.scala:189)
	at org.apache.spark.ml.feature.Bucketizer.transform(Bucketizer.scala:141)
	at line251821108a8a433da484ee31f166c83725.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-6079631:17)

{code}

",,apachespark,asolimando,bago.amirbekian,josephkb,mlnick,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23455,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 19:24:04 UTC 2018,,,,,,,,,,"0|i3q0t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/18 09:17;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20566;;;","12/Feb/18 17:47;josephkb;[~viirya]'s patch currently changes DefaultParamsWriter to save only the explicitly set Param values.  This means that loading a model into a new version of Spark could use different Param values if the default values have changed.

In the original design of persistence (see [SPARK-6725]), the goal was to make behavior exactly reproducible.  This means that default Param values do need to be saved.  I recommend that we maintain this guarantee.

I can see a couple of possibilities:
1. Simplest: Change the loading logic of Bucketizer so that it handles this edge case (by removing the value for inputCol when inputCols is set).  This may be best for Spark 2.3 since it's the fastest fix.
2. Reasonable: Change the saving logic of Bucketizer to handle this case.  This will be best in terms of fixing the edge case and being pretty quick to do.
3. Largest: Change DefaultParamsWriter to separate explicitly set values and default values.  Then update Bucketizer's loading logic to make use of this distinction.  I'm not a fan of this approach since it would involve shoving a huge change into branch-2.3 during late QA.

I'd vote strongly for the 2nd option now, and perhaps the 3rd option later on.  Opinions?;;;","13/Feb/18 02:11;viirya;I have no objection to [~josephkb]'s proposal (first 2nd and later 3rd).

 

The considering design is we should keep the default values of original Spark when saving the model, or use the default values of the Spark when loading the model. To keep the default values of original Spark, can make the behavior of the saved models reproducible. However, I have in mind that the behavior between loaded models and models created with current Spark can be different. E.g., The model ""foo"" from 2.1 with default value as ""a"" can reproducible behavior when loading back into 2.3. But it behaves differently with the same ""foo"" model created in 2.3 if the default value is changed to ""b"".

 

In other words, one is to keep the model behavior consistent before and after persistence even across Spark versions. Another one is to let the same kind of models has consistent behavior even they are coming from different Spark versions.

 

Current my patch follows the later one. I think the user should notice the change of default values in upgraded Spark, if they want to use old models. Btw, I also think of a rare but possible situation is, if we remove the default value from old version, the old models may not be easily loaded into new Spark.

 

 ;;;","13/Feb/18 02:21;viirya;For now, I think neither 3rd option or my current patch can be easily going into 2.3 because they are both not small change. So I also support to have the fixing of 2nd option first as a quick fix in 2.3. If no objection, I will prepare the fixing as a PR soon.;;;","13/Feb/18 04:59;josephkb;Thanks for reconsidering here [~viirya]!  I can help get the PR merged when ready.;;;","13/Feb/18 05:09;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20594;;;","13/Feb/18 11:46;mlnick;Should this be a blocker for 2.3? I think so since it should really be fixed before release.;;;","14/Feb/18 01:58;viirya;I agree with what [~mlnick] said.;;;","15/Feb/18 19:24;josephkb;Resolved in master and branch-2.3 with https://github.com/apache/spark/pull/20594;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
creating UnsafeKVExternalSorter with BytesToBytesMap may fail,SPARK-23376,13137454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,09/Feb/18 15:01,12/Feb/18 02:55,13/Jul/23 08:45,12/Feb/18 02:55,2.0.2,2.1.2,2.2.1,2.3.0,,,,,,2.2.2,2.3.0,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 12 02:55:42 UTC 2018,,,,,,,,,,"0|i3q027:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/18 15:20;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20561;;;","12/Feb/18 02:55;cloud_fan;Issue resolved by pull request 20561
[https://github.com/apache/spark/pull/20561];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicAllocation with failure in straggler task can lead to a hung spark job,SPARK-23365,13137333,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,09/Feb/18 03:27,29/Dec/20 04:14,13/Jul/23 08:45,27/Feb/18 19:13,2.1.2,2.2.1,2.3.0,,,,,,,2.3.1,2.4.0,,,,Scheduler,Spark Core,,,0,,,,"Dynamic Allocation can lead to a spark app getting stuck with 0 executors requested when the executors in the last tasks of a taskset fail (eg. with an OOM).

This happens when {{ExecutorAllocationManager}} s internal target number of executors gets out of sync with {{CoarseGrainedSchedulerBackend}} s target number.  {{EAM}} updates the {{CGSB}} in two ways: (1) it tracks how many tasks are active or pending in submitted stages, and computes how many executors would be needed for them.  And as tasks finish, it will actively decrease that count, informing the {{CGSB}} along the way.  (2) When it decides executors are inactive for long enough, then it requests that {{CGSB}} kill the executors -- this also tells the {{CGSB}} to update its target number of executors: https://github.com/apache/spark/blob/4df84c3f818aa536515729b442601e08c253ed35/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L622

So when there is just one task left, you could have the following sequence of events:
(1) the {{EAM}} sets the desired number of executors to 1, and updates the {{CGSB}} too
(2) while that final task is still running, the other executors cross the idle timeout, and the {{EAM}} requests the {{CGSB}} kill them
(3) now the {{EAM}} has a target of 1 executor, and the {{CGSB}} has a target of 0 executors

If the final task completed normally now, everything would be OK; the next taskset would get submitted, the {{EAM}} would increase the target number of executors and it would update the {{CGSB}}.

But if the executor for that final task failed (eg. an OOM), then the {{EAM}} thinks it [doesn't need to update anything|https://github.com/apache/spark/blob/4df84c3f818aa536515729b442601e08c253ed35/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L384-L386], because its target is already 1, which is all it needs for that final task; and the {{CGSB}} doesn't update anything either since its target is 0.

I think you can determine if this is the cause of a stuck app by looking for
{noformat}
yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
{noformat}
in the logs of the ApplicationMaster (at least on yarn).

You can reproduce this with this test app, run with {{--conf ""spark.dynamicAllocation.minExecutors=1"" --conf ""spark.dynamicAllocation.maxExecutors=5"" --conf ""spark.dynamicAllocation.executorIdleTimeout=5s""}}

{code}
import org.apache.spark.SparkEnv

sc.setLogLevel(""INFO"")

sc.parallelize(1 to 10000, 1000).count()

val execs = sc.parallelize(1 to 1000, 1000).map { _ => SparkEnv.get.executorId}.collect().toSet
val badExec = execs.head
println(""will kill exec "" + badExec)

sc.parallelize(1 to 5, 5).mapPartitions { itr =>
  val exec = SparkEnv.get.executorId
  if (exec == badExec) {
    Thread.sleep(20000) // long enough that all the other tasks finish, and the executors cross the idle timeout
    // now cause the executor to oom
    var buffers = Seq[Array[Byte]]()
    while(true) {
      buffers :+= new Array[Byte](1e8.toInt)
    }


    itr
  } else {
    itr
  }
}.collect()
{code}

*EDIT*: I adjusted the repro to cause an OOM on the bad executor, since {{sc.killExecutor}} doesn't play nice with dynamic allocation in other ways.",,apachespark,devaraj,irashid,kellyzly,vanzin,viirya,xuefuz,yumwang,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21834,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 29 04:14:40 UTC 2020,,,,,,,,,,"0|i3pzbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 22:14;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/20604;;;","27/Feb/18 19:13;vanzin;Issue resolved by pull request 20604
[https://github.com/apache/spark/pull/20604];;;","23/Mar/18 18:52;irashid;This is mostly a duplicate of  SPARK-21834, though I'm not marking it as such since both had committed changes, and I think this change is still good as useful cleanup.

The change from SPARK-21834 is actually sufficient to solve the problem described above.;;;","29/Dec/20 04:14;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30956;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Driver restart fails if it happens after 7 days from app submission,SPARK-23361,13137274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,08/Feb/18 21:57,17/May/20 18:13,13/Jul/23 08:45,23/Mar/18 06:00,2.1.0,,,,,,,,,2.4.0,,,,,Spark Core,YARN,,,0,,,,"If you submit an app that is supposed to run for > 7 days (so using \-\principal / \-\-keytab in cluster mode), and there's a failure that causes the driver to restart after 7 days (that being the default token lifetime for HDFS), the new driver will fail with an error like the following:

{noformat}
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (lots of uninteresting token info) can't be found in cache
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2123)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1253)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1249)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1249)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$6.apply(ApplicationMaster.scala:160)
{noformat}

Note: lines may not align with actual Apache code because that's our internal build.

This happens because as part of the app submission, the launcher provides delegation tokens to be used by the AM (=driver in this case), and those are expired at that point in time.",,apachespark,devaraj,jerryshao,mgaido,rajeshhadoop,roczei,Steven Rand,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27515,,,,,,,SPARK-27891,SPARK-23781,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 06:00:02 UTC 2018,,,,,,,,,,"0|i3pyy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/18 02:21;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20657;;;","23/Mar/18 06:00;jerryshao;Issue resolved by pull request 20657
[https://github.com/apache/spark/pull/20657];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSession.createDataFrame timestamps can be incorrect with non-Arrow codepath,SPARK-23360,13137260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,icexelloss,icexelloss,08/Feb/18 21:06,12/Dec/22 18:10,13/Jul/23 08:45,10/Feb/18 16:08,2.3.0,,,,,,,,,2.3.0,,,,,PySpark,,,,0,,,,"{code:java}
import datetime
import pandas as pd
import os

dt = [datetime.datetime(2015, 10, 31, 22, 30)]
pdf = pd.DataFrame({'time': dt})

os.environ['TZ'] = 'America/New_York'

df1 = spark.createDataFrame(pdf)
df1.show()

+-------------------+
|               time|
+-------------------+
|2015-10-31 21:30:00|
+-------------------+
{code}
Seems to related to this line here:

[https://github.com/apache/spark/blob/master/python/pyspark/sql/types.py#L1776]

It appears to be an issue with ""tzlocal()""

Wrong:
{code:java}
from_tz = ""America/New_York""
to_tz = ""tzlocal()""

s.apply(lambda ts:  ts.tz_localize(from_tz,ambiguous=False).tz_convert(to_tz).tz_localize(None)
if ts is not pd.NaT else pd.NaT)

0   2015-10-31 21:30:00
Name: time, dtype: datetime64[ns]
{code}
Correct:
{code:java}
from_tz = ""America/New_York""
to_tz = ""America/New_York""

s.apply(
lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
if ts is not pd.NaT else pd.NaT)

0   2015-10-31 22:30:00
Name: time, dtype: datetime64[ns]
{code}",,apachespark,icexelloss,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 10 16:08:58 UTC 2018,,,,,,,,,,"0|i3pyv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/18 21:07;icexelloss;cc [~cloud_fan] [~ueshin] [~hyukjin.kwon]

This is a regression from 2.2.1;;;","08/Feb/18 21:08;icexelloss;Also this works fine in Arrow path.;;;","08/Feb/18 22:08;icexelloss;cc [~bryanc] as well. I tried to fix this but didn't succeed. I don't understand the timestamp area in Spark too well. I think someone understands the area probably has a better chance of fixing this one than me.;;;","09/Feb/18 13:52;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20559;;;","10/Feb/18 16:08;gurwls223;Issue resolved by pull request 20559
[https://github.com/apache/spark/pull/20559];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the number of partitions is greater than 2^28, it will result in an error result",SPARK-23358,13137128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,08/Feb/18 11:29,12/Feb/18 14:33,13/Jul/23 08:45,09/Feb/18 14:45,2.3.0,,,,,,,,,2.2.2,2.3.0,,,,Spark Core,,,,0,,,,"In the `checkIndexAndDataFile`,the _blocks_ is the  _Int_ type,  when it is greater than 2^28, `blocks*8` will overflow, and this will result in an error result.
 In fact, `blocks` is actually the number of partitions.",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23391,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 12 01:49:04 UTC 2018,,,,,,,,,,"0|i3py1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/18 11:33;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/20544;;;","09/Feb/18 14:45;srowen;Issue resolved by pull request 20544
[https://github.com/apache/spark/pull/20544];;;","12/Feb/18 01:49;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/20581;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convertMetastore should not ignore table properties,SPARK-23355,13136960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,07/Feb/18 20:58,07/Nov/18 00:32,13/Jul/23 08:45,27/Apr/18 03:02,2.0.2,2.1.2,2.2.1,2.3.0,,,,,,2.4.0,,,,,SQL,,,,0,,,,"`convertMetastoreOrc/Parquet` ignores table properties.

This happens for a table created by `STORED AS ORC/PARQUET`.

Currently, `convertMetastoreParquet` is `true` by default. So, it's a well-known user-facing issue. For `convertMetastoreOrc`, it has been `false` and  SPARK-22279 tried to turn on that for Feature Parity with Parquet.

However, it's indeed a regression for the existing Hive ORC table users. So, it'll be reverted for Apache Spark 2.3 via https://github.com/apache/spark/pull/20536 .",,apachespark,cloud_fan,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,SPARK-22158,,,,,,,,,SPARK-17166,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 08 16:03:06 UTC 2018,,,,,,,,,,"0|i3px0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/18 21:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20522;;;","27/Apr/18 03:02;cloud_fan;Issue resolved by pull request 20522
[https://github.com/apache/spark/pull/20522];;;","08/May/18 16:03;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21269;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
checkpoint corruption in long running application,SPARK-23351,13136814,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,davidahern,davidahern,07/Feb/18 10:44,27/Feb/18 09:57,13/Jul/23 08:45,27/Feb/18 09:57,2.2.0,,,,,,,,,2.2.1,,,,,Structured Streaming,,,,0,,,,"hi, after leaving my (somewhat high volume) Structured Streaming application running for some time, i get the following exception.  The same exception also repeats when i try to restart the application.  The only way to get the application back running is to clear the checkpoint directory which is far from ideal.

Maybe a stream is not being flushed/closed properly internally by Spark when checkpointing?

 
 User class threw exception: org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 55 in stage 1.0 failed 4 times, most recent failure: Lost task 55.3 in stage 1.0 (TID 240, gbslixaacspa04u.metis.prd, executor 2): java.io.EOFException
 at java.io.DataInputStream.readInt(DataInputStream.java:392)
 at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(HDFSBackedStateStoreProvider.scala:481)
 at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:359)
 at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:358)
 at scala.Option.getOrElse(Option.scala:121)
 at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:358)
 at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:265)
 at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:200)
 at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:61)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
 at org.apache.spark.scheduler.Task.run(Task.scala:108)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)",,davidahern,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 27 09:57:42 UTC 2018,,,,,,,,,,"0|i3pw3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/18 21:57;zsxwing;What's your file system? HDFS?;;;","08/Feb/18 22:01;zsxwing;I believe this should be resolved in https://issues.apache.org/jira/browse/SPARK-21696. Could you try Spark 2.2.1?;;;","13/Feb/18 11:15;davidahern;hi, yes - it is HDFS

am on Cloudera... only 2.2.0 is available from them for now

will have to wait for the upgrade... think they only upgrade on major versions e.g. 2.3.0

just for the record, had another type of exception occur a while  back... probably related, but it was mostly the one i reported above

18/01/26 09:56:14 INFO yarn.Client: Application report for application_1513677310005_0840 (state: FINISHED) 18/01/26 09:56:14 INFO yarn.Client: client token: N/A diagnostics: User class threw exception: org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 156 in stage 1.0 failed 4 times, most recent failure: Lost task 156.3 in stage 1.0 (TID 392, gbslixaacspa06u.metis.prd, executor 1): java.io.EOFException: Stream ended prematurely at org.apache.spark.io.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:230) at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:203) at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125) at java.io.DataInputStream.read(DataInputStream.java:149) at org.spark_project.guava.io.ByteStreams.read(ByteStreams.java:899) at org.spark_project.guava.io.ByteStreams.readFully(ByteStreams.java:733) at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(HDFSBackedStateStoreProvider.scala:489) at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:359) at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:358) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:358) at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:265) at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:200) at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:61) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745);;;","13/Feb/18 22:01;zsxwing;[~davidahern] It's better to ask the vendor for support. They may have a different release cycle.;;;","13/Feb/18 22:17;srowen;[~davidahern] for the record, it's usually the opposite. Vendor branches like Cloudera say ""2.2.0"" but should be read like ""2.2.x"". They will as a rule only vary from the upstream branch in which commits go into a maintenance branch. This fix could be in a vendor 2.2.x release that isn't in an upstream one (or vice versa), or appear in a maintenance branch earlier from a vendor. That's in theory the value proposition; in this particular case I have no idea.;;;","27/Feb/18 09:56;davidahern;will close this assuming it's fixed in future version... will reopen if necessary;;;","27/Feb/18 09:57;davidahern;already fixed in future version... will test when available by Cloudera;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
append data using saveAsTable should adjust the data types,SPARK-23348,13136778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Feb/18 08:35,08/Feb/18 08:26,13/Jul/23 08:45,08/Feb/18 08:26,2.0.2,2.1.2,2.2.1,2.3.0,,,,,,2.3.0,,,,,SQL,,,,1,,,," 
{code:java}
Seq(1 -> ""a"").toDF(""i"", ""j"").write.saveAsTable(""t"")

Seq(""c"" -> 3).toDF(""i"", ""j"").write.mode(""append"").saveAsTable(""t"")

scala> sql(""select * from t"").show
{code}
 
This query will fail with a strange error:

{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 10.0 failed 1 times, most recent failure: Lost task 1.0 in stage 10.0 (TID 15, localhost, executor driver): java.lang.UnsupportedOperationException: Unimplemented type: IntegerType
 at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:473)
 at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:214)
 at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)
...
{code}
 
All Spark 2.X are the same. For Spark 1.6.3,
{code}
scala> sql(""select * from tx"").show
+----+---+
|   i|  j|
+----+---+
|null|  3|
|   1|  a|
+----+---+
{code}",,apachespark,cloud_fan,dongjoon,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 07 22:33:37 UTC 2018,,,,,,,,,,"0|i3pvvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/18 08:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20527;;;","07/Feb/18 22:27;dongjoon;[~cloud_fan], [~smilegator], [~sameerag].
Although this is not a regression at Spark 2.3, can we have this Apache Spark 2.3?;;;","07/Feb/18 22:33;sameerag;yes, +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: FileBasedDataSourceSuite,SPARK-23345,13136635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,vanzin,vanzin,06/Feb/18 18:42,07/Feb/18 17:49,13/Jul/23 08:45,07/Feb/18 17:49,2.4.0,,,,,,,,,2.3.0,,,,,SQL,Tests,,,0,,,,"See at least once:
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/87114/testReport/junit/org.apache.spark.sql/FileBasedDataSourceSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

{noformat}
Error Message
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 650 times over 10.011358752 seconds. Last failure message: There are 1 possibly leaked file streams..

Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 650 times over 10.011358752 seconds. Last failure message: There are 1 possibly leaked file streams..
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:421)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:439)
	at org.apache.spark.sql.FileBasedDataSourceSuite.eventually(FileBasedDataSourceSuite.scala:26)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:337)
	at org.apache.spark.sql.FileBasedDataSourceSuite.eventually(FileBasedDataSourceSuite.scala:26)
	at org.apache.spark.sql.test.SharedSparkSession$class.afterEach(SharedSparkSession.scala:114)
	at org.apache.spark.sql.FileBasedDataSourceSuite.afterEach(FileBasedDataSourceSuite.scala:26)
	at org.scalatest.BeforeAndAfterEach$$anonfun$1.apply$mcV$sp(BeforeAndAfterEach.scala:234)
{noformat}

Log file is too large to attach here, though.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 07 06:15:04 UTC 2018,,,,,,,,,,"0|i3pv07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/18 06:15;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20524;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache ORC to 1.4.3,SPARK-23340,13136311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,05/Feb/18 16:49,19/Apr/18 16:50,13/Jul/23 08:45,15/Feb/18 16:57,2.3.0,,,,,,,,,2.3.1,2.4.0,,,,Build,SQL,,,0,,,,"This issue updates Apache ORC dependencies to 1.4.3 released on February 9th.
Apache ORC 1.4.2 release removes unnecessary dependencies and 1.4.3 has 5 more patches including bug fixes (https://s.apache.org/Fll8).
Especially, the following ORC-285 is fixed at 1.4.3.

{code}
scala> val df = Seq(Array.empty[Float]).toDF()

scala> df.write.format(""orc"").save(""/tmp/floatarray"")

scala> spark.read.orc(""/tmp/floatarray"")
res1: org.apache.spark.sql.DataFrame = [value: array<float>]

scala> spark.read.orc(""/tmp/floatarray"").show()
18/02/12 22:09:10 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.io.IOException: Error reading file: file:/tmp/floatarray/part-00000-9c0b461b-4df1-4c23-aac1-3e4f349ac7d6-c000.snappy.orc
	at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1191)
	at org.apache.orc.mapreduce.OrcMapreduceRecordReader.ensureBatch(OrcMapreduceRecordReader.java:78)
...
Caused by: java.io.EOFException: Read past EOF for compressed stream Stream for column 2 kind DATA position: 0 length: 0 range: 0 offset: 0 limit: 0
{code}",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,SPARK-21783,,,,,,,,,,,,SPARK-23426,,,ORC-285,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 05:34:05 UTC 2018,,,,,,,,,,"0|i3pt0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/18 16:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20511;;;","18/Apr/18 05:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI SQL executions page throws NPE,SPARK-23330,13136108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,04/Feb/18 07:31,05/Feb/18 22:17,13/Jul/23 08:45,05/Feb/18 22:17,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Spark UI SQL executions page throws the following error and the page crashes:

{code}
 HTTP ERROR 500
 Problem accessing /SQL/. Reason:

Server Error
 Caused by:
 java.lang.NullPointerException
 at scala.collection.immutable.StringOps$.length$extension(StringOps.scala:47)
 at scala.collection.immutable.StringOps.length(StringOps.scala:47)
 at scala.collection.IndexedSeqOptimized$class.isEmpty(IndexedSeqOptimized.scala:27)
 at scala.collection.immutable.StringOps.isEmpty(StringOps.scala:29)
 at scala.collection.TraversableOnce$class.nonEmpty(TraversableOnce.scala:111)
 at scala.collection.immutable.StringOps.nonEmpty(StringOps.scala:29)
 at org.apache.spark.sql.execution.ui.ExecutionTable.descriptionCell(AllExecutionsPage.scala:182)
 at org.apache.spark.sql.execution.ui.ExecutionTable.row(AllExecutionsPage.scala:155)
 at org.apache.spark.sql.execution.ui.ExecutionTable$$anonfun$8.apply(AllExecutionsPage.scala:204)
 at org.apache.spark.sql.execution.ui.ExecutionTable$$anonfun$8.apply(AllExecutionsPage.scala:204)
 at org.apache.spark.ui.UIUtils$$anonfun$listingTable$2.apply(UIUtils.scala:339)
 at org.apache.spark.ui.UIUtils$$anonfun$listingTable$2.apply(UIUtils.scala:339)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
 at scala.collection.AbstractTraversable.map(Traversable.scala:104)
 at org.apache.spark.ui.UIUtils$.listingTable(UIUtils.scala:339)
 at org.apache.spark.sql.execution.ui.ExecutionTable.toNodeSeq(AllExecutionsPage.scala:203)
 at org.apache.spark.sql.execution.ui.AllExecutionsPage.render(AllExecutionsPage.scala:67)
 at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
 at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:82)
 at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
 at org.eclipse.jetty.server.Server.handle(Server.java:534)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
 at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
 at java.lang.Thread.run(Thread.java:748)
{code}

 Seems the bug is imported by [https://github.com/apache/spark/pull/19681/files#diff-a74d84702d8d47d5269e96740a55a3caR63]",,apachespark,jiangxb1987,sameerag,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 05 22:17:41 UTC 2018,,,,,,,,,,"0|i3prrr:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"04/Feb/18 07:59;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20502;;;","05/Feb/18 22:17;vanzin;Issue resolved by pull request 20502
[https://github.com/apache/spark/pull/20502];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Scheduler Delay"" of a task is confusing",SPARK-23326,13135980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,03/Feb/18 00:24,06/Feb/18 06:47,13/Jul/23 08:45,06/Feb/18 06:47,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Run the following code and check the UI
{code}
sc.makeRDD(1 to 1, 1).foreach { i => Thread.sleep(30000) }
{code}

You will see ""Scheduler Delay"" of a task is almost the same as ""Duration"" until the task finishes. That's really confusing.

In Spark 2.2,  ""Scheduler Delay"" will be 0 until the task finishes. This is also not correct but less confusing.",,apachespark,cloud_fan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 06 06:47:53 UTC 2018,,,,,,,,,,"0|i3pqzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/18 00:30;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20493;;;","06/Feb/18 06:47;cloud_fan;Issue resolved by pull request 20493
[https://github.com/apache/spark/pull/20493];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnalysisException after max iteration reached for IN query,SPARK-23316,13135737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bograd,bograd,bograd,02/Feb/18 10:37,05/Mar/18 07:04,13/Jul/23 08:45,20/Feb/18 20:42,2.3.0,2.4.0,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Query to reproduce:
{code:scala}
spark.range(10).where(""(id,id) in (select id, null from range(3))"").show
{code}

{code}
18/02/02 11:32:31 WARN BaseSessionStateBuilder$$anon$1: Max iterations (100) reached for batch Resolution
org.apache.spark.sql.AnalysisException: cannot resolve '(named_struct('id', `id`, 'id', `id`) IN (listquery()))' due to data type mismatch:
The data type of one or more elements in the left hand side of an IN subquery
is not compatible with the data type of the output of the subquery
Mismatched columns:
[]
Left side:
[bigint, bigint].
Right side:
[bigint, bigint].;;
{code}

The error message includes the last plan which contains ~100 useless Projects.
Does not happen in branch-2.2.
It has something to do with TypeCoercion, it is doing a futile attempt  to change nullability.",,apachespark,bograd,dkbiswal,dongjoon,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 20 20:42:03 UTC 2018,,,,,,,,,,"0|i3pphb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"02/Feb/18 11:29;bograd;I think it's related to {{In.checkInputTypes}} in 2.2: it does not check nullability while in 2.3 it does , by using {{DataType.equalsStructurally}};;;","02/Feb/18 11:42;bograd;I'll work on a fix;;;","08/Feb/18 15:13;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/20547;;;","08/Feb/18 15:22;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/20548;;;","20/Feb/18 20:42;dongjoon;Hi, [~smilegator].
Could you update the assignee with [~bograd]?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed to get output from canonicalized data source v2 related plans,SPARK-23315,13135702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,02/Feb/18 07:52,06/Feb/18 20:44,13/Jul/23 08:45,06/Feb/18 20:44,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 02 07:58:03 UTC 2018,,,,,,,,,,"0|i3pp9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/18 07:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20485;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI should sort jobs/stages with the completed timestamp before cleaning up them,SPARK-23307,13135605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,01/Feb/18 21:19,05/Feb/18 10:43,13/Jul/23 08:45,05/Feb/18 10:43,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"When you have a long running job, it may be deleted from UI quickly when it completes, if you happen to run a small job after it. It's pretty annoying when you run lots of jobs in the same driver concurrently (e.g., running multiple Structured Streaming queries). We should sort jobs/stages with the completed timestamp before cleaning up them.

In 2.2, Spark has a separated buffer for completed jobs/stages, so it doesn't need to sort the jobs/stages.

What's the behavior I expect:

Set ""spark.ui.retainedJobs"" to 10 and run the following codes, job 0 should be kept in the Spark UI.

 
{code:java}
new Thread() {
  override def run() {

    // job 0
    sc.makeRDD(1 to 1, 1).foreach { i =>
    Thread.sleep(10000)
   }
  }
}.start()

Thread.sleep(1000)

for (_ <- 1 to 20) {
  new Thread() {
    override def run() {
      sc.makeRDD(1 to 1, 1).foreach { i =>
      }
    }
  }.start()
}

Thread.sleep(15000)
  sc.makeRDD(1 to 1, 1).foreach { i =>
}

{code}",,ajbozarth,apachespark,cloud_fan,sameerag,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 05 10:43:52 UTC 2018,,,,,,,,,,"0|i3poo7:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"01/Feb/18 21:20;zsxwing;cc [~vanzin] [~cloud_fan];;;","01/Feb/18 21:41;sameerag;Bumping this to a blocker for 2.3;;;","01/Feb/18 22:21;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20481;;;","05/Feb/18 10:43;cloud_fan;Issue resolved by pull request 20481
[https://github.com/apache/spark/pull/20481];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in TaskMemoryManager,SPARK-23306,13135577,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhzhan,zhzhan,zhzhan,01/Feb/18 20:22,02/Feb/18 04:25,13/Jul/23 08:45,02/Feb/18 04:22,2.2.1,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"There is race condition in TaskMemoryManger, which may cause OOM.

The memory released may be taken by another task because there is a gap between releaseMemory and acquireMemory, e.g., UnifiedMemoryManager, causing the OOM. if the current is the only one that can perform spill. It can happen to BytesToBytesMap, as it only spill required bytes.",,apachespark,cloud_fan,zhzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 02 04:22:32 UTC 2018,,,,,,,,,,"0|i3pohz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/18 20:28;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20480;;;","02/Feb/18 04:22;cloud_fan;Issue resolved by pull request 20480
[https://github.com/apache/spark/pull/20480];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data source v2 column pruning with arbitrary expressions is broken,SPARK-23301,13135430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cloud_fan,cloud_fan,cloud_fan,01/Feb/18 14:10,05/Mar/18 06:46,13/Jul/23 08:45,01/Feb/18 18:51,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 01 14:41:04 UTC 2018,,,,,,,,,,"0|i3pnlb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"01/Feb/18 14:41;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20476;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
__repr__ broken for Rows instantiated with *args,SPARK-23299,13135397,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,oli-hall,oli-hall,01/Feb/18 11:29,12/Dec/22 18:10,13/Jul/23 08:45,06/May/19 17:04,1.5.0,2.2.0,,,,,,,,3.0.0,,,,,PySpark,,,,0,,,,"PySpark Rows throw an exception if instantiated without column names when `__repr__` is called. The most minimal reproducible example I've found is this:
{code:java}
> from pyspark.sql.types import Row
> Row(123)
<stack-trace snipped for brevity>
<v-env location>/lib/python2.7/site-packages/pyspark/sql/types.pyc in __repr__(self)
-> 1524             return ""<Row(%s)>"" % "", "".join(self)

TypeError: sequence item 0: expected string, int found{code}
This appears to be due to the implementation of `__repr__`, which works excellently for Rows created with column names, but for those without, assumes all values are strings ([link here|https://github.com/apache/spark/blob/master/python/pyspark/sql/types.py#L1584]).

This should be an easy fix, if the values are mapped to `str` first, all should be well (last line is the only modification):
{code:java}
def __repr__(self):
    """"""Printable representation of Row used in Python REPL.""""""
    if hasattr(self, ""__fields__""):
        return ""Row(%s)"" % "", "".join(""%s=%r"" % (k, v)
                                     for k, v in zip(self.__fields__, tuple(self)))
    else:
        ""<Row(%s)>"" % "", "".join(map(str, self))
{code}
This will yield the following:
{code:java}
> from pyspark.sql.types import Row
> Row('aaa', 123)
<Row(aaaa, 123)>
{code}
  ",Tested on OS X with Spark 1.5.0 as well as pip-installed `pyspark` 2.2.0. Code in question appears to still be in error on the master branch of the GitHub repository.,apachespark,holdenkarau,oli-hall,shashwat.anand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 06 17:04:26 UTC 2019,,,,,,,,,,"0|i3pndz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/18 07:40;shashwat.anand;Seems like a valid issue to me.  [~srowen] Should I send a PR for this ?;;;","04/Feb/18 09:55;gurwls223;Yup, just took a quick look and seems roughly correct. Please go ahead.;;;","04/Feb/18 10:00;gurwls223;Seems we should use {{repr}} instead of {{str}} BTW.;;;","04/Feb/18 10:26;shashwat.anand;[~hyukjin.kwon] But StructType is using _str_.  Should not we use either _repr_ at both places or _str_ at both?;;;","04/Feb/18 11:34;apachespark;User 'ashashwat' has created a pull request for this issue:
https://github.com/apache/spark/pull/20503;;;","13/Feb/18 08:11;shashwat.anand;[~hyukjin.kwon]   What do we do about this ?;;;","06/May/19 17:04;holdenkarau;Merged a fix for this for 3, we can continue the discussion around backporting and update the fix version if we do backport.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data source v2 self join fails,SPARK-23293,13135308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,01/Feb/18 05:05,02/Feb/18 01:10,13/Jul/23 08:45,02/Feb/18 01:10,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 01 05:10:03 UTC 2018,,,,,,,,,,"0|i3pmu7:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"01/Feb/18 05:10;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20466;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkR : substr : In SparkR dataframe , starting and ending position arguments in ""substr"" is giving wrong result  when the position is greater than 1",SPARK-23291,13135274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,narendra_k,narendra_k,01/Feb/18 01:17,12/Dec/22 18:11,13/Jul/23 08:45,07/Mar/18 17:38,2.1.2,2.2.0,2.2.1,2.3.0,,,,,,2.3.1,2.4.0,,,,SparkR,,,,0,,,,"Defect Description :

-----------------------------

For example ,an input string ""2017-12-01"" is read into a SparkR dataframe ""df"" with column name ""col1"".
 The target is to create a a new column named ""col2"" with the value ""12"" which is inside the string .""12"" can be extracted with ""starting position"" as ""6"" and ""Ending position"" as ""7""
 (the starting position of the first character is considered as ""1"" )

But,the current code that needs to be written is :
 
 df <- withColumn(df,""col2"",substr(df$col1,7,8)))

Observe that the first argument in the ""substr"" API , which indicates the 'starting position', is mentioned as ""7"" 
 Also, observe that the second argument in the ""substr"" API , which indicates the 'ending position', is mentioned as ""8""

i.e the number that should be mentioned to indicate the position should be the ""actual position + 1""

Expected behavior :

----------------------------

The code that needs to be written is :
 
 df <- withColumn(df,""col2"",substr(df$col1,6,7)))


Note :

-----------
 This defect is observed with only when the starting position is greater than 1.",,apachespark,cloud_fan,felixcheung,narendra_k,viirya,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 07 05:02:06 UTC 2018,,,,,,,,,,"0|i3pmmn:",9223372036854775807,,,,,felixcheung,,,,,,,,2.3.1,2.4.0,,,,,,,,,"01/Feb/18 03:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20464;;;","28/Mar/18 03:43;cloud_fan;shall we backport this bug fix to 2.3?;;;","29/Mar/18 08:26;viirya;Because it is related to behavior change, I'm hesitant to make a backport to 2.3 right now.;;;","04/May/18 01:56;gurwls223;[~cloud_fan], you feel like we should backport this? I was looking through PRs for backporting thing and seems we better backport it with a note in migration guide.
I think it was just a bad design mistake. It doesn't even match to R's one. Workaround should relatively easy if users are aware of this change.
If you feel about backporting it too, I would rather like to backport this.;;;","04/May/18 05:05;cloud_fan;Yea I think we should backport it.;;;","04/May/18 05:44;gurwls223;[~felixcheung] WDYT?;;;","04/May/18 21:02;yanboliang;This should be backported to Spark 2.3, as this is a bug fix and we can't wait several months for the next release. [~hyukjin.kwon] Do you like to send a PR? Thanks.;;;","06/May/18 08:20;gurwls223;Sure, will do.;;;","06/May/18 08:59;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21249;;;","06/May/18 09:04;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21250;;;","06/May/18 22:34;felixcheung;actually, I'm not sure we should backport this to a x.x.1 release.

yes, the behavior ""was unexpected"" but it has been around for the last 3 years, if I recall, since the very beginning. and it is not a regression per se.

either users don't care since it has never been reported, or (most likely) users have adopted to the behavior in which case we will break existing jobs in a patch release.

anyway, it's just my 2c.;;;","07/May/18 02:01;gurwls223;[~felixcheung], sure, I agree with that in general. However, we could probably think about this way too for this case specifically:

in other words, it has been wrong for 3 years, it requires weird codes for R specifically comparing to other languages APIs. IMHO, It's a bit subtlety and users might be adopted to this bugs rather than bothering to report this out (of course I guess with some nuisance). Think about this expr(""substr(...)"") and substr work differently. I am also seeing [expr(""substr(...)"") is suggested as an alternative of substr|https://stackoverflow.com/questions/37413122/use-of-substr-on-dataframe-column-in-sparkr?rq=1]  If it's clearly documented in the migration guide, I thought it can be fine.

Also, this substr case is pretty well understood and isolated.

As a reference, I recall a case - https://github.com/apache/spark/pull/20499#issuecomment-363863660. It sounds pretty a similar case with that. I was hesitant at that time too but after thinking for a while, I ended up with kind of agreeing that the backport is okay. It wasn't a regression at that time too.
;;;","07/May/18 05:02;felixcheung;I don't disagree the behavior issue. (ah, so someone did run into it in 2016)

If I recall, a few folks have brought up recently [https://semver.org/] I think it might be worthwhile whether we are following the principles of semantic versioning... or not.

I don't feel strongly about this so I'll leave others to comment.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inadvertent change in handling of DateType when converting to pandas dataframe,SPARK-23290,13135259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ueshin,amenck,amenck,31/Jan/18 23:32,12/Dec/22 18:10,13/Jul/23 08:45,06/Feb/18 09:30,2.3.0,,,,,,,,,2.3.0,,,,,PySpark,,,,0,,,,"In [this PR|https://github.com/apache/spark/pull/18664/files#diff-6fc344560230bf0ef711bb9b5573f1faR1968] there was a change in how `DateType` is being returned to users (line 1968 in dataframe.py). This can cause client code to fail, as in the following example from a python terminal:

{code:python}
>>> pdf = pd.DataFrame([['2015-01-01',1]], columns=['date', 'num'])
>>> pdf.dtypes
date    object
num      int64
dtype: object
>>> pdf['date'].apply(lambda d: dt.datetime.strptime(d, '%Y-%m-%d').date() )
0    2015-01-01
Name: date, dtype: object
>>> pdf = pd.DataFrame([['2015-01-01',1]], columns=['date', 'num'])
>>> pdf.dtypes
date    object
num      int64
dtype: object
>>> pdf['date'] = pd.to_datetime(pdf['date'])
>>> pdf.dtypes
date    datetime64[ns]
num              int64
dtype: object
>>> pdf['date'].apply(lambda d: dt.datetime.strptime(d, '%Y-%m-%d').date() )
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/amenck/anaconda2/lib/python2.7/site-packages/pandas/core/series.py"", line 2355, in apply
    mapped = lib.map_infer(values, f, convert=convert_dtype)
  File ""pandas/_libs/src/inference.pyx"", line 1574, in pandas._libs.lib.map_infer
  File ""<stdin>"", line 1, in <lambda>
TypeError: strptime() argument 1 must be string, not Timestamp
>>> 
{code}

Above we show both the old behavior (returning an ""object"" col) and the new behavior (returning a datetime column). Since there may be user code relying on the old behavior, I'd suggest reverting this specific part of this change. Also note that the NOTE on the docstring for the ""_to_corrected_pandas_type"" seems to be off, referring to the old behavior and not the current one.",,aash,amenck,apachespark,mlnick,robert3005,sameerag,smilegator,Steven Rand,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 06 09:30:13 UTC 2018,,,,,,,,,,"0|i3pmjb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"01/Feb/18 08:50;mlnick;cc [~bryanc];;;","02/Feb/18 05:06;ueshin;Thanks for the report!

I'm afraid I couldn't figure out what's going on because your example is something wrong.

In your first example, the dtype of {{pdf['date']}} seems {{object}}, but the actual type is {{str}}:

{code:python}
>>> pdf = pd.DataFrame([['2015-01-01',1]], columns=['date', 'num'])
>>> pdf.dtypes
date    object
num      int64
dtype: object
>>> type(pdf['date'][0])
<type 'str'>
{code}

So the lambda should work because the function in the lambda is for string type:

{code:python}
>>> pdf['date'].apply(lambda d: dt.datetime.strptime(d, '%Y-%m-%d').date() )
0    2015-01-01
Name: date, dtype: object
{code}

Whereas Spark returns {{datetime.date}} in 2.2 and {{pd.Timestamp}} in 2.3:

{code:python}
>>> df = spark.createDataFrame([('2015-01-01', 1)], ['date', 'num']).selectExpr(""cast(date as date)"", ""num"")
>>> df.printSchema()
root
 |-- date: date (nullable = true)
 |-- num: long (nullable = true)

>>> df.show()
+----------+---+
|      date|num|
+----------+---+
|2015-01-01|  1|
+----------+---+
{code}

in 2.2:

{code:python}
>>> pdf = df.toPandas()
>>> pdf.dtypes
date    object
num      int64
dtype: object
>>> type(pdf['date'][0])
<type 'datetime.date'>
{code}

in 2.3:

{code:python}
>>> pdf = df.toPandas()
>>> pdf.dtypes
date    datetime64[ns]
num              int64
dtype: object
>>> type(pdf['date'][0])
<class 'pandas._libs.tslib.Timestamp'>
{code}

In this case, the lambda shouldn't work anyway.

Could you provide some other example to elaborate the problem?

IIUC, {{datetime.date}} and {{pd.Timestamp}} are kind of compatible, so we can handle them in the same way. cc: [~bryanc]

Thanks!
;;;","02/Feb/18 19:42;sameerag;[~amenck] [~aash] any updates here?;;;","02/Feb/18 20:28;amenck;Hey [~ueshin] apologies, I tried to come up with a simpler example of the failure I saw, and ended up with an incorrect one! Here is a more straight-forward example of the failure in 2.3, specifically due to joining on columns with different (but similar) types:
{code}
>>> pdf = df.toPandas()
>>> pdf.dtypes
date    datetime64[ns]
num              int64
dtype: object
>>> type(pdf['date'][0])
<class 'pandas._libs.tslib.Timestamp'>
>>> user_provided_pdf
         date  num
0  2015-01-01    1
>>> user_provided_pdf.dtypes
date    object
num      int64
dtype: object
>>> type(user_provided_pdf['date'][0])
<type 'datetime.date'>
{code}
At this point, a simple example of change in functionality would be checking equality:
{code}
>>> pdf.loc[0,'date'] == user_provided_pdf.loc[0,'date']
False
{code}
In reality, I hit this when executing a join with a pandas dataframe obtained from another source:
{code}
>>> pdf.merge(user_provided_pdf, on=['date'], how='inner')
Empty DataFrame
Columns: [date, num_x, num_y]
Index: []
{code}
For 2.2, the equality above would hold and this join would produce a non-trivial output.;;;","05/Feb/18 08:09;ueshin;Thanks [~amenck] for clarifying.
I'll submit a pr for modifying to use {{datetime.date}} for date type and let's see feedback from the community.;;;","05/Feb/18 08:32;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20506;;;","06/Feb/18 07:48;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20515;;;","06/Feb/18 09:30;gurwls223;Issue resolved by pull request 20515
[https://github.com/apache/spark/pull/20515];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OneForOneBlockFetcher.DownloadCallback.onData may write just a part of data,SPARK-23289,13135241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,31/Jan/18 21:52,01/Feb/18 13:05,13/Jul/23 08:45,01/Feb/18 13:05,2.2.0,2.2.1,2.3.0,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,,,apachespark,cloud_fan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 01 13:05:59 UTC 2018,,,,,,,,,,"0|i3pmfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/18 21:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20461;;;","01/Feb/18 13:05;cloud_fan;Issue resolved by pull request 20461
[https://github.com/apache/spark/pull/20461];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect number of written records in structured streaming,SPARK-23288,13135151,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,bondyk,bondyk,31/Jan/18 17:46,21/Mar/18 17:07,13/Jul/23 08:45,21/Mar/18 17:07,2.2.0,,,,,,,,,2.3.1,2.4.0,,,,SQL,Structured Streaming,,,0,metrics,Metrics,,"I'm using SparkListener.onTaskEnd() to capture input and output metrics but it seems that number of written records ('taskEnd.taskMetrics().outputMetrics().recordsWritten()') is incorrect. Here is my stream construction:

 
{code:java}
StreamingQuery writeStream = session
        .readStream()
        .schema(RecordSchema.fromClass(TestRecord.class))
        .option(OPTION_KEY_DELIMITER, OPTION_VALUE_DELIMITER_TAB)
        .option(OPTION_KEY_QUOTE, OPTION_VALUE_QUOTATION_OFF)
        .csv(inputFolder.getRoot().toPath().toString())
        .as(Encoders.bean(TestRecord.class))
        .flatMap(
            ((FlatMapFunction<TestRecord, TestVendingRecord>) (u) -> {
                List<TestVendingRecord> resultIterable = new ArrayList<>();
                try {
                    TestVendingRecord result = transformer.convert(u);
                    resultIterable.add(result);
                } catch (Throwable t) {
                    System.err.println(""Ooops"");
                    t.printStackTrace();
                }

                return resultIterable.iterator();
            }),
            Encoders.bean(TestVendingRecord.class))
        .writeStream()
        .outputMode(OutputMode.Append())
        .format(""parquet"")
        .option(""path"", outputFolder.getRoot().toPath().toString())
        .option(""checkpointLocation"", checkpointFolder.getRoot().toPath().toString())
        .start();

    writeStream.processAllAvailable();
    writeStream.stop();
{code}
Tested it with one good and one bad (throwing exception in transformer.convert(u)) input records and it produces following metrics:

 
{code:java}
(TestMain.java:onTaskEnd(73)) - -----------status--> SUCCESS
(TestMain.java:onTaskEnd(75)) - -----------recordsWritten--> 0
(TestMain.java:onTaskEnd(76)) - -----------recordsRead-----> 2
(TestMain.java:onTaskEnd(83)) - taskEnd.taskInfo().accumulables():
(TestMain.java:onTaskEnd(84)) - name = duration total (min, med, max)
(TestMain.java:onTaskEnd(85)) - value =  323
(TestMain.java:onTaskEnd(84)) - name = number of output rows
(TestMain.java:onTaskEnd(85)) - value =  2
(TestMain.java:onTaskEnd(84)) - name = duration total (min, med, max)
(TestMain.java:onTaskEnd(85)) - value =  364
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.input.recordsRead
(TestMain.java:onTaskEnd(85)) - value =  2
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.input.bytesRead
(TestMain.java:onTaskEnd(85)) - value =  157
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.resultSerializationTime
(TestMain.java:onTaskEnd(85)) - value =  3
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.resultSize
(TestMain.java:onTaskEnd(85)) - value =  2396
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.executorCpuTime
(TestMain.java:onTaskEnd(85)) - value =  633807000
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.executorRunTime
(TestMain.java:onTaskEnd(85)) - value =  683
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.executorDeserializeCpuTime
(TestMain.java:onTaskEnd(85)) - value =  55662000
(TestMain.java:onTaskEnd(84)) - name = internal.metrics.executorDeserializeTime
(TestMain.java:onTaskEnd(85)) - value =  58
(TestMain.java:onTaskEnd(89)) - input records 2

Streaming query made progress: {
  ""id"" : ""1231f9cb-b2e8-4d10-804d-73d7826c1cb5"",
  ""runId"" : ""bd23b60c-93f9-4e17-b3bc-55403edce4e7"",
  ""name"" : null,
  ""timestamp"" : ""2018-01-26T14:44:05.362Z"",
  ""numInputRows"" : 2,
  ""processedRowsPerSecond"" : 0.8163265306122448,
  ""durationMs"" : {
    ""addBatch"" : 1994,
    ""getBatch"" : 126,
    ""getOffset"" : 52,
    ""queryPlanning"" : 220,
    ""triggerExecution"" : 2450,
    ""walCommit"" : 41
  },
  ""stateOperators"" : [ ],
  ""sources"" : [ {
    ""description"" : ""FileStreamSource[file:/var/folders/4w/zks_kfls2s3glmrj3f725p7hllyb5_/T/junit3661035412295337071]"",
    ""startOffset"" : null,
    ""endOffset"" : {
      ""logOffset"" : 0
    },
    ""numInputRows"" : 2,
    ""processedRowsPerSecond"" : 0.8163265306122448
  } ],
  ""sink"" : {
    ""description"" : ""FileSink[/var/folders/4w/zks_kfls2s3glmrj3f725p7hllyb5_/T/junit3785605384928624065]""
  }
}
{code}
The number of inputs is correct but the number of output records taken from taskEnd.taskMetrics().outputMetrics().recordsWritten() is zero. Accumulables (taskEnd.taskInfo().accumulables()) don't have a correct value as well - should be 1 but it shows 2 'number of output rows'.",,apachespark,bondyk,cloud_fan,gsomogyi,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 21 17:07:14 UTC 2018,,,,,,,,,,"0|i3plvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/18 16:44;gsomogyi;I'm working on this issue.;;;","16/Feb/18 16:45;gsomogyi;Seems like no statsTrackers created in FileStreamSink.;;;","20/Feb/18 10:24;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20639;;;","06/Mar/18 04:30;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20745;;;","21/Mar/18 17:07;cloud_fan;Issue resolved by pull request 20745
[https://github.com/apache/spark/pull/20745];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query produces results in incorrect order when a composite order by clause refers to both original columns and aliases,SPARK-23281,13135023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,31/Jan/18 08:56,01/Feb/18 04:50,13/Jul/23 08:45,01/Feb/18 04:49,2.3.0,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,0,,,,"Here is the test snippet.

{code}
scala> Seq[(Integer, Integer)](
     |         (1, 1),
     |         (1, 3),
     |         (2, 3),
     |         (3, 3),
     |         (4, null),
     |         (5, null)
     |       ).toDF(""key"", ""value"").createOrReplaceTempView(""src"")

scala> sql(
     |         """"""
     |           |SELECT MAX(value) as value, key as col2
     |           |FROM src
     |           |GROUP BY key
     |           |ORDER BY value desc, key
     |         """""".stripMargin).show
+-----+----+
|value|col2|
+-----+----+
|    3|   3|
|    3|   2|
|    3|   1|
| null|   5|
| null|   4|
+-----+----+
{code}

Here is the explain output :
{code}
== Parsed Logical Plan ==
'Sort ['value DESC NULLS LAST, 'key ASC NULLS FIRST], true
+- 'Aggregate ['key], ['MAX('value) AS value#9, 'key AS col2#10]
   +- 'UnresolvedRelation `src`

== Analyzed Logical Plan ==
value: int, col2: int
Project [value#9, col2#10]
+- Sort [value#9 DESC NULLS LAST, col2#10 DESC NULLS LAST], true
   +- Aggregate [key#5], [max(value#6) AS value#9, key#5 AS col2#10]
      +- SubqueryAlias src
         +- Project [_1#2 AS key#5, _2#3 AS value#6]
            +- LocalRelation [_1#2, _2#3]
{code}

The sort direction should be ascending for the 2nd column. Instead its being changed
to descending in Analyzer.resolveAggregateFunctions.

The above testcase models TPCDS-Q71 and thus we have the same issue in Q71 as well.
",,apachespark,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 31 09:25:05 UTC 2018,,,,,,,,,,"0|i3pl33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/18 09:25;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20453;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive/tests have been failing when run locally on the laptop (Mac) with OOM ,SPARK-23275,13134861,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,30/Jan/18 19:36,09/Feb/18 21:23,13/Jul/23 08:45,30/Jan/18 22:12,2.3.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"hive tests have been failing when they are run locally (Mac Os)  after a recent change in the trunk. After running the tests for some time, the test fails with OOM with  Error: unable to create new native thread. 

I noticed the thread count goes all the way up to 2000+ after which we start getting these OOM errors. Most of the threads seem to be related to the connection pool in hive metastore (BoneCP-xxxxx-xxxx ). This behaviour change is happening after we made the following change to HiveClientImpl.reset()

{code}
 def reset(): Unit = withHiveState {
    try {
      // code
    } finally {
      runSqlHive(""USE default"")  ===> this is causing the issue
    }
{code}


I am proposing to temporarily back-out part of a fix made to address SPARK-23000 to resolve this issue while we work-out the exact reason for this sudden increase in thread counts.
",,apachespark,dkbiswal,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 09 21:23:04 UTC 2018,,,,,,,,,,"0|i3pk33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/18 19:52;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20441;;;","09/Feb/18 21:23;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/20562;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplaceExceptWithFilter fails on dataframes filtered on same column,SPARK-23274,13134804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,onursatici,onursatici,30/Jan/18 16:18,31/Jan/18 19:40,13/Jul/23 08:45,31/Jan/18 04:07,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Currently affects:
{code:java}
$ git tag --contains 01f6ba0e7a
v2.3.0-rc1
v2.3.0-rc2
{code}
Steps to reproduce:
{code:java}
$ cat test.csv
a,b
1,2
1,3
2,2
2,4
{code}
{code:java}
val df = spark.read.format(""csv"").option(""header"", ""true"").load(""test.csv"")
val df1 = df.filter($""a"" === 1)
val df2 = df.filter($""a"" === 2)
df1.select(""b"").except(df2.select(""b"")).show
{code}
results in:
{code:java}
java.util.NoSuchElementException: key not found: a
  at scala.collection.MapLike$class.default(MapLike.scala:228)
  at scala.collection.AbstractMap.default(Map.scala:59)
  at scala.collection.MapLike$class.apply(MapLike.scala:141)
  at scala.collection.AbstractMap.apply(Map.scala:59)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$$anonfun$org$apache$spark$sql$catalyst$optimizer$ReplaceExceptWithFilter$$transformCondition$1.applyOrElse(ReplaceExceptWithFilter.scala:60)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$$anonfun$org$apache$spark$sql$catalyst$optimizer$ReplaceExceptWithFilter$$transformCondition$1.applyOrElse(ReplaceExceptWithFilter.scala:60)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$.org$apache$spark$sql$catalyst$optimizer$ReplaceExceptWithFilter$$transformCondition(ReplaceExceptWithFilter.scala:60)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$$anonfun$apply$1.applyOrElse(ReplaceExceptWithFilter.scala:50)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$$anonfun$apply$1.applyOrElse(ReplaceExceptWithFilter.scala:48)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$.apply(ReplaceExceptWithFilter.scala:48)
  at org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithFilter$.apply(ReplaceExceptWithFilter.scala:41)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:92)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:89)
  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:89)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:81)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:81)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:79)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:81)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:90)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:90)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3141)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2426)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2640)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:237)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:668)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:627)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
  ... 49 elided
{code}",,aash,apachespark,f2005870@gmail.com,henryr,onursatici,sameerag,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 31 19:40:49 UTC 2018,,,,,,,,,,"0|i3pjqn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"30/Jan/18 18:09;aash;Suspect this regression was introduced by [https://github.com/apache/spark/commit/01f6ba0e7a] ;;;","30/Jan/18 18:20;smilegator;Since this is a regression, I will try to fix it ASAP;;;","31/Jan/18 00:55;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20444;;;","31/Jan/18 19:40;aash;Many thanks for the fast fix [~smilegator]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Parquet output contains only ""_SUCCESS"" file after empty DataFrame saving ",SPARK-23271,13134725,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,pmz0178,pmz0178,30/Jan/18 11:30,12/Dec/22 18:11,13/Jul/23 08:45,08/Mar/18 22:59,2.2.0,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,"Sophisticated case, reproduced only if read empty CSV file without header with assigned schema.

Steps for reproduce (Scala):
{code:java}
val anySchema = StructType(StructField(""anyName"", StringType, nullable = false) :: Nil)

val inputDF = spark.read.schema(anySchema).csv(inputFolderWithEmptyCSVFile)
inputDF.write.parquet(outputFolderName)

// Exception: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;
val actualDF = spark.read.parquet(outputFolderName)
 
{code}
*Actual:* Only ""_SUCCESS"" file in output directory

*Expected*: at least one Parquet file with schema.

Project for reproduce is attached.",,apachespark,cloud_fan,dkbiswal,pmz0178,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15393,,,,,,,,,,,"30/Jan/18 11:30;pmz0178;parquet-empty-output.zip;https://issues.apache.org/jira/secure/attachment/12908319/parquet-empty-output.zip",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 22:59:08 UTC 2018,,,,,,,,,,"0|i3pj93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/18 15:04;srowen;That might be 'by design' but by symmetry you might well expect writing an empty file produces something you can read. Then again, there's nothing to write, and reading nothing should be an error.;;;","30/Jan/18 15:31;pmz0178;[~srowen]
In example schema is provided, and this schema have to be written to output, with no data.

if use such empty dataframe instead of CSV:
{code:java}
val inputDF = List.empty[String].toDF()
{code}
in output directory present ""parquet"" file, and output is read without errors; for me it looks correct.

Behaviour for CSV file is different.

Looks like it is bug - empty DataFrame cannot be written differently, depending on source.

 ;;;","31/Jan/18 13:31;gurwls223;To be clear, it's orthogonal with SPARK-15393 if I understood correctly, by the way.;;;","01/Feb/18 08:45;pmz0178;Yes, SPARK-15393 is similar, linked in JIRA to this issue.
Such code can be used as workaround:
{code}
    if (originalDF.rdd.isEmpty) {
      originalDF.sparkSession.createDataFrame(List.empty[Row].asJava, originalDF.schema)
    } else {
      originalDF
    }
{code};;;","02/Feb/18 01:58;dkbiswal;[~hyukjin.kwon]
 I took a look at this. To the best of my knowledge, the difference in the behaviour between these two cases is because of the following :
 case 1
{code:java}
scala> List.empty[String].toDF().rdd.partitions.length
res18: Int = 1
{code}
Case 2
{code:java}
scala> val anySchema = StructType(StructField(""anyName"", StringType, nullable = false) :: Nil)
anySchema: org.apache.spark.sql.types.StructType = StructType(StructField(anyName,StringType,false))
scala> spark.read.schema(anySchema).csv(""/tmp/empty_folder"").rdd.partitions.length
res22: Int = 0
{code}
For the 2nd case, since number of partitions = 0, we don't call the write task (the task has logic to create the empty parquet file).
 I tried to repartition the input RDD after [here|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L180] if input number of partitions = 0, before setting up the write job and things seem to work well.

Would this be a reasonable way to fix this ? Appreciate your feedback.;;;","06/Feb/18 07:46;smilegator;For this behavior, I remember [~zsxwing] and I had a discussion a few months ago. However, I forgot the conclusion we made. : );;;","06/Feb/18 15:21;smilegator;After a discussion with [~cloud_fan], we think the behavior should be consistent regardless the number of RDD partitions. Thanks!;;;","06/Feb/18 19:09;dkbiswal;Thank you [~smilegator]. I will try to create a PR to fix this by trying to repartition the RDD before setting up the write job. 
We can discuss whether its the right approach to fix this issue in the PR.;;;","07/Feb/18 07:54;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20525;;;","08/Feb/18 18:50;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20551;;;","08/Mar/18 22:59;cloud_fan;Issue resolved by pull request 20525
[https://github.com/apache/spark/pull/20525];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase spark.sql.codegen.hugeMethodLimit to 65535,SPARK-23267,13134678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,30/Jan/18 08:38,30/Jan/18 19:37,13/Jul/23 08:45,30/Jan/18 19:37,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Still saw the performance regression introduced by `spark.sql.codegen.hugeMethodLimit` in our internal workloads. There are two major issues in the current solution.
 * The size of the complied byte code is not identical to the bytecode size of the method. The detection is still not accurate. 
 * The bytecode size of a single operator (e.g., `SerializeFromObject`) could still exceed 8K limit. We saw the performance regression in such scenario. 

Since it is close to the release of 2.3, we decide to increase it to 64K for avoiding the perf regression.",,apachespark,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 30 08:42:04 UTC 2018,,,,,,,,,,"0|i3piyn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"30/Jan/18 08:42;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20434;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create table stored as parquet should update table size if automatic update table size is enabled,SPARK-23263,13134643,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuming,yumwang,yumwang,30/Jan/18 03:43,12/Dec/22 18:10,13/Jul/23 08:45,20/Jun/19 05:20,2.4.0,,,,,,,,,3.0.0,,,,,SQL,,,,0,,,,"How to reproduce:

{noformat}
bin/spark-sql --conf spark.sql.statistics.size.autoUpdate.enabled=true
{noformat}

{code:sql}
spark-sql> create table test_create_parquet stored as parquet as select 1;
spark-sql> desc extended test_create_parquet;
{code}
The table statistics will not exists.


 ",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 20 05:20:04 UTC 2019,,,,,,,,,,"0|i3piqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/18 03:51;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/20430;;;","20/Jun/19 05:20;gurwls223;Issue resolved by pull request 20430
[https://github.com/apache/spark/pull/20430];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in JavaDoc/ScalaDoc for DataFrameWriter,SPARK-23250,13134259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,CCInCharge,CCInCharge,CCInCharge,28/Jan/18 00:48,28/Jan/18 20:56,13/Jul/23 08:45,28/Jan/18 20:56,2.1.1,2.1.2,2.2.0,2.2.1,,,,,,2.3.0,,,,,Documentation,,,,0,,,,"JavaDoc/ScalaDoc for several methods in the DataFrameWriter class denote ""This is applicable for all file-based data sources (e.g. Parquet, JSON) *staring* Spark 2.1.0"" - should be ""starting Spark 2.1.0"". This typo is not present in the Spark 2.1.0 docs.",,apachespark,CCInCharge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 28 20:56:31 UTC 2018,,,,,,,,,,"0|i3pgdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/18 01:04;srowen;It should be ""starting with Spark 2.1.0"" even. We don't make JIRAs for these but go ahead with a PR now.;;;","28/Jan/18 01:27;apachespark;User 'CCInCharge' has created a pull request for this issue:
https://github.com/apache/spark/pull/20417;;;","28/Jan/18 20:56;srowen;Issue resolved by pull request 20417
[https://github.com/apache/spark/pull/20417];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaContinuousSourceSuite may hang forever,SPARK-23245,13134154,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,zsxwing,zsxwing,27/Jan/18 00:18,27/Jan/18 07:06,13/Jul/23 08:45,27/Jan/18 07:06,2.3.0,,,,,,,,,2.3.0,,,,,Structured Streaming,Tests,,,0,,,,"The following stream execution thread is holding the lock ""IncrementalExecution"".

{code}

""stream execution thread for [id = 83790664-fd66-4645-b55a-37c17897c691, runId = febf6c2a-1372-4c83-998c-90984a9a02c2]"" #2653 daemon prio=5 os_prio=0 tid=0x00007ff511ae2000 nid=0xcde1 waiting on condition [0x00007ff32ebbb000]
 java.lang.Thread.State: WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for <0x000000071a25fa80> (a scala.concurrent.impl.Promise$CompletionLatch)
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
 at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
 at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
 at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
 at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:222)
 at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:731)
 at org.apache.spark.SparkContext.runJob(SparkContext.scala:2109)
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2.scala:78)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:135)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
 at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$3.apply(SparkPlan.scala:167)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
 at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:164)
 at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
 at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:112)
 - locked <0x000000071a256e10> (a org.apache.spark.sql.execution.streaming.IncrementalExecution)
 at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)
 at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3$$anonfun$apply$1.apply(ContinuousExecution.scala:273)
 at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3$$anonfun$apply$1.apply(ContinuousExecution.scala:273)
 at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:88)
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:124)
 at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3.apply(ContinuousExecution.scala:273)
 at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3.apply(ContinuousExecution.scala:273)
 at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
 at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:60)
 at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runContinuous(ContinuousExecution.scala:271)
 at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runActivatedStream(ContinuousExecution.scala:94)
 at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:291)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:201)

{code}

 

But the test thread is waiting for the same lock

{code}

""pool-1-thread-1-ScalaTest-running-KafkaContinuousSourceSuite"" #20 prio=5 os_prio=0 tid=0x00007ff5b4f1e800 nid=0x5566 waiting for monitor entry [0x00007ff51cffb000]
 java.lang.Thread.State: BLOCKED (on object monitor)
 at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:109)
 - waiting to lock <0x000000071a256e10> (a org.apache.spark.sql.execution.streaming.IncrementalExecution)
 at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:109)
 at org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1$$anonfun$apply$25.apply(StreamTest.scala:475)
 at org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1$$anonfun$apply$25.apply(StreamTest.scala:475)
 at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:395)
 at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:409)
 at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:439)
 at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:479)
 at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:337)
 at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:479)
 at org.apache.spark.sql.streaming.StreamTest$class.eventually$1(StreamTest.scala:378)
 at org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1.apply(StreamTest.scala:474)
 at org.apache.spark.sql.streaming.StreamTest$$anonfun$liftedTree1$1$1.apply(StreamTest.scala:432)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
 at org.apache.spark.sql.streaming.StreamTest$class.liftedTree1$1(StreamTest.scala:432)
 at org.apache.spark.sql.streaming.StreamTest$class.testStream(StreamTest.scala:431)
 - locked <0x00000007199f3438> (a org.apache.spark.sql.kafka010.KafkaContinuousSourceSuite)
 at org.apache.spark.sql.kafka010.KafkaSourceTest.testStream(KafkaSourceSuite.scala:50)
 at org.apache.spark.sql.kafka010.KafkaSourceSuiteBase.org$apache$spark$sql$kafka010$KafkaSourceSuiteBase$$testFromEarliestOffsets(KafkaSourceSuite.scala:1056)
 at org.apache.spark.sql.kafka010.KafkaSourceSuiteBase$$anonfun$38$$anonfun$apply$8.apply$mcV$sp(KafkaSourceSuite.scala:668)
 at org.apache.spark.sql.kafka010.KafkaSourceSuiteBase$$anonfun$38$$anonfun$apply$8.apply(KafkaSourceSuite.scala:665)
 at org.apache.spark.sql.kafka010.KafkaSourceSuiteBase$$anonfun$38$$anonfun$apply$8.apply(KafkaSourceSuite.scala:665)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
 at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:108)
 at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
 at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
 at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
 at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
 at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
 at org.apache.spark.sql.kafka010.KafkaSourceTest.org$scalatest$BeforeAndAfterEach$$super$runTest(KafkaSourceSuite.scala:50)
 at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:221)
 at org.apache.spark.sql.kafka010.KafkaSourceTest.runTest(KafkaSourceSuite.scala:50)
 at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
 at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
 at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
 at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
 at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
 at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
 at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
 at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
 at org.scalatest.Suite$class.run(Suite.scala:1147)
 at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
 at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
 at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
 at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
 at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
 at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
 at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
 at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
 at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
 at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
 at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:480)
 at sbt.ForkMain$Run$2.call(ForkMain.java:296)
 at sbt.ForkMain$Run$2.call(ForkMain.java:286)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)

{code}

 

The deadlock here is StreamTest is waiting for the lock and cannot stop the query and hence hang forever.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 27 07:06:30 UTC 2018,,,,,,,,,,"0|i3pfqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/18 00:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20413;;;","27/Jan/18 07:06;zsxwing;Issue resolved by pull request 20413
[https://github.com/apache/spark/pull/20413];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle+Repartition on an RDD could lead to incorrect answers,SPARK-23243,13134138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,jiangxb1987,jiangxb1987,26/Jan/18 23:00,11/Sep/19 19:05,13/Jul/23 08:45,05/Sep/18 22:37,1.6.0,2.0.0,2.1.0,2.2.0,2.3.0,,,,,2.2.3,2.3.2,2.4.0,,,Spark Core,,,,0,correctness,,,"The RDD repartition also uses the round-robin way to distribute data, this can also cause incorrect answers on RDD workload the similar way as in https://issues.apache.org/jira/browse/SPARK-23207

The approach that fixes DataFrame.repartition() doesn't apply on the RDD repartition issue, as discussed in https://github.com/apache/spark/pull/20393#issuecomment-360912451

We track for alternative solutions for this issue in this task.",,apachespark,bersprockets,darabos,dongjoon,felixcheung,irashid,jiangxb1987,joshrosen,koert,maropu,mauzhang,pingsutw,rdub,shivaram,Teng Peng,tgraves,XuanYuan,yumwang,zhuqi,,,,,,,,,,,,,,,,SPARK-25156,,,,,,,SPARK-25341,SPARK-25342,SPARK-28699,SPARK-23207,SPARK-29042,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 10 14:23:59 UTC 2018,,,,,,,,,,"0|i3pfmv:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"27/Jan/18 00:43;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20414;;;","02/Jul/18 12:40;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/21698;;;","15/Aug/18 20:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22112;;;","06/Sep/18 14:44;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22354;;;","08/Sep/18 18:23;bersprockets;Any plans to back port this to 2.2?;;;","08/Sep/18 18:31;bersprockets;BTW, I took a stab at back porting it to 2.2, but to get it to work I had to also back port SPARK-20715. So my version has an additional 398 changed lines (from the additional back port of SPARK-20715). I can post that, but thought maybe someone might have a smaller version.;;;","10/Sep/18 14:23;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/22382;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't run tests in KafkaSourceSuiteBase twice,SPARK-23242,13134123,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,26/Jan/18 22:15,27/Jan/18 00:10,13/Jul/23 08:45,27/Jan/18 00:10,2.3.0,,,,,,,,,2.3.0,,,,,Structured Streaming,Tests,,,0,,,,"KafkaSourceSuiteBase should be abstract class, otherwise KafkaSourceSuiteBase will also run.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 27 00:10:20 UTC 2018,,,,,,,,,,"0|i3pfjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/18 22:18;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20412;;;","27/Jan/18 00:10;zsxwing;Issue resolved by pull request 20412
[https://github.com/apache/spark/pull/20412];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonWorkerFactory issues unhelpful message when pyspark.daemon produces bogus stdout,SPARK-23240,13134095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,26/Jan/18 20:03,12/Dec/22 18:10,13/Jul/23 08:45,20/Feb/18 11:26,2.2.1,,,,,,,,,2.4.0,,,,,PySpark,,,,0,,,,"Environmental issues or site-local customizations (i.e., sitecustomize.py present in the python install directory) can interfere with daemon.py’s output to stdout. PythonWorkerFactory produces unhelpful messages when this happens, causing some head scratching before the actual issue is determined.

Case #1: Extraneous data in pyspark.daemon’s stdout. In this case, PythonWorkerFactory uses the output as the daemon’s port number and ends up throwing an exception when creating the socket:
{noformat}
java.lang.IllegalArgumentException: port out of range:1819239265
	at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143)
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:188)
	at java.net.Socket.<init>(Socket.java:244)
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:78)
{noformat}
Case #2: No data in pyspark.daemon’s stdout. In this case, PythonWorkerFactory throws an EOFException exception reading the from the Process input stream.

The second case is somewhat less mysterious than the first, because PythonWorkerFactory also displays the stderr from the python process.

When there is unexpected or missing output in pyspark.daemon’s stdout, PythonWorkerFactory should say so.

 ",,apachespark,bersprockets,henryr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 20 11:26:57 UTC 2018,,,,,,,,,,"0|i3pfdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/18 21:16;bersprockets;I will be making a pull request.;;;","28/Jan/18 04:37;gurwls223;Actually [~bersprockets], now there is a workaround for a custom fix within daemon and worker - https://github.com/apache/spark/pull/20151 from 2.4.0.;;;","28/Jan/18 19:38;bersprockets;Hi [~hyukjin.kwon],

I am not sure this update covers the case where python site-local customizations puts arbitrary data into stdout before the daemon module (whatever it is) is able to put a port number into stdout. What happens in that case is that PythonWorkerFactory ends up reading the arbitrary data as the port number, since it exists in the python process's stdout ahead of the actual port number. My proposed pull request will not fix that, but it will produce an error message that will explicitly implicate the daemon's python process as the source of the bad port number.;;;","29/Jan/18 18:40;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/20424;;;","06/Feb/18 14:38;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/20519;;;","10/Feb/18 18:26;bersprockets;A little background. A Spark installation had a Python sitecustomize.py like this:

 
{code:java}
try:
   import flotilla
 except ImportError as e:
   print e{code}
 

(flotilla is not the real python module, I just use that as an example).

Because flotilla was not installed on the user's cluster, the first output in daemon's stdout was:

 
{noformat}
No module named flotilla{noformat}
 

In fact, this is what I get when I run pyspark.daemon with this sitecustomize.py installed:
{noformat}
bash-3.2$ python -m pyspark.daemon
python -m pyspark.daemon
No module named flotilla
^@^@\325{noformat}
Therefore, PythonWorkerFactory.startDaemon reads 'No m', or 0x4e6f206d or 1315905645, as the port number.

Here's what happens when I run a pyspark action with the above sitecustomize.py installed:
{noformat}
>>> text_file = sc.textFile(""/Users/bruce/ncdc_gsod"").count()
odule named flotilla
18/02/10 09:44:27 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.IllegalArgumentException: port out of range:1315905645
 at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143)
 at java.net.InetSocketAddress.<init>(InetSocketAddress.java:188)
 at java.net.Socket.<init>(Socket.java:244){noformat}




 ;;;","20/Feb/18 11:26;gurwls223;Issue resolved by pull request 20424
[https://github.com/apache/spark/pull/20424];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
asNondeterministic in Python UDF not being set when the UDF is called at least once,SPARK-23233,13134028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,26/Jan/18 15:00,12/Dec/22 17:50,13/Jul/23 08:45,27/Jan/18 19:27,2.3.0,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,0,,,,"With this diff

{code}
diff --git a/python/pyspark/sql/udf.py b/python/pyspark/sql/udf.py
index de96846c5c7..026a78bf547 100644
--- a/python/pyspark/sql/udf.py
+++ b/python/pyspark/sql/udf.py
@@ -180,6 +180,7 @@ class UserDefinedFunction(object):
         wrapper.deterministic = self.deterministic
         wrapper.asNondeterministic = functools.wraps(
             self.asNondeterministic)(lambda: self.asNondeterministic()._wrapped())
+        wrapper._unwrapped = lambda: self
         return wrapper

     def asNondeterministic(self):
{code}

{code:java}
>>> from pyspark.sql.functions import udf
>>> f = udf(lambda x: x)
>>> spark.range(1).select(f(""id""))
DataFrame[<lambda>(id): string]
>>> f._unwrapped()._judf_placeholder.udfDeterministic()
True
>>> ndf = f.asNondeterministic()
>>> ndf.deterministic
False
>>> spark.range(1).select(ndf(""id""))
DataFrame[<lambda>(id): string]
>>> ndf._unwrapped()._judf_placeholder.udfDeterministic()
True
{code}

Seems we don't actually update the {{deterministic}} once it's called due to cache in Python side. ",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 26 15:07:03 UTC 2018,,,,,,,,,,"0|i3peyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/18 15:07;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/20409;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When hive.default.fileformat is other kinds of file types, create textfile table cause a serde error",SPARK-23230,13134003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,26/Jan/18 12:44,14/Feb/18 05:00,13/Jul/23 08:45,13/Feb/18 00:04,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.2.0,2.2.1,,2.2.2,2.3.0,,,,SQL,,,,0,,,,"When hive.default.fileformat is other kinds of file types, create textfile table cause a serde error.
 We should take the default type of textfile and sequencefile both as org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.
{code:java}
set hive.default.fileformat=orc;
create table tbl( i string ) stored as textfile;
desc formatted tbl;

Serde Library org.apache.hadoop.hive.ql.io.orc.OrcSerde
InputFormat  org.apache.hadoop.mapred.TextInputFormat
OutputFormat  org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat{code}
 
{code:java}
set hive.default.fileformat=orc;
create table tbl stored as textfile
as
select  1



{code}
{{It failed because it used the wrong SERDE}}
{code:java}
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.orc.OrcSerde$OrcSerdeRow cannot be cast to org.apache.hadoop.io.BytesWritable
	at org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat$1.write(HiveIgnoreKeyTextOutputFormat.java:91)
	at org.apache.spark.sql.hive.execution.HiveOutputWriter.write(HiveFileFormat.scala:149)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:327)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)
	... 16 more
{code}
 ",,apachespark,dzcxzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 13 01:41:04 UTC 2018,,,,,,,,,,"0|i3pet3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/18 12:46;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/20406;;;","13/Feb/18 01:41;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/20593;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stacking dataset transforms performs poorly,SPARK-23223,13133910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,26/Jan/18 03:29,12/Dec/22 18:10,13/Jul/23 08:45,29/Jan/18 17:02,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"It is a common pattern to apply multiple transforms to a {{Dataset}} (using {{Dataset.withColumn}} for example. This is currently quite expensive because we run {{CheckAnalysis}} on the full plan and create an encoder for each intermediate {{Dataset}}.

{{CheckAnalysis}} only needs to be run for the newly added plan components, and not for the full plan. The addition of the {{AnalysisBarrier}} created this issue.",,apachespark,f2005870@gmail.com,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17006,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 28 10:06:12 UTC 2018,,,,,,,,,,"0|i3pe8n:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"26/Jan/18 03:42;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/20402;;;","28/Jan/18 10:06;gurwls223;I think this fixes SPARK-17006 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: DataFrameRangeSuite,SPARK-23222,13133876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,vanzin,vanzin,25/Jan/18 23:01,30/Jan/18 13:02,13/Jul/23 08:45,30/Jan/18 13:02,2.4.0,,,,,,,,,2.3.0,,,,,SQL,Tests,,,0,,,,"I've seen this test fail a few times in unrelated PRs. e.g.:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/86605/testReport/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/86656/testReport/

{noformat}
Error Message
org.scalatest.exceptions.TestFailedException: Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$class.intercept(Assertions.scala:822)
	at org.scalatest.FunSuite.intercept(FunSuite.scala:1560)
	at org.apache.spark.sql.DataFrameRangeSuite$$anonfun$2$$anonfun$apply$mcV$sp$4$$anonfun$apply$2.apply$mcV$sp(DataFrameRangeSuite.scala:168)
	at org.apache.spark.sql.catalyst.plans.PlanTestBase$class.withSQLConf(PlanTest.scala:176)
	at org.apache.spark.sql.DataFrameRangeSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(DataFrameRangeSuite.scala:33)
	at org.apache.spark.sql.test.SQLTestUtilsBase$class.withSQLConf(SQLTestUtils.scala:167)
	at org.apache.spark.sql.DataFrameRangeSuite.withSQLConf(DataFrameRangeSuite.scala:33)
	at org.apache.spark.sql.DataFrameRangeSuite$$anonfun$2$$anonfun$apply$mcV$sp$4.apply(DataFrameRangeSuite.scala:166)
	at org.apache.spark.sql.DataFrameRangeSuite$$anonfun$2$$anonfun$apply$mcV$sp$4.apply(DataFrameRangeSuite.scala:165)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.DataFrameRangeSuite$$anonfun$2.apply$mcV$sp(DataFrameRangeSuite.scala:165)
	at org.apache.spark.sql.DataFrameRangeSuite$$anonfun$2.apply(DataFrameRangeSuite.scala:154)
	at org.apache.spark.sql.DataFrameRangeSuite$$anonfun$2.apply(DataFrameRangeSuite.scala:154)
{noformat}",,apachespark,cloud_fan,mgaido,smurakozi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 30 13:02:02 UTC 2018,,,,,,,,,,"0|i3pe13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/18 06:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20431;;;","30/Jan/18 13:02;cloud_fan;Issue resolved by pull request 20431
[https://github.com/apache/spark/pull/20431];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cached data should not carry extra hint info,SPARK-23214,13133638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,25/Jan/18 10:02,27/Jan/18 00:48,13/Jul/23 08:45,27/Jan/18 00:48,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 25 10:12:05 UTC 2018,,,,,,,,,,"0|i3pck7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/18 10:12;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20394;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveDelegationTokenProvider throws an exception if Hive jars are not the classpath,SPARK-23209,13133560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,stakiar,stakiar,25/Jan/18 01:27,29/Jan/18 22:10,13/Jul/23 08:45,29/Jan/18 22:09,2.3.0,,,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,"While doing some Hive-on-Spark testing against the Spark 2.3.0 release candidates we came across a bug (see HIVE-18436).

Stack-trace:

{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConf
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.getDelegationTokenProviders(HadoopDelegationTokenManager.scala:68)
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.<init>(HadoopDelegationTokenManager.scala:54)
        at org.apache.spark.deploy.yarn.security.YARNHadoopDelegationTokenManager.<init>(YARNHadoopDelegationTokenManager.scala:44)
        at org.apache.spark.deploy.yarn.Client.<init>(Client.scala:123)
        at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1502)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 10 more
{code}

Looks like the bug was introduced by SPARK-20434. SPARK-20434 changed {{HiveDelegationTokenProvider}} so that it constructs {{o.a.h.hive.conf.HiveConf}} inside {{HiveCredentialProvider#hiveConf}} rather than trying to manually load the class via the class loader. Looks like with the new code the JVM tries to load {{HiveConf}} as soon as {{HiveDelegationTokenProvider}} is referenced. Since there is no try-catch around the construction of {{HiveDelegationTokenProvider}} a {{ClassNotFoundException}} is thrown, which causes spark-submit to crash. Spark's {{docs/running-on-yarn.md}} says ""a Hive token will be obtained if Hive is on the classpath"". This behavior would seem to contradict that.","OSX, Java(TM) SE Runtime Environment (build 1.8.0_92-b14), Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)",apachespark,dongjoon,dougb,irashid,sameerag,stakiar,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-18436,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 29 22:09:58 UTC 2018,,,,,,,,,,"0|i3pc2v:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"25/Jan/18 16:25;stakiar;Setting {{spark.security.credentials.hive.enabled}} doesn't help either because {{HiveDelegationTokenProvider}} is created first and then the configs are read to filter out the different token providers.;;;","25/Jan/18 21:31;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20399;;;","29/Jan/18 22:09;irashid;Issue resolved by pull request 20399
[https://github.com/apache/spark/pull/20399];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenArrayData produces illegal code,SPARK-23208,13133544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hvanhovell,hvanhovell,hvanhovell,25/Jan/18 00:13,25/Jan/18 08:42,13/Jul/23 08:45,25/Jan/18 08:42,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The GenArrayData.genCodeToCreateArrayData produces illegal java code when code splitting is enabled. This is caused by a typo on the following line: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala#L114
",,apachespark,cloud_fan,dongjoon,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 25 08:42:02 UTC 2018,,,,,,,,,,"0|i3pbzb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"25/Jan/18 01:15;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/20391;;;","25/Jan/18 08:42;cloud_fan;Issue resolved by pull request 20391
[https://github.com/apache/spark/pull/20391];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle+Repartition on an DataFrame could lead to incorrect answers,SPARK-23207,13133526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,24/Jan/18 23:21,08/Jun/22 08:36,13/Jul/23 08:45,26/Jan/18 23:03,1.6.0,2.0.0,2.1.0,2.2.0,2.3.0,,,,,2.1.4,2.2.3,2.3.0,,,SQL,,,,1,correctness,,,"Currently shuffle repartition uses RoundRobinPartitioning, the generated result is nondeterministic since the sequence of input rows are not determined.

The bug can be triggered when there is a repartition call following a shuffle (which would lead to non-deterministic row ordering), as the pattern shows below:
upstream stage -> repartition stage -> result stage
(-> indicate a shuffle)
When one of the executors process goes down, some tasks on the repartition stage will be retried and generate inconsistent ordering, and some tasks of the result stage will be retried generating different data.

The following code returns 931532, instead of 1000000:
{code:java}
import scala.sys.process._

import org.apache.spark.TaskContext
val res = spark.range(0, 1000 * 1000, 1).repartition(200).map { x =>
  x
}.repartition(200).map { x =>
  if (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId < 2) {
    throw new Exception(""pkill -f java"".!!)
  }
  x
}
res.distinct().count()
{code}",,apachespark,bersprockets,darabos,f2005870@gmail.com,felixcheung,igor.berman,irashid,jiangxb1987,jonathak,joshrosen,maropu,rdub,robert3005,sameerag,shivaram,tgraves,Wasikowski,yucai,zhuqi,,,,,,,,,,,,,,,,SPARK-25156,,,,,,,SPARK-23243,SPARK-28699,,SPARK-25114,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 08 08:20:25 UTC 2022,,,,,,,,,,"0|i3pbvb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"24/Jan/18 23:29;jiangxb1987;I'm working on this.;;;","25/Jan/18 09:04;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20393;;;","26/Jan/18 23:03;sameerag;Issue resolved by pull request 20393 https://github.com/apache/spark/pull/20393;;;","29/Jan/18 19:08;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20426;;;","07/Aug/18 19:59;tgraves;does this affect spark 2.2 and < ? from the description it sounds like it, in which case we should backport.;;;","09/Aug/18 14:14;tgraves;[~jiangxb1987] ^;;;","09/Aug/18 14:25;jiangxb1987;This affects the 2.2 and lower versions, the reason why we didn't backport the patch is that it can cause huge perf regression to `repartition()` operation, and chance to hit this correctness bug is small. cc [~smilegator][~sameerag];;;","09/Aug/18 14:58;tgraves;ok, I guess I disagree with that. Any correctness bug is very bad in my opinion, corrupt/lost data is much worse than taking a performance hit as corrupt/lost data could easily result in to lost revenue or errors in business critical data.  ;;;","09/Aug/18 16:31;irashid;yeah I agree with Tom, silent data loss is a major bug.  I don't actually think the chance to hit this is so small.;;;","11/Aug/18 03:57;bersprockets;I can help out here. I will make a PR for branch-2.2 in the morning.;;;","11/Aug/18 16:20;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/22079;;;","14/Aug/18 07:11;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/22101;;;","23/Aug/18 16:13;darabos;Sorry, could you clarify the fix version please? ""Affects version"" and ""Fix version"" are both set to 2.3.0. And GitHub only shows [https://github.com/apache/spark/commit/94c67a76ec1fda908a671a47a2a1fa63b3ab1b06] on master, not on a release tag. Thanks! (Also thanks for the fix.);;;","23/Aug/18 21:47;bersprockets;Will we be back-porting this to 2.1, or does the 18 month EOL period stand?;;;","23/Aug/18 23:27;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/22211;;;","15/Nov/18 11:41;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/23046;;;","08/Jun/22 08:20;igor.berman;We are still facing this issue in production with v3.1.2 at very large workloads. This happens very rarely, but still happens.
Current trials to reproduce this problem with above reproduction failed, so at this point no reproduction, will update if we will find one

we are running on mesos and with dynamic allocation
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ImageSchema.readImages incorrectly sets alpha channel to 255 for four-channel images,SPARK-23205,13133510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Siddharth Murching,Siddharth Murching,Siddharth Murching,24/Jan/18 22:21,26/Jan/18 00:16,13/Jul/23 08:45,26/Jan/18 00:16,2.3.0,,,,,,,,,2.3.0,,,,,ML,MLlib,,,0,,,,"When parsing raw image data in ImageSchema.decode(), we use a [java.awt.Color constructor|https://docs.oracle.com/javase/7/docs/api/java/awt/Color.html#Color(int)] that sets alpha = 255, even for four-channel images.

See the offending line here: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/image/ImageSchema.scala#L172

A fix is to simply update the line to: 

val color = new Color(img.getRGB(w, h), nChannels == 4)


instead of

val color = new Color(img.getRGB(w, h))",,apachespark,Siddharth Murching,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 26 00:16:14 UTC 2018,,,,,,,,,,"0|i3pbrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/18 22:40;Siddharth Murching;I'm working on a PR to address this issue if that's alright :);;;","24/Jan/18 22:41;apachespark;User 'smurching' has created a pull request for this issue:
https://github.com/apache/spark/pull/20389;;;","26/Jan/18 00:16;srowen;Issue resolved by pull request 20389
[https://github.com/apache/spark/pull/20389];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reset configuration when restarting from checkpoints,SPARK-23200,13133333,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,foxish,foxish,24/Jan/18 10:56,17/May/20 18:25,13/Jul/23 08:45,19/Sep/18 05:10,2.4.0,,,,,,,,,2.4.0,,,,,Kubernetes,Spark Core,,,1,,,,"Streaming workloads and restarting from checkpoints may need additional changes, i.e. resetting properties -  see https://github.com/apache-spark-on-k8s/spark/pull/516",,apachespark,cloud_fan,felixcheung,foxish,jerryshao,liyinan926,rxin,skonto,ssaavedra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 19 05:10:47 UTC 2018,,,,,,,,,,"0|i3paon:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"24/Jan/18 12:29;apachespark;User 'ssaavedra' has created a pull request for this issue:
https://github.com/apache/spark/pull/20383;;;","26/Jan/18 07:27;jerryshao;Issue resolved by pull request 20383

https://github.com/apache/spark/pull/20383;;;","13/Mar/18 18:30;foxish;Looks like the change was reverted. [~ssaavedra], can you propose this change again with the cleanup? We should target it for 2.4.;;;","10/Sep/18 14:16;cloud_fan;Is there any followup here? This seems an important fix to run structural streaming with K8s. cc [~tdas] [~zsxwing];;;","11/Sep/18 04:11;felixcheung;probably need someone to rebuild on the current config names...;;;","11/Sep/18 08:33;apachespark;User 'ssaavedra' has created a pull request for this issue:
https://github.com/apache/spark/pull/22392;;;","11/Sep/18 08:34;apachespark;User 'ssaavedra' has created a pull request for this issue:
https://github.com/apache/spark/pull/22392;;;","11/Sep/18 08:41;ssaavedra;It seems there wasn't much interest in having streaming working on k8s :)

I don't currently have an available k8s cluster in which to test the PR, but if I get bandwidth to spawn one I'll test this myself in the next days. Otherwise, I'll build Spark and release the Docker images in Docker Hub for someone else with the resources to reproduce this.

I recommend you to use the twitter example and have checkpointing configured on a s3a:// bucket.

Steps: launch with spark-submit once, wait for some checkpoint files to spawn on the bucket, then remove the driver pod, and then re-send with spark-submit again. Check in the logs that the driver loaded successfully and was able to revive the workers and that the names are new (no names from the old instances are shown in the logs as missing).;;;","14/Sep/18 00:44;skonto;This is important and should have been a bug not an improvement. Checkpointing needs to work as it is used by default in many cases in production. We should have an integration test for it.

 ;;;","14/Sep/18 01:02;skonto;[~cloud_fan]I think this should go in 2.4 even though its a bit late, the remaining PR is trivial. A few properties need to be restored from the checkpoint, and of course it needs testing. I can do the testing if we can get it in 2.4 soon. [~foxish] thoughts?;;;","17/Sep/18 02:50;cloud_fan;We should definitely merge it to branch 2.4, but I won't block the release since it's not that critical and it's still in progress. After it's merged, feel free to vote -1 on the RC voting email to include this change, if necessary.;;;","19/Sep/18 05:10;liyinan926;Issue resolved by pull request 22392
[https://github.com/apache/spark/pull/22392];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix KafkaContinuousSourceStressForDontFailOnDataLossSuite to test ContinuousExecution,SPARK-23198,13133239,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,24/Jan/18 01:20,25/Jan/18 19:15,13/Jul/23 08:45,24/Jan/18 20:59,2.3.0,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,0,,,,"Currently, `KafkaContinuousSourceStressForDontFailOnDataLossSuite` runs on `MicroBatchExecution`. It should test `ContinuousExecution`.",,apachespark,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 25 19:15:04 UTC 2018,,,,,,,,,,"0|i3pa3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/18 01:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20374;;;","24/Jan/18 20:59;zsxwing;Issue resolved by pull request 20374
[https://github.com/apache/spark/pull/20374];;;","25/Jan/18 19:15;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20398;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hint is lost after using cached data,SPARK-23192,13133115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,23/Jan/18 16:48,23/Feb/18 06:33,13/Jul/23 08:45,23/Jan/18 22:57,2.2.1,2.3.0,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The hint of the plan segment is lost, if the plan segment is replaced by the cached data. 

{noformat}
      val df1 = spark.createDataFrame(Seq((1, ""4""), (2, ""2""))).toDF(""key"", ""value"")
      val df2 = spark.createDataFrame(Seq((1, ""1""), (2, ""2""))).toDF(""key"", ""value"")
      df2.cache()
      val df3 = df1.join(broadcast(df2), Seq(""key""), ""inner"")
{noformat}

Hint is lost in {{df3}}. The physical join algorithm will not respect the hint due to the loss.",,apachespark,dongjoon,maropu,mshen,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 00:15:13 UTC 2018,,,,,,,,,,"0|i3p9cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/18 16:51;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20365;;;","23/Jan/18 18:07;dongjoon;Hi, [~smilegator].
I'm confused. You gave +1 for RC2 but marked this one as 2.3.0.
Does this mean -1 for RC2?;;;","23/Jan/18 18:29;smilegator;This is not a regression. It does not block 2.3.0 release.;;;","23/Jan/18 18:42;dongjoon;Thank you for confirming, [~smilegator]!;;;","24/Jan/18 00:15;dongjoon;Sorry, but this should be marked as 2.3.1 until RC3 starts to vote.
cc [~smilegator] and [~sameerag];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Workers registration failes in case of network drop,SPARK-23191,13133079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Ngone51,neeraj20gupta,neeraj20gupta,23/Jan/18 15:16,22/Sep/20 14:26,13/Jul/23 08:45,28/May/19 04:01,1.6.3,2.2.1,2.3.0,,,,,,,3.0.0,,,,,Spark Core,,,,0,,,,"We have a 3 node cluster. We were facing issues of multiple driver running in some scenario in production.

On further investigation we were able to reproduce iin both 1.6.3 and 2.2.1 versions the scenario with following steps:-
 # Setup a 3 node cluster. Start master and slaves.
 # On any node where the worker process is running block the connections on port 7077 using iptables.
{code:java}
iptables -A OUTPUT -p tcp --dport 7077 -j DROP
{code}

 # After about 10-15 secs we get the error on node that it is unable to connect to master.
{code:java}
2018-01-23 12:08:51,639 [rpc-client-1-1] WARN  org.apache.spark.network.server.TransportChannelHandler - Exception in connection from <servername>
java.io.IOException: Connection timed out
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
        at java.lang.Thread.run(Thread.java:745)
2018-01-23 12:08:51,647 [dispatcher-event-loop-0] ERROR org.apache.spark.deploy.worker.Worker - Connection to master failed! Waiting for master to reconnect...
2018-01-23 12:08:51,647 [dispatcher-event-loop-0] ERROR org.apache.spark.deploy.worker.Worker - Connection to master failed! Waiting for master to reconnect...

{code}

 # Once we get this exception we renable the connections to port 7077 using
{code:java}
iptables -D OUTPUT -p tcp --dport 7077 -j DROP
{code}

 # Worker tries to register again with master but is unable to do so. It gives following error

{code:java}
2018-01-23 12:08:58,657 [worker-register-master-threadpool-2] WARN  org.apache.spark.deploy.worker.Worker - Failed to connect to master <servername>:7077
org.apache.spark.SparkException: Exception thrown in awaitResult:
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
        at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100)
        at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108)
        at org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1.run(Worker.scala:241)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to <servername>:7077
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
        at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197)
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
        ... 4 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection timed out: <servername>:7077
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
        ... 1 more
2018-01-23 12:09:03,705 [dispatcher-event-loop-5] ERROR org.apache.spark.deploy.worker.Worker - Worker registration failed: Duplicate worker ID
2018-01-23 12:09:03,705 [dispatcher-event-loop-5] ERROR org.apache.spark.deploy.worker.Worker - Worker registration failed: Duplicate worker ID{code}

 # The worker state is changed to DEAD in spark UI. As a result of which duplicate driver is launched.","OS:- Centos 6.9(64 bit)

 ",cloud_fan,neeraj20gupta,Ngone51,suj1th,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16190,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 28 04:01:42 UTC 2019,,,,,,,,,,"0|i3p94f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/18 08:09;suj1th;This is a known race condition, in which the reconnection attempt made by the worker after the network outage is seen as a request to register a duplicate worker on the (same) master and hence, the attempt fails. Details on this can be found in SPARK-4592. Although the race condition is resolved for the case in which a new master replaces the unresponsive master, it still exists when the same old master recovers, which is the case here.;;;","29/Apr/19 09:21;zuo.tingbing9;we faced the same issue in standalone HA mode. Could you please take a view on this issue?
{code:java}
2019-03-15 20:22:10,474 INFO Worker: Master has changed, new master is at spark://vmax17:7077 
2019-03-15 20:22:14,862 INFO Worker: Master with url spark://vmax18:7077 requested this worker to reconnect.
2019-03-15 20:22:14,863 INFO Worker: Connecting to master vmax18:7077... 
2019-03-15 20:22:14,863 INFO Worker: Connecting to master vmax17:7077... 
2019-03-15 20:22:14,865 INFO Worker: Master with url spark://vmax18:7077 requested this worker to reconnect.
2019-03-15 20:22:14,865 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already. 
2019-03-15 20:22:14,868 INFO Worker: Master with url spark://vmax18:7077 requested this worker to reconnect. 
2019-03-15 20:22:14,868 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already. 
2019-03-15 20:22:14,871 INFO Worker: Master with url spark://vmax18:7077 requested this worker to reconnect. 
2019-03-15 20:22:14,871 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already. 
2019-03-15 20:22:14,879 ERROR Worker: Worker registration failed: Duplicate worker ID
2019-03-15 20:22:14,891 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,891 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,893 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,893 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,893 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,896 INFO ShutdownHookManager: Shutdown hook called 
2019-03-15 20:22:14,898 INFO ShutdownHookManager: Deleting directory /data4/zdh/spark/tmp/spark-c578bf32-6a5e-44a5-843b-c796f44648ee 
2019-03-15 20:22:14,908 INFO ShutdownHookManager: Deleting directory /data3/zdh/spark/tmp/spark-7e57e77d-cbb7-47d3-a6dd-737b57788533 
2019-03-15 20:22:14,920 INFO ShutdownHookManager: Deleting directory /data2/zdh/spark/tmp/spark-0beebf20-abbd-4d99-a401-3ef0e88e0b05{code}
 

[~andrewor14]  [~cloud_fan] [~vanzin];;;","29/Apr/19 13:22;cloud_fan;[~Ngone51] can you take a look please?;;;","29/Apr/19 13:42;Ngone51;[~cloud_fan] Ok, I'll have a deep look after 5.1 holiday.;;;","07/May/19 12:20;Ngone51;Hi [~neeraj20gupta]

What do you mean by _multiple driver running in some scenario_ & _As a result of which duplicate driver is launched ?_

Do you mean there're multiple drivers running concurrently for one app ? ;;;","08/May/19 02:19;zuo.tingbing9;See these detail logs, master changed from vmax18 to vmax17.

In master vmax18,  worker be removed because got no heartbeat but soon got heartbeat and asking to re-register with master vmax18(will  tryRegisterAllMaster() which include master vmax17).

In the same time, worker has bean registered with master vmax17 when master vmax17 got leadership.

So Worker registration failed: Duplicate worker ID.

 

spark-mr-master-vmax18.log：
{code:java}
2019-03-15 20:22:09,441 INFO ZooKeeperLeaderElectionAgent: We have lost leadership
2019-03-15 20:22:14,544 WARN Master: Removing worker-20190218183101-vmax18-33129 because we got no heartbeat in 60 seconds
2019-03-15 20:22:14,544 INFO Master: Removing worker worker-20190218183101-vmax18-33129 on vmax18:33129
2019-03-15 20:22:14,864 WARN Master: Got heartbeat from unregistered worker worker-20190218183101-vmax18-33129. Asking it to re-register.
2019-03-15 20:22:14,975 ERROR Master: Leadership has been revoked -- master shutting down.
{code}
 

spark-mr-master-vmax17.log:
{code:java}
2019-03-15 20:22:14,870 INFO Master: Registering worker vmax18:33129 with 21 cores, 125.0 GB RAM
2019-03-15 20:22:15,261 INFO Master: vmax18:33129 got disassociated, removing it.
2019-03-15 20:22:15,263 INFO Master: Removing worker worker-20190218183101-vmax18-33129 on vmax18:33129
2019-03-15 20:22:15,311 ERROR Inbox: Ignoring error
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /spark/master_status/worker_worker-20190218183101-vmax18-33129
{code}
 

spark-mr-worker-vmax18.log:
{code:java}
2019-03-15 20:22:10,474 INFO Worker: Master has changed, new master is at spark://vmax17:7077
2019-03-15 20:22:14,862 INFO Worker: Master with url spark://vmax18:7077 requested this worker to reconnect.
2019-03-15 20:22:14,865 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already.
2019-03-15 20:22:14,879 ERROR Worker: Worker registration failed: Duplicate worker ID
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process!
2019-03-15 20:22:14,896 INFO ShutdownHookManager: Shutdown hook called{code}
 

PS, this will result another issue: The leader will always in COMPLETING_RECOVERY state.

worker-vmax18 shut down cause duplicate worker ID,and clear the worker's node on persist Engine(we use zookeeper). Then the new leader(master-vmax17) find the worker died and trying to remove it ,and try to clear the node on zookeeper,but the node has been removed yet during worker-vmax18 shut down ,so {color:#ff0000}*an exception was thrown in function completeRecovery()* *. Then the leader will always in COMPLETING_RECOVERY state.*{color}

 ;;;","11/May/19 05:18;neeraj20gupta;Hi [~Ngone51],

Yes in my case multiple drivers were running concurrently for one app.;;;","13/May/19 03:34;Ngone51;Hi [~neeraj20gupta]  Can you explain more about the part of _running concurrently for one app ?_ 

In my understanding, if the worker shipped with drivers exited due to  duplicate resgister, those drivers would also be killed. 

So, how does it(_running concurrently for one app_) happens ? ;;;","14/May/19 13:56;neeraj20gupta;Hi [~Ngone51],

In our case we were running the driver with ""supervise"" flag. So during the network glitch that driver was assumed to be dead and a new driver was spawned. 
However at that time(after the network glitch was over) we saw that the old driver was also in running state. This might be some boundary condition.
As a result of this we did a fix to remove supervise flag and create monitoring/alerting for the driver process.;;;","28/May/19 04:01;cloud_fan;Issue resolved by pull request 24569
[https://github.com/apache/spark/pull/24569];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reflect stage level blacklisting on executor tab ,SPARK-23189,13133006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,23/Jan/18 10:21,13/Feb/18 15:55,13/Jul/23 08:45,13/Feb/18 15:55,2.1.1,,,,,,,,,2.4.0,,,,,Web UI,,,,0,,,,"This issue is the came during working on SPARK-22577 where the conclusion was not only stage tab should reflect stage and application level backlisting but also the executor tab should be extended with stage level backlisting information.

As [~irashid] and [~tgraves] are discussed the backlisted stages should be listed for an executor like ""*stage[ , ,...]*"". One idea was to list only the most recent 3 of the blacklisted stages another was list all the active stages which are blacklisted.  

 ",,ajbozarth,apachespark,attilapiros,f2005870@gmail.com,irashid,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22577,,,,,,,,,,,"25/Jan/18 15:30;attilapiros;afterStageCompleted.png;https://issues.apache.org/jira/secure/attachment/12907714/afterStageCompleted.png","25/Jan/18 15:30;attilapiros;backlisted.png;https://issues.apache.org/jira/secure/attachment/12907713/backlisted.png","25/Jan/18 16:44;attilapiros;multiple_stages.png;https://issues.apache.org/jira/secure/attachment/12907728/multiple_stages.png","30/Jan/18 10:52;attilapiros;multiple_stages_1.png;https://issues.apache.org/jira/secure/attachment/12908314/multiple_stages_1.png","30/Jan/18 10:52;attilapiros;multiple_stages_2.png;https://issues.apache.org/jira/secure/attachment/12908315/multiple_stages_2.png","30/Jan/18 10:52;attilapiros;multiple_stages_3.png;https://issues.apache.org/jira/secure/attachment/12908316/multiple_stages_3.png",,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 13 15:55:20 UTC 2018,,,,,,,,,,"0|i3p8o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/18 10:22;attilapiros;I am working on this issue.;;;","23/Jan/18 13:26;attilapiros;In case of listing only the active stages only I have only one performance concern / question. It is easy to store the blacklisted stageId-s for a live executor during blacklisting. But n case of stage completion without introducing an extra map from stage ID to executor ID I can only find the relevant executors with a complete search.

Moreover I have an idea: what about showing just the last 3 by default but having a tooltip containing all the stages in case of hovering above the item? [~ajbozarth] is using a tooltip an option on the UI? I have seen some in the event timeline, but within a table I have found none.;;;","23/Jan/18 14:08;tgraves;[~attilapiros] what do you mean by the last 3?  Are you talking for the dead nodes or just referring to tracking all stages?

I was thinking only showing active stages in the executor page as blacklisted.  If its not active and it didn't get promoted to application then the user shouldn't care as much and they can see by going to the stages page.  [~irashid] thoughts?;;;","23/Jan/18 15:03;irashid;yeah I think just active stages might be fine.  In general, that will not be a long list (though of course, it could be long ...).

honestly, I am trying to understand exactly what this will be used for.  If you just want to know which executors / nodes might be bad, looking at ""failed tasks"" is probably better.  If its to see what is going on an active stage, it seems like you'd look at the stage page for that particular stage instead (as attila is handling in SPARK-22577).

The other use I could think of is finding ""repeat offenders"", if there are some executors which keep getting put on the taskset blacklist, but for some reason don't get put on the application blacklist (eg, wrong thresholds).  But for that, you'd want to see the blacklisting for stages even after they had completed.;;;","23/Jan/18 20:19;ajbozarth;The tooltip feature does exist, but I agree with [~irashid] that I'm not sure about the use case here. The executors page is dynamically created using the rest api, which would entail adding the blacklisted stages list to the api. I still think this is a cool idea, I'm just not sure if its necessary.;;;","23/Jan/18 20:28;tgraves;Personally I use the executors page a lot so that is why I think it would be nice to have it match the existing state of the active stages.  For instance I go there many times as soon as I load an application to see how many active executors it has (with dynamic allocation), I can quickly see how many are active/dead and in this case possibly blacklisted and compare that to how many tasks are running in the various states.  

Like I mentioned in the pr for the stages page though, I'm ok with waiting on this and see how just having it in stages page goes. ;;;","23/Jan/18 21:08;attilapiros;[~tgraves] I see so only active stages are interesting. Is it ok to find the relevant executors at stage completion by iterating through all the executors? So how many executors should we calculate typically? 

[~irashid], [~ajbozarth] thanks for the info then I forget the tooltip here.

I already started the coding but the Javascript part a bit slows me down still in the beginning of next week we will have something to see. ;;;","24/Jan/18 17:42;irashid;[~tgraves] -- why do you use the executors page instead of the stage page for the active stage?  Is it because you're typically running lots of jobs simultaneously?  Or is it just because its easier to navigate directly to the executors page -- no need to have a jobid / stageId?

If its just having a convenient spot to navigate to, we could easily at a ""lastStage"" / ""lastJob"" redirect, that would be pretty trivial.  Or maybe there is some other summary view we're missing to capture the current state of the cluster -- the ""executors"" page may be close to what you want know but perhaps we're actually missing something else.

I'm not trying to block this change or anything, just want to avoid UI clutter and think a bit about the right way to add this.;;;","24/Jan/18 18:06;tgraves;for large jobs the specific stage page is a pain to navigate.  Most of the time I go  there i immediately shrink the executors table just to be able to see task table more easily.  All of those pages are a pain to navigate and find things in my opinion, which is why we had a pr to change to datatables but got back burnered based on vanzin request because changes in history server. that is done now so I think could be done now but don't have time at the moment. But that is just one reason.  

Most of the time I can take a quick look at the job or stages tab (list of all stages) to see what is happening at a high level,  jump to executors tab to see what is going there at a high level, and then only if needed load the specific stages page.  It might just be the way I use it.  It also depends on what the user is complaining about as to where I go first.  I think if we fixed the stage page to be more usable that might change.   Like I already said I'm fine with waiting on this.;;;","24/Jan/18 19:11;irashid;OK, since nobody feels strongly and to avoid bike-shedding here, my suggestion to move forward is Attila goes ahead with this since it should be a small change, but if it complicated for whatever reason its probably not worth sinking a lot of time into.

I'd rather we just fixed the stage page to be better -- I don't like adding this if we plan on ripping it out later.  But also realize we don't have a ton of contributors working on the UI now, so who knows when the other UI stuff will happen, so we can make this small change, since I think you probably have a pretty good sense of what would be useful in these pages.
;;;","25/Jan/18 15:35;attilapiros;I have uploaded some images about the very first results:

- The [^backlisted.png] is when one stage is backlisted for the executor.
- The [^afterStageCompleted.png] when the stage is completed and the UI is updated.

[~irashid], [~tgraves] Any suggestions? ;;;","25/Jan/18 16:17;irashid;looks reasonable to me.  How do you differentiate application level blacklisting from stage-level blacklisting on the executors page?;;;","25/Jan/18 17:04;attilapiros;I have introduced a new attribute for live executors called blacklistedInStages.

It is a set containing the StageIDs where the executor was blacklisted.

So on the UI the label can be one of the followings:
- ""Blacklisted"" when the executor is blacklisted application level (old flag)
- ""Dead"" when the executor is not Blacklisted and not Active
- ""Blacklisted in Stages: [...]"" when the executor is Active but the blacklistedInStages set is not empty  
- ""Active"" when the executor is Active and blacklistedInStages set is empty;;;","26/Jan/18 15:00;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/20408;;;","13/Feb/18 15:55;irashid;Issue resolved by pull request 20408
[https://github.com/apache/spark/pull/20408];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initialize DriverManager first before loading Drivers,SPARK-23186,13132920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,23/Jan/18 04:10,10/Feb/18 02:30,13/Jul/23 08:45,09/Feb/18 04:56,2.2.1,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,0,,,,"Since some JDBC Drivers have class initialization code to call `DriverManager`, we need to initialize DriverManager first in order to avoid potential deadlock situation like the following or STORM-2527.

{code}
Thread 9587: (state = BLOCKED)
 - sun.reflect.NativeConstructorAccessorImpl.newInstance0(java.lang.reflect.Constructor, java.lang.Object[]) @bci=0 (Compiled frame; information may be imprecise)
 - sun.reflect.NativeConstructorAccessorImpl.newInstance(java.lang.Object[]) @bci=85, line=62 (Compiled frame)
 - sun.reflect.DelegatingConstructorAccessorImpl.newInstance(java.lang.Object[]) @bci=5, line=45 (Compiled frame)
 - java.lang.reflect.Constructor.newInstance(java.lang.Object[]) @bci=79, line=423 (Compiled frame)
 - java.lang.Class.newInstance() @bci=138, line=442 (Compiled frame)
 - java.util.ServiceLoader$LazyIterator.nextService() @bci=119, line=380 (Interpreted frame)
 - java.util.ServiceLoader$LazyIterator.next() @bci=11, line=404 (Interpreted frame)
 - java.util.ServiceLoader$1.next() @bci=37, line=480 (Interpreted frame)
 - java.sql.DriverManager$2.run() @bci=21, line=603 (Interpreted frame)
 - java.sql.DriverManager$2.run() @bci=1, line=583 (Interpreted frame)
 - java.security.AccessController.doPrivileged(java.security.PrivilegedAction) @bci=0 (Compiled frame)
 - java.sql.DriverManager.loadInitialDrivers() @bci=27, line=583 (Interpreted frame)
 - java.sql.DriverManager.<clinit>() @bci=32, line=101 (Interpreted frame)
 - org.apache.phoenix.mapreduce.util.ConnectionUtil.getConnection(java.lang.String, java.lang.Integer, java.lang.String, java.util.Properties) @bci=12, line=98 (Interpreted frame)
 - org.apache.phoenix.mapreduce.util.ConnectionUtil.getInputConnection(org.apache.hadoop.conf.Configuration, java.util.Properties) @bci=22, line=57 (Interpreted frame)
 - org.apache.phoenix.mapreduce.PhoenixInputFormat.getQueryPlan(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.conf.Configuration) @bci=61, line=116 (Interpreted frame)
 - org.apache.phoenix.mapreduce.PhoenixInputFormat.createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext) @bci=10, line=71 (Interpreted frame)
 - org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(org.apache.spark.rdd.NewHadoopRDD, org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=233, line=156 (Interpreted frame)

Thread 9170: (state = BLOCKED)
 - org.apache.phoenix.jdbc.PhoenixDriver.<clinit>() @bci=35, line=125 (Interpreted frame)
 - sun.reflect.NativeConstructorAccessorImpl.newInstance0(java.lang.reflect.Constructor, java.lang.Object[]) @bci=0 (Compiled frame)
 - sun.reflect.NativeConstructorAccessorImpl.newInstance(java.lang.Object[]) @bci=85, line=62 (Compiled frame)
 - sun.reflect.DelegatingConstructorAccessorImpl.newInstance(java.lang.Object[]) @bci=5, line=45 (Compiled frame)
 - java.lang.reflect.Constructor.newInstance(java.lang.Object[]) @bci=79, line=423 (Compiled frame)
 - java.lang.Class.newInstance() @bci=138, line=442 (Compiled frame)
 - org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(java.lang.String) @bci=89, line=46 (Interpreted frame)
 - org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$2.apply() @bci=7, line=53 (Interpreted frame)
 - org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$2.apply() @bci=1, line=52 (Interpreted frame)
 - org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anon$1.<init>(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD, org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=81, line=347 (Interpreted frame)
 - org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=7, line=339 (Interpreted frame)
{code}",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 09 21:35:05 UTC 2018,,,,,,,,,,"0|i3p85b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/18 04:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20357;;;","23/Jan/18 05:04;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20359;;;","23/Jan/18 07:28;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20357;;;","09/Feb/18 04:56;cloud_fan;Issue resolved by pull request 20359
[https://github.com/apache/spark/pull/20359];;;","09/Feb/18 21:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20563;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark parameter-less UDFs raise exception if applied after distinct,SPARK-23177,13132665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,Wasikowski,Wasikowski,22/Jan/18 12:01,12/Dec/22 18:10,13/Jul/23 08:45,24/Jan/18 02:45,2.1.2,2.2.0,2.2.1,,,,,,,2.3.0,,,,,PySpark,,,,0,,,,"It seems there is an issue with UDFs that take no arguments, but only if UDF is applied after {{distinct()}} operation.

Here is the short example, that allows reproduce an issue in PySpark shell:
{code:java}
import pyspark.sql.functions as f
import uuid

df = spark.createDataFrame([(1,2), (3,4)])
f_udf = f.udf(lambda: str(uuid.uuid4()))
df.distinct().withColumn(""a"", f_udf()).show()
{code}
and it raises the following exception:
{noformat}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/spark/python/pyspark/sql/dataframe.py"", line 336, in show
    print(self._jdf.showString(n, 20))
  File ""/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
  File ""/opt/spark/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o54.showString.
: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: pythonUDF0#16
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$33.apply(HashAggregateExec.scala:475)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$33.apply(HashAggregateExec.scala:474)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:474)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:612)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:311)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)
	at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2150)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2363)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Couldn't find pythonUDF0#16 in [_1#0L,_2#1L]
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 63 more
{noformat}
It is also worth to mention, that the same code without {{distinct}} does not cause an error.
 Furthermore, if the UDF takes at least one argument then an exception is not raised as well.",,apachespark,Wasikowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 07:12:04 UTC 2018,,,,,,,,,,"0|i3p6kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/18 07:15;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20360;;;","24/Jan/18 02:45;gurwls223;Fixed in https://github.com/apache/spark/pull/20360;;;","24/Jan/18 07:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
from_json can produce nulls for fields which are marked as non-nullable,SPARK-23173,13132556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mswit,hvanhovell,hvanhovell,22/Jan/18 01:43,12/Dec/22 18:11,13/Jul/23 08:45,10/Mar/18 14:05,2.2.1,,,,,,,,,2.3.1,2.4.0,,,,SQL,,,,0,release-notes,,,"The {{from_json}} function uses a schema to convert a string into a Spark SQL struct. This schema can contain non-nullable fields. The underlying {{JsonToStructs}} expression does not check if a resulting struct respects the nullability of the schema. This leads to very weird problems in consuming expressions. In our case parquet writing would produce an illegal parquet file.

There are roughly solutions here:
 # Assume that each field in schema passed to {{from_json}} is nullable, and ignore the nullability information set in the passed schema.
 # Validate the object during runtime, and fail execution if the data is null where we are not expecting this.
I currently am slightly in favor of option 1, since this is the more performant option and a lot easier to do.

WDYT? cc [~rxin] [~marmbrus] [~hyukjin.kwon] [~brkyvz]",,apachespark,brkyvz,cloud_fan,hvanhovell,maropu,mswit,rxin@databricks.com,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10848,,,SPARK-17763,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 10 14:05:19 UTC 2018,,,,,,,,,,"0|i3p5wf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/18 04:16;gurwls223;I believe this one is related with SPARK-17763. My first try was 2 (roughly 1.5 years ago).

The root cause seems the same - I just double checked the Jackson parsers produce {{null}} regardless of the nullability.

Just FYI, there was a related discussion in SPARK-16472 too. If I understood correctly, seems the guys roughly rather prefer to leave it as nullable rather than failure during runtime. 

So, +1 for 1. to me given the past discussion and It seems simplest and less invasion.

cc [~cloud_fan] too who was in the discussion of SPARK-16472.;;;","22/Jan/18 07:04;cloud_fan;+1 on proposal 1.;;;","22/Jan/18 08:21;viirya;+1 for 1 too.;;;","22/Jan/18 13:01;brkyvz;In terms of usability, I prefer 1. In terms of the viewpoint of a data engineer, I would like 2 as well if that's not too hard.

 

Basically, if I expect that my data doesn't have nulls, but is suddenly outputting them, I would rather have it fail initially (or get written out to the \_corrupt\_record column).

In an ideal world, I should be able to either permit nullable fields (Option 1), or have the record be written out as corrupt.;;;","22/Jan/18 15:16;mswit;I think starting with option 1 is a good idea for a remedy of the corruption issue.

Verifying the data would certainly be good and it can be done in at least two approaches:

(1) detect incorrect data and fail

(2) write rejected data to a separate file/column as Burak suggests

 

(1) can even be orthogonal if the verification is done at the level of parquet encoding. It would help avoid the corruption with all sources, not just JSON.;;;","25/Jan/18 12:16;mswit;I'm going to work on this.;;;","25/Jan/18 20:04;rxin@databricks.com;Yea I agree with you Herman.

On Sun, Jan 21, 2018 at 5:44 PM Herman van Hovell (JIRA) <jira@apache.org>

;;;","28/Feb/18 12:51;apachespark;User 'mswit-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/20694;;;","10/Mar/18 14:05;gurwls223;Fixed in https://github.com/apache/spark/pull/20694;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
withColumn fails for a column that is a result of mapped DataSet,SPARK-23157,13132223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,henryr,kretes,kretes,19/Jan/18 16:24,12/Dec/22 18:11,13/Jul/23 08:45,01/Feb/18 02:16,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Having 

{code:java}
case class R(id: String)
val ds = spark.createDataset(Seq(R(""1"")))
{code}

This works:
{code}
scala> ds.withColumn(""n"", ds.col(""id""))
res16: org.apache.spark.sql.DataFrame = [id: string, n: string]
{code}

but when we map over ds it fails:
{code}
scala> ds.withColumn(""n"", ds.map(a => a).col(""id""))
org.apache.spark.sql.AnalysisException: resolved attribute(s) id#55 missing from id#4 in operator !Project [id#4, id#55 AS n#57];;
!Project [id#4, id#55 AS n#57]
+- LocalRelation [id#4]

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:347)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2884)
  at org.apache.spark.sql.Dataset.select(Dataset.scala:1150)
  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:1905)
  ... 48 elided
{code}",,apachespark,henryr,kretes,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 01 02:16:23 UTC 2018,,,,,,,,,,"0|i3p4ev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/18 01:31;henryr;I'm not sure if this should actually be expected to work. {{Dataset.map()}} will always return a dataset with a logical plan that's different to the original, so {{ds.map(a => a).col(""id"")}} has an expression that refers to an attribute ID that isn't produced by the original dataset. It seems like the requirement for {{ds.withColumn()}} is that the column argument is an expression over {{ds}}'s logical plan.

You get the same error doing the following, which is more explicit about these being two separate datasets.
{code:java}
scala> val ds = spark.createDataset(Seq(R(""1"")))
ds: org.apache.spark.sql.Dataset[R] = [id: string]

scala> val ds2 = spark.createDataset(Seq(R(""1"")))
ds2: org.apache.spark.sql.Dataset[R] = [id: string]

scala> ds.withColumn(""id2"", ds2.col(""id""))
org.apache.spark.sql.AnalysisException: Resolved attribute(s) id#113 missing from id#1 in operator !Project [id#1, id#113 AS id2#115]. Attribute(s) with the same name appear in the operation: id. Please check if the right attribute(s) are used.;;
!Project [id#1, id#113 AS id2#115]
+- LocalRelation [id#1]

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:41)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:297)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:70)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3286)
  at org.apache.spark.sql.Dataset.select(Dataset.scala:1303)
  at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2185)
  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2152)
  ... 49 elided
{code}
If the {{map}} function weren't the identity, would you expect this still to work?;;;","29/Jan/18 10:15;kretes;[~henryr] I see what you mean in your example.

However I would expect my example to work both with an identity and with any arbitrary function. My expectations for this API come from e.g. pandas where I can operate on columns and if I have the same number of elements in column - I can use it to in multiple Datasets.

And since in my example root Dataset is the same and there is no shuffling/filtering or any other operation that may change the order or number of rows - adding column that is an effect of some operation on same root Dataset should be possible.;;;","29/Jan/18 12:04;srowen;Agree this should not work . You are selecting a column from a different Dataset. Happening to work because a number of cols matches or the function is the identity sounds like as much way to write bugs as convenience;;;","29/Jan/18 20:24;henryr;[~kretes] - I can see an argument for the behaviour you're describing, but that's not the way the API is apparently intended. Like Sean says, there are way too many ways to shoot yourself in the foot if you can stitch together arbitrary datasets like this if the Datasets are column-wise incompatible, and allowing the relatively small subset of cases where it would work would lead to a more confusing API, IMO. 

The documentation for {{withColumn()}} could be updated to make this clearer; if I get a moment today I'll submit a PR. ;;;","29/Jan/18 21:57;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/20429;;;","30/Jan/18 22:52;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/20443;;;","01/Feb/18 02:16;gurwls223;Issue resolved by pull request 20443
[https://github.com/apache/spark/pull/20443];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid guard condition in org.apache.spark.ml.classification.Classifier,SPARK-23152,13131983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tovbinm,tovbinm,tovbinm,18/Jan/18 19:29,24/Jan/18 18:15,13/Jul/23 08:45,24/Jan/18 18:15,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.1.3,2.3.0,2.3.1,2.4.0,,,,,ML,MLlib,,,0,easyfix,,,"When fitting a classifier that extends ""org.apache.spark.ml.classification.Classifier"" (NaiveBayes, DecisionTreeClassifier, RandomForestClassifier) a misleading NullPointerException is thrown.

Steps to reproduce: 
{code:java}
val data = spark.createDataset(Seq.empty[(Double, org.apache.spark.ml.linalg.Vector)])
new DecisionTreeClassifier().setLabelCol(""_1"").setFeaturesCol(""_2"").fit(data)
{code}
 The error: 
{code:java}
java.lang.NullPointerException: Value at index 0 is null

at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:165)
at org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:115)
at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:102)
at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)
at org.apache.spark.ml.Predictor.fit(Predictor.scala:118){code}
  

The problem happens due to an incorrect guard condition in function getNumClasses at org.apache.spark.ml.classification.Classifier:106
{code:java}
val maxLabelRow: Array[Row] = dataset.select(max($(labelCol))).take(1)
if (maxLabelRow.isEmpty) {
  throw new SparkException(""ML algorithm was given empty dataset."")
}
{code}
When the input data is empty the result ""maxLabelRow"" array is not. Instead it contains a single Row(null) element.

 

Proposed solution: the condition can be modified to verify that.
{code:java}
if (maxLabelRow.isEmpty || maxLabelRow(0).get(0) == null) {
  throw new SparkException(""ML algorithm was given empty dataset."")
}
{code}
 

 ",,apachespark,tovbinm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 18:15:11 UTC 2018,,,,,,,,,,"0|i3p2xr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 20:46;apachespark;User 'tovbinm' has created a pull request for this issue:
https://github.com/apache/spark/pull/20321;;;","24/Jan/18 18:15;srowen;Issue resolved by pull request 20321
[https://github.com/apache/spark/pull/20321];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.read.csv with multiline=true gives FileNotFoundException if path contains spaces,SPARK-23148,13131871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,henryr,bograd,bograd,18/Jan/18 13:03,12/Dec/22 18:11,13/Jul/23 08:45,24/Jan/18 12:20,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Repro code:
{code:java}
spark.range(10).write.csv(""/tmp/a b c/a.csv"")
spark.read.option(""multiLine"", false).csv(""/tmp/a b c/a.csv"").count
10
spark.read.option(""multiLine"", true).csv(""/tmp/a b c/a.csv"").count
java.io.FileNotFoundException: File file:/tmp/a%20b%20c/a.csv/part-00000-cf84f9b2-5fe6-4f54-a130-a1737689db00-c000.csv does not exist
{code}

Trying to manually escape fails in a different place:
{code}
spark.read.option(""multiLine"", true).csv(""/tmp/a%20b%20c/a.csv"").count
org.apache.spark.sql.AnalysisException: Path does not exist: file:/tmp/a%20b%20c/a.csv;
  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:683)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:387)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:387)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:381)
{code}",,apachespark,bograd,guanzhe,henryr,highfei2011@126.com,jomach,ksunitha,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23814,,,,,,,SPARK-30647,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 26 21:28:57 UTC 2020,,,,,,,,,,"0|i3p28v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/18 06:39;highfei2011@126.com;If you must do this, I think it is best to add escaping it.;;;","19/Jan/18 10:11;bograd;I updated the description with manul escape, if that is what you meant;;;","19/Jan/18 11:27;guanzhe;Looks like the same problem with [SPARK-21996|https://issues.apache.org/jira/browse/SPARK-21996?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel]. I get the problem fixed by making the following change:

=============DataSourceScanExec.scala=========

line:441

Seq(PartitionedFile(
 partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) 

==>

Seq(PartitionedFile(
partition.values, file.getPath.toString, 0, file.getLen, hosts)) 

==========================================

 

Can you help to make a pull request?;;;","19/Jan/18 23:25;henryr;It seems like the problem is that {{CodecStreams.createInputStreamWithCloseResource}} can't properly handle a {{path}} argument that's URL-encoded. We could add an overload for {{createInputStreamWithCloseResource(Configuration, Path)}} and then pass {{new Path(new URI(path))}} from {{CSVDataSource.readFile()}}. This has the benefit of being a more localised change (and doesn't change the 'contract' that comes from {{FileScanRDD}} currently having URL-encoded pathnames everywhere). A strawman commit is [here|https://github.com/henryr/spark/commit/b8c51418ee7d4bca18179fd863f7f4885c98c0ef].;;;","20/Jan/18 14:58;gurwls223;Anyone want to open a PR with regression test for both JSON and CSV? I took a quick look and I think we can remove the previous one too. I realised actually I made that function.;;;","23/Jan/18 01:27;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/20355;;;","24/Jan/18 12:20;gurwls223;Issue resolved by pull request 20355
[https://github.com/apache/spark/pull/20355];;;","26/Jan/20 21:28;jomach;Hi [~hyukjin.kwon] and [~henryr]  , I have the same problem if I create a custom data source 

```

class ImageFileValidator extends FileFormat with DataSourceRegister with Serializable

```

So the Problem Needs to be in some other places. 

Here my trace: 

 

I created https://issues.apache.org/jira/browse/SPARK-30647
{code:java}
 org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 213, localhost, executor driver): java.io.FileNotFoundException: File file:somePath/0019_leftImg8%20bit.png does not exist
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
   at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
   at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
   at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
   at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
   at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
   at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
   at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125)
   at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
   at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)
   at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
   at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
   at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
   at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)

{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage page will throw exception when there's no complete tasks,SPARK-23147,13131863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,18/Jan/18 12:51,18/Jan/18 18:20,13/Jul/23 08:45,18/Jan/18 18:20,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Current Stage page's task table will throw an exception when there's no complete tasks, to reproduce this issue, user could submit code like:

{code}
sc.parallelize(1 to 20, 20).map { i => Thread.sleep(10000); i }.collect()
{code}

Then open the UI and click into stage details. Below is the screenshot.

 !Screen Shot 2018-01-18 at 8.50.08 PM.png! 

Deep dive into the code, found that current UI can only show the completed tasks, it is different from 2.2 code.
",,apachespark,jerryshao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 12:51;jerryshao;Screen Shot 2018-01-18 at 8.50.08 PM.png;https://issues.apache.org/jira/secure/attachment/12906616/Screen+Shot+2018-01-18+at+8.50.08+PM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 18:20:10 UTC 2018,,,,,,,,,,"0|i3p273:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 13:17;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/20315;;;","18/Jan/18 18:20;vanzin;Issue resolved by pull request 20315
[https://github.com/apache/spark/pull/20315];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSourceV2Strategy is missing in HiveSessionStateBuilder,SPARK-23140,13131774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,18/Jan/18 03:36,18/Jan/18 11:22,13/Jul/23 08:45,18/Jan/18 11:22,2.3.0,,,,,,,,,2.3.0,,,,,SQL,Structured Streaming,,,0,,,,"DataSourceV2Strategy is not added into HiveSessionStateBuilder's planner, which will lead to exception when playing continuous query:

{noformat}
ERROR ContinuousExecution: Query abc [id = 5cb6404a-e907-4662-b5d7-20037ccd6947, runId = 617b8dea-018e-4082-935e-98d98d473fdd] terminated with error
java.lang.AssertionError: assertion failed: No plan for WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.ContinuousMemoryWriter@3dba6d7c
+- StreamingDataSourceV2Relation [timestamp#15, value#16L], org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousReader@62ceac53

	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$2.apply(ContinuousExecution.scala:221)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$2.apply(ContinuousExecution.scala:212)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:271)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runContinuous(ContinuousExecution.scala:212)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runActivatedStream(ContinuousExecution.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:279)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
{noformat}
",,apachespark,cloud_fan,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 11:22:41 UTC 2018,,,,,,,,,,"0|i3p1nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 03:41;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/20305;;;","18/Jan/18 11:22;cloud_fan;Issue resolved by pull request 20305
[https://github.com/apache/spark/pull/20305];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accumulators don't show up properly in the Stages page anymore,SPARK-23135,13131674,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,brkyvz,brkyvz,17/Jan/18 19:29,19/Jan/18 21:19,13/Jul/23 08:45,19/Jan/18 21:19,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"Didn't do a lot of digging but may be caused by:

[https://github.com/apache/spark/commit/1c70da3bfbb4016e394de2c73eb0db7cdd9a6968#diff-0d37752c6ec3d902aeff701771b4e932]

 

!webUIAccumulatorRegression.png!"," 

 

 ",apachespark,brkyvz,sameerag,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/18 19:37;brkyvz;webUIAccumulatorRegression.png;https://issues.apache.org/jira/secure/attachment/12906457/webUIAccumulatorRegression.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 19 21:18:58 UTC 2018,,,,,,,,,,"0|i3p11b:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"17/Jan/18 19:38;brkyvz;cc [~vanzin];;;","17/Jan/18 19:58;vanzin;I'll try to take a look at the code, but do you have code to replicate the issue?

I tried the following and the page renders fine:

{code}
scala> val acc1 = sc.longAccumulator(""acc1"")
acc1: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(acc1), value: 0)

scala> val acc2 = sc.longAccumulator(""acc2"")
acc2: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(acc2), value: 0)

scala> sc.parallelize(1 to 10, 10).map { i =>
     |   acc1.add(i)
     |   acc2.add(i * 2)
     |   i
     | }.collect()
{code}
;;;","17/Jan/18 20:03;vanzin;(By fine I mean the table renders correctly; the accumulator still shows ""Some(blah)"" which is wrong, though.);;;","17/Jan/18 20:20;vanzin;Nevermind, I was able to get the wrong table after some tries.;;;","17/Jan/18 20:36;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20299;;;","19/Jan/18 21:18;sameerag;Issue resolved by pull request 20299 https://github.com/apache/spark/pull/20299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark options are not passed to the Executor in Docker context,SPARK-23133,13131645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,akorzhuev,akorzhuev,akorzhuev,17/Jan/18 17:44,17/May/20 18:24,13/Jul/23 08:45,18/Jan/18 22:01,2.3.0,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,0,,,,"Reproduce:
 # Build image with `bin/docker-image-tool.sh`.
 # Submit application to k8s. Set executor options, e.g. ` --conf ""spark.executor. extraJavaOptions=-Djava.security.auth.login.config=./jaas.conf""`
 # Visit Spark UI on executor and notice that option is not set.

Expected behavior: options from spark-submit should be correctly passed to executor.

Cause:

`SPARK_EXECUTOR_JAVA_OPTS` is not defined in `entrypoint.sh`

https://github.com/apache/spark/blob/branch-2.3/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L70

[https://github.com/apache/spark/blob/branch-2.3/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L44-L45]",Running Spark on K8s using supplied Docker image.,akorzhuev,apachespark,foxish,vanzin,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 22:01:43 UTC 2018,,,,,,,,,,"0|i3p0uv:",9223372036854775807,,,,,vanzin,,,,,,,,,,,,,,,,,,"17/Jan/18 18:52;apachespark;User 'andrusha' has created a pull request for this issue:
https://github.com/apache/spark/pull/20296;;;","18/Jan/18 19:48;foxish;Thanks for submitting the PR fixing this.;;;","18/Jan/18 21:18;apachespark;User 'foxish' has created a pull request for this issue:
https://github.com/apache/spark/pull/20322;;;","18/Jan/18 22:01;vanzin;Issue resolved by pull request 20322
[https://github.com/apache/spark/pull/20322];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the Spark Streaming app is running for a period of time, the page is incorrectly reported when accessing '/ jobs /' or '/ jobs / job /? Id = 13' and ui can not be accessed.",SPARK-23121,13131443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smurakozi,guoxiaolongzte,guoxiaolongzte,17/Jan/18 03:16,20/Feb/18 19:58,13/Jul/23 08:45,22/Jan/18 18:36,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,,,,"When the Spark Streaming app is running for a period of time, the page is incorrectly reported when accessing '/ jobs /' or '/ jobs / job /? Id = 13' and ui can not be accessed.

 

Test command:

./bin/spark-submit --class org.apache.spark.examples.streaming.HdfsWordCount ./examples/jars/spark-examples_2.11-2.4.0-SNAPSHOT.jar /spark

 

The app is running for a period of time,  ui can not be accessed, please see attachment.

 

 ",,apachespark,guoxiaolongzte,sameerag,smurakozi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23184,,,,,,,,,,,,,,,,,,"17/Jan/18 03:18;guoxiaolongzte;1.png;https://issues.apache.org/jira/secure/attachment/12906324/1.png","17/Jan/18 03:18;guoxiaolongzte;2.png;https://issues.apache.org/jira/secure/attachment/12906325/2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 22 18:36:57 UTC 2018,,,,,,,,,,"0|i3ozmn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"17/Jan/18 03:21;srowen;Isn't this because the job info is no longer retained by the UI? You can configure how many it remembers, but it's not infinite.;;;","17/Jan/18 03:29;apachespark;User 'guoxiaolongzte' has created a pull request for this issue:
https://github.com/apache/spark/pull/20287;;;","17/Jan/18 03:30;guoxiaolongzte;The problem is the page is down and it has not been able to recover.;;;","17/Jan/18 03:50;srowen;You're just saying there is an error in another way.The question is whether it's expected. I think it is, given your description. The fix is not OK (swallowing all exceptions); unless you're trying to refine cases like this to return a better response like 404, I think this should be closed.;;;","19/Jan/18 12:55;apachespark;User 'smurakozi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20330;;;","19/Jan/18 13:27;smurakozi;[~guoxiaolongzte] found two separate problems, bot triggered by having a high number of jobs/stages. In such a situation the store of the history server drops various objects to save memory. It may happen that the job itself is in the store, but its stages or the RDDOperationGraph are not. In such cases rendering of the all jobs and the job pages fails.

As a consequence, the jobs page may become inaccessible if the cluster processes many jobs, so I think the priority of this issue should be increased.

What do you think [~srowen] ?;;;","19/Jan/18 14:06;srowen;Yes that sounds right. But doesn't it just cause an error when displaying pages for old jobs? it would be an 'error' of some kind no matter what, whether a 404 or ""not found"" message. It can be improved but didn't sound like it mattered beyond that.;;;","19/Jan/18 14:11;smurakozi;One issue is with displaying old jobs. Depending on how old a job is it may or may not be displayed correctly.

The bigger issue is that the main jobs page can also be affected. ;;;","22/Jan/18 18:36;vanzin;Issue resolved by pull request 20330
[https://github.com/apache/spark/pull/20330];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix API annotation in DataSource V2 for streaming,SPARK-23119,13131434,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,17/Jan/18 02:08,18/Jan/18 00:40,13/Jul/23 08:45,18/Jan/18 00:40,2.3.0,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,0,,,,,,apachespark,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 00:40:28 UTC 2018,,,,,,,,,,"0|i3ozkn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"17/Jan/18 02:19;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/20286;;;","18/Jan/18 00:40;zsxwing;Issue resolved by pull request 20286
[https://github.com/apache/spark/pull/20286];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LevelDB store not iterating correctly when indexed value has negative value,SPARK-23103,13131377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,16/Jan/18 22:17,22/Jan/18 21:43,13/Jul/23 08:45,19/Jan/18 19:33,2.3.0,,,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,"Marking as minor since I don't believe we currently have anything that needs to store negative values in indexed fields. But I wrote a unit test and got:

 
{noformat}
[error] Test org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues failed: java.lang.AssertionError: expected:<[-50, 0, 50]> but was:<[[0, -50, 50]]>, took 0.025 sec
{noformat}",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 22 21:43:07 UTC 2018,,,,,,,,,,"0|i3oz87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/18 22:57;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20284;;;","17/Jan/18 20:06;irashid;-[~vanzin] is this really minor, not a blocker for 2.3?  Or you think its doesn't matter since nothing we care about for sorting in SHS ever has negative values?-

EDIT:  argh I'm sorry, I should have actually read your description first;;;","17/Jan/18 20:12;vanzin;Given the unit test failure in the PR it might not be as minor; but I haven't looked at that yet (looking at another bug still).;;;","19/Jan/18 19:33;irashid;Issue resolved by pull request 20284
[https://github.com/apache/spark/pull/20284];;;","22/Jan/18 21:43;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20353;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decorrelation of scalar subquery fails with java.util.NoSuchElementException.,SPARK-23095,13131353,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,16/Jan/18 21:12,17/Jan/18 02:00,13/Jul/23 08:45,17/Jan/18 02:00,2.2.2,2.3.0,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The following SQL involving scalar correlated query returns a map exception.
{code:java}
SELECT t1a
FROM   t1
WHERE  t1a = (SELECT   count
              FROM     t2
              WHERE    t2c = t1c
              HAVING   count >= 1)

{code}
{code:java}
 
key not found: ExprId(278,786682bb-41f9-4bd5-a397-928272cc8e4e) java.util.NoSuchElementException: key not found: ExprId(278,786682bb-41f9-4bd5-a397-928272cc8e4e)         at scala.collection.MapLike$class.default(MapLike.scala:228)         at scala.collection.AbstractMap.default(Map.scala:59)         at scala.collection.MapLike$class.apply(MapLike.scala:141)         at scala.collection.AbstractMap.apply(Map.scala:59)         at org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery$.org$apache$spark$sql$catalyst$optimizer$RewriteCorrelatedScalarSubquery$$evalSubqueryOnZeroTups(subquery.scala:378)         at org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery$$anonfun$org$apache$spark$sql$catalyst$optimizer$RewriteCorrelatedScalarSubquery$$constructLeftJoins$1.apply(subquery.scala:430)         at org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery$$anonfun$org$apache$spark$sql$catalyst$optimizer$RewriteCorrelatedScalarSubquery$$constructLeftJoins$1.apply(subquery.scala:426)
{code}

In this case, after evaluating the HAVING clause ""count(*) > 1"" statically
against the binding of aggregtation result on empty input, we determine
that this query will not have a the count bug. We should simply return
the evalSubqueryOnZeroTups with empty value.",,apachespark,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 16 21:26:04 UTC 2018,,,,,,,,,,"0|i3oz2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/18 21:26;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20283;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Json Readers choose wrong encoding when bad records are present and fail,SPARK-23094,13131346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,brkyvz,brkyvz,16/Jan/18 20:54,12/Dec/22 18:11,13/Jul/23 08:45,29/Apr/18 03:27,2.2.1,,,,,,,,,2.4.0,,,,,SQL,,,,0,,,,The cases described in SPARK-16548 and SPARK-20549 handled the JsonParser code paths for expressions but not the readers. We should also cover reader code paths reading files with bad characters.,,apachespark,brkyvz,dongjoon,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23428,,,SPARK-16548,SPARK-20549,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 07 03:40:04 UTC 2018,,,,,,,,,,"0|i3oz1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 01:04;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/20302;;;","18/Jan/18 22:36;gurwls223;Issue resolved by pull request 20302
[https://github.com/apache/spark/pull/20302];;;","14/Feb/18 23:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20614;;;","19/Feb/18 22:04;dongjoon;Since this is reverted and not a regression, I'll remove the fixed version to unblock RC4.;;;","04/Apr/18 21:43;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/20937;;;","29/Apr/18 03:27;gurwls223;Issue resolved by pull request 20937
[https://github.com/apache/spark/pull/20937];;;","07/May/18 03:40;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/21254;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to create operation log session directory"" when parent directory not present",SPARK-23089,13131267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,seano,seano,16/Jan/18 15:30,19/Jan/18 11:48,13/Jul/23 08:45,19/Jan/18 11:48,2.2.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"When creating a session directory, Thrift should create the parent directory _(i.e. /tmp/hive/operation_logs)_ if it is not present.

It's common for operators to clean-up old and empty directories in /tmp, or to have tools (systemd-tmpfiles or tmpwatch) that do it automatically.

This was fixed in HIVE-12262 but not in Spark Thrift as seen by this:
{code}18/01/15 14:22:49 WARN HiveSessionImpl: Unable to create operation log session directory: /tmp/hive/operation_logs/683a6318-adc4-42c4-b665-11dad14d7ec7{code}

Resolved by manually creating /tmp/hive/operation_logs/","/usr/hdp/2.6.3.0-235/spark2/jars/spark-hive-thriftserver_2.11-2.2.0.2.6.3.0-235.jar

$ cat /etc/redhat-release
Red Hat Enterprise Linux Server release 7.4 (Maipo)

$ ps aux|grep ^hive.*spark.*thrift
hive     1468503  0.9  0.5 13319628 1411676 ?    Sl   Jan15  10:18 /usr/java/default/bin/java -Dhdp.version=2.6.3.0-235 -cp /usr/hdp/current/spark2-thriftserver/conf/:/usr/hdp/current/spark2-thriftserver/jars/*:/usr/hdp/current/hadoop-client/conf/ -Xmx2048m org.apache.spark.deploy.SparkSubmit --properties-file /usr/hdp/current/spark2-thriftserver/conf/spark-thrift-sparkconf.conf --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server spark-internal
",apachespark,mgaido,seano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 16 19:47:05 UTC 2018,,,,,,,,,,"0|i3oyjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/18 19:47;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20281;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server not showing incomplete/running applications,SPARK-23088,13131251,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adobe_pmackles,adobe_pmackles,adobe_pmackles,16/Jan/18 14:38,09/Nov/18 01:59,13/Jul/23 08:45,30/Jan/18 03:18,2.1.2,2.2.1,,,,,,,,2.4.0,,,,,Spark Core,Web UI,,,0,,,,"History server not showing incomplete/running applications when _spark.history.ui.maxApplications_ property is set to a value that is smaller than the total number of applications.

I believe this is because any applications where completed=false wind up at the end of the list of apps returned by the /applications endpoint and when _spark.history.ui.maxApplications_ is set, that list gets truncated and the running apps are never returned.

The fix I have in mind is to modify the history template to start passing the _status_ parameter when calling the /applications endpoint (status=completed is the default).

I am running Spark in a Mesos environment but I don't think that is relevant to this issue",,adobe_pmackles,ajbozarth,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25975,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 30 03:18:01 UTC 2018,,,,,,,,,,"0|i3oygf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/18 14:42;apachespark;User 'pmackles' has created a pull request for this issue:
https://github.com/apache/spark/pull/20335;;;","30/Jan/18 03:18;jerryshao;Issue resolved by pull request 20335
https://github.com/apache/spark/pull/20335;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckCartesianProduct too restrictive when condition is constant folded to false/null,SPARK-23087,13131227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,juliuszsompolski,juliuszsompolski,16/Jan/18 13:00,21/Jan/18 06:41,13/Jul/23 08:45,21/Jan/18 06:41,2.2.1,2.3.0,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Running
{code}
sql(""SELECT id as a FROM RANGE(10)"").createOrReplaceTempView(""A"")
sql(""SELECT NULL as a FROM RANGE(10)"").createOrReplaceTempView(""NULLTAB"")
sql(""SELECT 1 as goo FROM A LEFT OUTER JOIN NULLTAB ON A.a = NULLTAB.a"").collect()
{code}
results in:
{code}
org.apache.spark.sql.AnalysisException: Detected cartesian product for LEFT OUTER join between logical plans
Project
+- Range (0, 10, step=1, splits=None)
and
Project
+- Range (0, 10, step=1, splits=None)
Join condition is missing or trivial.
Use the CROSS JOIN syntax to allow cartesian products between these relations.;
  at 
 org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)
{code}

This is because NULLTAB.a is constant folded to null, and then the condition is constant folded altogether:
{code}
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.NullPropagation ===
GlobalLimit 21                                      
 +- LocalLimit 21                                    
    +- Project [1 AS goo#28]                         
!      +- Join LeftOuter, (a#0L = null)              
          :- Project [id#1L AS a#0L]                 
          :  +- Range (0, 10, step=1, splits=None)   
          +- Project                                  
             +- Range (0, 10, step=1, splits=None) 

GlobalLimit 21
+- LocalLimit 21
   +- Project [1 AS goo#28]
      +- Join LeftOuter, null
         :- Project [id#1L AS a#0L]
         :  +- Range (0, 10, step=1, splits=None)
         +- Project
            +- Range (0, 10, step=1, splits=None)
{code}

And then CheckCartesianProduct doesn't like it, even though the condition does not produce a cartesian product, but evaluates to null.",,apachespark,joshrosen,juliuszsompolski,ksunitha,mgaido,xiayunsun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 19 21:19:04 UTC 2018,,,,,,,,,,"0|i3oyb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/18 21:19;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20333;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error message for built-in functions,SPARK-23080,13131062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mgaido,mgaido,mgaido,15/Jan/18 15:58,12/Dec/22 18:10,13/Jul/23 08:45,16/Jan/18 02:48,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"When a user puts the wrong number of parameters in a function, an AnalysisException is thrown. If the function is a UDF, he user is told how many parameters the function expected and how many he/she put. If the function, instead, is a built-in one, no information about the number of parameters expected and the actual one is provided. This can help in some cases, to debug the errors (eg. bad quotes escaping may lead to a different number of parameters than expected, etc. etc.)",,apachespark,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 16 02:48:28 UTC 2018,,,,,,,,,,"0|i3oxaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/18 16:16;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20271;;;","16/Jan/18 02:48;gurwls223;Issue resolved by pull request 20271
[https://github.com/apache/spark/pull/20271];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix query constraints propagation with aliases,SPARK-23079,13131048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,15/Jan/18 15:08,18/Jan/18 02:59,13/Jul/23 08:45,18/Jan/18 02:59,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Previously, PR #19201 fix the problem of non-converging constraints.

After that PR #19149 improve the loop and constraints is inferred only once.

So the problem of non-converging constraints is gone.

Also, in current code, the case below will fail.

```

spark.range(5).write.saveAsTable(""t"")
val t = spark.read.table(""t"")
val left = t.withColumn(""xid"", $""id"" + lit(1)).as(""x"")
val right = t.withColumnRenamed(""id"", ""xid"").as(""y"")
val df = left.join(right, ""xid"").filter(""id = 3"").toDF()
checkAnswer(df, Row(4, 3))

```

Because of `aliasMap` replace all the aliased child. See the test case in PR for details.

 

 ",,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 02:59:10 UTC 2018,,,,,,,,,,"0|i3ox7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/18 15:18;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20270;;;","16/Jan/18 17:10;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20278;;;","18/Jan/18 02:59;cloud_fan;Issue resolved by pull request 20278
[https://github.com/apache/spark/pull/20278];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump previousSparkVersion in MimaBuild.scala to be 2.2.0,SPARK-23070,13130773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,14/Jan/18 07:49,15/Jan/18 14:49,13/Jul/23 08:45,15/Jan/18 14:49,2.3.0,,,,,,,,,,,,,,SQL,,,,0,,,,,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 14 08:10:05 UTC 2018,,,,,,,,,,"0|i3ow9j:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"14/Jan/18 08:10;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20264;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R API doc empty in Spark 2.3.0 RC1,SPARK-23065,13130724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sameerag,felixcheung,felixcheung,13/Jan/18 06:23,14/Jan/18 06:42,13/Jul/23 08:45,14/Jan/18 06:42,2.3.0,,,,,,,,,2.3.0,,,,,SparkR,,,,0,,,,"[~sameerag]

https://dist.apache.org/repos/dist/dev/spark/v2.3.0-rc1-docs/_site/api/R/index.html

Did it fail to build?",,felixcheung,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/18 00:48;sameerag;Screen Shot 2018-01-13 at 3.15.48 PM.png;https://issues.apache.org/jira/secure/attachment/12906010/Screen+Shot+2018-01-13+at+3.15.48+PM.png","14/Jan/18 00:49;sameerag;Screen Shot 2018-01-13 at 3.16.06 PM.png;https://issues.apache.org/jira/secure/attachment/12906009/Screen+Shot+2018-01-13+at+3.16.06+PM.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 14 06:42:48 UTC 2018,,,,,,,,,,"0|i3ovyn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"13/Jan/18 09:52;sameerag;Thanks for verifying this [~felixcheung]! While the build didn't fail in an obvious way, it seems like it only uploaded partially generated R docs. I tried running it manually and it turned out to be a roxygen version issue:

{code}
~/dev/spark/spark/docs SKIP_SCALADOC=1 SKIP_PYTHONDOC=1 SKIP_SQLDOC=1 jekyll build
Configuration file: /Users/sameer/dev/spark/spark/docs/_config.yml
       Deprecation: The 'gems' configuration option has been renamed to 'plugins'. Please update your config file accordingly.
Moving to R directory and building roxygen docs.
Using Scala 2.11
Using R_SCRIPT_PATH = /usr/local/bin
++++ dirname /Users/sameer/dev/spark/spark/R/install-dev.sh
+++ cd /Users/sameer/dev/spark/spark/R
+++ pwd
++ FWDIR=/Users/sameer/dev/spark/spark/R
++ LIB_DIR=/Users/sameer/dev/spark/spark/R/lib
++ mkdir -p /Users/sameer/dev/spark/spark/R/lib
++ pushd /Users/sameer/dev/spark/spark/R
++ . /Users/sameer/dev/spark/spark/R/find-r.sh
+++ '[' -z /usr/local/bin ']'
++ . /Users/sameer/dev/spark/spark/R/create-rd.sh
+++ set -o pipefail
+++ set -e
+++++ dirname /Users/sameer/dev/spark/spark/R/create-rd.sh
++++ cd /Users/sameer/dev/spark/spark/R
++++ pwd
+++ FWDIR=/Users/sameer/dev/spark/spark/R
+++ pushd /Users/sameer/dev/spark/spark/R
+++ . /Users/sameer/dev/spark/spark/R/find-r.sh
++++ '[' -z /usr/local/bin ']'
+++ /usr/local/bin/Rscript -e ' if(""devtools"" %in% rownames(installed.packages())) { library(devtools); devtools::document(pkg=""./pkg"", roclets=c(""rd"")) }'
Error: ‘roxygen2’ >= 5.0.0 must be installed for this functionality.
Execution halted
jekyll 3.7.0 | Error:  R doc generation failed
{code}

I've fixed this issue and updated the docs at https://dist.apache.org/repos/dist/dev/spark/v2.3.0-rc1-docs/_site/index.html. Can you please verify if everything looks okay now?;;;","13/Jan/18 17:34;felixcheung;Was the Jekyll error failed the doc build? Just want to make sure error like this is discoverable.

I’m still seeing the empty header page can you check?
https://dist.apache.org/repos/dist/dev/spark/v2.3.0-rc1-docs/_site/api/R/index.html

Vs
https://spark.apache.org/docs/latest/api/R/index.html


;;;","14/Jan/18 00:52;sameerag;No, the jekyll error didn't fail the doc build. We should investigate why that happened.

Style wise, both https://dist.apache.org/repos/dist/dev/spark/v2.3.0-rc1-docs/_site/api/R/index.html and https://spark.apache.org/docs/latest/api/R/index.html look identical to me (attached screenshots). Is there something that I'm missing? Could it be your local browser cache?

By the way, FWIW, the R logo looks better in https://spark.apache.org/docs/2.2.0/api/R/index.html so it seems like something might've changed in 2.2.1.



;;;","14/Jan/18 01:08;felixcheung;Something was definitely cached - not sure what since I have forced refresh.

Anyway the links are back now (when I open in another browse), let me review more closely. Thanks


;;;","14/Jan/18 06:42;felixcheung;I have checked the doc, looks good, but I have a few small fixes. will open another JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct some improper with view related method usage,SPARK-23059,13130551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xubo245,xubo245,xubo245,12/Jan/18 15:27,29/Jan/18 16:59,13/Jul/23 08:45,29/Jan/18 16:59,2.2.1,,,,,,,,,2.4.0,,,,,SQL,Tests,,,0,,,,"And correct some improper usage like: 
{code:java}
 test(""list global temp views"") {
    try {
      sql(""CREATE GLOBAL TEMP VIEW v1 AS SELECT 3, 4"")
      sql(""CREATE TEMP VIEW v2 AS SELECT 1, 2"")

      checkAnswer(sql(s""SHOW TABLES IN $globalTempDB""),
        Row(globalTempDB, ""v1"", true) ::
        Row("""", ""v2"", true) :: Nil)

      assert(spark.catalog.listTables(globalTempDB).collect().toSeq.map(_.name) == Seq(""v1"", ""v2""))
    } finally {
      spark.catalog.dropTempView(""v1"")
      spark.catalog.dropGlobalTempView(""v2"")
    }
  }
{code}
",,apachespark,xubo245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 12 15:39:40 UTC 2018,,,,,,,,,,"0|i3ouwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/18 15:39;apachespark;User 'xubo245' has created a pull request for this issue:
https://github.com/apache/spark/pull/20250;;;","12/Jan/18 15:39;xubo245;split from https://github.com/apache/spark/pull/20228#issuecomment-357266852, according to [~dongjoon] committer review;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaContinuousSourceSuite Kafka column types test failing,SPARK-23055,13130459,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sameerag,mgaido,mgaido,12/Jan/18 08:26,12/Jan/18 23:41,13/Jul/23 08:45,12/Jan/18 23:21,2.3.0,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,0,,,,"KafkaContinuousSourceSuite Kafka column types test fails (https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/85998/testReport/junit/org.apache.spark.sql.kafka010/KafkaContinuousSourceSuite/Kafka_column_types/).

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/4009/
{code}
[info] KafkaContinuousSourceSuite:
[info] - cannot stop Kafka stream (1 second, 279 milliseconds)
[info] - assign from latest offsets (failOnDataLoss: true) (9 seconds, 202 milliseconds)
[info] - assign from earliest offsets (failOnDataLoss: true) (8 seconds, 108 milliseconds)
[info] - assign from specific offsets (failOnDataLoss: true) (4 seconds, 102 milliseconds)
[info] - subscribing topic by name from latest offsets (failOnDataLoss: true) (12 seconds, 125 milliseconds)
[info] - subscribing topic by name from earliest offsets (failOnDataLoss: true) (12 seconds, 69 milliseconds)
[info] - subscribing topic by name from specific offsets (failOnDataLoss: true) (5 seconds, 935 milliseconds)
[info] - subscribing topic by pattern from latest offsets (failOnDataLoss: true) (13 seconds, 70 milliseconds)
[info] - subscribing topic by pattern from earliest offsets (failOnDataLoss: true) (13 seconds, 122 milliseconds)
[info] - subscribing topic by pattern from specific offsets (failOnDataLoss: true) (7 seconds, 877 milliseconds)
[info] - assign from latest offsets (failOnDataLoss: false) (12 seconds, 201 milliseconds)
[info] - assign from earliest offsets (failOnDataLoss: false) (12 seconds, 82 milliseconds)
[info] - assign from specific offsets (failOnDataLoss: false) (8 seconds, 530 milliseconds)
[info] - subscribing topic by name from latest offsets (failOnDataLoss: false) (18 seconds, 339 milliseconds)
[info] - subscribing topic by name from earliest offsets (failOnDataLoss: false) (17 seconds, 397 milliseconds)
[info] - subscribing topic by name from specific offsets (failOnDataLoss: false) (8 seconds, 926 milliseconds)
[info] - subscribing topic by pattern from latest offsets (failOnDataLoss: false) (20 seconds, 198 milliseconds)
Build timed out (after 255 minutes). Marking the build as aborted.
Build was aborted
Archiving artifacts
[info] - subscribing topic by pattern from earliest offsets (failOnDataLoss: false) *** FAILED *** (2 hours, 24 minutes, 19 seconds)
[info]   Error while stopping stream: 
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/4010/console
{code}
[info] KafkaContinuousSourceSuite:
[info] - cannot stop Kafka stream (1 second, 238 milliseconds)
[info] - assign from latest offsets (failOnDataLoss: true) (9 seconds, 516 milliseconds)
[info] - assign from earliest offsets (failOnDataLoss: true) (7 seconds, 961 milliseconds)
[info] - assign from specific offsets (failOnDataLoss: true) (4 seconds, 193 milliseconds)
[info] - subscribing topic by name from latest offsets (failOnDataLoss: true) (11 seconds, 443 milliseconds)
[info] - subscribing topic by name from earliest offsets (failOnDataLoss: true) (12 seconds, 674 milliseconds)
[info] - subscribing topic by name from specific offsets (failOnDataLoss: true) (6 seconds, 13 milliseconds)
[info] - subscribing topic by pattern from latest offsets (failOnDataLoss: true) (13 seconds, 185 milliseconds)
Build timed out (after 255 minutes). Marking the build as aborted.
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7/4011/consoleFull
{code}
[info] KafkaContinuousSourceSuite:
[info] - cannot stop Kafka stream (1 second, 551 milliseconds)
[info] - assign from latest offsets (failOnDataLoss: true) (8 seconds, 560 milliseconds)
[info] - assign from earliest offsets (failOnDataLoss: true) (8 seconds, 40 milliseconds)
[info] - assign from specific offsets (failOnDataLoss: true) (4 seconds, 373 milliseconds)
[info] - subscribing topic by name from latest offsets (failOnDataLoss: true) (12 seconds, 872 milliseconds)
[info] - subscribing topic by name from earliest offsets (failOnDataLoss: true) (13 seconds, 338 milliseconds)
[info] - subscribing topic by name from specific offsets (failOnDataLoss: true) (6 seconds, 999 milliseconds)
[info] - subscribing topic by pattern from latest offsets (failOnDataLoss: true) (15 seconds, 393 milliseconds)
[info] - subscribing topic by pattern from earliest offsets (failOnDataLoss: true) (14 seconds, 482 milliseconds)
Build timed out (after 255 minutes). Marking the build as aborted.
Build was aborted
Archiving artifacts
[info] - subscribing topic by pattern from specific offsets (failOnDataLoss: true) *** FAILED *** (2 hours, 24 minutes, 1 second)
[info]   Error while stopping stream: 
.. tim
{code}",,attilapiros,dongjoon,mgaido,sameerag,smurakozi,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22908,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 12 23:41:49 UTC 2018,,,,,,,,,,"0|i3oubz:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"12/Jan/18 22:20;dongjoon;During investigating this, I found that the suite itself hangs, too. I updated the hang cases.
cc [~tdas], [~smilegator];;;","12/Jan/18 22:22;dongjoon;cc [~sameerag] since he's a release manager for 2.3.;;;","12/Jan/18 23:20;sameerag;I've reverted https://github.com/apache/spark/pull/20096 and re-opened SPARK-22908 to deflake the builds. Thanks!;;;","12/Jan/18 23:39;dongjoon;Thank you, [~sameerag].;;;","12/Jan/18 23:41;tdas;Thanks [~dongjoon] and [~sameerag] for handling this. [~joseph.torres] and I will fix the issue and redo the PR. Sorry for the trouble.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results of casting UserDefinedType to String,SPARK-23054,13130441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,12/Jan/18 07:58,19/Jan/18 03:41,13/Jul/23 08:45,19/Jan/18 03:41,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"{code}
>>> from pyspark.ml.classification import MultilayerPerceptronClassifier
>>> from pyspark.ml.linalg import Vectors
>>> df = spark.createDataFrame([(0.0, Vectors.dense([0.0, 0.0])), (1.0, Vectors.dense([0.0, 1.0]))], [""label"", ""features""])
>>> df.selectExpr(""CAST(features AS STRING)"").show(truncate = False)
+-------------------------------------------+
|features                                   |
+-------------------------------------------+
|[6,1,0,0,2800000020,2,0,0,0]               |
|[6,1,0,0,2800000020,2,0,0,3ff0000000000000]|
+-------------------------------------------+
{code}",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 05:44:05 UTC 2018,,,,,,,,,,"0|i3ou87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/18 08:09;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20246;;;","18/Jan/18 05:44;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
taskBinarySerialization and task partitions calculate in DagScheduler.submitMissingTasks should keep the same RDD checkpoint status,SPARK-23053,13130411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivoson,ivoson,ivoson,12/Jan/18 04:43,23/Feb/18 06:32,13/Jul/23 08:45,13/Feb/18 16:07,2.1.0,,,,,,,,,2.1.3,2.2.2,2.3.0,,,Scheduler,Spark Core,,,0,,,,"When we run concurrent jobs using the same rdd which is marked to do checkpoint. If one job has finished running the job, and start the process of RDD.doCheckpoint, while another job is submitted, then submitStage and submitMissingTasks will be called. In [submitMissingTasks|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L961], will serialize taskBinaryBytes and calculate task partitions which are both affected by the status of checkpoint, if the former is calculated before doCheckpoint finished, while the latter is calculated after doCheckpoint finished, when run task, rdd.compute will be called, for some rdds with particular partition type such as [MapWithStateRDD|https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala] who will do partition type cast, will get a ClassCastException because the part params is actually a CheckpointRDDPartition.
This error occurs because rdd.doCheckpoint occurs in the same thread that called sc.runJob, while the task serialization occurs in the DAGSchedulers event loop.",,apachespark,irashid,ivoson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 18 15:21:05 UTC 2018,,,,,,,,,,"0|i3ou1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/18 04:52;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/20244;;;","06/Feb/18 11:47;ivoson;here is the stack trace of exception.

{code:java}
java.lang.ClassCastException: org.apache.spark.rdd.CheckpointRDDPartition cannot be cast to org.apache.spark.streaming.rdd.MapWithStateRDDPartition
at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:152)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
{code}
;;;","09/Feb/18 05:48;ivoson;the following is a repro case, for clarity 
{code:java}
/** Wrapped rdd partition. */
class WrappedPartition(val partition: Partition) extends Partition {
  def index: Int = partition.index
}

/**
 * An RDD with a particular defined Partition which is WrappedPartition.
 * The compute method will cast the split to WrappedPartition. The cast operation will be
 * used in this test suite.
 */
class WrappedRDD(parent: RDD[Int]) extends RDD[Int](parent) {
  protected def getPartitions: Array[Partition] = {
    parent.partitions.map(p => new WrappedPartition(p))
  }

  def compute(split: Partition, context: TaskContext): Iterator[Int] = {
    parent.compute(split.asInstanceOf[WrappedPartition].partition, context)
  }
}
{code}
{code:java}
/**
 * In this repro, we simulate the scene in concurrent jobs using the same
 * rdd which is marked to do checkpoint:
 * Job one has already finished the spark job, and start the process of doCheckpoint;
 * Job two is submitted, and submitMissingTasks is called.
 * In submitMissingTasks, if taskSerialization is called before doCheckpoint is done,
 * while part calculates from stage.rdd.partitions is called after doCheckpoint is done,
 * we may get a ClassCastException when execute the task because of some rdd will do
 * Partition cast.
 *
 * With this test case, just want to indicate that we should do taskSerialization and
 * part calculate in submitMissingTasks with the same rdd checkpoint status.
 */
repro(""SPARK-23053: avoid ClassCastException in concurrent execution with checkpoint"") {
  // set checkpointDir.
  val checkpointDir = Utils.createTempDir()
  sc.setCheckpointDir(checkpointDir.toString)

  // Semaphores to control the process sequence for the two threads below.
  val doCheckpointStarted = new Semaphore(0)
  val taskBinaryBytesFinished = new Semaphore(0)
  val checkpointStateUpdated = new Semaphore(0)

  val rdd = new WrappedRDD(sc.makeRDD(1 to 100, 4))
  rdd.checkpoint()

  val checkpointRunnable = new Runnable {
    override def run() = {
      // Simulate what RDD.doCheckpoint() does here.
      rdd.doCheckpointCalled = true
      val checkpointData = rdd.checkpointData.get
      RDDCheckpointData.synchronized {
        if (checkpointData.cpState == CheckpointState.Initialized) {
          checkpointData.cpState = CheckpointState.CheckpointingInProgress
        }
      }

      val newRDD = checkpointData.doCheckpoint()

      // Release doCheckpointStarted after job triggered in checkpoint finished, so
      // that taskBinary serialization can start.
      doCheckpointStarted.release()
      // Wait until taskBinary serialization finished in submitMissingTasksThread.
      taskBinaryBytesFinished.acquire()

      // Update our state and truncate the RDD lineage.
      RDDCheckpointData.synchronized {
        checkpointData.cpRDD = Some(newRDD)
        checkpointData.cpState = CheckpointState.Checkpointed
        rdd.markCheckpointed()
      }
      checkpointStateUpdated.release()
    }
  }

  val submitMissingTasksRunnable = new Runnable {
    override def run() = {
      // Simulate the process of submitMissingTasks.
      // Wait until doCheckpoint job running finished, but checkpoint status not changed.
      doCheckpointStarted.acquire()

      val ser = SparkEnv.get.closureSerializer.newInstance()

      // Simulate task serialization while submitMissingTasks.
      // Task serialized with rdd checkpoint not finished.
      val cleanedFunc = sc.clean(Utils.getIteratorSize _)
      val func = (ctx: TaskContext, it: Iterator[Int]) => cleanedFunc(it)
      val taskBinaryBytes = JavaUtils.bufferToArray(
        ser.serialize((rdd, func): AnyRef))
      // Because partition calculate is in a synchronized block, so in the fixed code
      // partition is calculated here.
      val correctPart = rdd.partitions(0)

      // Release taskBinaryBytesFinished so changing checkpoint status to Checkpointed will
      // be done in checkpointThread.
      taskBinaryBytesFinished.release()
      // Wait until checkpoint status changed to Checkpointed in checkpointThread.
      checkpointStateUpdated.acquire()

      // Now we're done simulating the interleaving that might happen within the scheduler,
      // we'll check to make sure the final state is OK by simulating a couple steps that
      // normally happen on the executor.
      // Part calculated with rdd checkpoint already finished.
      val errPart = rdd.partitions(0)

      // TaskBinary will be deserialized when run task in executor.
      val (taskRdd, taskFunc) = ser.deserialize[(RDD[Int], (TaskContext, Iterator[Int]) => Unit)](
        ByteBuffer.wrap(taskBinaryBytes), Thread.currentThread.getContextClassLoader)

      val taskContext = mock(classOf[TaskContext])
      doNothing().when(taskContext).killTaskIfInterrupted()

      // Make sure our test case is setup correctly -- we expect a ClassCastException here
      // if we use the rdd.partitions after checkpointing was done, but our binary bytes is
      // from before it finished.
      intercept[ClassCastException] {
        // Triggered when runTask in executor.
        taskRdd.iterator(errPart, taskContext)
      }

      // Execute successfully with correctPart.
      taskRdd.iterator(correctPart, taskContext)
    }
  }

  try {
    new Thread(checkpointRunnable).start()
    val submitMissingTasksThread = new Thread(submitMissingTasksRunnable)
    submitMissingTasksThread.start()
    submitMissingTasksThread.join()
  } finally {
    Utils.deleteRecursively(checkpointDir)
  }
}
{code}
 ;;;","13/Feb/18 16:03;irashid;Fixed by https://github.com/apache/spark/pull/20244

I set the fix version to 2.3.1, because we're in the middle of voting for RC3.  If we cut another RC this would actually be fixed in 2.3.0.;;;","18/Feb/18 15:21;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/20635;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
job description in Spark UI is broken ,SPARK-23051,13130381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smurakozi,zsxwing,zsxwing,12/Jan/18 01:16,20/Feb/18 19:58,13/Jul/23 08:45,14/Jan/18 14:33,2.3.0,,,,,,,,,2.3.0,,,,,Web UI,,,,0,regression,,,"In previous versions, Spark UI will use the stage description if the job description is not set. But right now it’s just empty.

Reproducer: Just run the following codes in spark shell and check the UI:
{code}
val q = spark.readStream.format(""rate"").load().writeStream.format(""console"").start()
Thread.sleep(2000)
q.stop()
{code}",,ajbozarth,apachespark,cloud_fan,sameerag,smurakozi,suchithjn22,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23470,,,,,,,,,,,,"12/Jan/18 16:42;smurakozi;Spark-23051-after.png;https://issues.apache.org/jira/secure/attachment/12905888/Spark-23051-after.png","12/Jan/18 16:42;smurakozi;Spark-23051-before.png;https://issues.apache.org/jira/secure/attachment/12905887/Spark-23051-before.png","12/Jan/18 16:42;smurakozi;in-2.2.png;https://issues.apache.org/jira/secure/attachment/12905889/in-2.2.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 14 14:33:00 UTC 2018,,,,,,,,,,"0|i3otv3:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"12/Jan/18 01:18;zsxwing;I marked this is a blocker since it's a regression.;;;","12/Jan/18 01:22;cloud_fan;cc [~vanzin];;;","12/Jan/18 16:14;apachespark;User 'smurakozi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20251;;;","12/Jan/18 16:42;smurakozi;Screenshots showing the behavior in 2.2 and in 2.3 before and after the fix;;;","14/Jan/18 14:33;srowen;Issue resolved by pull request 20251
[https://github.com/apache/spark/pull/20251];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`spark.sql.files.ignoreCorruptFiles` should work for ORC files,SPARK-23049,13130344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/Jan/18 22:46,15/Jan/18 04:59,13/Jul/23 08:45,15/Jan/18 04:59,2.0.2,2.1.2,2.2.1,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"When `spark.sql.files.ignoreCorruptFiles=true`, we should ignore corrupted ORC files.",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 15 04:59:07 UTC 2018,,,,,,,,,,"0|i3otmv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 22:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20240;;;","15/Jan/18 04:59;cloud_fan;Issue resolved by pull request 20240
[https://github.com/apache/spark/pull/20240];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
merge script has bug when assigning jiras to non-contributors,SPARK-23044,13130216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,irashid,irashid,irashid,11/Jan/18 15:30,17/Jan/18 00:25,13/Jul/23 08:45,17/Jan/18 00:25,2.3.0,,,,,,,,,2.4.0,,,,,Project Infra,,,,0,,,,"as reported by [~jerryshao]:

bq. Hi Imran Rashid, looks like the changes will throw an exception when the assignee is not yet a contributor. Please see the stack.

{noformat}
Traceback (most recent call last):
  File ""./dev/merge_spark_pr.py"", line 501, in <module>
    main()
  File ""./dev/merge_spark_pr.py"", line 487, in main
    resolve_jira_issues(title, merged_refs, jira_comment)
  File ""./dev/merge_spark_pr.py"", line 327, in resolve_jira_issues
    resolve_jira_issue(merge_branches, comment, jira_id)
  File ""./dev/merge_spark_pr.py"", line 245, in resolve_jira_issue
    cur_assignee = choose_jira_assignee(issue, asf_jira)
  File ""./dev/merge_spark_pr.py"", line 317, in choose_jira_assignee
    asf_jira.assign_issue(issue.key, assignee.key)
  File ""/Library/Python/2.7/site-packages/jira/client.py"", line 108, in wrapper
    result = func(*arg_list, **kwargs)

{noformat}",,apachespark,irashid,Simon_poortman@icloud.com,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22921,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 17 00:25:36 UTC 2018,,,,,,,,,,"0|i3osuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 15:47;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/20236;;;","17/Jan/18 00:25;vanzin;Issue resolved by pull request 20236
[https://github.com/apache/spark/pull/20236];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use OneHotEncoderModel to encode labels in MultilayerPerceptronClassifier,SPARK-23042,13130161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,11/Jan/18 10:44,17/Aug/18 18:41,13/Jul/23 08:45,17/Aug/18 18:41,2.3.0,,,,,,,,,2.4.0,,,,,ML,,,,0,,,,"In MultilayerPerceptronClassifier, we use RDD operation to encode labels for now. I think we should use ML's OneHotEncoderEstimator/Model to do the encoding.",,apachespark,dbtsai,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 17 18:41:14 UTC 2018,,,,,,,,,,"0|i3osi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 10:49;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20232;;;","17/Aug/18 18:41;dbtsai;Issue resolved by pull request 20232
[https://github.com/apache/spark/pull/20232];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update docker/spark-test (JDK/OS),SPARK-23038,13130086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,11/Jan/18 07:13,14/Jan/18 07:32,13/Jul/23 08:45,14/Jan/18 07:27,2.2.1,,,,,,,,,2.2.2,2.3.0,2.4.0,,,Tests,,,,0,,,,"This issue aims to update the followings in `docker/spark-test`.
- JDK7 -> JDK8: Spark 2.2+ supports JDK8 only.
- Ubuntu 12.04.5 LTS(precise) -> Ubuntu 16.04.3 LTS(xeniel): The end of life of `precise` was April 28, 2017.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 11 07:25:04 UTC 2018,,,,,,,,,,"0|i3os1j:",9223372036854775807,,,,,,,,,,,,,2.2.2,2.3.0,,,,,,,,,"11/Jan/18 07:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20230;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RFormula should not use deprecated OneHotEncoder and should include VectorSizeHint in pipeline,SPARK-23037,13130074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,11/Jan/18 06:07,16/Jan/18 20:58,13/Jul/23 08:45,16/Jan/18 20:58,2.3.0,,,,,,,,,2.3.0,,,,,ML,,,,0,,,,,,apachespark,bago.amirbekian,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21926,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 11 06:08:03 UTC 2018,,,,,,,,,,"0|i3orz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 06:08;apachespark;User 'MrBago' has created a pull request for this issue:
https://github.com/apache/spark/pull/20229;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix improper information of TempTableAlreadyExistsException,SPARK-23035,13130034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xubo245,xubo245,xubo245,11/Jan/18 01:36,16/Jan/18 01:18,13/Jul/23 08:45,15/Jan/18 15:15,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,," 

Problem: it throw TempTableAlreadyExistsException and output ""Temporary table '$table' already exists"" when we create temp view by using org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager#create, it's improper.
{code:java}
 /**
   * Creates a global temp view, or issue an exception if the view already exists and
   * `overrideIfExists` is false.
   */
  def create(
      name: String,
      viewDefinition: LogicalPlan,
      overrideIfExists: Boolean): Unit = synchronized {
    if (!overrideIfExists && viewDefinitions.contains(name)) {
      throw new TempTableAlreadyExistsException(name)
    }
    viewDefinitions.put(name, viewDefinition)
  }
{code}

No need to fix: 
 warning: TEMPORARY TABLE ... USING ... is deprecated and use TempViewAlreadyExistsException when create temp view
There are warning when run test: test(""rename temporary view - destination table with database name"")

02:11:38.136 WARN org.apache.spark.sql.execution.SparkSqlAstBuilder: CREATE TEMPORARY TABLE ... USING ... is deprecated, please use CREATE TEMPORARY VIEW ... USING ... instead
 other test cases also have this warning",,apachespark,xubo245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 11 02:06:04 UTC 2018,,,,,,,,,,"0|i3orrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 02:06;apachespark;User 'xubo245' has created a pull request for this issue:
https://github.com/apache/spark/pull/20227;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump master branch version to 2.4.0-SNAPSHOT,SPARK-23028,13129877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,10/Jan/18 15:01,12/Jan/18 16:38,13/Jul/23 08:45,12/Jan/18 16:38,2.4.0,,,,,,,,,2.4.0,,,,,Build,,,,0,,,,,,apachespark,cloud_fan,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 12 16:38:49 UTC 2018,,,,,,,,,,"0|i3oqtj:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"10/Jan/18 15:04;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20222;;;","12/Jan/18 16:38;cloud_fan;Issue resolved by pull request 20222
[https://github.com/apache/spark/pull/20222];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSet with scala.Null causes Exception,SPARK-23025,13129790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,wabu,wabu,10/Jan/18 09:07,12/Jan/18 10:06,13/Jul/23 08:45,12/Jan/18 10:06,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"When creating a DataSet over a case class containing a field of type scala.Null, there is an exception thrown. As far as I can see, spark sql would support a Schema(NullType, true), but it fails inside the {{schemaFor}} function with a {{MatchError}}.
I would expect spark to return a DataSet with a NullType for that field. 


h5. Minimal Exampe

{code}
case class Foo(foo: Int, bar: Null)
val ds = Seq(Foo(42, null)).toDS()
{code}

h5. Exception
{code}
scala.MatchError: scala.Null (of class scala.reflect.internal.Types$ClassNoArgsTypeRef)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:713)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:704)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:56)
  at org.apache.spark.sql.catalyst.ScalaReflection$class.cleanUpReflectionObjects(ScalaReflection.scala:809)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:39)
  at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:703)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$deserializerFor$1$$anonfun$9.apply(ScalaReflection.scala:391)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$deserializerFor$1$$anonfun$9.apply(ScalaReflection.scala:390)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$deserializerFor$1.apply(ScalaReflection.scala:390)
  at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$deserializerFor$1.apply(ScalaReflection.scala:148)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:56)
  at org.apache.spark.sql.catalyst.ScalaReflection$class.cleanUpReflectionObjects(ScalaReflection.scala:809)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:39)
  at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$deserializerFor(ScalaReflection.scala:148)
  at org.apache.spark.sql.catalyst.ScalaReflection$.deserializerFor(ScalaReflection.scala:136)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:72)
  at org.apache.spark.sql.Encoders$.product(Encoders.scala:275)
  at org.apache.spark.sql.LowPrioritySQLImplicits$class.newProductEncoder(SQLImplicits.scala:233)
  at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:33)
  ... 42 elided 
{code}

h5. Background Info

To handle our data in a type-safe fashion, we have generated AVRO schemas and corresponding scala case classes for our domain data. As some fields only contain null values, this results in fields with scala.Null as a type. Moving our pipeline to DataSets/structured streaming, case classes with Null types begin to give problems, even trough NullType is known to spark SQL.",,apachespark,mgaido,wabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 12:12:04 UTC 2018,,,,,,,,,,"0|i3oqa7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/18 12:12;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20219;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results of printing Array/Map/Struct in showString,SPARK-23023,13129737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,10/Jan/18 03:05,24/Oct/18 18:48,13/Jul/23 08:45,15/Jan/18 08:29,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The current `Datset.showString` prints rows thru `RowEncoder` deserializers like;
{code}
scala> Seq(Seq(Seq(1, 2), Seq(3), Seq(4, 5, 6))).toDF(""a"").show(false)
+------------------------------------------------------------+
|a                                                           |
+------------------------------------------------------------+
|[WrappedArray(1, 2), WrappedArray(3), WrappedArray(4, 5, 6)]|
+------------------------------------------------------------+
{code}
This result is incorrect because the correct one is;
{code}
scala> Seq(Seq(Seq(1, 2), Seq(3), Seq(4, 5, 6))).toDF(""a"").show(false)
+------------------------+
|a                       |
+------------------------+
|[[1, 2], [3], [4, 5, 6]]|
+------------------------+
{code}",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25824,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 15 08:29:58 UTC 2018,,,,,,,,,,"0|i3opyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/18 03:13;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20214;;;","15/Jan/18 08:29;cloud_fan;Issue resolved by pull request 20214
[https://github.com/apache/spark/pull/20214];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnalysisBarrier should not cut off the explain output for Parsed Logical Plan,SPARK-23021,13129724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,rednaxelafx,rednaxelafx,10/Jan/18 01:24,14/Jan/18 14:34,13/Jul/23 08:45,14/Jan/18 14:33,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"In PR#20094 as a follow up to SPARK-20392, there were some fixes to the handling of {{AnalysisBarrier}}, but there seem to be more cases that need to be fixed.

One such case is that right now the Parsed Logical Plan in explain output would be cutoff by {{AnalysisBarrier}}, e.g.
{code:none}
scala> val df1 = spark.range(1).select('id as 'x, 'id + 1 as 'y).repartition(1).select('x === 'y)
df1: org.apache.spark.sql.DataFrame = [(x = y): boolean]

scala> df1.explain(true)
== Parsed Logical Plan ==
'Project [('x = 'y) AS (x = y)#22]
+- AnalysisBarrier Repartition 1, true

== Analyzed Logical Plan ==
(x = y): boolean
Project [(x#16L = y#17L) AS (x = y)#22]
+- Repartition 1, true
   +- Project [id#13L AS x#16L, (id#13L + cast(1 as bigint)) AS y#17L]
      +- Range (0, 1, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [(x#16L = y#17L) AS (x = y)#22]
+- Repartition 1, true
   +- Project [id#13L AS x#16L, (id#13L + 1) AS y#17L]
      +- Range (0, 1, step=1, splits=Some(8))

== Physical Plan ==
*Project [(x#16L = y#17L) AS (x = y)#22]
+- Exchange RoundRobinPartitioning(1)
   +- *Project [id#13L AS x#16L, (id#13L + 1) AS y#17L]
      +- *Range (0, 1, step=1, splits=8)
{code}
",,apachespark,cloud_fan,maropu,rednaxelafx,robert3005,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 12 18:08:42 UTC 2018,,,,,,,,,,"0|i3opvj:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"12/Jan/18 06:02;maropu;hi, kris, you're working on it?
I think we just forget to override innerChildren in AnalysisBarrier.
{code}

>> Before
scala> Seq((1, 1)).toDF(""a"", ""b"").groupBy(""a"").count().sample(0.1).explain(true)
== Parsed Logical Plan ==
Sample 0.0, 0.1, false, -7661439431999668039
+- AnalysisBarrier Aggregate [a#5], [a#5, count(1) AS count#14L]

>> After
scala> Seq((1, 1)).toDF(""a"", ""b"").groupBy(""a"").count().sample(0.1).explain(true)
== Parsed Logical Plan ==
Sample 0.0, 0.1, false, -5086223488015741426
+- AnalysisBarrier
      +- Aggregate [a#5], [a#5, count(1) AS count#14L]
         +- Project [_1#2 AS a#5, _2#3 AS b#6]
            +- LocalRelation [_1#2, _2#3]
{code};;;","12/Jan/18 06:29;rednaxelafx;Hi [~maropu]-san,

Thanks for looking at it! No, I only noticed this behavior while working on something else. I wasn't planning on working on this ticket, unless no one takes it and I get too curious ;-)

Would you like to submit that patch as a PR?

Thanks,
Kris;;;","12/Jan/18 06:37;maropu;yea, sure, I have bandwidth to do :))
Is the fix above is correct? cc: [~smilegator] [~viirya]
Or, we need to totally cut off AnalysisBarrier in explain results?;;;","12/Jan/18 07:58;rednaxelafx;Hi [~maropu]-san,

Thanks! I'm not familiar with this part of the code, but IIUC there's two points to this:
1. The explain output shouldn't be changed because of the newly added {{AnalysisBarrier}}. The current behavior (with the cutoff) should be considered a regression, although it's not a major behavioral one.
2. The reason why {{AnalysisBarrier}} didn't override {{innerChildren}} was by design: it wanted to cut off the recursion to help avoid stack overflows when the analyzer needs to walk the plan tree.

[~cloud_fan] may have recently fixed a similar issue where the ""analyzed logical plan"" was cut off by {{AnalysisBarrier}} in the same way. Is that the case, [~cloud_fan]?;;;","12/Jan/18 10:40;cloud_fan;`AnalysisBarrier` didn't override `children` by design. I think `innerChildren` is fine, as it's for explain/UI only.;;;","12/Jan/18 10:50;maropu;ok, I'll make a pr that way;;;","12/Jan/18 12:58;viirya;To override {{innerChildren}} sounds good to me. [~maropu] Please ping me if you submit the PR. Thanks.;;;","12/Jan/18 14:37;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20247;;;","12/Jan/18 18:08;rednaxelafx;Thank you very much for explaining the differences between {{children}} and {{innerChildren}}, [~viirya] and [~cloud_fan]! TIL :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-enable Flaky Test: org.apache.spark.launcher.SparkLauncherSuite.testInProcessLauncher,SPARK-23020,13129723,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,sameerag,sameerag,10/Jan/18 01:06,04/May/18 16:54,13/Jul/23 08:45,02/Feb/18 04:02,2.4.0,,,,,,,,,2.3.1,2.4.0,,,,Tests,,,,1,,,,https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-2.3-test-maven-hadoop-2.7/42/testReport/junit/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/,,apachespark,cloud_fan,dongjoon,mgaido,sameerag,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23322,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 04 16:54:57 UTC 2018,,,,,,,,,,"0|i3opvb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,"10/Jan/18 01:06;sameerag;cc [~vanzin];;;","10/Jan/18 18:12;vanzin;I think I found the race in the code, now need to figure out how to fix it... :-/;;;","10/Jan/18 20:56;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20223;;;","16/Jan/18 06:42;sameerag;Issue resolved by pull request 20223
[https://github.com/apache/spark/pull/20223];;;","17/Jan/18 06:24;sameerag;I had to revert this patch as it broke [{{YarnClusterSuite.timeout to get SparkContext in cluster mode triggers failure}}| https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/94/testReport/org.apache.spark.deploy.yarn/YarnClusterSuite/timeout_to_get_SparkContext_in_cluster_mode_triggers_failure/history/] 

[{{SparkLauncherSuite.testInProcessLauncher}}|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/90/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/] seems to be still flaky.;;;","17/Jan/18 08:20;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20291;;;","17/Jan/18 17:30;vanzin;Bummer. I'll try to take another look later today.;;;","17/Jan/18 19:45;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20297;;;","22/Jan/18 06:51;cloud_fan;Issue resolved by pull request 20297
[https://github.com/apache/spark/pull/20297];;;","24/Jan/18 04:15;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20376;;;","24/Jan/18 18:17;sameerag;FYI The {{SparkLauncherSuite}} test is still failing occasionally (a lot less common though): https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.6/142/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/;;;","24/Jan/18 18:21;vanzin;Argh. Feel free to disable it in branch-2.3; please leave it on on master so we can get more info while I look at it.;;;","24/Jan/18 22:15;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20388;;;","29/Jan/18 07:05;sameerag;I'm sorry but the flakiness in the test still refuses to go away: [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/154/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/.|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.7/154/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/]

 

Per Marcelo's suggestion, I'm going to (only) disable this test in 2.3. The master builds are failing similarly ([https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/4426/testReport/junit/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/)] so I hope it'll not hinder any investigation.;;;","29/Jan/18 17:41;vanzin;:-/

It's getting harder and harder to reproduce these races locally... this one may take a while.;;;","31/Jan/18 22:09;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20462;;;","02/Feb/18 04:02;cloud_fan;Issue resolved by pull request 20462
[https://github.com/apache/spark/pull/20462];;;","06/Mar/18 00:27;vanzin;Things look pretty stable on master, so I'll post a backport for 2.3.1 so we get the fixes in the next maintenance release.
https://amplab.cs.berkeley.edu/jenkins/user/vanzin/my-views/view/Spark/job/spark-master-test-maven-hadoop-2.7/4571/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/;;;","06/Mar/18 00:34;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20743;;;","04/May/18 16:22;dongjoon;Hi, All.

This seems to fail again in branch 2.3. Can we disable this in branch-2.3 for Apache Spark 2.3.1 at least?
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.6/lastCompletedBuild/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/;;;","04/May/18 16:54;vanzin;If that still fails somewhere it means there is still a bug somewhere. I don't think disabling the test is the right thing unless it's actually common enough that it's causing problems. Lots of our tests are flaky.

That's like the only failure recently, BTW.
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.3-test-maven-hadoop-2.6/lastCompletedBuild/testReport/org.apache.spark.launcher/SparkLauncherSuite/testInProcessLauncher/history/;;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky Test: org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD,SPARK-23019,13129721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Gengliang.Wang,sameerag,sameerag,10/Jan/18 00:53,10/Jan/18 17:52,13/Jul/23 08:45,10/Jan/18 17:52,2.3.0,,,,,,,,,2.3.0,,,,,Java API,Tests,,,0,,,,"{{org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD}} has been failing due to multiple spark contexts: https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-2.3-test-maven-hadoop-2.6/

{code}
Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.<init>(SparkContext.scala:116)
org.apache.spark.launcher.SparkLauncherSuite$InProcessTestApp.main(SparkLauncherSuite.java:182)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.launcher.InProcessAppHandle.lambda$start$0(InProcessAppHandle.java:63)
java.lang.Thread.run(Thread.java:748)
{code}",,apachespark,mgaido,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 13:26:05 UTC 2018,,,,,,,,,,"0|i3opuv:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"10/Jan/18 13:26;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20221;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark creatDataFrame causes Pandas warning of assignment to a copy of a reference,SPARK-23018,13129710,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,09/Jan/18 23:48,10/Jan/18 05:00,13/Jul/23 08:45,10/Jan/18 05:00,2.3.0,,,,,,,,,2.3.0,,,,,PySpark,,,,0,,,,"When calling {{SparkSession.createDataFrame}} with a Pandas DataFrame as input (with Arrow disabled) a Pandas warning is raised when the DataFrame is a slice:

{noformat}
In [1]: import numpy as np
   ...: import pandas as pd
   ...: pdf = pd.DataFrame(np.random.rand(100, 2))
   ...: 

In [2]: df = spark.createDataFrame(pdf[:10])
/home/bryan/git/spark/python/pyspark/sql/session.py:476: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  pdf[column] = s
{noformat}

This doesn't seem to cause a bug in this case, but might for others.  It could be avoided by only assigning the series if it was a modified timestamp field.",,apachespark,bryanc,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 05:00:52 UTC 2018,,,,,,,,,,"0|i3opsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/18 23:48;bryanc;I can submit a PR;;;","10/Jan/18 00:03;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/20213;;;","10/Jan/18 05:00;ueshin;Issue resolved by pull request 20213
[https://github.com/apache/spark/pull/20213];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark should not assume Pandas cols are a basestring type,SPARK-23009,13129638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,09/Jan/18 19:19,12/Dec/22 18:10,13/Jul/23 08:45,10/Jan/18 05:57,2.3.0,,,,,,,,,2.3.0,,,,,PySpark,,,,0,,,,"When calling {{SparkSession.createDataFrame}} using a Pandas DataFrame as input, Spark assumes that the columns will either be a {{str}} type or {{unicode}} type.  They can actually be any type that a dict can key off of.  If they are not a {{basestr}} type, then a confusing AttributeError is thrown:

{noformat}
In [16]: pdf = pd.DataFrame(np.random.rand(4, 2))

In [17]: pdf
Out[17]: 
          0         1
0  0.145171  0.482940
1  0.151336  0.299861
2  0.220338  0.830133
3  0.001659  0.513787

In [18]: pdf.columns
Out[18]: RangeIndex(start=0, stop=2, step=1)

In [19]: df = spark.createDataFrame(pdf)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-18-11bcb07e0e39> in <module>()
----> 1 df = spark.createDataFrame(pdf)

/home/bryan/git/spark/python/pyspark/sql/session.pyc in createDataFrame(self, data, schema, samplingRatio, verifySchema)
    646             # If no schema supplied by user then get the names of columns only
    647             if schema is None:
--> 648                 schema = [x.encode('utf-8') if not isinstance(x, str) else x for x in data.columns]
    649 
    650             if self.conf.get(""spark.sql.execution.arrow.enabled"", ""false"").lower() == ""true"" \

AttributeError: 'int' object has no attribute 'encode'
{noformat}",,apachespark,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 05:57:14 UTC 2018,,,,,,,,,,"0|i3opc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/18 19:20;bryanc;I can put in a fix for this;;;","09/Jan/18 20:03;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/20210;;;","10/Jan/18 05:57;gurwls223;Fixed in https://github.com/apache/spark/pull/20210;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add schema evolution test suite for file-based data sources,SPARK-23007,13129604,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,09/Jan/18 18:30,20/May/21 18:36,13/Jul/23 08:45,12/Jul/18 21:10,2.2.1,,,,,,,,,2.4.0,,,,,SQL,Tests,,,0,,,,"A schema can evolve in several ways and the followings are already supported in file-based data sources.

   1. Add a column
   2. Remove a column
   3. Change a column position
   4. Change a column type

This issue aims to guarantee users a backward-compatible schema evolution coverage on file-based data sources and to prevent future regressions by *adding schema evolution test suites explicitly*.

Here, we consider safe evolution without data loss. For example, data type evolution should be from small types to larger types like `int`-to-`long`, not vice versa.

As of today, in the master branch, file-based data sources have schema evolution coverages like the followings.

|| File Format || Coverage     || Note                                                   ||
| TEXT         | N/A          | Schema consists of a single string column.             |
| CSV          | 1, 2, 4      |                                                        |
| JSON         | 1, 2, 3, 4   |                                                        |
| ORC          | 1, 2, 3, 4   | Native vectorized ORC reader has the widest coverage.  |
| PARQUET      | 1, 2, 3      |                                                        |",,apachespark,dongjoon,dougb,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,SPARK-35461,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 18:36:04 UTC 2018,,,,,,,,,,"0|i3op4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/18 18:32;dongjoon;I hope this should be in Apache Spark 2.3.0 since this is only test suites.;;;","09/Jan/18 18:36;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20208;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Structured Streaming raise ""llegalStateException: Cannot remove after already committed or aborted""",SPARK-23004,13129503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,secfree,secfree,09/Jan/18 11:31,13/Jul/18 05:30,13/Jul/23 08:45,23/Apr/18 20:21,2.1.0,2.1.1,2.1.2,2.2.0,2.2.1,2.3.0,,,,2.3.1,2.4.0,,,,Structured Streaming,,,,0,,,,"A structured streaming query with a streaming aggregation can throw the following error in rare cases. 
{code:java}
java.lang.IllegalStateException: Cannot remove after already committed or aborted at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$verify(HDFSBackedStateStoreProvider.scala:659 ) at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.remove(HDFSBackedStateStoreProvider.scala:143) at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3$$anon$1.hasNext(statefulOperators.scala:233) at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:191) at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:80) at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:111) at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:103) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 18-01-05 13:29:57 WARN TaskSetManager:66 Lost task 68.0 in stage 1933.0 (TID 196171, localhost, executor driver): java.lang.IllegalStateException: Cannot remove after already committed or aborted at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$verify(HDFSBackedStateStoreProvider.scala:659){code}
 

This can happen when the following conditions are accidentally hit. 
 # Streaming aggregation with aggregation function that is a subset of {{[TypedImperativeAggregation|https://github.com/apache/spark/blob/76b8b840ddc951ee6203f9cccd2c2b9671c1b5e8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala#L473]}} (for example, {{collect_set}}, {{collect_list}}, {{percentile}}, etc.). 
 # Query running in {{update}} mode
 # After the shuffle, a partition has exactly 128 records. 

This happens because of the following. 
 # The {{StateStoreSaveExec}} used in streaming aggregations has the [following logic|https://github.com/apache/spark/blob/76b8b840ddc951ee6203f9cccd2c2b9671c1b5e8/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala#L359] when used in {{update}} mode.
 ## There is an iterator that reads data from its parent iterator and updates the StateStore.
 ## When the parent iterator is fully consumed (i.e. {{baseIterator.hasNext}} returns false) then all state changes are committed by calling {{StateStore.commit}}. 
 ## The implementation of {{StateStore.commit()}} in {{HDFSBackedStateStore}} does not allow itself to be called twice. However, the logic is such that, if {{hasNext}} is called multiple times after {{baseIterator.hasNext}} has returned false then each time it will call {{StateStore.commit}}.
 ## For most aggregation functions, this is okay because {{hasNext}} is only called once. But thats not the case with {{ImperativeTypedAggregates}}.
 # {{ImperativeTypedAggregates}} are executed using {{ObjectHashAggregateExec}} which will try to use two kinds of hashmaps for aggregations. 
 ## It will first try to use an unsorted hashmap. If the size of the hashmap increases beyond a certain threshold (default 128), then it will switch to using a sorted hashmap. 
 ## The [switching logic|https://github.com/apache/spark/blob/76b8b840ddc951ee6203f9cccd2c2b9671c1b5e8/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala] in {{ObjectAggregationIterator}} (used by {{ObjectHashAggregateExec}})  is such that when the number of records matches the threshold (i.e. 128), it will end up calling the {{iterator.hasNext}} twice.

When combined with the above two conditions are combined, it leads to the above error. This latent bug has existed since Spark 2.1 when {{ObjectHashAggregateExec}} was introduced in Spark.

 ",Run on yarn or local both raise the exception.,apachespark,mgaido,secfree,tdas,xiayunsun,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 23 20:21:12 UTC 2018,,,,,,,,,,"0|i3ooif:",9223372036854775807,,,,,,,,,,,,,2.3.1,2.4.0,,,,,,,,,"09/Jan/18 16:56;srowen;There's no info here about how you get into this situation. Do you have a reproduction or any context?;;;","23/Apr/18 00:42;tdas;[~joshrosen] hit the issue as well, and thanks to him I could reproduce it. I am updating the description of the Jira to include more details. This is a long-standing crazy issue!;;;","23/Apr/18 00:46;secfree;secfree 赞了您的邮件。
Spark by Readdle
;;;","23/Apr/18 01:30;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/21124;;;","23/Apr/18 20:21;tdas;Issue resolved by pull request 21124
[https://github.com/apache/spark/pull/21124];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when running desc database,SPARK-23001,13129471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,guanzhe,guanzhe,09/Jan/18 09:15,11/Jan/18 10:19,13/Jul/23 08:45,11/Jan/18 10:19,2.2.0,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,0,,,,"I have a database named 'gz_test'. When I run ""desc database gz_test"", spark throws a NullPointerException.
{code}
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:39)
	at org.apache.spark.sql.execution.LocalTableScanExec$$anonfun$unsafeRows$1.apply(LocalTableScanExec.scala:41)
	at org.apache.spark.sql.execution.LocalTableScanExec$$anonfun$unsafeRows$1.apply(LocalTableScanExec.scala:41)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.LocalTableScanExec.unsafeRows$lzycompute(LocalTableScanExec.scala:41)
	at org.apache.spark.sql.execution.LocalTableScanExec.unsafeRows(LocalTableScanExec.scala:36)
	at org.apache.spark.sql.execution.LocalTableScanExec.executeCollect(LocalTableScanExec.scala:67)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:298)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:340)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:248)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:39)
	at org.apache.spark.sql.execution.LocalTableScanExec$$anonfun$unsafeRows$1.apply(LocalTableScanExec.scala:41)
	at org.apache.spark.sql.execution.LocalTableScanExec$$anonfun$unsafeRows$1.apply(LocalTableScanExec.scala:41)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.LocalTableScanExec.unsafeRows$lzycompute(LocalTableScanExec.scala:41)
	at org.apache.spark.sql.execution.LocalTableScanExec.unsafeRows(LocalTableScanExec.scala:36)
	at org.apache.spark.sql.execution.LocalTableScanExec.executeCollect(LocalTableScanExec.scala:67)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:298)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:340)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:248)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}

I noticed that the run() method in *DescribeDatabaseCommand* will return a three-row result, which include ""Database Name"", ""Description"" and ""Location"". In my case, the ""Description"" of this database is null. And I think that may cause this problem.  ",,apachespark,cloud_fan,guanzhe,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 11 10:19:14 UTC 2018,,,,,,,,,,"0|i3oobb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/18 11:01;guanzhe;The exception disappeared after I made the following change in *DescribeDatabaseCommand*:
{code: title=ddl.scala | borderStyle=solid}
  // line 165-168
  override val output: Seq[Attribute] = {
    AttributeReference(""database_description_item"", StringType, nullable = true)() ::
      AttributeReference(""database_description_value"", StringType, nullable = true)() :: Nil
  }
{code}

If there is any other influence when I set nullable=true?;;;","09/Jan/18 16:58;srowen;[~smilegator] I think you created this part. Seems reasonable?;;;","10/Jan/18 03:19;smilegator;Let me do a quick fix.;;;","10/Jan/18 03:51;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20215;;;","11/Jan/18 10:19;cloud_fan;Issue resolved by pull request 20215
[https://github.com/apache/spark/pull/20215];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test suite DataSourceWithHiveMetastoreCatalogSuite in Spark 2.3,SPARK-23000,13129443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,09/Jan/18 04:47,22/Jan/18 05:03,13/Jul/23 08:45,22/Jan/18 05:03,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-2.3-test-sbt-hadoop-2.6/

The test suite DataSourceWithHiveMetastoreCatalogSuite of Branch 2.3 always failed in hadoop 2.6 ",,apachespark,cloud_fan,sameerag,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 22 05:03:17 UTC 2018,,,,,,,,,,"0|i3oo53:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"09/Jan/18 04:51;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20196;;;","09/Jan/18 08:33;cloud_fan;Issue resolved by pull request 20196
[https://github.com/apache/spark/pull/20196];;;","09/Jan/18 16:19;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20207;;;","10/Jan/18 11:31;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20218;;;","11/Jan/18 08:07;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20231;;;","16/Jan/18 00:54;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/20273;;;","19/Jan/18 08:06;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20328;;;","22/Jan/18 05:03;sameerag;Issue resolved by [https://github.com/apache/spark/pull/20328];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Value for SPARK_MOUNTED_CLASSPATH in executor pods is not set,SPARK-22998,13129417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyinan926,liyinan926,liyinan926,09/Jan/18 00:51,17/May/20 18:26,13/Jul/23 08:45,09/Jan/18 09:39,2.3.0,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,0,,,,"The environment variable {{SPARK_MOUNTED_CLASSPATH}} is referenced by the executor's Dockerfile, but is not set by the k8s scheduler backend.",,apachespark,liyinan926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24599,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 00:57:05 UTC 2018,,,,,,,,,,"0|i3onzb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"09/Jan/18 00:57;apachespark;User 'liyinan926' has created a pull request for this issue:
https://github.com/apache/spark/pull/20193;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove assumption of cluster domain in Kubernetes mode,SPARK-22992,13129305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,foxish,foxish,foxish,08/Jan/18 18:13,17/May/20 18:23,13/Jul/23 08:45,08/Jan/18 21:02,2.3.0,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,0,,,,"We assume in some places that the cluster domain is ""cluster.local"", which will lead to situations where people want to do something similar to https://github.com/apache-spark-on-k8s/spark/pull/593

Since we already assume kubernetes.default.svc is resolvable, we can do the same for the driver as well.",,apachespark,foxish,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 08 21:02:20 UTC 2018,,,,,,,,,,"0|i3onan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 18:19;apachespark;User 'foxish' has created a pull request for this issue:
https://github.com/apache/spark/pull/20187;;;","08/Jan/18 21:02;vanzin;Issue resolved by pull request 20187
[https://github.com/apache/spark/pull/20187];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix method isFairScheduler in JobsTab and StagesTab,SPARK-22990,13129248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,08/Jan/18 13:55,09/Jan/18 02:45,13/Jul/23 08:45,09/Jan/18 02:45,2.2.1,,,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,"In current implementation, the function `isFairScheduler` is always false, since it is comparing String with `SchedulingMode`",,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 02:45:52 UTC 2018,,,,,,,,,,"0|i3omxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 13:57;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20186;;;","09/Jan/18 02:45;cloud_fan;Issue resolved by pull request 20186
[https://github.com/apache/spark/pull/20186];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid instantiating multiple instances of broadcast variables ,SPARK-22986,13129146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ho3rexqj,ho3rexqj,ho3rexqj,08/Jan/18 03:09,12/Jan/18 07:46,13/Jul/23 08:45,12/Jan/18 07:46,2.2.1,,,,,,,,,2.3.0,,,,,Spark Core,,,,0,,,,"When resources happen to be constrained on an executor the first time a broadcast variable is instantiated it is persisted to disk by the BlockManager.  Consequently, every subsequent call to TorrentBroadcast::readBroadcastBlock from other instances of that broadcast variable spawns another instance of the underlying value.  That is, broadcast variables are spawned once per executor *unless* memory is constrained, in which case every instance of a broadcast variable is provided with a unique copy of the underlying value.

The fix I propose is to explicitly cache the underlying values using weak references (in a ReferenceMap) - note, however, that I couldn't find a clean approach to creating the cache container here.  I added that to BroadcastManager as a package-private field for want of a better solution, however if something more appropriate already exists in the project for that purpose please let me know.

The above issue was terminating our team's applications erratically - effectively, we were distributing roughly 1 GiB of data through a broadcast variable and under certain conditions memory was constrained the first time the broadcast variable was loaded on an executor.  As such, the executor attempted to spawn several additional copies of the broadcast variable (we were using 8 worker threads on the executor) which quickly led to the task failing as the result of an OOM exception.",,apachespark,ho3rexqj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 08 03:12:05 UTC 2018,,,,,,,,,,"0|i3ombr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 03:12;apachespark;User 'ho3rexqj' has created a pull request for this issue:
https://github.com/apache/spark/pull/20183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix argument escaping bug in from_utc_timestamp / to_utc_timestamp codegen,SPARK-22985,13129132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,08/Jan/18 00:17,08/Jan/18 19:53,13/Jul/23 08:45,08/Jan/18 03:42,2.0.0,2.1.0,2.2.0,2.3.0,,,,,,2.3.0,,,,,SQL,,,,0,,,,"The from_utc_timestamp and to_utc_timestamp expressions do not properly escape their timezone argument in codegen, leading to compilation errors instead of analysis errors.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 08 00:30:04 UTC 2018,,,,,,,,,,"0|i3om8n:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"08/Jan/18 00:30;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/20182;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect bitmap copying and offset shifting in GenerateUnsafeRowJoiner,SPARK-22984,13129129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,07/Jan/18 23:49,09/Jan/18 04:04,13/Jul/23 08:45,09/Jan/18 04:04,1.5.0,1.6.0,2.0.0,2.1.0,2.2.0,2.3.0,,,,2.2.2,2.3.0,,,,SQL,,,,0,correctness,,,"The following query returns an incorrect answer:

{code}
set spark.sql.autoBroadcastJoinThreshold=-1;

create table a as select * from values 1;
create table b as select * from values 2;

SELECT
t3.col1,
t1.col1
FROM a t1
CROSS JOIN b t2
CROSS JOIN b t3
{code}

This should return the row {{2, 1}} but instead it returns {{null, 1}}. If you permute the order of the columns in the select statement or the order of the joins then it returns a valid answer (i.e. one without incorrect NULLs).

This turns out to be due to two longstanding bugs in GenerateUnsafeRowJoiner, which I'll describe in more detail in my PR.",,apachespark,joshrosen,maropu,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 08 00:05:05 UTC 2018,,,,,,,,,,"0|i3om7z:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"08/Jan/18 00:05;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/20181;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't push filters beneath aggregates with empty grouping expressions,SPARK-22983,13129128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,07/Jan/18 23:41,08/Jan/18 08:06,13/Jul/23 08:45,08/Jan/18 08:06,2.1.0,2.2.0,2.3.0,,,,,,,2.2.2,2.3.0,,,,SQL,,,,0,correctness,,,"The following SQL query should return zero rows, but in Spark it actually returns one row:

{code}
SELECT 1 from (
  SELECT 1 AS z,
  MIN(a.x)
  FROM (select 1 as x) a
  WHERE false
) b
where b.z != b.z
{code}

The problem stems from the `PushDownPredicate` rule: when this rule encounters a filter on top of an Aggregate operator, e.g. `Filter(Agg(...))`, it removes the original filter and adds a new filter onto Aggregate's child, e.g. `Agg(Filter(...))`. This is often okay, but the case above is a counterexample: because there is no explicit `GROUP BY`, we are implicitly computing a global aggregate over the entire table so the original filter was not acting like a `HAVING` clause filtering the number of groups: if we push this filter then it fails to actually reduce the cardinality of the Aggregate output, leading to the wrong answer.

A simple fix is to never push down filters beneath aggregates when there are no grouping expressions.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 07 23:46:05 UTC 2018,,,,,,,,,,"0|i3om7r:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"07/Jan/18 23:46;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/20180;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unsafe asynchronous close() call from FileDownloadChannel,SPARK-22982,13129125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,07/Jan/18 23:09,18/Jan/18 18:28,13/Jul/23 08:45,10/Jan/18 07:10,1.6.0,2.0.0,2.1.0,2.2.0,,,,,,2.2.2,2.3.0,,,,Spark Core,,,,0,correctness,,,"Spark's Netty-based file transfer code contains an asynchronous IO bug which may lead to incorrect query results.

At a high-level, the problem is that an unsafe asynchronous `close()` of a pipe's source channel creates a race condition where file transfer code closes a file descriptor then attempts to read from it. If the closed file descriptor's number has been reused by an `open()` call then this invalid read may cause unrelated file operations to return incorrect results due to reading different data than intended.

I have a small, surgical fix for this bug and will submit a PR with more description on the specific race condition / underlying bug.",,aash,apachespark,cloud_fan,joshrosen,Kotomi,maropu,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 18:28:25 UTC 2018,,,,,,,,,,"0|i1u552:zzy",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"07/Jan/18 23:31;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/20179;;;","10/Jan/18 07:10;cloud_fan;Issue resolved by pull request 20179
[https://github.com/apache/spark/pull/20179];;;","10/Jan/18 13:16;srowen;Does this affect earlier branches in the same way? Seems important to back port if so. ;;;","10/Jan/18 17:14;joshrosen;In theory this affects all 1.6.0+ versions.

It's going to be much harder to trigger in 1.6.0 because we shouldn't have as many Janino-induced remote ClassNotFoundExceptions to trigger the error path.

We should probably port to other 2.x branches, with one caveat: we need to make sure that my fix isn't relying on JRE 8.x functionality because I think at least some of those older branches still have support for Java 7.;;;","10/Jan/18 17:16;srowen;Agreed. java.nio should be OK as that was introduced in Java 7. Spark 2.2 requires Java 8, so that much is safe.;;;","18/Jan/18 18:28;aash;[~joshrosen] do you have some example stacktraces of what this bug can cause?  Several of our clusters hit what I think is this problem earlier this month, see below for details.

 

For a few days in January (4th through 12th) on our AWS infra, we observed massively degraded disk read throughput (down to 33% of previous peaks).  During this time, we also began observing intermittent exceptions coming from Spark at read time of parquet files that a previous Spark job had written.  When the read throughput recovered on the 12th, we stopped observing the exceptions and haven't seen them since.

At first we observed this stacktrace when reading .snappy.parquet files:
{noformat}
java.lang.RuntimeException: java.io.IOException: could not read page Page [bytes.size=1048641, valueCount=29945, uncompressedSize=1048641] in col [my_column] BINARY
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:493)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:486)
	at org.apache.parquet.column.page.DataPageV1.accept(DataPageV1.java:96)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPage(VectorizedColumnReader.java:486)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:157)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:229)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:137)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:398)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:191)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:80)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:109)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: could not read page Page [bytes.size=1048641, valueCount=29945, uncompressedSize=1048641] in col [my_column] BINARY
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV1(VectorizedColumnReader.java:562)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.access$000(VectorizedColumnReader.java:47)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:490)
	... 31 more
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:547)
	at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:69)
	at org.apache.parquet.hadoop.codec.NonBlockedDecompressorStream.read(NonBlockedDecompressorStream.java:51)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readFully(DataInputStream.java:169)
	at org.apache.parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:253)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV1(VectorizedColumnReader.java:555)
	... 33 more{noformat}
and saw a similar exception when attempting to read the file with parquet-tools instead of Spark, indicating the file itself was corrupted, not the read process:
{noformat}
parquet.io.ParquetDecodingException: Can not read value at 11074077 in block 1 in file file:/path/to/part-00852-4f6b3ec3-ae6d-41ff-919b-a2ef4ea3dfa0-c000.snappy.parquet
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)
        at parquet.hadoop.ParquetReader.read(ParquetReader.java:124)
        at parquet.tools.command.CatCommand.execute(CatCommand.java:54)
        at parquet.tools.Main.main(Main.java:219)
Caused by: parquet.io.ParquetDecodingException: could not read page Page [bytes.size=1048598, valueCount=83618, uncompressedSize=1048598] in col [longitude] BINARY
        at parquet.column.impl.ColumnReaderImpl.readPageV1(ColumnReaderImpl.java:568)
        at parquet.column.impl.ColumnReaderImpl.access$300(ColumnReaderImpl.java:57)
        at parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:516)
        at parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:513)
        at parquet.column.page.DataPageV1.accept(DataPageV1.java:96)
        at parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:513)
        at parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:505)
        at parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:607)
        at parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:407)
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:209)
        ... 3 more
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:516)
        at parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:69)
        at parquet.hadoop.codec.NonBlockedDecompressorStream.read(NonBlockedDecompressorStream.java:51)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readFully(DataInputStream.java:169)
        at parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:204)
        at parquet.column.impl.ColumnReaderImpl.readPageV1(ColumnReaderImpl.java:557)
        ... 12 more
Can not read value at 11074077 in block 1 in file file:/path/to/part-00852-4f6b3ec3-ae6d-41ff-919b-a2ef4ea3dfa0-c000.snappy.parquet
{noformat}
Thinking it might have something to do with snappy, we set {{spark.sql.parquet.compression.codec: gzip}} and then observed this exception, again intermittently:
{noformat}
java.lang.IndexOutOfBoundsException
at java.nio.Buffer.checkIndex(Buffer.java:540)
at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readUnsignedVarInt(VectorizedRleValuesReader.java:532)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readNextGroup(VectorizedRleValuesReader.java:588)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readIntegers(VectorizedRleValuesReader.java:467)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readIntegers(VectorizedRleValuesReader.java:438)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:163)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:229)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:137)
at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:398)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:108)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){noformat}
 

This occurred in clusters with Spark versions ranging from a few weeks ago on the apache master branch to all the way back to Spark 1.6.1.  All in AWS infra, we did not observe this outside of AWS.  One cluster saw failures in about 1% of builds.

My hypothesis is that the Spectre/Meltdown patches on AWS, which in an early iteration caused large I/O performance degradations, caused Spark to hold file handles open for longer than previously, and subsequently hit this bug more frequently than before.  We hadn't seen the exceptions previously, and when another silent AWS patch went out around Jan 12th that fixed I/O performance, we stopped seeing the exceptions.

Do these observations match some of the observations your team observed while diving into this bug?  Or am I totally off-base and latching onto an unrelated issue hoping it's the same as mine?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results of casting Struct to String,SPARK-22981,13129058,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,07/Jan/18 06:27,09/Jan/18 14:02,13/Jul/23 08:45,09/Jan/18 14:02,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"{code}
scala> val df = Seq(((1, ""a""), 0), ((2, ""b""), 0)).toDF(""a"", ""b"")
df: org.apache.spark.sql.DataFrame = [a: struct<_1: int, _2: string>, b: int]

scala> df.write.saveAsTable(""t"")
                                                                                
scala> sql(""SELECT CAST(a AS STRING) FROM t"").show
+-------------------+
|                  a|
+-------------------+
|[0,1,1800000001,61]|
|[0,2,1800000001,62]|
+-------------------+
{code}",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 14:02:36 UTC 2018,,,,,,,,,,"0|i3olsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/18 06:38;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20176;;;","09/Jan/18 14:02;cloud_fan;Issue resolved by pull request 20176
[https://github.com/apache/spark/pull/20176];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrameWriter operations do not show details in SQL tab,SPARK-22977,13128978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,yumwang,yumwang,06/Jan/18 04:01,27/Dec/18 23:05,13/Jul/23 08:45,12/Feb/18 14:09,2.3.0,,,,,,,,,2.3.0,,,,,SQL,Web UI,,,0,,,,"When CreateHiveTableAsSelectCommand or  InsertIntoHiveTable, SQL tab don't show details after [SPARK-20213|https://issues.apache.org/jira/browse/SPARK-20213].

*Before*:
!before.png!

*After*:
!after.png!",,ajbozarth,apachespark,cloud_fan,mgaido,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20213,SPARK-26437,,,,,,,,,,"06/Jan/18 04:22;yumwang;after.png;https://issues.apache.org/jira/secure/attachment/12904922/after.png","06/Jan/18 04:22;yumwang;before.png;https://issues.apache.org/jira/secure/attachment/12904921/before.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 12 14:09:19 UTC 2018,,,,,,,,,,"0|i3olb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/18 05:13;yumwang;cc [~cloud_fan];;;","06/Feb/18 17:42;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20521;;;","12/Feb/18 14:09;cloud_fan;Issue resolved by pull request 20521
[https://github.com/apache/spark/pull/20521];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker cleanup can remove running driver directories,SPARK-22976,13128952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rspitzer,rspitzer,rspitzer,06/Jan/18 00:22,22/Jan/18 04:32,13/Jul/23 08:45,22/Jan/18 04:30,1.0.2,,,,,,,,,2.3.0,,,,,Deploy,Spark Core,,,0,,,,"Spark Standalone worker cleanup finds directories to remove with a listFiles command

This includes both application directories and driver directories from cluster mode submitted applications. 

A directory is considered to not be part of a running app if the worker does not have an executor with a matching ID.

https://github.com/apache/spark/blob/v2.2.1/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L432
{code}
      val appIds = executors.values.map(_.appId).toSet
      val isAppStillRunning = appIds.contains(appIdFromDir)
{code}


If a driver has been started on a node, but all of the executors are on other nodes, the worker running the driver will always assume that the driver directory is not part of a running app.

Consider a two node spark cluster with Worker A and Worker B where each node has a single core available. We submit our application in deploy mode cluster, the driver begins running on Worker A while the Executor starts on B.

Worker A has a cleanup triggered and looks and finds it has a directory
{code}
/var/lib/spark/worker/driver-20180105234824-0000
{code}

Worker A check's it's executor list and finds no entries which match this since it has no corresponding executors for this application. Worker A then removes the directory even though it may still be actively running.

I think this could be fixed by modifying line 432 to be
{code}
      val appIds = executors.values.map(_.appId).toSet ++ drivers.values.map(_.driverId)
{code}

I'll run a test and submit a PR soon.
",,apachespark,bcantoni,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 17 20:21:05 UTC 2018,,,,,,,,,,"0|i3ol5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 23:52;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/20190;;;","08/Jan/18 23:52;rspitzer;Made a PR against 2.0 but it's valid against all versions up to master;;;","17/Jan/18 20:21;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/20298;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetricsReporter producing NullPointerException when there was no progress reported,SPARK-22975,13128885,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,bondyk,bondyk,05/Jan/18 18:56,12/Jan/18 19:27,13/Jul/23 08:45,12/Jan/18 19:26,2.2.0,2.2.1,,,,,,,,2.2.2,2.3.0,,,,Structured Streaming,,,,0,,,,"The exception occurs in MetricsReporter when it tries to register gauges using lastProgress of each stream.
{code:java}
  registerGauge(""inputRate-total"", () => stream.lastProgress.inputRowsPerSecond)
  registerGauge(""processingRate-total"", () => stream.lastProgress.inputRowsPerSecond)
  registerGauge(""latency"", () => stream.lastProgress.durationMs.get(""triggerExecution"").longValue())
{code}
In case if a stream doesn't have any progress reported than following exception occurs:
{noformat}
18/01/05 17:45:57 ERROR ScheduledReporter: RuntimeException thrown from CloudwatchReporter#report. Exception was suppressed.
java.lang.NullPointerException
	at org.apache.spark.sql.execution.streaming.MetricsReporter$$anonfun$1.apply$mcD$sp(MetricsReporter.scala:42)
	at org.apache.spark.sql.execution.streaming.MetricsReporter$$anonfun$1.apply(MetricsReporter.scala:42)
	at org.apache.spark.sql.execution.streaming.MetricsReporter$$anonfun$1.apply(MetricsReporter.scala:42)
	at org.apache.spark.sql.execution.streaming.MetricsReporter$$anon$1.getValue(MetricsReporter.scala:49)
	at amazon.nexus.spark.metrics.cloudwatch.CloudwatchReporter.lambda$createNumericGaugeMetricDatumStream$0(CloudwatchReporter.java:146)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.lambda$entryConsumer$0(Collections.java:1575)
	at java.util.TreeMap$EntrySpliterator.forEachRemaining(TreeMap.java:2969)
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet$UnmodifiableEntrySetSpliterator.forEachRemaining(Collections.java:1600)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:312)
	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:510)
	at amazon.nexus.spark.metrics.cloudwatch.CloudwatchReporter.partitionIntoSublists(CloudwatchReporter.java:390)
	at amazon.nexus.spark.metrics.cloudwatch.CloudwatchReporter.report(CloudwatchReporter.java:137)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at com.codahale.metrics.ScheduledReporter$1.run(ScheduledReporter.java:117)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,apachespark,bondyk,mgaido,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 12 19:26:30 UTC 2018,,,,,,,,,,"0|i3okqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 22:40;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20189;;;","12/Jan/18 19:26;zsxwing;Issue resolved by pull request 20189
[https://github.com/apache/spark/pull/20189];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CountVectorModel does not attach attributes to output column,SPARK-22974,13128882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,thewilliamzhang,thewilliamzhang,05/Jan/18 18:39,03/May/19 17:49,13/Jul/23 08:45,14/Aug/18 05:05,2.2.1,,,,,,,,,2.4.0,,,,,ML,,,,0,,,,"If CountVectorModel transforms columns, the output column will not have attributes attached to it. If later on, those output columns are used in Interaction transformer, an exception will be thrown:
{quote}""org.apache.spark.SparkException: Vector attributes must be defined for interaction.""
{quote}
To reproduce it:
{quote}import org.apache.spark.ml.feature._
 import org.apache.spark.sql.functions._

val df = spark.createDataFrame(Seq(
 (0, Array(""a"", ""b"", ""c""), Array(""1"", ""2"")),
 (1, Array(""a"", ""b"", ""b"", ""c"", ""a"", ""d""), Array(""1"", ""2"", ""3""))
 )).toDF(""id"", ""words"", ""nums"")

val cvModel: CountVectorizerModel = new CountVectorizer()
 .setInputCol(""nums"")
 .setOutputCol(""features2"")
 .setVocabSize(4)
 .setMinDF(0)
 .fit(df)

val cvm = new CountVectorizerModel(Array(""a"", ""b"", ""c""))
 .setInputCol(""words"")
 .setOutputCol(""features1"")

val df1 = cvm.transform(df)
 val df2 = cvModel.transform(df1)

val interaction = new Interaction().setInputCols(Array(""features1"", ""features2"")).setOutputCol(""features"")
 val df3 = interaction.transform(df2)
{quote}",,apachespark,dbtsai,mgaido,thewilliamzhang,xiayunsun,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 03 17:49:00 UTC 2019,,,,,,,,,,"0|i3okpr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 08:32;xiayunsun;do we define the expected behavior here as Interaction of the occurrences? 

`CountVectorizer` returns a `SparseVector`, for example, {{(3,[0,1,2],[1.0,1.0,1.0])}}

I think it's possible to attach numerical attribute to  [1.0, 1.0, 1.0] then interaction will work. ;;;","18/Jan/18 09:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/20313;;;","14/Aug/18 05:05;dbtsai;Issue resolved by pull request 20313
[https://github.com/apache/spark/pull/20313];;;","03/May/19 17:49;yuhaoyan;On a business trip from April 29th to May 3rd . Please expect delayed email response. Conctact +1 669 243 8273for anything urgent.

Thanks,
Yuhao
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results of casting Map to String,SPARK-22973,13128829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,05/Jan/18 15:35,07/Jan/18 05:43,13/Jul/23 08:45,07/Jan/18 05:43,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"{code}
scala> Seq(Map(1 -> ""a"", 2 -> ""b"")).toDF(""a"").write.saveAsTable(""t"")
scala> sql(""SELECT cast(a as String) FROM t"").show(false)
+----------------------------------------------------------------+
|a                                                               |
+----------------------------------------------------------------+
|org.apache.spark.sql.catalyst.expressions.UnsafeMapData@38bdd75d|
+----------------------------------------------------------------+
{code}",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 07 05:43:11 UTC 2018,,,,,,,,,,"0|i3oke7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/18 15:45;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20166;;;","07/Jan/18 05:43;cloud_fan;Issue resolved by pull request 20166
[https://github.com/apache/spark/pull/20166];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Couldn't find corresponding Hive SerDe for data source provider org.apache.spark.sql.hive.orc.,SPARK-22972,13128797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xubo245,xubo245,xubo245,05/Jan/18 13:05,10/Jan/18 15:34,13/Jul/23 08:45,09/Jan/18 02:21,2.2.1,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,0,,,,"
*There is error when running test code:*

{code:java}
  test(""create orc table"") {
    spark.sql(
      s""""""CREATE TABLE normal_orc_as_source_hive
         |USING org.apache.spark.sql.hive.orc
         |OPTIONS (
         |  PATH '${new File(orcTableAsDir.getAbsolutePath).toURI}'
         |)
       """""".stripMargin)
    val df = spark.sql(""select * from normal_orc_as_source_hive"")
    spark.sql(""desc formatted normal_orc_as_source_hive"").show()
  }
{code}

*warning:*

{code:java}
05:00:44.038 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider org.apache.spark.sql.hive.orc. Persisting data source table `default`.`normal_orc_as_source_hive` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.

{code}

Root cause analysis:   
ORC related code is incorrect in HiveSerDe :

{code:java}

org.apache.spark.sql.internal.HiveSerDe#sourceToSerDe
{code}


{code:java}
  def sourceToSerDe(source: String): Option[HiveSerDe] = {
    val key = source.toLowerCase(Locale.ROOT) match {
      case s if s.startsWith(""org.apache.spark.sql.parquet"") => ""parquet""
      case s if s.startsWith(""org.apache.spark.sql.orc"") => ""orc""
      case s if s.equals(""orcfile"") => ""orc""
      case s if s.equals(""parquetfile"") => ""parquet""
      case s if s.equals(""avrofile"") => ""avro""
      case s => s
    }
{code}

Solution:

change ""org.apache.spark.sql.orc“  to  ""org.apache.spark.sql.hive.orc""




",,apachespark,xubo245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 09 04:15:04 UTC 2018,,,,,,,,,,"0|i3ok73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/18 13:29;apachespark;User 'xubo245' has created a pull request for this issue:
https://github.com/apache/spark/pull/20165;;;","09/Jan/18 04:15;apachespark;User 'xubo245' has created a pull request for this issue:
https://github.com/apache/spark/pull/20195;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalStateException: No current assignment for partition kssh-2,SPARK-22968,13128697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,1028344078@qq.com,1028344078@qq.com,05/Jan/18 05:28,14/Oct/19 16:54,13/Jul/23 08:45,18/Apr/18 02:13,2.1.1,,,,,,,,,2.4.0,,,,,Spark Core,,,,0,,,,"*Kafka Broker:*
{code:java}
   message.max.bytes : 2621440  
{code}

*Spark Streaming+Kafka Code:*
{code:java}
, ""max.partition.fetch.bytes"" -> (5242880: java.lang.Integer) //default: 1048576
, ""request.timeout.ms"" -> (90000: java.lang.Integer) //default: 60000
, ""session.timeout.ms"" -> (60000: java.lang.Integer) //default: 30000
, ""heartbeat.interval.ms"" -> (5000: java.lang.Integer)
, ""receive.buffer.bytes"" -> (10485760: java.lang.Integer)
{code}


*Error message:*
{code:java}
8/01/05 09:48:27 INFO internals.ConsumerCoordinator: Revoking previously assigned partitions [kssh-7, kssh-4, kssh-3, kssh-6, kssh-5, kssh-0, kssh-2, kssh-1] for group use_a_separate_group_id_for_each_stream
18/01/05 09:48:27 INFO internals.AbstractCoordinator: (Re-)joining group use_a_separate_group_id_for_each_stream
18/01/05 09:48:27 INFO internals.AbstractCoordinator: Successfully joined group use_a_separate_group_id_for_each_stream with generation 4
18/01/05 09:48:27 INFO internals.ConsumerCoordinator: Setting newly assigned partitions [kssh-7, kssh-4, kssh-6, kssh-5] for group use_a_separate_group_id_for_each_stream
18/01/05 09:48:27 ERROR scheduler.JobScheduler: Error generating jobs for time 1515116907000 ms
java.lang.IllegalStateException: No current assignment for partition kssh-2
	at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:231)
	at org.apache.kafka.clients.consumer.internals.SubscriptionState.needOffsetReset(SubscriptionState.java:295)
	at org.apache.kafka.clients.consumer.KafkaConsumer.seekToEnd(KafkaConsumer.java:1169)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.latestOffsets(DirectKafkaInputDStream.scala:197)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:214)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
18/01/05 09:48:27 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
18/01/05 09:48:27 INFO streaming.StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
18/01/05 09:48:27 INFO scheduler.ReceiverTracker: ReceiverTracker stopped
18/01/05 09:48:27 INFO scheduler.JobGenerator: Stopping JobGenerator immediately
18/01/05 09:48:27 INFO util.RecurringTimer: Stopped timer for JobGenerator after time 1515116907000
18/01/05 09:48:27 INFO scheduler.JobGenerator: Stopped JobGenerator
18/01/05 09:48:27 INFO scheduler.JobScheduler: Stopped JobScheduler
{code}
","Kafka:  0.10.0  (CDH5.12.0)
Apache Spark 2.1.1

Spark streaming+Kafka",1028344078@qq.com,apachespark,koeninger,,,,,,,,,,,,,,,,,345600,345600,,0%,345600,345600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 02:13:36 UTC 2018,,,,,,,,,,"0|i3ojlb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 01:28;1028344078@qq.com;[~srowen] Thanks for quick response.
I turn up the parameter ""max.partition.fetch.bytes""  from 5242880 to10485760.
In the later days, i'll look at it again.;;;","28/Feb/18 01:47;1028344078@qq.com;It's been a long time.I close it first.;;;","27/Mar/18 08:25;1028344078@qq.com;This issue is reappear.;;;","30/Mar/18 06:36;1028344078@qq.com;Adjust these parameters,keep monitor.

“request.timeout.ms“ -> (210000: java.lang.Integer)
“session.timeout.ms“ -> (180000: java.lang.Integer) 
“heartbeat.interval.ms“ -> (6000: java.lang.Integer);;;","11/Apr/18 07:20;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21038;;;","11/Apr/18 07:57;1028344078@qq.com;[~apachespark] Thank you very much. ;;;","18/Apr/18 02:13;koeninger;Issue resolved by pull request 21038
[https://github.com/apache/spark/pull/21038];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VersionSuite failed on Windows caused by Windows format path,SPARK-22967,13128687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ngone51,Ngone51,Ngone51,05/Jan/18 03:16,12/Dec/22 18:11,13/Jul/23 08:45,11/Jan/18 13:18,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,05/Jan/18 00:00,0,build,test,windows,"On Windows system, two unit test case would fail while running VersionSuite (""A simple set of tests that call the methods of a `HiveClient`, loading different version of hive from maven central."")

Failed A : test(s""$version: read avro file containing decimal"") 

{code:java}
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string);
{code}

Failed B: test(s""$version: SPARK-17920: Insert into/overwrite avro table"")

{code:java}
Unable to infer the schema. The schema specification is required to create the table `default`.`tab2`.;
org.apache.spark.sql.AnalysisException: Unable to infer the schema. The schema specification is required to create the table `default`.`tab2`.;
{code}

As I deep into this problem, I found it is related to ParserUtils#unescapeSQLString().

These are two lines at the beginning of Failed A:

{code:java}
val url = Thread.currentThread().getContextClassLoader.getResource(""avroDecimal"")
val location = new File(url.getFile)
{code}

And in my environment，`location` (path value) is

{code:java}
D:\workspace\IdeaProjects\spark\sql\hive\target\scala-2.11\test-classes\avroDecimal
{code}

And then, in SparkSqlParser#visitCreateHiveTable()#L1128:

{code:java}
val location = Option(ctx.locationSpec).map(visitLocationSpec)
{code}
This line want to get LocationSepcContext's content first, which is equal to `location` above.
Then, the content is passed to visitLocationSpec(), and passed to unescapeSQLString()
finally.

Lets' have a look at unescapeSQLString():
{code:java}
/** Unescape baskslash-escaped string enclosed by quotes. */
  def unescapeSQLString(b: String): String = {
    var enclosure: Character = null
    val sb = new StringBuilder(b.length())

    def appendEscapedChar(n: Char) {
      n match {
        case '0' => sb.append('\u0000')
        case '\'' => sb.append('\'')
        case '""' => sb.append('\""')
        case 'b' => sb.append('\b')
        case 'n' => sb.append('\n')
        case 'r' => sb.append('\r')
        case 't' => sb.append('\t')
        case 'Z' => sb.append('\u001A')
        case '\\' => sb.append('\\')
        // The following 2 lines are exactly what MySQL does TODO: why do we do this?
        case '%' => sb.append(""\\%"")
        case '_' => sb.append(""\\_"")
        case _ => sb.append(n)
      }
    }

    var i = 0
    val strLength = b.length
    while (i < strLength) {
      val currentChar = b.charAt(i)
      if (enclosure == null) {
        if (currentChar == '\'' || currentChar == '\""') {
          enclosure = currentChar
        }
      } else if (enclosure == currentChar) {
        enclosure = null
      } else if (currentChar == '\\') {

        if ((i + 6 < strLength) && b.charAt(i + 1) == 'u') {
          // \u0000 style character literals.

          val base = i + 2
          val code = (0 until 4).foldLeft(0) { (mid, j) =>
            val digit = Character.digit(b.charAt(j + base), 16)
            (mid << 4) + digit
          }
          sb.append(code.asInstanceOf[Char])
          i += 5
        } else if (i + 4 < strLength) {
          // \000 style character literals.

          val i1 = b.charAt(i + 1)
          val i2 = b.charAt(i + 2)
          val i3 = b.charAt(i + 3)

          if ((i1 >= '0' && i1 <= '1') && (i2 >= '0' && i2 <= '7') && (i3 >= '0' && i3 <= '7')) {
            val tmp = ((i3 - '0') + ((i2 - '0') << 3) + ((i1 - '0') << 6)).asInstanceOf[Char]
            sb.append(tmp)
            i += 3
          } else {
            appendEscapedChar(i1)
            i += 1
          }
        } else if (i + 2 < strLength) {
          // escaped character literals.
          val n = b.charAt(i + 1)
          appendEscapedChar(n)
          i += 1
        }
      } else {
        // non-escaped character literals.
        sb.append(currentChar)
      }
      i += 1
    }
    sb.toString()
  }
{code}
 Again, here, variable `b` is equal to content and `location`, is valued of 

{code:java}
D:\workspace\IdeaProjects\spark\sql\hive\target\scala-2.11\test-classes\avroDecimal
{code}

And we can make sense from the unescapeSQLString()' strategies that it transform  the String ""\t"" into a escape character '\t' and remove all backslashes.
So, our original correct location resulted in:

{code:java}
D:workspaceIdeaProjectssparksqlhive\targetscala-2.11\test-classesavroDecimal
{code}
 after unescapeSQLString() completed.
Note that, here, [ \t ] is no longer a string, but a escape character. 

Then, return into SparkSqlParser#visitCreateHiveTable(), and move to L1134:

{code:java}
val locUri = location.map(CatalogUtils.stringToURI(_))
{code}

`location` is passed to stringToURI(), and resulted in:

{code:java}
file:/D:workspaceIdeaProjectssparksqlhive%09argetscala-2.11%09est-classesavroDecimal
{code}

finally, as  escape character '\t'  is transformed into URI code '%09'.

Although, I'm not clearly about how this wrong path directly caused that exception, as I almostly know nothing about Hive, I can verify that this wrong path is the real factor to cause this exception.

When I append these lines(in order to fix the wrong path) after HiveExternalCatalog#doCreateTable()Line236-240:

{code:java}
if (tableLocation.get.getPath.startsWith(""/D"")) {
     tableLocation = Some(CatalogUtils.stringToURI(
        ""file:/D:/workspace/IdeaProjects/spark/sql/hive/target/scala-2.11/test-classes/avroDecimal""))
    }
{code}
 
then, failed unit test A will pass, excluding test B.

And below is the stack trace of the Exception:

{code:java}
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:602)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:469)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:467)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:467)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:273)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:210)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:256)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:467)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply(HiveExternalCatalog.scala:216)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply(HiveExternalCatalog.scala:216)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.doCreateTable(HiveExternalCatalog.scala:216)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.createTable(ExternalCatalog.scala:119)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:304)
	at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:128)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$51.apply(Dataset.scala:3196)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3195)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:71)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$24$$anonfun$apply$mcV$sp$3.apply$mcV$sp(VersionsSuite.scala:829)
	at org.apache.spark.sql.hive.client.VersionsSuite.withTable(VersionsSuite.scala:70)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$24.apply$mcV$sp(VersionsSuite.scala:828)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$24.apply(VersionsSuite.scala:805)
	at org.apache.spark.sql.hive.client.VersionsSuite$$anonfun$6$$anonfun$apply$24.apply(VersionsSuite.scala:805)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite$class.run(Suite.scala:1147)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
Caused by: MetaException(message:java.lang.IllegalArgumentException: Can not create a Path from an empty string)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1121)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)
	at com.sun.proxy.$Proxy31.create_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:482)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:471)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy32.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:596)
	... 78 more
Caused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127)
	at org.apache.hadoop.fs.Path.<init>(Path.java:184)
	at org.apache.hadoop.fs.Path.getParent(Path.java:357)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:427)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:690)
	at org.apache.hadoop.hive.metastore.Warehouse.mkdirs(Warehouse.java:194)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1059)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1107)
	... 93 more

{code}

As for test B, I did'n do a careful inspection, but I find a same wrong path as test A. So, I guess exceptions were  caused by the same factor.
 





",Windos7,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Thu Jan 11 13:18:43 UTC 2018,,,,,,,,,,"0|hzzy7p:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/18 06:50;Ngone51;@[~srowen];;;","05/Jan/18 15:02;gurwls223;Yup, it's a known issue. I have been fixing the tests in batch so far and usually the fix is about replacing the path to URI form. Would you like to open a PR? I can review.;;;","06/Jan/18 05:16;Ngone51;I'd like to open a PR, but I'm not 100% sure how to fix this bug yet. As you say:

{code:java}
fix is about replacing the path to URI form
{code}

But this Windows' path goes wrong before the stringToURI() called (as I mentioned above). So, should we fix it before URI transform happen ?;;;","08/Jan/18 01:24;gurwls223;Ah, I meant to fix the tests to use URI forms instead of Windows file path if they are not specific to test the path forms themselves.;;;","08/Jan/18 11:48;Ngone51;I understand what you mean now, and things with Hive go well after I tried this. But, another wired problem arise.

Tmp dir was created at the beginning of test B(mentioned above):

{code:java}
protected def withTempDir(f: File => Unit): Unit = {
   val dir = Utils.createTempDir().getCanonicalFile
   try f(dir) finally Utils.deleteRecursively(dir)
  }
{code}

And, it would be deleted in finally clause.

And test B will run with below Hive visions sequently:

{code:java}
private val versions = Seq(""0.12"", ""0.13"", ""0.14"", ""1.0"", ""1.1"", ""1.2"", ""2.0"", ""2.1"")
{code}

And each version will delete the tmp dir successfully except the version 0.12. And when I try to delete this tmp file manualy, then, Windows warning me that this file may open in another program. It seems that an open stream is occupying this file.

But, this tmp file could be deleted after another version test start running.

And, I tried to exchange the order between 0.12 and 0.13, but the result remains the same.

That's really make me confused. Maybe, there's something incompatible with version 0.12.



;;;","08/Jan/18 12:07;gurwls223;There's a annoying problem on Windows. Every file _must_ be closed before being removed as it holds an exclusive lock. In theory, those temp directories are guaranteed to be removed at JVM's shutdown hook. I think we can ignore that stuff for now. Does that make the tests fail?;;;","08/Jan/18 13:00;Ngone51;Yeah, it fails the test.;;;","08/Jan/18 13:13;gurwls223;Hm .. I haven't taken a look for it closely yet but can we ignore it in that case in anyway? If it's difficult or impossible, I think we can just skip on Windows as it's going to fail anyway. I did this several times. For example, I think you can refer https://github.com/apache/spark/pull/16999.

I know it take a while to investigate to check if other tests are failed on Windows but it should be really nicer if we can identify some more test cases failed on Windows fix them in a batch.;;;","08/Jan/18 14:04;Ngone51;Maybe, we can give a warning rather than an exception, since the main goal of the tests have been achieved.

I will check it again tomorrow...
 
And I know another case(ChildProcAppHanlderSuite) may related to Windows, also. I can post it once I have time.;;;","08/Jan/18 15:54;gurwls223;Yea, that sounds good roughly. Will check the PR once you opened.;;;","09/Jan/18 08:21;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/20199;;;","11/Jan/18 13:18;gurwls223;Fixed in https://github.com/apache/spark/pull/20199;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes app fails if local files are used,SPARK-22962,13128556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyinan926,vanzin,vanzin,04/Jan/18 18:12,17/May/20 18:24,13/Jul/23 08:45,18/Jan/18 22:44,2.3.0,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,0,,,,"If you try to start a Spark app on kubernetes using a local file as the app resource, for example, it will fail:

{code}
./bin/spark-submit [[bunch of arguments]] /path/to/local/file.jar
{code}

{noformat}
+ /sbin/tini -s -- /bin/sh -c 'SPARK_CLASSPATH=""${SPARK_HOME}/jars/*"" &&     env | grep SPARK_JAVA_OPT_ | sed '\''s/[^=]*=\(.*\)/\1/g'
\'' > /tmp/java_opts.txt &&     readarray -t SPARK_DRIVER_JAVA_OPTS < /tmp/java_opts.txt &&     if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x}
 ]; then SPARK_CLASSPATH=""$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH""; fi &&     if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SP
ARK_CLASSPATH=""$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH""; fi &&     if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R ""$SPARK
_MOUNTED_FILES_DIR/."" .; fi &&     ${JAVA_HOME}/bin/java ""${SPARK_DRIVER_JAVA_OPTS[@]}"" -cp ""$SPARK_CLASSPATH"" -Xms$SPARK_DRIVER_MEMOR
Y -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $SPARK_DRIVER_ARGS'
Error: Could not find or load main class com.cloudera.spark.tests.Sleeper
{noformat}

Using an http server to provide the app jar solves the problem.

The k8s backend should either somehow make these files available to the cluster or error out with a more user-friendly message if that feature is not yet available.",,apachespark,foxish,liyinan926,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 22:44:51 UTC 2018,,,,,,,,,,"0|i3oipz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/18 19:54;foxish;This is the resource staging server use-case. We'll upstream this in the 2.4.0 timeframe.;;;","18/Jan/18 20:15;liyinan926;I agree that before we upstream the staging server, we should fail the submission if a user uses local resources. [~vanzin], if it's not too late to get into 2.3, I'm gonna file a PR for this.;;;","18/Jan/18 20:18;vanzin;Yes send a PR.;;;","18/Jan/18 20:26;foxish;I think this isn't super critical for this release, mostly a usability thing. If it's small enough, it makes sense, but if it introduces risk and we have to redo manual testing, I'd vote against getting this into 2.3.;;;","18/Jan/18 20:43;apachespark;User 'liyinan926' has created a pull request for this issue:
https://github.com/apache/spark/pull/20320;;;","18/Jan/18 22:44;vanzin;Issue resolved by pull request 20320
[https://github.com/apache/spark/pull/20320];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constant columns no longer picked as constraints in 2.3,SPARK-22961,13128551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a.ionescu,a.ionescu,a.ionescu,04/Jan/18 17:54,31/Jan/18 07:50,13/Jul/23 08:45,05/Jan/18 13:37,2.3.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,constraints,optimizer,regression,"We're no longer picking up {{x = 2}} as a constraint from something like {{df.withColumn(""x"", lit(2))}}

The unit test below succeeds in {{branch-2.2}}:
{code}
test(""constraints should be inferred from aliased literals"") {
    val originalLeft = testRelation.subquery('left).as(""left"")
    val optimizedLeft = testRelation.subquery('left).where(IsNotNull('a) && 'a <=> 2).as(""left"")

    val right = Project(Seq(Literal(2).as(""two"")), testRelation.subquery('right)).as(""right"")
    val condition = Some(""left.a"".attr === ""right.two"".attr)

    val original = originalLeft.join(right, Inner, condition)
    val correct = optimizedLeft.join(right, Inner, condition)

    comparePlans(Optimize.execute(original.analyze), correct.analyze)
  }
{code}
but fails in {{branch-2.3}} with:
{code}
== FAIL: Plans do not match ===
 'Join Inner, (two#0 = a#0)                     'Join Inner, (two#0 = a#0)
!:- Filter isnotnull(a#0)                       :- Filter ((2 <=> a#0) && isnotnull(a#0))
 :  +- LocalRelation <empty>, [a#0, b#0, c#0]   :  +- LocalRelation <empty>, [a#0, b#0, c#0]
 +- Project [2 AS two#0]                        +- Project [2 AS two#0]
    +- LocalRelation <empty>, [a#0, b#0, c#0]      +- LocalRelation <empty>, [a#0, b#0, c#0] 
{code}",,a.ionescu,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 18:08:04 UTC 2018,,,,,,,,,,"0|i3oiov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 18:08;apachespark;User 'adrian-ionescu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApproxQuantile breaks if the number of rows exceeds MaxInt,SPARK-22957,13128482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,04/Jan/18 11:53,05/Jan/18 02:17,13/Jul/23 08:45,05/Jan/18 02:17,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"ApproxQuantile overflows when number of rows exceeds 2.147B (max int32).

If you run ApproxQuantile on a dataframe with 3B rows of 1 to 3B and ask it for 1/6 quantiles, it should return [0.5B, 1B, 1.5B, 2B, 2.5B, 3B]. However, in the [implementation of ApproxQuantile|https://github.com/apache/spark/blob/v2.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/QuantileSummaries.scala#L195], it calls .toInt on the target rank, which overflows at 2.147B.",,apachespark,cloud_fan,juliuszsompolski,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 05 02:17:42 UTC 2018,,,,,,,,,,"0|i3oi9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 13:25;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/20152;;;","05/Jan/18 02:17;cloud_fan;Issue resolved by pull request 20152
[https://github.com/apache/spark/pull/20152];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union Stream Failover Cause `IllegalStateException`,SPARK-22956,13128481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,04/Jan/18 11:47,16/Sep/18 01:55,13/Jul/23 08:45,16/Jan/18 06:02,2.1.0,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,0,,,,"When we union 2 streams from kafka or other sources, while one of them have no continues data coming and in the same time task restart, this will cause an `IllegalStateException`. This mainly cause because the code in [MicroBatchExecution|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L190]  , while one stream has no continues data, its comittedOffset same with availableOffset during `populateStartOffsets`, and `currentPartitionOffsets` not properly handled in KafkaSource. Also, maybe we should also consider this scenario in other Source.",,apachespark,XuanYuan,yanlin-Lynn,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 16 06:02:00 UTC 2018,,,,,,,,,,"0|i3oi9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 12:00;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/20150;;;","16/Jan/18 06:02;zsxwing;Issue resolved by pull request 20150
[https://github.com/apache/spark/pull/20150];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error generating jobs when Stopping JobGenerator gracefully,SPARK-22955,13128464,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,choojoyq,zhaoshijie,zhaoshijie,04/Jan/18 10:19,06/Sep/19 22:44,13/Jul/23 08:45,03/Sep/19 18:02,2.2.0,,,,,,,,,2.4.5,3.0.0,,,,DStreams,,,,1,,,,"when I stop a spark-streaming application with parameter 
 spark.streaming.stopGracefullyOnShutdown, I get ERROR as follows:

{code:java}
2018-01-04 17:31:17,524 ERROR org.apache.spark.deploy.yarn.ApplicationMaster: RECEIVED SIGNAL TERM
2018-01-04 17:31:17,527 INFO org.apache.spark.streaming.StreamingContext: Invoking stop(stopGracefully=true) from shutdown hook
2018-01-04 17:31:17,530 INFO org.apache.spark.streaming.scheduler.ReceiverTracker: ReceiverTracker stopped
2018-01-04 17:31:17,531 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopping JobGenerator gracefully
2018-01-04 17:31:17,532 INFO org.apache.spark.streaming.scheduler.JobGenerator: Waiting for all received blocks to be consumed for job generation
2018-01-04 17:31:17,533 INFO org.apache.spark.streaming.scheduler.JobGenerator: Waited for all received blocks to be consumed for job generation
2018-01-04 17:31:17,747 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1515058267000 ms
2018-01-04 17:31:18,302 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1515058268000 ms
2018-01-04 17:31:18,785 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1515058269000 ms
2018-01-04 17:31:19,001 INFO org.apache.spark.streaming.util.RecurringTimer: Stopped timer for JobGenerator after time 1515058279000
2018-01-04 17:31:19,200 INFO org.apache.spark.streaming.scheduler.JobScheduler: Added jobs for time 1515058270000 ms
2018-01-04 17:31:19,207 INFO org.apache.spark.streaming.scheduler.JobGenerator: Stopped generation timer
2018-01-04 17:31:19,207 INFO org.apache.spark.streaming.scheduler.JobGenerator: Waiting for jobs to be processed and checkpoints to be written
2018-01-04 17:31:19,210 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error generating jobs for time 1515058271000 ms
java.lang.IllegalStateException: This consumer has already been closed.
	at org.apache.kafka.clients.consumer.KafkaConsumer.ensureNotClosed(KafkaConsumer.java:1417)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1428)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:929)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.paranoidPoll(DirectKafkaInputDStream.scala:161)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.latestOffsets(DirectKafkaInputDStream.scala:180)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:208)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

It looks like KafkaConsumer close before JobGenerator stop , then I view the source code, 
!https://raw.githubusercontent.com/smdfj/picture/master/spark/JobGenerator.png!



I find graph.stop() will stop Dstream,then close KafkaConsumer,but JobGenerator.eventLoop has not stopped,so error occured.",,choojoyq,gsomogyi,zhaoshijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 18:02:28 UTC 2019,,,,,,,,,,"0|i3oi5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 14:31;srowen;[~tdas] should the order of these two stops be reversed? the comment on stopping the event loop suggests it needs to happen 'first', also.;;;","20/Aug/19 10:40;choojoyq;The issue is still exists, submitted PR in order to fix it.;;;","03/Sep/19 18:02;srowen;Resolved by https://github.com/apache/spark/pull/25511;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated secret volumes in Spark pods when init-containers are used,SPARK-22953,13128410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyinan926,liyinan926,liyinan926,04/Jan/18 05:10,17/May/20 18:24,13/Jul/23 08:45,04/Jan/18 23:36,2.3.0,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,0,,,,"User-specified secrets are mounted into both the main container and init-container (when it is used) in a Spark driver/executor pod, using the {{MountSecretsBootstrap}}. Because {{MountSecretsBootstrap}} always adds the secret volumes to the pod, the same secret volumes get added twice, one when mounting the secrets to the main container, and the other when mounting the secrets to the init-container. See https://github.com/apache-spark-on-k8s/spark/issues/594.",,apachespark,liyinan926,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 23:36:02 UTC 2018,,,,,,,,,,"0|i3ohtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 06:34;apachespark;User 'liyinan926' has created a pull request for this issue:
https://github.com/apache/spark/pull/20148;;;","04/Jan/18 23:06;apachespark;User 'liyinan926' has created a pull request for this issue:
https://github.com/apache/spark/pull/20159;;;","04/Jan/18 23:36;vanzin;Issue resolved by pull request 20159
[https://github.com/apache/spark/pull/20159];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
count() after dropDuplicates() on emptyDataFrame returns incorrect value,SPARK-22951,13128392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fengliu@databricks.com,belbis,belbis,04/Jan/18 02:13,02/Mar/20 07:19,13/Jul/23 08:45,10/Jan/18 22:25,1.6.2,2.1.3,2.2.0,2.3.0,,,,,,2.2.3,2.3.0,,,,SQL,,,,0,correctness,,,"here is a minimal Spark Application to reproduce:

{code}
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}


object DropDupesApp extends App {
  
  override def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
      .setAppName(""test"")
      .setMaster(""local"")
    val sc = new SparkContext(conf)
    val sql = SQLContext.getOrCreate(sc)
    assert(sql.emptyDataFrame.count == 0) // expected
    assert(sql.emptyDataFrame.dropDuplicates.count == 1) // unexpected
  }
  
}
{code}",,apachespark,belbis,dongjoon,lian cheng,liufeng.ee@gmail.com,mgaido,smurakozi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 18:22:24 UTC 2019,,,,,,,,,,"0|i3ohpj:",9223372036854775807,,,,,,,,,,,,,2.2.3,2.3.0,,,,,,,,,"04/Jan/18 11:34;smurakozi;Adding 2.2.0 to affected version:

{code}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_152)
Type in expressions to have them evaluated.
Type :help for more information.

scala> Seq.empty[String].toDF.dropDuplicates.count
res0: Long = 0

scala> spark.emptyDataset[String].dropDuplicates.count
res1: Long = 0

scala> sc.emptyRDD[String].toDF.dropDuplicates.count
res2: Long = 0

scala> spark.emptyDataFrame.dropDuplicates.count
res3: Long = 1
{code};;;","06/Jan/18 11:03;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/20174;;;","10/Jan/18 22:25;lian cheng;Issue resolved by pull request 20174
[https://github.com/apache/spark/pull/20174];;;","03/Jan/19 04:38;dongjoon;Hi, [~fengliu@databricks.com] and [~lian cheng].
Since this is a correctness issue reported on branch-2.2, I'll backport this for Spark 2.2.3.;;;","03/Jan/19 18:22;dongjoon;This is backported to `branch-2.2` via https://github.com/apache/spark/pull/23434 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user classpath first cause no class found exception,SPARK-22950,13128389,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,04/Jan/18 02:08,04/Jan/18 11:14,13/Jul/23 08:45,04/Jan/18 11:14,2.1.0,2.2.1,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"
{code:java}
bin/spark-submit --class some.class.enbaled.hive.Support --master yarn --deploy-mode client  --conf spark.driver.userClassPathFirst=true  the.jar
{code}

spark.driver.userClassPathFirst=true with default builtin hive jars cause classnotfoundexception during initializing hive client
{code:java}
Please make sure that jars for your version of hive and hadoop are included in the paths passed to spark.sql.hive.metastore.jars.
    at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:270)
    at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
    at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
    at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
    at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:194)
    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)
    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:194)
    at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
    at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:193)
    at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
    at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
    at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
    at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
    at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
    at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
    at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
    at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1050)
    ... 29 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
    ... 46 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/session/SessionState
    at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:136)
    ... 51 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.session.SessionState
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:221)
    at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:210)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 52 more
{code}",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 02:23:04 UTC 2018,,,,,,,,,,"0|i3ohov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 02:23;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/20145;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce memory requirement for TrainValidationSplit,SPARK-22949,13128359,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,03/Jan/18 23:21,05/Jan/18 06:45,13/Jul/23 08:45,05/Jan/18 06:45,2.3.0,,,,,,,,,2.3.0,2.4.0,,,,ML,,,,0,,,,"There was a fix in {{ CrossValidator }} to reduce memory usage on the driver, the same patch to be applied to {{ TrainValidationSplit }}.",,apachespark,bago.amirbekian,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22707,SPARK-21535,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 05 06:45:58 UTC 2018,,,,,,,,,,"0|i3ohi7:",9223372036854775807,,,,,,,,,,,,,2.3.0,2.4.0,,,,,,,,,"03/Jan/18 23:26;apachespark;User 'MrBago' has created a pull request for this issue:
https://github.com/apache/spark/pull/20143;;;","05/Jan/18 06:45;josephkb;Resolved by https://github.com/apache/spark/pull/20143;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""SparkPodInitContainer"" shouldn't be in ""rest"" package",SPARK-22948,13128338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,vanzin,vanzin,vanzin,03/Jan/18 22:25,17/May/20 18:26,13/Jul/23 08:45,04/Jan/18 23:00,2.3.0,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,0,,,,"Just noticed while playing with the code that this class is in {{org.apache.spark.deploy.rest.k8s}}; ""rest"" doesn't make sense here since there's no REST server (and it's the only class in there, too).",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 23:00:47 UTC 2018,,,,,,,,,,"0|i3ohdj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 19:05;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20156;;;","04/Jan/18 23:00;vanzin;Issue resolved by pull request 20156
[https://github.com/apache/spark/pull/20156];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Recursive withColumn calls cause org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB",SPARK-22946,13128259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hchopra,hchopra,03/Jan/18 15:27,11/Jan/18 10:08,13/Jul/23 08:45,11/Jan/18 10:06,2.2.0,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Recursive calls to withColumn, for updating the same column causes _org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB
_

This can be reproduced in Spark 1.x and also in the latest version we are using (Spark 2.2.0) 
code to reproduce: 

import org.apache.spark.sql.functions._
var df =sc.parallelize(Seq((""123"",""CustOne""),(""456"",""CustTwo""))).toDF(""ID"",""CustName"")
(1 to 20).foreach(x => {
df = df.withColumn(""ID"", when(col(""ID"") === ""123"", lit(""678"")).otherwise(col(""ID"")))
println(""==== ""+x)
df.show
})

Stack dump:

        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.org$apache$spark$sql$cata
lyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:555)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerato
r.scala:575)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerato
r.scala:572)
        at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java
:3599)
        at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 28 more
Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/cataly
st/expressions/GeneratedClass$SpecificUnsafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)V
"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows
 beyond 64 KB
        at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
        at org.codehaus.janino.CodeContext.write(CodeContext.java:854)
        at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:10242)
        at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:9912)
        at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:9897)
        at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3317)
        at org.codehaus.janino.UnitCompiler.access$8500(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$10.visitLocalVariableAccess(UnitCompiler.java:3285)
        at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:3189)
        at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
        at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3313)
        at org.codehaus.janino.UnitCompiler.access$8000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$10.visitAmbiguousName(UnitCompiler.java:3280)
        at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3138)
        at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
        at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2669)
        at org.codehaus.janino.UnitCompiler.access$4500(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$7.visitAssignment(UnitCompiler.java:2619)
        at org.codehaus.janino.Java$Assignment.accept(Java.java:3405)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2654)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1643)
        at org.codehaus.janino.UnitCompiler.access$1100(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitExpressionStatement(UnitCompiler.java:936)
        at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2097)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
        at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
        at org.codehaus.janino.Java$Block.accept(Java.java:2012)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1753)
        at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
        at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
        at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:3
47)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.org$apache$spark$sql$cata
lyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:550)
        ... 32 more",,cloud_fan,hchopra,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 11 10:08:04 UTC 2018,,,,,,,,,,"0|i3ogvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 10:08;cloud_fan;This should have been fixed during https://issues.apache.org/jira/browse/SPARK-22510;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test suite HiveExternalCatalogVersionsSuite fails on platforms that don't have wget installed,SPARK-22940,13128039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,02/Jan/18 18:23,05/Jan/18 17:59,13/Jul/23 08:45,05/Jan/18 17:59,2.2.1,,,,,,,,,2.3.0,,,,,Tests,,,,0,,,,"On platforms that don't have wget installed (e.g., Mac OS X), test suite org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite throws an exception and aborts:

java.io.IOException: Cannot run program ""wget"": error=2, No such file or directory

HiveExternalCatalogVersionsSuite uses wget to download older versions of Spark for compatibility testing. First it uses wget to find a suitable mirror, and then it uses wget to download a tar file from the mirror.

There are several ways to fix this (in reverse order of difficulty of implementation)

1. Require Mac OS X users to install wget if they wish to run unit tests (or at the very least if they wish to run HiveExternalCatalogVersionsSuite). Also, update documentation to make this requirement explicit.
2. Fall back on curl when wget is not available.
3. Use an HTTP library to query for a suitable mirror and download the tar file.

Number 2 is easy to implement, and I did so to get the unit test to run. But it relies on another external program if wget is not installed.

Number 3 is probably slightly more complex to implement and requires more corner-case checking (e.g, redirects, etc.).

",MacOS Sierra 10.12.6,apachespark,bersprockets,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 05 17:59:33 UTC 2018,,,,,,,,,,"0|i3ofjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/18 18:47;srowen;Agreed. Many other scripts require wget, though at least one will use either. The easiest option is just to document that you need wget, which is simple enough. I forget whether I have it via XCode tools or brew, but, brew install wget should always be sufficient.

Using a library is just fine too and probably more desirable within the test code.

Feel free to open a PR for any of the options, I'd say.;;;","04/Jan/18 04:54;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/20147;;;","05/Jan/18 17:59;vanzin;Issue resolved by pull request 20147
[https://github.com/apache/spark/pull/20147];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make optional clauses order insensitive for CREATE TABLE SQL statement,SPARK-22934,13127845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,01/Jan/18 07:02,03/Jan/18 14:11,13/Jul/23 08:45,03/Jan/18 14:11,2.2.1,,,,,,,,,2.3.0,,,,,SQL,,,,0,,,,"Each time, when I write a complex Create Table statement, I have to open the .g4 file to find the EXACT order of clauses in CREATE TABLE statement. When the order is not right, I will get A strange confusing error message generated from ALTR4. 
{noformat}
CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db_name.]table_name
    [(col_name1 col_type1 [COMMENT col_comment1], ...)]
    USING datasource
    [OPTIONS (key1=val1, key2=val2, ...)]
    [PARTITIONED BY (col_name1, col_name2, ...)]
    [CLUSTERED BY (col_name3, col_name4, ...) INTO num_buckets BUCKETS]
    [LOCATION path]
    [COMMENT table_comment]
    [TBLPROPERTIES (key1=val1, key2=val2, ...)]
    [AS select_statement]
{noformat}

The proposal is to make the following clauses order insensitive. 
{noformat}
    [OPTIONS (key1=val1, key2=val2, ...)]
    [PARTITIONED BY (col_name1, col_name2, ...)]
    [CLUSTERED BY (col_name3, col_name4, ...) INTO num_buckets BUCKETS]
    [LOCATION path]
    [COMMENT table_comment]
    [TBLPROPERTIES (key1=val1, key2=val2, ...)]
{noformat}

The same idea is also applicable to Create Hive Table. 
{noformat}
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
    [(col_name1[:] col_type1 [COMMENT col_comment1], ...)]
    [COMMENT table_comment]
    [PARTITIONED BY (col_name2[:] col_type2 [COMMENT col_comment2], ...)]
    [ROW FORMAT row_format]
    [STORED AS file_format]
    [LOCATION path]
    [TBLPROPERTIES (key1=val1, key2=val2, ...)]
    [AS select_statement]
{noformat}

The proposal is to make the following clauses order insensitive. 
{noformat}
    [COMMENT table_comment]
    [PARTITIONED BY (col_name2[:] col_type2 [COMMENT col_comment2], ...)]
    [ROW FORMAT row_format]
    [STORED AS file_format]
    [LOCATION path]
    [TBLPROPERTIES (key1=val1, key2=val2, ...)]
{noformat}
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 01 07:07:03 UTC 2018,,,,,,,,,,"0|i3oecn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,"01/Jan/18 07:07;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20133;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
