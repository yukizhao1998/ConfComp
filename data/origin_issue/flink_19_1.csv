Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Use reserved IP as unrouteable IP in RestClientTest,FLINK-12646,13235954,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,victor-wong,victor-wong,victor-wong,28/May/19 09:45,12/Jun/19 07:48,13/Jul/23 08:05,11/Jun/19 13:00,,,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / REST,,28/May/19 00:00,0,pull-request-available,,,"In
{code:java}
org.apache.flink.runtime.rest.RestClientTest#testConnectionTimeout
{code}
, we use a ""unroutableIp"" with a value of  ""10.255.255.1"" for test.

But sometimes this IP is reachable in a private network of a company, which is the case for me. As a result, this test failed with a following exception: 

 
{code:java}
java.lang.AssertionError: Expected: an instance of org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException but: <org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.255.255.1:80> is a org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.junit.Assert.assertThat(Assert.java:956) at org.junit.Assert.assertThat(Assert.java:923) at org.apache.flink.runtime.rest.RestClientTest.testConnectionTimeout(RestClientTest.java:76) ...
{code}
 

 

Can we change the `unroutableIp` to a reserved IP address, i.e ""240.0.0.0"", which is described as  _Reserved for future use_ in [wikipedia|https://en.wikipedia.org/wiki/Reserved_IP_addresses] 

Or change the assertion? ",,trohrmann,victor-wong,,,,,,,,,,,,,,,,,"jiasheng55 commented on pull request #8663: [FLINK-12646][runtime] Change the test IP of RestClientTest to 240.0.0.0
URL: https://github.com/apache/flink/pull/8663
 
 
   
   ## What is the purpose of the change
   
   Fix broken tests of RestClientTest
   
   ## Brief change log
   
   Change the test IP of RestClientTest from 10.255.255.1 to 240.0.0.0.
   
   
   ## Verifying this change
   
   
   This change is already covered by existing tests, such as `org.apache.flink.runtime.rest.RestClientTest#testConnectionTimeout`.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jun/19 22:15;githubbot;600","tillrohrmann commented on pull request #8663: [FLINK-12646][runtime] Change the test IP of RestClientTest to 240.0.0.0
URL: https://github.com/apache/flink/pull/8663
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/19 12:59;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 11 13:00:13 UTC 2019,,,,,,,,,,"0|z0350w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/19 12:58;trohrmann;Sounds like a good idea to me [~victor-wong]. Do you wanna contribute this fix?;;;","07/Jun/19 22:19;victor-wong;I'd love to, [https://github.com/apache/flink/pull/8663];;;","11/Jun/19 13:00;trohrmann;Fixed via
1.9.0: 7ee00fd31c5d373d376fd4d8aa498fc8fe267edb
1.8.1: 5dec698fcdf5503b608d6cc0757c40054a46b2f9
1.7.3: dce00e173ac6cda9541d4378dcddeb827857c44c;;;",,,,,,,,,,,,,,,,,,,,,
OutputBufferPoolUsageGauge can fail with NPE,FLINK-12642,13235932,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,chesnay,chesnay,28/May/19 08:07,02/Oct/19 17:48,13/Jul/23 08:05,31/May/19 20:25,1.9.0,,,,,,,,1.9.0,,,,Runtime / Metrics,Runtime / Network,,0,pull-request-available,,,"The result partition metrics are initialized before {{ResultPartitiion#setup}} was called. If a reporter tries to access a {{OutputBufferPoolUsageGauge}} in between it will fail with an NPE since the bufferpool of the partition is still null.

{code}
2019-05-27 14:49:47,031 WARN  org.apache.flink.runtime.metrics.MetricRegistryImpl           - Error while reporting metrics
java.lang.NullPointerException
	at org.apache.flink.runtime.io.network.metrics.OutputBufferPoolUsageGauge.getValue(OutputBufferPoolUsageGauge.java:41)
	at org.apache.flink.runtime.io.network.metrics.OutputBufferPoolUsageGauge.getValue(OutputBufferPoolUsageGauge.java:27)
	at org.apache.flink.metrics.slf4j.Slf4jReporter.tryReport(Slf4jReporter.java:114)
	at org.apache.flink.metrics.slf4j.Slf4jReporter.report(Slf4jReporter.java:80)
	at org.apache.flink.runtime.metrics.MetricRegistryImpl$ReporterTask.run(MetricRegistryImpl.java:436)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:514)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:300)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
	at java.base/java.lang.Thread.run(Thread.java:844)
{code}",,aitozi,azagrebin,zjwang,,,,,,,,,,,,,,,,"azagrebin commented on pull request #8574: [FLINK-12642][network][metrics] Fix In/OutputBufferPoolUsageGauge failure with NullPointerException if BufferPool has not been inited yet
URL: https://github.com/apache/flink/pull/8574
 
 
   ## What is the purpose of the change
   
   The result partition metrics are initialised before `ResultPartitiion#setup` was called. If a reporter tries to access a In/OutputBufferPoolUsageGauge in between it will fail with an `NullPointerException` since the `BufferPool` of the partition is still `null`. Currently, the quick fix is to return zero metrics until the `BufferPool` is initialised. When we have a single-threaded access from `Task#run`, we can merge partition/gate create and setup then it should not be the case anymore.
   
   ## Brief change log
   
   Check the partition `BufferPool` is not `null` in `In/OutputBufferPoolUsageGauge#getValue` and return zero metrics if it is `null`.
   
   ## Verifying this change
   
   Trivial fix.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/May/19 10:16;githubbot;600","zentol commented on pull request #8574: [FLINK-12642][network][metrics] Fix In/OutputBufferPoolUsageGauge failure with NullPointerException if BufferPool has not been inited yet
URL: https://github.com/apache/flink/pull/8574
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/May/19 20:24;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 31 20:25:10 UTC 2019,,,,,,,,,,"0|z034w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/19 08:08;chesnay;[~zjwang] [~azagrebin];;;","28/May/19 09:11;zjwang;Thanks for reporting this [~chesnay]!

I think we could check whether the `BufferPool` is null before calling detail getter methods in `OutputBufferPoolUsageGauge`.

If you agree I could take it to submit the PR.;;;","28/May/19 09:17;chesnay;Checking for null is a fair solution for the time being, but I'd prefer it metrics didn't have access to uninitialized resources, ;;;","29/May/19 20:17;azagrebin; When we have a single-threaded access from Task.run, we can merge partition/gate create and setup then it should not be case anymore.;;;","30/May/19 02:13;zjwang;Yes, that is the ideal way which we confirmed before to make creation and setup together. If the task thread model is already ready, we could try this way.;;;","31/May/19 20:25;chesnay;master: bc16485cc89fbe5b0dd1534737d0b5cd1ced885b;;;",,,,,,,,,,,,,,,,,,
REST API stability test does not fail on compatible modifications,FLINK-12636,13235816,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/19 12:45,28/May/19 17:06,13/Jul/23 08:05,28/May/19 17:06,1.9.0,,,,,,,,1.9.0,,,,Runtime / REST,Tests,,0,,,,The stability test does not fail properly if the API was modified in a compatible way. The test should still fail until the snapshot was regenerated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 28 17:06:00 UTC 2019,,,,,,,,,,"0|z0346o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/19 17:06;chesnay;master: 4fe936dc8f65d57645eea908337653c76d83f400;;;",,,,,,,,,,,,,,,,,,,,,,,
REST API stability test does not cover jar upload,FLINK-12635,13235812,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/19 12:40,28/May/19 17:06,13/Jul/23 08:05,28/May/19 17:06,1.9.0,,,,,,,,1.9.0,,,,Runtime / REST,Tests,,0,pull-request-available,,,"The stability test for the REST API currently resides in flink-runtime, but the jar upload is handled via an extension in runtime-web. Since this extension cannot be loaded in flink-runtime it isn't covered by the test.",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8554: [FLINK-12635][rest] Cover jar upload in stability test
URL: https://github.com/apache/flink/pull/8554
 
 
   ## What is the purpose of the change
   
   Extends the stability test to also check for new calls being added (to force developers to update the snapshot), and moves the test to `flink-runtime-web` to also cover the jar upload.
   
   
   ## Brief change log
   
   * also check for new calls in stability test
   * regenerate snapshot (one call was already missing)
   * move test to flink-runtime-web
   * regenerate snapshot
   
   ## Verifying this change
   
   You can revert the snapshot changes, which will cause the test to fail. Instructions will be printed to update the snapshots; upon following them the test will succeed again.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/May/19 13:05;githubbot;600","zentol commented on pull request #8554: [FLINK-12635][rest] Cover jar upload in stability test
URL: https://github.com/apache/flink/pull/8554
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/May/19 17:05;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 28 17:06:08 UTC 2019,,,,,,,,,,"0|z0345s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/19 17:06;chesnay;master: 3558bac8b2cd9609642414a0bf96d622653d144f;;;",,,,,,,,,,,,,,,,,,,,,,,
Scala process function example doesn't compile,FLINK-12622,13235566,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chaiyq,chaiyq,chaiyq,25/May/19 00:53,23/Oct/19 10:52,13/Jul/23 08:05,23/Oct/19 10:52,1.8.0,,,,,,,,1.10.0,,,,API / DataStream,Documentation,,0,pull-request-available,,,"The process function defined as below in windows.md : 

{quote}class MyProcessWindowFunction extends ProcessWindowFunction[(String, Long), String, String, TimeWindow] {

  def process(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]): () = {
    var count = 0L
    for (in <- input) {
      count = count + 1
    }
    out.collect(s""Window ${context.window} count: $count"")
  }
}{quote}

The process function defined in ProcessWindowFunction  has a return vlue of Unit , But the override in MyProcessWindowFunction doesn't match it well. When compiling MyProcessWindowFunction , it comes an error like the following :

{quote}Error:(37, 109) '=>' expected but '=' found.
  def process(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]) : ()  = {{quote}",,chaiyq,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8540: [FLINK-12622][doc]Change the return value for process function in MyProcessWindowFunction
URL: https://github.com/apache/flink/pull/8540
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Oct/19 10:52;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 10:52:45 UTC 2019,,,,,,,,,,"0|z032n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/19 01:41;chaiyq;[https://github.com/apache/flink/pull/8540/commits];;;","23/Oct/19 10:52;chesnay;master: 048ddc104d6917170a701a5ea4a6e83a50817c9d;;;",,,,,,,,,,,,,,,,,,,,,,
Correct the flink pom `artifactId` config and scala-free check logic,FLINK-12602,13235106,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunjincheng121,sunjincheng121,sunjincheng121,23/May/19 10:47,02/Oct/19 17:48,13/Jul/23 08:05,18/Aug/19 13:25,1.9.0,,,,,,,,1.9.0,,,,Build System,,,0,pull-request-available,,,"I find a shell issue in `verify_scala_suffixes.sh`(line 145) as follows:
{code}
grep ""${module}_\d\+\.\d\+</artifactId>"" ""{}""
{code}
This code want to find out all modules that the module's `artifactId`  with a `scala_binary_version` suffix. 
but the problem is our all `artifactId` value is in the pattern of `XXX_${scala.binary.version}`, such as:
{code}
<artifactId>flink-tests_${scala.binary.version}</artifactId>
{code}
then the result always empty, so this check did not take effect.

I have already initiated a discussion of the issue. Please check the Mail thread here for details.
http://mail-archives.apache.org/mod_mbox/flink-dev/201905.mbox/%3CCAJSjTKw+8McSC0FvNeyaOVL_TTrr_UUOsX-TFGxj5GfQp1AUtQ@mail.gmail.com%3E",,aljoscha,sunjincheng121,trohrmann,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #8563: [FLINK-12602][travis] Correct the flink pom `artifactId` config and s…
URL: https://github.com/apache/flink/pull/8563
 
 
   ## What is the purpose of the change
   I find a shell issue in `verify_scala_suffixes.sh`(line 145) as follows:
   
   `grep ""${module}_\d\+\.\d\+</artifactId>"" ""{}""`
   
   This code want to find out all modules that the module's `artifactId`  with a `scala_binary_version` suffix. 
   but the problem is our all `artifactId` value is in the pattern of `XXX_${scala.binary.version}`, such as:
   
   `<artifactId>flink-tests_${scala.binary.version}</artifactId>`
   
   then the result always empty, so this check did not take effect.
   
   So, we need to correct `artifactId `of some of the modules and correct the check logic for scala-free.
   
   ## Brief change log
     - remove the scala version suffix for connector-hive and queryable-state-client-java
     - add the scala dependencies for table-api-scala and flink-sql-connectors
     - correct the scala-free check logic in `verify_scala_suffixes.sh`
   
   NOTE:
   We have two ways handling of the connector:
    1. Improve the script to check any (compile) dependencies with a scala-suffix(which we mentioned above).
    2. Add the `flink-streaming-java` dependency wich  `provided`  scope for the corresponding connectors.
   
   For approach 1, we should add check logic:
    1. The command of `dependency:tree` should add an option: ` -Dincludes=org.apache.flink:_2.1::  ` such as `org.apache.flink:flink-streaming-java_2.11`.
     2. The command of `grep ` also need to add the logic: `E ""org.scala-lang| org.apache.flink:[^:]+_2\.1[0-9]""` , also for test `grep --invert-match ""org.apache.flink:[^:]_2\.1[0-9]:.:.*:test""`.
   
   For approach 2, we should add the dependency of `link-streaming-java` for `flink-sql-connector-elasticsearch6 flink-sql-connector-kafka flink-sql-connector-kafka-0.10 flink-sql-connector-kafka-0.11 flink-sql-connector-kafka-0.9`.  
   
   For now, I think the change logic in approach 1 is a bit complex(a lot of filtering processing logic). So, I prefer the approach 2, due to even we add some new module in the future we should well know whether we should add the scala-suffix for the `artifactId`, then manually add dependencies on for the scala in the pom. 
   
   So, in this PR I add the dependency of `link-streaming-java` for `flink-sql-connectors`.
   
   ## Verifying this change
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes)
     - If yes, how is the feature documented? (docs)
       1. correct the artifactId in `queryable_state.md/queryable_state.zh.md`.
     
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/May/19 10:11;githubbot;600","sunjincheng121 commented on pull request #8563: [FLINK-12602][travis] Correct the flink pom `artifactId` config and s…
URL: https://github.com/apache/flink/pull/8563
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 01:22;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 13:25:49 UTC 2019,,,,,,,,,,"0|z02zsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/19 10:51;chesnay;Mirroring my comment from the ML; there's no reason to keep that thread going now that we have a JIRA.

There are 3 modules that have a suffix, which don't need it:

 * flink-connector-hive

 * flink-queryable-state-client-java

 * flink-table-api-scala

All other modules do need it since they have dependencies with a scala-suffix (mostly on runtime and streaming-java).

Your change does make sense to me, but there's likely another issue in the preceding logic that determines which module is scala-free.
flink-tests for example should not be considered scala-free, since it relies on flink-runtime which contains scala and hence the scala-lang dependencies, but it apparently is given that your change detects something. ;;;","23/May/19 11:05;sunjincheng121;Thank you for confirming this is an issue，and thanks a lot for your double check. Due to the script check logic is incorrect, most of the outputs are incorrect.

You are right and I just pointed out the problem of the scala-free check and not mentioned the final solution which I think we should discuss it. 

I tried to understand you and have modified the script a bit and have got the same results as you mentioned(In my local).
The main changes are as follows: 
1) Adds test modules which should be checked: !flink-fs-tests,!flink-yarn-tests,!flink-tests
2) Marks the modules as infected which depend on scala trasitively or depend on modules suffixed with `_{scala_version}`

I think we should discuss the rule of how to check whether a module is scala-free or not.
If I understand you correctly, the rule in your mind may be as follows:

1) All the modules should check the dependencies(excluding the dependencies introduced by the test code). Regarding to modules flink-fs-tests,
   flink-yarn-tests and flink-tests, the dependencies introduced by the test code should also be checked.
2) The checking rule is whether a module depends on scala trasitively or depends on modules suffixed with `_{scala_version}`.
Following the above rules, we can get results you mentioned(Only 3 modules with incorrect artifact id).

Open question:

Currently, all the test code are also released into the repository, such as http://central.maven.org/maven2/org/apache/flink/flink-avro/1.8.0/flink-avro-1.8.0-tests.jar.
Users can also depend on these jars. My question is why we need to check the test dependencies for modules flink-fs-tests, flink-yarn-tests and flink-tests, 
but not check the test dependencies for other modules?

The solution:
If we follow the rule you mentioned, the change is as follows:
1) Correct the check logic for the script
2) Correct the artifact id for modules:  flink-connector-hive, flink-queryable-state-client-java
3) Add the scala dependencies for the module flink-table-api-scala due we plan to add the scala code(I discussed with Timo and Aljoscha)

If we should check other test code for other modules, maybe we need more changes. But it depends on the above open question.;;;","23/May/19 11:55;chesnay;Ha, you got me there. flink-tests, flink-fs-tests and flink-yarn-tests also don't need a suffix.

The purpose of the scala-suffix is to ensure that _users_ depending on a module do not inadvertently pull in scala dependencies.
So, if you depend on module ""MyModule"", then there should be no transitive dependency pulled in that depends on a specific scala version.
If it did you'd have no control over which version is actually pulled in.

Dependencies that have a {{test}} scope however are _not_ transitive and never pulled in by the user.
That's why we can ignore those. This is why flink-avro does not have a suffix, and why flink-tests in fact does not need one.

Now, let's consider what is wrong with the script.
As you laid out correctly, we do not check correctly for the scala suffix in scala-free modules. With this change alone 11 modules are marked as having wrong suffixes:

# flink-connector-hive
# flink-fs-tests
# flink-queryable-state-client-java
# flink-sql-connector-elasticsearch6
# flink-sql-connector-kafka
# flink-sql-connector-kafka-0.10
# flink-sql-connector-kafka-0.11
# flink-sql-connector-kafka-0.9
# flink-table-api-scala
# flink-tests
# flink-yarn-tests

fs-tests, tests, yarn-tests do not need a suffix because they only have test dependencies.

table-api-scala does not need a suffix _right now_ but will obviously need it in the future since it is supposed to contain scala code. As you pointed out correctly, let's add the scala dependencies now to future-proof this puppy.

connector-hive and queryable-state-client-java are genuinely free of scala dependencies, and we can remove the suffix as is.

The sql-connectors _do_ all need a suffix since they all rely on connectors which also have one, which need it themselves since they rely on runtime/streaming-java.
My suspicion is that these are tagged because the script works against dependency-reduced poms, where provided dependencies are removed. runtime/streaming-java are usually marked as provided, so they and their transitive dependencies, including scala, are omitted from the pom.
As a result, as you pointed out, we not only have to check for scala dependencies but for any (compile) dependencies with a scala-suffix.;;;","27/May/19 11:02;sunjincheng121;Sounds good [~chesnay]!

I have a few updates on the handling of the connector, we have two ways to deal with it:
 # Improve the script to check any (compile) dependencies with a scala-suffix(which we mentioned above).
 # Add the `flink-streaming-java` dependency wich  `provided`  scope for the corresponding connectors.

For approach 1, we should add check logic:
 * The command of `dependency:tree` should add an option: ` -Dincludes=org.apache.flink:*_2.1*::  ` such as `org.apache.flink:flink-streaming-java_2.11`.
 * The command of `grep ` also need to add the logic: `-E ""org.scala-lang|- org.apache.flink:[^:]+_2\.1[0-9]""` , also for test `grep --invert-match ""org.apache.flink:[^:]*_2\.1[0-9]:.*:.*:test""`.

For approach 2, we should add the dependency of `link-streaming-java` for `flink-sql-connector-elasticsearch6 flink-sql-connector-kafka flink-sql-connector-kafka-0.10 flink-sql-connector-kafka-0.11 flink-sql-connector-kafka-0.9`.  And I have prepared the changs [here|https://github.com/sunjincheng121/flink/pull/96]

For now, I think the change logic in approach 1 is a bit complex(a lot of filtering processing logic). So, I prefer the approach 2, due to even we add some new module in the future we should well know whether we should add the scala-suffix for the `artifactId`, then manually add dependencies on for the scala in the pom. 

What do you think?;;;","29/May/19 10:12;sunjincheng121;I have open the PR, I appreciate if you can have a look at it :);;;","18/Aug/19 13:25;trohrmann;Fixed via https://github.com/apache/flink/commit/57621703b7342442f3a0ec62315ce1cefa0a8287;;;",,,,,,,,,,,,,,,,,,
Flink-shaded's shade-sources profile does not work anymore,FLINK-12598,13235064,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,nkruber,nkruber,23/May/19 07:23,14/Nov/19 10:28,13/Jul/23 08:05,13/Nov/19 14:51,shaded-7.0,,,,,,,,shaded-9.0,,,,BuildSystem / Shaded,,,0,pull-request-available,,,"{code}
> mvn clean package -Pshade-sources
...
[INFO] --- maven-shade-plugin:3.0.0:shade (shade-flink) @ flink-shaded-hadoop-2 ---
[INFO] Excluding org.apache.commons:commons-compress:jar:1.18 from the shaded jar.
[INFO] Excluding org.apache.avro:avro:jar:1.8.2 from the shaded jar.
[INFO] Including org.codehaus.jackson:jackson-core-asl:jar:1.9.13 in the shaded jar.
[INFO] Including org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13 in the shaded jar.
[INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.7 from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.4 from the shaded jar.
[INFO] Excluding org.tukaani:xz:jar:1.5 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.7 from the shaded jar.
[INFO] Including org.apache.hadoop:hadoop-common:jar:2.4.1 in the shaded jar.
[INFO] Including org.apache.hadoop:hadoop-annotations:jar:2.4.1 in the shaded jar.
[INFO] Including com.google.guava:guava:jar:11.0.2 in the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.3.1 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-math3:jar:3.5 from the shaded jar.
[INFO] Excluding xmlenc:xmlenc:jar:0.52 from the shaded jar.
[INFO] Including commons-httpclient:commons-httpclient:jar:3.1 in the shaded jar.
[INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
[INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
[INFO] Excluding commons-net:commons-net:jar:3.1 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
[INFO] Excluding javax.servlet:servlet-api:jar:2.5 from the shaded jar.
[INFO] Excluding commons-el:commons-el:jar:1.0 from the shaded jar.
[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
[INFO] Excluding log4j:log4j:jar:1.2.17 from the shaded jar.
[INFO] Including net.java.dev.jets3t:jets3t:jar:0.9.0 in the shaded jar.
[INFO] Including org.apache.httpcomponents:httpclient:jar:4.5.3 in the shaded jar.
[INFO] Including org.apache.httpcomponents:httpcore:jar:4.4.6 in the shaded jar.
[INFO] Excluding com.jamesmurty.utils:java-xmlbuilder:jar:0.4 from the shaded jar.
[INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
[INFO] Excluding commons-configuration:commons-configuration:jar:1.7 from the shaded jar.
[INFO] Excluding commons-digester:commons-digester:jar:1.8.1 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-log4j12:jar:1.7.15 from the shaded jar.
[INFO] Including com.google.protobuf:protobuf-java:jar:2.5.0 in the shaded jar.
[INFO] Including org.apache.hadoop:hadoop-auth:jar:2.4.1 in the shaded jar.
[INFO] Excluding com.jcraft:jsch:jar:0.1.42 from the shaded jar.
[INFO] Including com.google.code.findbugs:jsr305:jar:1.3.9 in the shaded jar.
[WARNING] Could not get sources for com.google.code.findbugs:jsr305:jar:1.3.9:compile
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] flink-shaded 7.0 ................................... SUCCESS [  0.771 s]
[INFO] flink-shaded-force-shading 7.0 ..................... SUCCESS [  0.951 s]
[INFO] flink-shaded-asm-6 6.2.1-7.0 ....................... SUCCESS [  1.469 s]
[INFO] flink-shaded-guava-18 18.0-7.0 ..................... SKIPPED
[INFO] flink-shaded-netty-4 4.1.32.Final-7.0 .............. SKIPPED
[INFO] flink-shaded-netty-tcnative-dynamic 2.0.25.Final-7.0 SUCCESS [  2.195 s]
[INFO] flink-shaded-jackson-parent 2.9.8-7.0 .............. SUCCESS [  0.161 s]
[INFO] flink-shaded-jackson-2 2.9.8-7.0 ................... SKIPPED
[INFO] flink-shaded-jackson-module-jsonSchema-2 2.9.8-7.0 . SKIPPED
[INFO] flink-shaded-hadoop-2 2.4.1-7.0 .................... FAILURE [  2.597 s]
[INFO] flink-shaded-hadoop-2-uber 2.4.1-7.0 ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3.499 s (Wall Clock)
[INFO] Finished at: 2019-05-23T09:22:04+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.: NullPointerException -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-shaded-hadoop-2
{code}",,nkruber,,,,,,,,,,,,,,,,,,"NicoK commented on pull request #63: [FLINK-12598] update maven-shade-plugin to version 3.1.1
URL: https://github.com/apache/flink-shaded/pull/63
 
 
   This fixes a potential NPE (seems to appear only with some maven versions) when
   trying to shade sources with a dependency being unavailable.
   
   Steps to reproduce the previous error (using maven 3.6.1):
   ```
   > mvn clean package -Pshade-sources
   ...
   [INFO] --- maven-shade-plugin:3.0.0:shade (shade-flink) @ flink-shaded-hadoop-2 ---
   [INFO] Excluding org.apache.commons:commons-compress:jar:1.18 from the shaded jar.
   [INFO] Excluding org.apache.avro:avro:jar:1.8.2 from the shaded jar.
   [INFO] Including org.codehaus.jackson:jackson-core-asl:jar:1.9.13 in the shaded jar.
   [INFO] Including org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13 in the shaded jar.
   [INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.7 from the shaded jar.
   [INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.4 from the shaded jar.
   [INFO] Excluding org.tukaani:xz:jar:1.5 from the shaded jar.
   [INFO] Excluding org.slf4j:slf4j-api:jar:1.7.7 from the shaded jar.
   [INFO] Including org.apache.hadoop:hadoop-common:jar:2.4.1 in the shaded jar.
   [INFO] Including org.apache.hadoop:hadoop-annotations:jar:2.4.1 in the shaded jar.
   [INFO] Including com.google.guava:guava:jar:11.0.2 in the shaded jar.
   [INFO] Excluding commons-cli:commons-cli:jar:1.3.1 from the shaded jar.
   [INFO] Excluding org.apache.commons:commons-math3:jar:3.5 from the shaded jar.
   [INFO] Excluding xmlenc:xmlenc:jar:0.52 from the shaded jar.
   [INFO] Including commons-httpclient:commons-httpclient:jar:3.1 in the shaded jar.
   [INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
   [INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
   [INFO] Excluding commons-net:commons-net:jar:3.1 from the shaded jar.
   [INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
   [INFO] Excluding javax.servlet:servlet-api:jar:2.5 from the shaded jar.
   [INFO] Excluding commons-el:commons-el:jar:1.0 from the shaded jar.
   [INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
   [INFO] Excluding log4j:log4j:jar:1.2.17 from the shaded jar.
   [INFO] Including net.java.dev.jets3t:jets3t:jar:0.9.0 in the shaded jar.
   [INFO] Including org.apache.httpcomponents:httpclient:jar:4.5.3 in the shaded jar.
   [INFO] Including org.apache.httpcomponents:httpcore:jar:4.4.6 in the shaded jar.
   [INFO] Excluding com.jamesmurty.utils:java-xmlbuilder:jar:0.4 from the shaded jar.
   [INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
   [INFO] Excluding commons-configuration:commons-configuration:jar:1.7 from the shaded jar.
   [INFO] Excluding commons-digester:commons-digester:jar:1.8.1 from the shaded jar.
   [INFO] Excluding org.slf4j:slf4j-log4j12:jar:1.7.15 from the shaded jar.
   [INFO] Including com.google.protobuf:protobuf-java:jar:2.5.0 in the shaded jar.
   [INFO] Including org.apache.hadoop:hadoop-auth:jar:2.4.1 in the shaded jar.
   [INFO] Excluding com.jcraft:jsch:jar:0.1.42 from the shaded jar.
   [INFO] Including com.google.code.findbugs:jsr305:jar:1.3.9 in the shaded jar.
   [WARNING] Could not get sources for com.google.code.findbugs:jsr305:jar:1.3.9:compile
   [INFO] ------------------------------------------------------------------------
   [INFO] Reactor Summary:
   [INFO] 
   [INFO] flink-shaded 7.0 ................................... SUCCESS [  0.771 s]
   [INFO] flink-shaded-force-shading 7.0 ..................... SUCCESS [  0.951 s]
   [INFO] flink-shaded-asm-6 6.2.1-7.0 ....................... SUCCESS [  1.469 s]
   [INFO] flink-shaded-guava-18 18.0-7.0 ..................... SKIPPED
   [INFO] flink-shaded-netty-4 4.1.32.Final-7.0 .............. SKIPPED
   [INFO] flink-shaded-netty-tcnative-dynamic 2.0.25.Final-7.0 SUCCESS [  2.195 s]
   [INFO] flink-shaded-jackson-parent 2.9.8-7.0 .............. SUCCESS [  0.161 s]
   [INFO] flink-shaded-jackson-2 2.9.8-7.0 ................... SKIPPED
   [INFO] flink-shaded-jackson-module-jsonSchema-2 2.9.8-7.0 . SKIPPED
   [INFO] flink-shaded-hadoop-2 2.4.1-7.0 .................... FAILURE [  2.597 s]
   [INFO] flink-shaded-hadoop-2-uber 2.4.1-7.0 ............... SKIPPED
   [INFO] ------------------------------------------------------------------------
   [INFO] BUILD FAILURE
   [INFO] ------------------------------------------------------------------------
   [INFO] Total time:  3.499 s (Wall Clock)
   [INFO] Finished at: 2019-05-23T09:22:04+02:00
   [INFO] ------------------------------------------------------------------------
   [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.: NullPointerException -> [Help 1]
   [ERROR] 
   [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
   [ERROR] Re-run Maven using the -X switch to enable full debug logging.
   [ERROR] 
   [ERROR] For more information about the errors and possible solutions, please read the following articles:
   [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
   [ERROR] 
   [ERROR] After correcting the problems, you can resume the build with the command
   [ERROR]   mvn <goals> -rf :flink-shaded-hadoop-2
   ```
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/May/19 09:11;githubbot;600","NicoK commented on pull request #65: [FLINK-12598] update maven-shade-plugin to version 3.1.1 for the shade-sources profile
URL: https://github.com/apache/flink-shaded/pull/65
 
 
   In contrast to #63, this one only updates the maven-shade-plugin for the shade-sources build probile.
   
   This fixes a potential NPE (seems to appear only with some maven versions) when
   trying to shade sources with a dependency being unavailable.
   
   Steps to reproduce the previous error (using maven 3.6.1):
   ```
   > mvn clean package -Pshade-sources
   ...
   [INFO] --- maven-shade-plugin:3.0.0:shade (shade-flink) @ flink-shaded-hadoop-2 ---
   [INFO] Excluding org.apache.commons:commons-compress:jar:1.18 from the shaded jar.
   [INFO] Excluding org.apache.avro:avro:jar:1.8.2 from the shaded jar.
   [INFO] Including org.codehaus.jackson:jackson-core-asl:jar:1.9.13 in the shaded jar.
   [INFO] Including org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13 in the shaded jar.
   [INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.7 from the shaded jar.
   [INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.4 from the shaded jar.
   [INFO] Excluding org.tukaani:xz:jar:1.5 from the shaded jar.
   [INFO] Excluding org.slf4j:slf4j-api:jar:1.7.7 from the shaded jar.
   [INFO] Including org.apache.hadoop:hadoop-common:jar:2.4.1 in the shaded jar.
   [INFO] Including org.apache.hadoop:hadoop-annotations:jar:2.4.1 in the shaded jar.
   [INFO] Including com.google.guava:guava:jar:11.0.2 in the shaded jar.
   [INFO] Excluding commons-cli:commons-cli:jar:1.3.1 from the shaded jar.
   [INFO] Excluding org.apache.commons:commons-math3:jar:3.5 from the shaded jar.
   [INFO] Excluding xmlenc:xmlenc:jar:0.52 from the shaded jar.
   [INFO] Including commons-httpclient:commons-httpclient:jar:3.1 in the shaded jar.
   [INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
   [INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
   [INFO] Excluding commons-net:commons-net:jar:3.1 from the shaded jar.
   [INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
   [INFO] Excluding javax.servlet:servlet-api:jar:2.5 from the shaded jar.
   [INFO] Excluding commons-el:commons-el:jar:1.0 from the shaded jar.
   [INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
   [INFO] Excluding log4j:log4j:jar:1.2.17 from the shaded jar.
   [INFO] Including net.java.dev.jets3t:jets3t:jar:0.9.0 in the shaded jar.
   [INFO] Including org.apache.httpcomponents:httpclient:jar:4.5.3 in the shaded jar.
   [INFO] Including org.apache.httpcomponents:httpcore:jar:4.4.6 in the shaded jar.
   [INFO] Excluding com.jamesmurty.utils:java-xmlbuilder:jar:0.4 from the shaded jar.
   [INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
   [INFO] Excluding commons-configuration:commons-configuration:jar:1.7 from the shaded jar.
   [INFO] Excluding commons-digester:commons-digester:jar:1.8.1 from the shaded jar.
   [INFO] Excluding org.slf4j:slf4j-log4j12:jar:1.7.15 from the shaded jar.
   [INFO] Including com.google.protobuf:protobuf-java:jar:2.5.0 in the shaded jar.
   [INFO] Including org.apache.hadoop:hadoop-auth:jar:2.4.1 in the shaded jar.
   [INFO] Excluding com.jcraft:jsch:jar:0.1.42 from the shaded jar.
   [INFO] Including com.google.code.findbugs:jsr305:jar:1.3.9 in the shaded jar.
   [WARNING] Could not get sources for com.google.code.findbugs:jsr305:jar:1.3.9:compile
   [INFO] ------------------------------------------------------------------------
   [INFO] Reactor Summary:
   [INFO] 
   [INFO] flink-shaded 7.0 ................................... SUCCESS [  0.771 s]
   [INFO] flink-shaded-force-shading 7.0 ..................... SUCCESS [  0.951 s]
   [INFO] flink-shaded-asm-6 6.2.1-7.0 ....................... SUCCESS [  1.469 s]
   [INFO] flink-shaded-guava-18 18.0-7.0 ..................... SKIPPED
   [INFO] flink-shaded-netty-4 4.1.32.Final-7.0 .............. SKIPPED
   [INFO] flink-shaded-netty-tcnative-dynamic 2.0.25.Final-7.0 SUCCESS [  2.195 s]
   [INFO] flink-shaded-jackson-parent 2.9.8-7.0 .............. SUCCESS [  0.161 s]
   [INFO] flink-shaded-jackson-2 2.9.8-7.0 ................... SKIPPED
   [INFO] flink-shaded-jackson-module-jsonSchema-2 2.9.8-7.0 . SKIPPED
   [INFO] flink-shaded-hadoop-2 2.4.1-7.0 .................... FAILURE [  2.597 s]
   [INFO] flink-shaded-hadoop-2-uber 2.4.1-7.0 ............... SKIPPED
   [INFO] ------------------------------------------------------------------------
   [INFO] BUILD FAILURE
   [INFO] ------------------------------------------------------------------------
   [INFO] Total time:  3.499 s (Wall Clock)
   [INFO] Finished at: 2019-05-23T09:22:04+02:00
   [INFO] ------------------------------------------------------------------------
   [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.: NullPointerException -> [Help 1]
   [ERROR] 
   [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
   [ERROR] Re-run Maven using the -X switch to enable full debug logging.
   [ERROR] 
   [ERROR] For more information about the errors and possible solutions, please read the following articles:
   [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
   [ERROR] 
   [ERROR] After correcting the problems, you can resume the build with the command
   [ERROR]   mvn <goals> -rf :flink-shaded-hadoop-2
   ```
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/May/19 09:18;githubbot;600","zentol commented on pull request #65: [FLINK-12598] update maven-shade-plugin to version 3.1.1 for the shade-sources profile
URL: https://github.com/apache/flink-shaded/pull/65
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/May/19 09:57;githubbot;600","zentol commented on pull request #63: [FLINK-12598] update maven-shade-plugin to version 3.1.1
URL: https://github.com/apache/flink-shaded/pull/63
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 10:28;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 13 14:51:52 UTC 2019,,,,,,,,,,"0|z02zjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/19 08:44;chesnay;MIrroring my comment from the ML:

Worked for me; could you try again without parallel builds?

Which maven version are you using? ;;;","23/May/19 08:57;nkruber;Disabling parallel builds did not help but it could indeed be a bug in maven. It could be related to MSHADE-247 which, however, should have been fixed with maven 3.1. Seems related to this line:
{code}
[WARNING] Could not get sources for com.google.code.findbugs:jsr305:jar:1.3.9:compile
{code}

{code}
> /usr/bin/mvn -v
Apache Maven 3.6.1 (d66c9c0b3152b2e69ee9bac180bb8fcc8e6af555; 2019-04-04T21:00:29+02:00)
Maven home: /usr/share/java/maven
Java version: 1.8.0_212, vendor: IcedTea, runtime: /usr/lib64/jvm/java-1.8.0-openjdk-1.8.0/jre
Default locale: en_GB, platform encoding: UTF-8
OS name: ""linux"", version: ""5.1.3-1-default"", arch: ""amd64"", family: ""unix""
{code}

Detailed stacktrace:
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.: NullPointerException -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call (MultiThreadedBuilder.java:202)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call (MultiThreadedBuilder.java:198)
    at java.util.concurrent.FutureTask.run (FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)
    at java.util.concurrent.FutureTask.run (FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)
    at java.lang.Thread.run (Thread.java:748)
Caused by: org.apache.maven.plugin.PluginExecutionException: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:148)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call (MultiThreadedBuilder.java:202)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call (MultiThreadedBuilder.java:198)
    at java.util.concurrent.FutureTask.run (FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)
    at java.util.concurrent.FutureTask.run (FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)
    at java.lang.Thread.run (Thread.java:748)
Caused by: java.lang.NullPointerException
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.resolveArtifactSources (ShadeMojo.java:730)
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.processArtifactSelectors (ShadeMojo.java:627)
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute (ShadeMojo.java:425)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call (MultiThreadedBuilder.java:202)
    at org.apache.maven.lifecycle.internal.builder.multithreaded.MultiThreadedBuilder$1.call (MultiThreadedBuilder.java:198)
    at java.util.concurrent.FutureTask.run (FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call (Executors.java:511)
    at java.util.concurrent.FutureTask.run (FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)
    at java.lang.Thread.run (Thread.java:748)
{code};;;","23/May/19 09:01;chesnay;On 3.2.5 maven maven keeps going even if sources could not be found.;;;","23/May/19 09:01;nkruber;ok, that actually gives a clue: in our `pom.xml` we depend on maven-shade-plugin 3.1.1 but somewhere version 3.0.0 must be used:

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.: NullPointerException -> [Help 1]
{code};;;","23/May/19 09:05;nkruber;Version 3.1.1 is only used for the Java9 profile while 3.0.0 is used in general. Changing that to 3.1.1 also fixes the problem. I can create a PR if you'd like that change.;;;","23/May/19 09:10;chesnay;Slight correction: we use 3.0.0 by default, except for asm6 / jdk9 builds which use 3.1.1 .

However this presents a simple work-around; simply bump the shade-plugin version. 

I'd prefer this for the time being; I'd first have to dig through the shade-plugin JIRA to find potential issues before bumping the plugin in general :/;;;","27/May/19 09:36;nkruber;FYI: since this only affects the hadoop-based subprojects, there is also another workaround for now:

mvn clean package -Pshade-sources -pl '!flink-shaded-hadoop-2,!flink-shaded-hadoop-2-uber'
;;;","27/May/19 09:58;chesnay;Workaround merged to master in 7d91dadc52e02de0e101f6eb8ec7e514d3de000d.;;;","13/Nov/19 14:51;chesnay;WE now use version 3.1.1 of the shade plugin, in line with the main Flink project.

shaded-master: 6b99fa615733daea68ec2028a2dffaf93810c9dc;;;",,,,,,,,,,,,,,,
KinesisDataFetcherTest.testOriginalExceptionIsPreservedWhenInterruptedDuringShutdown deadlocks,FLINK-12595,13235055,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rehevkor5,dwysakowicz,dwysakowicz,23/May/19 06:44,22/Jul/19 16:48,13/Jul/23 08:05,22/Jul/19 16:43,1.9.0,,,,,,,,1.9.0,,,,Connectors / Kinesis,Tests,,0,pull-request-available,test-stability,,"https://api.travis-ci.org/v3/job/535738122/log.txt

",,aljoscha,dwysakowicz,maguowei,rehevkor5,thw,trohrmann,,,,,,,,,,,,,"rehevkor5 commented on pull request #9187: [FLINK-12595][kinesis] Interrupt thread at right time to avoid deadlock
URL: https://github.com/apache/flink/pull/9187
 
 
   - Inside testOriginalExceptionIsPreservedWhenInterruptedDuringShutdown,
   consumerThread.interrupt() was getting absorbed inside
   KinesisDataFetcher's while(running) loop, therefore
   TestableKinesisDataFetcherForShardConsumerException's awaitTermination()
   wasn't getting interrupted by it. This led to deadlock, with
   KinesisDataFetcher waiting on the test code to send the interrupt, and
   the test code waiting for KinesisDataFetcher to throw the expected
   exception.
   - Now, the test code waits until KinesisDataFetcher is inside
   awaitTermination() before producing the interrupt, so it can be sure
   that the interrupt it produces will be received/handled inside
   awaitTermination().
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/19 20:07;githubbot;600","tweise commented on pull request #9187: [FLINK-12595][kinesis] Interrupt thread at right time to avoid deadlock
URL: https://github.com/apache/flink/pull/9187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 16:40;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,FLINK-11568,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 16:48:19 UTC 2019,,,,,,,,,,"0|z02zhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/19 12:29;trohrmann;Another instance: https://api.travis-ci.org/v3/job/546124948/log.txt;;;","17/Jul/19 16:37;thw;This issues seems related to FLINK-11568

[~rehevkor5] could you take a look please?

 ;;;","20/Jul/19 18:23;rehevkor5;Sorry about that! Had to draw out a big sequence diagram, but I have a hypothesis about the issue. It looks like KinesisDataFetcher (on the FlinkKinesisConsumer thread) could by chance pause inside the while(running) loop, before Thread.sleep(). Then, the KinesisShardConsumer thread might interrupt that thread via KinesisDataFetcher#mainThread.interrupt() and then trigger shutdownWaiter, allowing the tests's fetcher.waitUntilShutdown() to complete, after which the test would also interrupt the FlinkKinesisConsumer thread. Then, when the KinesisDataFetcher code resumes, it calls Thread.sleep() and catches/ignores the interrupted state (caused by both interrupts), then it exits the while(running) loop due to running == false, and gets to our awaitTermination() code which waits forever because it has already absorbed the test's interrupt.

This can be reproduced by adding code like this to the KinesisDataFetcher beneath the if(running && discoveryIntervalMillis !=0) line in order to force a longer delay in that thread:
{code:java}
boolean wasInterrupted = false;
int interruptionCount = 0;
for (int i = 0; i < 4; i++) {
   try {
      Thread.sleep(4000);
   } catch (InterruptedException ie) {
      wasInterrupted = true;
      interruptionCount++;
   }
}
if (wasInterrupted) {
   // Restore the interrupted state
   Thread.currentThread().interrupt();
}
System.out.println(""KinesisDataFetcher was interrupted "" + interruptionCount + "" times during the "" +
   ""while(running) loop."");
System.out.flush();
{code}
You'll likely see that it gets interrupted twice, and the test deadlocks with the same stacks as the logs provided above.

I have 02a0cf3d4e checked out to reproduce the issue matching the provided logs. I assume it's best to write this patch against HEAD of master, and let you handle backporting it? Let me know if that's not the case. I'll post again once I have a PR to address my hypothesis.;;;","20/Jul/19 18:26;rehevkor5;Another thing I noticed: FlinkKinesisConsumer#sourceContext.close() is not called if fetcher.runFetcher() throws an exception. This seems like it might be a problem? Should that be moved into a ""finally"" block? Do you think I should submit a separate issue for that?;;;","20/Jul/19 20:12;rehevkor5;Created [https://github.com/apache/flink/pull/9187];;;","22/Jul/19 16:46;thw;[~rehevkor5] thanks for fixing this! 

Regarding FlinkKinesisConsumer#sourceContext.close() : It's a separate discussion. I think that when the fetcher exits with an exception, we should not expect any further (orderly) cleanup.

 ;;;","22/Jul/19 16:48;thw;cherry pick for release-1.9: 7ea55e967bc450b3b744edcaea23834646e439cd

 ;;;",,,,,,,,,,,,,,,,,
StreamTableEnvironment object has no attribute connect,FLINK-12592,13234916,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sunjincheng121,trohrmann,trohrmann,22/May/19 14:39,28/Jun/19 12:17,13/Jul/23 08:05,28/Jun/19 12:17,1.9.0,,,,,,,,1.9.0,,,,API / Python,Tests,,0,pull-request-available,test-stability,,"The Python build module failed on Travis with the following problem: {{'StreamTableEnvironment' object has no attribute 'connect'}}.

https://api.travis-ci.org/v3/job/535684431/log.txt",,sunjincheng121,trohrmann,,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #8525: [FLINK-12592][python] Add `--force` for python install.
URL: https://github.com/apache/flink/pull/8525
 
 
   ## What is the purpose of the change
   For ensuring overwrite the python file when install, in this PR only  Add `--force` for python install command.
   
   ## Brief change log
   
     - Add `--force` for python install command.
   
   
   ## Verifying this change
   
   This change does not need any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? ( not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/May/19 04:37;githubbot;600","asfgit commented on pull request #8525: [FLINK-12592][python] Add `--force` for python install.
URL: https://github.com/apache/flink/pull/8525
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/May/19 05:37;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"24/May/19 02:55;sunjincheng121;image-2019-05-24-10-55-04-214.png;https://issues.apache.org/jira/secure/attachment/12969581/image-2019-05-24-10-55-04-214.png","24/May/19 02:55;sunjincheng121;image-2019-05-24-10-55-14-534.png;https://issues.apache.org/jira/secure/attachment/12969580/image-2019-05-24-10-55-14-534.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 28 12:17:41 UTC 2019,,,,,,,,,,"0|z02ymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/19 06:16;sunjincheng121;Thanks [~till.rohrmann]! I'll try to fix it. :);;;","24/May/19 02:52;sunjincheng121;It seems the python code not be updated, due to the `connect` is added in FLINK-12439, if using the old python code, will throw the exception which log show:

!image-2019-05-24-10-55-14-534.png!

So, I proposal change the tox command: from `python setup.py install` to `python setup.py install --force`, for force installation (overwrite any existing files). and let's see what will be happened.
 What do you think?;;;","24/May/19 05:38;sunjincheng121;PR #8525 merged in master: 873bb4d6388194267656f40528fea8d2c6f1d450

Keep opening status for this JIRA, Observe if the problem has been resolved.;;;","28/Jun/19 12:17;sunjincheng121;This problem has disappeared, So close this JIRA;;;",,,,,,,,,,,,,,,,,,,,
Synchronization issue in StatsDReporterTest,FLINK-12591,13234880,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,SleePy,srichter,srichter,22/May/19 11:23,24/May/19 07:54,13/Jul/23 08:05,24/May/19 07:54,1.9.0,,,,,,,,1.9.0,,,,Runtime / Metrics,Tests,,0,pull-request-available,,,"Problem can be reproduced by looping the test locally in the IDE. See also

https://api.travis-ci.org/v3/job/535675153/log.txt",,srichter,,,,,,,,,,,,,,,,,,"ifndef-SleePy commented on pull request #8524: [FLINK-12591][test] Fix unstable test case StatsDReporterTest
URL: https://github.com/apache/flink/pull/8524
 
 
   ## What is the purpose of the change
   
   * `StatsDReporterTest` is unstable, fix it
   
   ## Brief change log
   
   * The reason of unstable is that, there is a race condition of `DatagramSocketReceiver.waitUntilNumLines`.
   * We need to get the lock outside the loop since `Object.wait` might wake up without a notification. It might miss the notification when checking the loop condition caused by self wakeup.
   
   
   ## Verifying this change
   
   * This change is already covered by existing tests
   * I have verified it by looping executing `StatsDReporterTest.testStatsDMetersReportingOfNegativeValues`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/May/19 04:12;githubbot;600","zentol commented on pull request #8524: [FLINK-12591][test] Fix unstable test case StatsDReporterTest
URL: https://github.com/apache/flink/pull/8524
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/May/19 07:54;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 24 07:54:48 UTC 2019,,,,,,,,,,"0|z02yeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/19 07:54;chesnay;master: 0d7a5a41c60765f2e63fd337d222f53697bf91b3;;;",,,,,,,,,,,,,,,,,,,,,,,
SQL end-to-end test fails on Travis,FLINK-12589,13234840,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,chesnay,chesnay,22/May/19 08:39,22/May/19 09:33,13/Jul/23 08:05,22/May/19 09:33,1.9.0,,,,,,,,1.9.0,,,,Table SQL / API,,,0,,,,"https://travis-ci.org/apache/flink/jobs/535231108

{code}
==============================================================================
Running 'Streaming SQL end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-37856266807
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT
Starting cluster.
Starting standalonesession daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
[INFO] 1 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 2 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 3 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting execution of program
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER NOT NULL correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
expression type is RecordType(INTEGER correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
set is rel#150:LogicalWindowAggregate.NONE(input=HepRelVertex#139,group={},correct=SUM($1),w$start=start('w$),w$end=end('w$),w$rowtime=rowtime('w$),w$proctime=proctime('w$))
expression is LogicalWindowAggregate#167
	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:234)
	at org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.onMatch(ExtendedAggregateExtractProjectRule.java:90)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:559)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:418)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:214)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:201)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlanner(TableEnvImpl.scala:282)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlannerSequentially(TableEnvImpl.scala:248)
	at org.apache.flink.table.api.TableEnvImpl.optimizeNormalizeLogicalPlan(TableEnvImpl.scala:204)
	at org.apache.flink.table.api.StreamTableEnvImpl.optimize(StreamTableEnvImpl.scala:738)
	at org.apache.flink.table.api.StreamTableEnvImpl.translate(StreamTableEnvImpl.scala:787)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:100)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:83)
	at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:269)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:742)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:272)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:204)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:983)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1056)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1056)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12411,,FLINK-12249,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-05-22 08:39:21.0,,,,,,,,,,"0|z02y5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stderr and stdout are reversed in OptimizerPlanEnvironment,FLINK-12586,13234798,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fan_li_ya,shinhira_kazunori,shinhira_kazunori,22/May/19 05:11,23/May/19 09:58,13/Jul/23 08:05,23/May/19 09:58,1.7.2,1.8.0,,,,,,,1.9.0,,,,Command Line Client,,,0,pull-request-available,,,"In OptimizerPlanEnvironment#getOptimizedPlan method, it looks like that stdout is output as System.err and stderr is output as System.out.

[https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/program/OptimizerPlanEnvironment.java#L107-L108]

 

I think, It should be like as bellow.
{code:java}
throw new ProgramInvocationException(

""The program plan could not be fetched - the program aborted pre-maturely.""

+ ""\n\nSystem.err: "" + (stdout.length() == 0 ? ""(none)"" : stderr)

+ ""\n\nSystem.out: "" + (stderr.length() == 0 ? ""(none)"" : stdout));
{code}",,fan_li_ya,shinhira_kazunori,,,,,,,,,,,,,,,,,"liyafan82 commented on pull request #8516: [FLINK-12586][Command Line Client]Stderr and stdout are reversed in OptimizerPlanEnvironment
URL: https://github.com/apache/flink/pull/8516
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix issue 12586: In OptimizerPlanEnvironment#getOptimizedPlan method, it looks like that stdout is output as System.err and stderr is output as System.out.
   
   ## Brief change log
   
     - Change the method OptimizerPlanEnvironment#getOptimizedPlan
     
   
   
   ## Verifying this change
   
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/May/19 02:35;githubbot;600","zentol commented on pull request #8516: [FLINK-12586][Command Line Client]Stderr and stdout are reversed in OptimizerPlanEnvironment
URL: https://github.com/apache/flink/pull/8516
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/May/19 09:57;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 23 09:58:07 UTC 2019,,,,,,,,,,"0|z02xwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/19 02:36;fan_li_ya;Hi [~shinhira_kazunori], thanks for finding this problem. I have provided a fix. Please take a look.;;;","23/May/19 05:02;shinhira_kazunori;Thank you for your code [~fan_li_ya].

It is exactly what I expected.

It looks good to me.;;;","23/May/19 09:58;chesnay;master: 396e93cc4c5fd4086d715234589b3a991a2c809b;;;",,,,,,,,,,,,,,,,,,,,,
inputQueueLength metric does not work for LocalInputChannels,FLINK-12576,13234610,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,pnowojski,pnowojski,21/May/19 13:19,15/Oct/19 03:45,13/Jul/23 08:05,14/Oct/19 10:30,1.6.4,1.7.2,1.8.0,1.9.0,,,,,1.9.0,,,,Runtime / Metrics,Runtime / Network,,0,pull-request-available,,,"Currently {{inputQueueLength}} ignores LocalInputChannels ({{SingleInputGate#getNumberOfQueuedBuffers}}). This can can cause mistakes when looking for causes of back pressure (If task is back pressuring whole Flink job, but there is a data skew and only local input channels are being used).",,aitozi,alpinegizmo,gaoyunhaii,jark,kevin.cyj,kisimple,pnowojski,zjwang,,,,,,,,,,,"Aitozi commented on pull request #8559: [FLINK-12576]Take localInputChannel into account when complute inputQueueLength
URL: https://github.com/apache/flink/pull/8559
 
 
   ## What is the purpose of the change
   
   Take localInputChannel into account when complute inputQueueLength.
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/May/19 16:55;githubbot;600","zhijiangW commented on pull request #8559: [FLINK-12576][Network, Metrics]Take localInputChannel into account when compute inputQueueLength
URL: https://github.com/apache/flink/pull/8559
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 11:25;githubbot;600","pnowojski commented on pull request #9895: [FLINK-12576][docs,metrics] Document that input pool usage metrics ignore LocalInputChannels
URL: https://github.com/apache/flink/pull/9895
 
 
   This is just a simple documentation update.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / **docs** / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 10:22;githubbot;600","zhijiangW commented on pull request #9895: [FLINK-12576][docs,metrics] Document that input pool usage metrics ignore LocalInputChannels
URL: https://github.com/apache/flink/pull/9895
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Oct/19 03:45;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,FLINK-12674,,,,,,,"24/Sep/19 13:27;alpinegizmo;Screen Shot 2019-09-24 at 3.11.15 PM.png;https://issues.apache.org/jira/secure/attachment/12981185/Screen+Shot+2019-09-24+at+3.11.15+PM.png","24/Sep/19 13:26;alpinegizmo;Screen Shot 2019-09-24 at 3.13.05 PM.png;https://issues.apache.org/jira/secure/attachment/12981184/Screen+Shot+2019-09-24+at+3.13.05+PM.png","24/Sep/19 13:25;alpinegizmo;Screen Shot 2019-09-24 at 3.22.36 PM.png;https://issues.apache.org/jira/secure/attachment/12981181/Screen+Shot+2019-09-24+at+3.22.36+PM.png","24/Sep/19 13:25;alpinegizmo;Screen Shot 2019-09-24 at 3.22.53 PM.png;https://issues.apache.org/jira/secure/attachment/12981182/Screen+Shot+2019-09-24+at+3.22.53+PM.png","24/Sep/19 14:40;alpinegizmo;flink-1.8-2-single-slot-TMs-input.png;https://issues.apache.org/jira/secure/attachment/12981197/flink-1.8-2-single-slot-TMs-input.png","24/Sep/19 14:40;alpinegizmo;flink-1.8-2-single-slot-TMs-output.png;https://issues.apache.org/jira/secure/attachment/12981196/flink-1.8-2-single-slot-TMs-output.png","24/Sep/19 14:52;alpinegizmo;flink-1.8-input-subtasks.png;https://issues.apache.org/jira/secure/attachment/12981201/flink-1.8-input-subtasks.png","24/Sep/19 14:52;alpinegizmo;flink-1.8-output-subtasks.png;https://issues.apache.org/jira/secure/attachment/12981200/flink-1.8-output-subtasks.png","26/Sep/19 03:34;kevin.cyj;image-2019-09-26-11-34-24-878.png;https://issues.apache.org/jira/secure/attachment/12981385/image-2019-09-26-11-34-24-878.png","26/Sep/19 03:36;kevin.cyj;image-2019-09-26-11-36-06-027.png;https://issues.apache.org/jira/secure/attachment/12981386/image-2019-09-26-11-36-06-027.png",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 14 10:30:53 UTC 2019,,,,,,,,,,"0|z02wqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/19 15:47;aitozi;Hi, [~pnowojski] I check the code, for localInputChannel do we just have to count the buffer queued in resultsubpartition? I think the outQueueLength of localInputChannel should equal to the inputQueueLength, right?;;;","29/May/19 13:25;pnowojski;Yes, I think you are right. For example for {{PipelinedSubpartition}} this should return {{PipelinedSubpartition.buffers.size()}};;;","05/Aug/19 12:03;zjwang;Fix in master : 302fc1d5de38bb6db99ddc45470efcc9ebd782bc

Fix in release-1.9 : 2a6fb9af7bca5a55d1dc9c55b779eea38b43e1a2;;;","23/Sep/19 14:56;alpinegizmo;To reproduce,


git clone --branch backpressure-with-2-TMs https://github.com/alpinegizmo/flink-playgrounds.git
cd flink-playgrounds/operations-playground
docker-compose build
docker-compose up -d

You will find a job with these 5 operators

(1) kafka -> (2) timestamps / watermarks -> (3) keyBy + backpressure map -> (4) keyBy + window -> (5) kafka

where #3, the backpressure map, causes severe backpressure every other minute. The job is running with parallelism of 2 throughout; up until the first keyBy all the traffic is on the subtasks with 0 as their index. 

In this backpressure-with-2-TMs branch there are two TMs each with one slot. You will observe that all of the output metrics for the 0-index watermarking subtask rise to 1 during the even-numbered minutes, and fall to 0 during the odd numbered minutes, as expected. 

If I run this with one TM with 2 slots, all of the input metrics for the backpressure operator are always zero. 

To confirm that the metrics do work in the non-local case, I created this
backpressure-with-2-TMs branch where there are 2 single-slot TMs. In this case the input metrics for subtask 0 of the backpressure operator are always 0, but the input metrics for subtask 1 of that operator rise and fall every minute, as they should. Since subtask 0 is handling 2x as many records as subtask 1, I conclude that the local input metrics are still broken.

;;;","24/Sep/19 03:34;zjwang;Thanks for reporting this [~alpinegizmo]

I want to confirm two things:

1. The input metric here is for {{inputQueueLength}}?

2. Have you tried whether this problem exists before release-1.9, especially for the case of non-local in 2 single-slot TMs.

This ticket actually made two mainly changes before. One is for considering the input metric (inputQueueLength) for local input channel. The other is that the metric value is got out of synchronized way instead for remote input channel. So I wonder whether it would cause visibility issue for metric reporter thread. But it seems that this issue only happens for the parallelism of backpressure operator in your testing.

 ;;;","24/Sep/19 13:29;alpinegizmo;I just did some more careful testing, this time with

taskmanager.network.memory.buffers-per-channel:1
taskmanager.network.memory.floating-buffers-per-gate:1

which I think is as low as the buffering can go. 

Here are the various input and output metrics, running on Flink 1.9 with 2 single-slot TMs:

 !Screen Shot 2019-09-24 at 3.22.53 PM.png! 
 !Screen Shot 2019-09-24 at 3.22.36 PM.png! 

Running on Flink 1.9 with a single two-slot TM looks like this:

 !Screen Shot 2019-09-24 at 3.13.05 PM.png! 
 !Screen Shot 2019-09-24 at 3.11.15 PM.png! 

I'll see if I can repeat the case you asked about on Flink 1.8.;;;","24/Sep/19 14:41;alpinegizmo;Here are the results for Flink 1.8 with two single-slot TMs.

 !flink-1.8-2-single-slot-TMs-output.png! 
 !flink-1.8-2-single-slot-TMs-input.png! ;;;","24/Sep/19 14:53;alpinegizmo;Here's what's going on overall. Only one of the output subtasks has any traffic, and the keyby sends 2/3 of it in one direction, and the remaining 1/3 the other way.

 !flink-1.8-output-subtasks.png! 
 !flink-1.8-input-subtasks.png! ;;;","26/Sep/19 03:40;kevin.cyj;[~alpinegizmo] I tried the instruction gave above, but the problem did not reproduce.

The instruction I used is:

!image-2019-09-26-11-36-06-027.png!

Here is my result:

!image-2019-09-26-11-34-24-878.png!

Input queue length of both the local and remote channel are not always zero. Did I do something wrong?;;;","26/Sep/19 08:35;alpinegizmo;Ok, I see what's going on now, at least to some extent. I see now that the input queue length metric is behaving as documented.

I wasn't focused on the input queue length metric when I re-opened this ticket – I was only looking at the inPoolUsage and exclusive and floating buffer metrics. Is it the case that these metrics are also intended to ignore local input channels? If so, then I guess the only bug is in the documentation, which fails to explain this.;;;","26/Sep/19 08:36;kevin.cyj;[~alpinegizmo] Because only one of the two upstream task will emit records to the downstream task. One of downstream task will always get its input from local input channel and no buffer in the bufferPool for input will be used. So inPoolUsage is always zero for one of the channels should be what's expected. 

For one two-slot TM case, both input channels for the Backpressure vertex are local input channel. So the inPoolUsage should be also zero.;;;","26/Sep/19 09:11;zjwang;Thanks for reproducing the case [~kevin.cyj]

My previous concern was also for the specific input metric before, so I pointed out the first question before:

> 1. The input metric here is for {{inputQueueLength}}?

If it was the inputQueueLength case, there must be a potential bug and actually this Jira ticket was motivated for this metric in LocalInputChannel before.

If it was the case of inPoolUsage, it can be explained reasonable as always 0 for local channel, because it fetches the buffer directly from upstream's partition queue, so its buffer pool is never used and always be 0. ;;;","26/Sep/19 14:31;pnowojski;I think we should document this ""feature"", that {{inPoolUsage}} ignores local channels and in such cases, to rule out some mistakes, it's best to check the {{inputQueueLength}} value as well. Especially that I haven't found this being mentioned anywhere:
https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html
https://flink.apache.org/2019/07/23/flink-network-stack-2.html;;;","13/Oct/19 15:40;pnowojski;[~jark], why did you move the fix version from 1.9.0 to 1.9.2? As far as I remember, this was fixed in 1.9.0;;;","14/Oct/19 02:50;jark;Hi [~pnowojski], I was moving ""OPEN"" 1.9.0 issues to 1.9.2, because 1.9.0 has already been released. 
I didn't notice this is an ""REOPEN"" issue. So I reset it back to 1.9.0 now. 

Btw, if the conclusion of the ""REOPEN"" issus is there are still something to fix, then what the fixVersion should be?
Should we open another issue to track this? ;;;","14/Oct/19 10:12;pnowojski;I think it depends. If the original issue was not fixed, then re-opening and removing the incorrect fix version is ok. In this case, the conclusion was that everything works as it should, so keeping the previous fix version makes more sense, as the code behaves/will behave in the same way in 1.9.2 as in 1.9.0.;;;","14/Oct/19 10:30;pnowojski;The issue was actually fixed as expected in 1.9.0. To avoid confusion documentation was improved: https://github.com/apache/flink/pull/9895;;;",,,,,,,
Fix a bug in SqlDateTimeUtils#parseToTimeMillis,FLINK-12553,13234214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fan_li_ya,fan_li_ya,fan_li_ya,20/May/19 07:16,13/Apr/21 20:40,13/Jul/23 08:05,21/May/19 02:15,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,0,pull-request-available,,,"If parameter ""1999-12-31 12:34:56.123"" is used, it should return 123. But it returns 1230 now.",,fan_li_ya,jark,,,,,,,,,,,,,,,,,"liyafan82 commented on pull request #8483: [FLINK-12553][Table SQL / Runtime]Fix a bug in SqlDateTimeUtils#parseToTimeMillis
URL: https://github.com/apache/flink/pull/8483
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix a bug in SqlDateTimeUtils#parseToTimeMillis: If parameter ""1999-12-31 12:34:56.123"" is used, it should return 123. But it returns 230 now.
   
   
   ## Brief change log
   
   *(for example:)*
     - Fix the bug in class SqlDateTimeUtils
     - Add a class for unit test: SqlDateTimeUtilsTest
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - run test SqlDateTimeUtilsTest#testParseToTimeMillis
     
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/May/19 07:23;githubbot;600","asfgit commented on pull request #8483: [FLINK-12553][Table SQL / Runtime]Fix a bug in SqlDateTimeUtils#parseToTimeMillis
URL: https://github.com/apache/flink/pull/8483
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/May/19 02:17;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 02:15:27 UTC 2019,,,,,,,,,,"0|z02uao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/19 02:15;jark;Fixed in 1.9.0: a86bce5a176144e06d0120b804f3af986c325ebf
;;;",,,,,,,,,,,,,,,,,,,,,,,
Deadlock when the task thread downloads jars using BlobClient,FLINK-12547,13233972,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunhaibotb,sunhaibotb,sunhaibotb,17/May/19 14:16,04/Jun/19 02:42,13/Jul/23 08:05,03/Jun/19 13:06,1.8.0,,,,,,,,1.8.1,1.9.0,,,Runtime / Coordination,,,0,pull-request-available,,,"The jstack is as follows (this jstack is from an old Flink version, but the master branch has the same problem).
{code:java}
""Source: Custom Source (76/400)"" #68 prio=5 os_prio=0 tid=0x00007f8139cd3000 nid=0xe2 runnable [0x00007f80da5fd000]
java.lang.Thread.State: RUNNABLE
at java.net.SocketInputStream.socketRead0(Native Method)
at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
at java.net.SocketInputStream.read(SocketInputStream.java:170)
at java.net.SocketInputStream.read(SocketInputStream.java:141)
at org.apache.flink.runtime.blob.BlobInputStream.read(BlobInputStream.java:152)
at org.apache.flink.runtime.blob.BlobInputStream.read(BlobInputStream.java:140)
at org.apache.flink.runtime.blob.BlobClient.downloadFromBlobServer(BlobClient.java:164)
at org.apache.flink.runtime.blob.AbstractBlobCache.getFileInternal(AbstractBlobCache.java:181)
at org.apache.flink.runtime.blob.PermanentBlobCache.getFile(PermanentBlobCache.java:206)
at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.registerTask(BlobLibraryCacheManager.java:120)
- locked <0x000000062cf2a188> (a java.lang.Object)
at org.apache.flink.runtime.taskmanager.Task.createUserCodeClassloader(Task.java:968)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:604)
at java.lang.Thread.run(Thread.java:834)

Locked ownable synchronizers:
- None
{code}
 

The reason is that SO_TIMEOUT is not set in the socket connection of the blob client. When the network packet loss seriously due to the high CPU load of the machine, the blob client connection fails to perceive that the server has been disconnected, which results in blocking in the native method. ",,aitozi,QiLuo,sunhaibotb,trohrmann,uce,,,,,,,,,,,,,,"sunhaibotb commented on pull request #8484: [FLINK-12547] Add connection and socket timeouts for the blob client
URL: https://github.com/apache/flink/pull/8484
 
 
   ## What is the purpose of the change
   
   When the network packet loss seriously due to the high CPU load of the machine, the blob client connection fails to perceive that the server has been disconnected, which results in locking in the native method `java.net.SocketInputStream.socketRead0`. This pull request adds connection and socket timeouts for the blob client to fix this issue.
   
   ## Brief change log
   
     - Add configurable connection and socket timeouts for the blob client.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - Added unit test for checking the socket timeout is effective.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/May/19 08:19;githubbot;600","tillrohrmann commented on pull request #8484: [FLINK-12547] Add connection and socket timeouts for the blob client
URL: https://github.com/apache/flink/pull/8484
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jun/19 13:06;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,FLINK-12426,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 03 13:06:25 UTC 2019,,,,,,,,,,"0|z02ssg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/19 01:13;QiLuo;Hi Haibo,

 

We had added SO_TIMEOUT to the blob socket (SO_TIMEOUT = 30min) in our internal Flink release, but this deadlock issue still occurs as described in https://issues.apache.org/jira/browse/FLINK-12426. The TM hangs for over an hour (longer than the SO_TIMEOUT).

 

We've observed that the JM is under heavy CPU load during the deadlock issue, but recovers after a few minutes. This looks pretty strange to me.

 

Thanks,

Qi;;;","27/May/19 06:02;sunhaibotb;[~QiLuo],

Because the blob client has a retry mechanism, I understand that ""The TM hangs for over an hour (longer than the SO_TIMEOUT)"" is possible, but it does not mean that SO_TIMEOUT does not work. In addition, it is not excluded that there may be other reasons leading to hang.

 

`30 minutes` is too longer, and I suggest to set SO_TIMEOUT to a smaller value.;;;","03/Jun/19 13:06;trohrmann;Fixed via

1.9.0: de31f49b4b835b71cdf99f93c89e33a84113b272
1.8.1: ce734359ca093be17b31f2396b30f7303812ea7f;;;",,,,,,,,,,,,,,,,,,,,,
Deadlock while releasing memory and requesting segment concurrent in SpillableSubpartition,FLINK-12544,13233940,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjwang,zjwang,zjwang,17/May/19 10:44,06/Jun/19 09:14,13/Jul/23 08:05,04/Jun/19 13:25,,,,,,,,,1.8.1,,,,Runtime / Network,,,0,pull-request-available,,,"It is reported by flink user, and the original jstack is as following:

 
{code:java}
// ""CoGroup (2/2)"":
                at org.apache.flink.runtime.io.network.NetworkEnvironment.registerTask(NetworkEnvironment.java:213)
                - waiting to lock <0x000000062bf859b8> (a java.lang.Object)
                at org.apache.flink.runtime.taskmanager.Task.run(Task.java:614)
                at java.lang.Thread.run(Thread.java:745)
""CoGroup (1/2)"":
                at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:277)
                - waiting to lock <0x000000063fdf4888> (a java.util.ArrayDeque)
                at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:172)
                at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release0(AbstractReferenceCountedByteBuf.java:95)
                at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:84)
                at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:147)
                at org.apache.flink.runtime.io.network.buffer.BufferConsumer.close(BufferConsumer.java:121)
                at org.apache.flink.runtime.io.network.partition.SpillableSubpartition.spillFinishedBufferConsumers(SpillableSubpartition.java:274)
                at org.apache.flink.runtime.io.network.partition.SpillableSubpartition.releaseMemory(SpillableSubpartition.java:239)
                - locked <0x000000063fdf4ac8> (a java.util.ArrayDeque)
                at org.apache.flink.runtime.io.network.partition.ResultPartition.releaseMemory(ResultPartition.java:371)
                at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.setNumBuffers(LocalBufferPool.java:375)
                at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.redistributeBuffers(NetworkBufferPool.java:408)
                at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:297)
                - locked <0x000000063c785350> (a java.lang.Object)
                at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:259)
                at org.apache.flink.runtime.io.network.NetworkEnvironment.setupInputGate(NetworkEnvironment.java:272)
                at org.apache.flink.runtime.io.network.NetworkEnvironment.registerTask(NetworkEnvironment.java:224)
                - locked <0x000000062bf859b8> (a java.lang.Object)
                at org.apache.flink.runtime.taskmanager.Task.run(Task.java:614)
                at java.lang.Thread.run(Thread.java:745)
""DataSource  (1/1)"":
                at org.apache.flink.runtime.io.network.partition.SpillableSubpartition.releaseMemory(SpillableSubpartition.java:227)
                - waiting to lock <0x000000063fdf4ac8> (a java.util.ArrayDeque)
                at org.apache.flink.runtime.io.network.partition.ResultPartition.releaseMemory(ResultPartition.java:371)
                at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegment(LocalBufferPool.java:257)
                - locked <0x000000063fdf4888> (a java.util.ArrayDeque)
                at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:218)
                at org.apache.flink.runtime.io.network.api.writer.RecordWriter.requestNewBufferBuilder(RecordWriter.java:213)
                at org.apache.flink.runtime.io.network.api.writer.RecordWriter.sendToTarget(RecordWriter.java:144)
                at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
                at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)
                at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)
                at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:193)
                at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
                at java.lang.Thread.run(Thread.java:745)
{code}
Based on the above stack, it happens in the following scenario:
 * taskA: emit -> requestBufferBuilder -> synchronized in LocalBufferPool -> SpillableSubpartition#releaseMemory -> wait for  synchronized in  SpillableSubpartition
 * submit TaskB:  trigger taskA releaseMemory ->  SpillableSubpartition#releaseMemory ->  synchronized in  SpillableSubpartition ->  SpillableSubpartition#spillFinishedBufferConsumers -> bufferConsumer#close -> LocalBufferPool#recycle -> wait for synchronized in LocalBufferPool

 ",,nkruber,pnowojski,ram_krish,trohrmann,uce,zjwang,,,,,,,,,,,,,"zhijiangW commented on pull request #8557: [FLINK-12544][network] Fix the deadlock while releasing memory triggered by multiple threads in SpillableSubpartition
URL: https://github.com/apache/flink/pull/8557
 
 
   ## What is the purpose of the change
   
   *Fix the deadlock while releasing memory triggered by multiple threads in `SpillableSubpartition`.*
   
   ## Brief change log
   
     - *Make the call of `releaseMemory(1)` outside `synchronized` in `LocalBufferPool#requestMemorySegment`*
     - *Refactor the `NoOpTaskActions` as a separate class for using in other tests*
   
   ## Verifying this change
   
   This change is covered by new test `SpillableSubpartitionTest#testConcurrentRequestAndReleaseMemory`*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/May/19 10:50;githubbot;600","pnowojski commented on pull request #8557: [FLINK-12544][network] Fix the deadlock while releasing memory triggered by multiple threads in SpillableSubpartition
URL: https://github.com/apache/flink/pull/8557
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jun/19 12:51;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-12740,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 04 13:27:29 UTC 2019,,,,,,,,,,"0|z02slc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/19 13:27;zjwang;Merged into release-1.8 : 61fa1005adc61b6f9999c3439fdd127e5f25adea;;;",,,,,,,,,,,,,,,,,,,,,,,
Update Travis base image from trusty to xenial,FLINK-12516,13233452,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,15/May/19 11:23,16/May/19 08:11,13/Jul/23 08:05,16/May/19 08:11,1.9.0,,,,,,,,1.9.0,,,,Travis,,,0,pull-request-available,,,"Currently, our Travis tests are running with a {{trusty}} environment which is officially not supported by Ubuntu anymore. It also brings old system libraries which blocks unit tests for FLINK-11579.

I propose to update to xenial (Ubuntu 16.04 LTS) which is still supported until 2021-04.

Since Travis doesn't support oraclejdk8 on xenial, however, this also implies a switch to openJDK 8.",,nkruber,,,,,,,,,,,,,,,,,,"NicoK commented on pull request #8448: [FLINK-12516][travis] use Ubuntu 16.04 LTS (Xenial) and change to openJDK8
URL: https://github.com/apache/flink/pull/8448
 
 
   ## What is the purpose of the change
   
   This changes the test environment from trusty to xenial which is still
   supported until 2021-04 and provides more up-to-date system libraries.
   
   Since oraclejdk8 is not available in Travis' xenial image and we can't switch
   to oraclejdk9 yet, this also changes the JDK to openJDK8.
   
   ## Brief change log
   
   - change Travis' distribution to `xenial`
   - change Travis' JDK to `openjdk8`
   
   ## Verifying this change
   
   Run Travis tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 11:31;githubbot;600","zentol commented on pull request #8448: [FLINK-12516][travis] use Ubuntu 16.04 LTS (Xenial) and change to openJDK8
URL: https://github.com/apache/flink/pull/8448
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/May/19 08:10;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,FLINK-12517,FLINK-12518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 16 08:11:00 UTC 2019,,,,,,,,,,"0|z02pko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/19 08:11;chesnay;master: 54c1af104be68e683e4bdfd41f4839f6597e83ca;;;",,,,,,,,,,,,,,,,,,,,,,,
Sync flink-shaded-hadoop with dependency management entries,FLINK-12515,13233434,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunjincheng121,chesnay,chesnay,15/May/19 09:40,17/May/19 11:39,13/Jul/23 08:05,17/May/19 11:39,shaded-7.0,,,,,,,,shaded-7.0,,,,BuildSystem / Shaded,,,0,pull-request-available,,,"When we moved {{flink-shaded-hadoop}} to {{flink-shaded}} we did not account for {{dependencyManagement}} entries in the root pom which affected the versions of multiple transitive dependencies.

As a result the set of bundled dependencies has changed.
We should go over the existing dependency management entries, determine which affect the bundling of hadoop, and copy them into the respective {{flink-shaded-hadoop}} module.",,,,,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #62:  [FLINK-12515][Build System, BuildSystem / Shaded] Sync flink-shaded-hadoop with dependency management entries
URL: https://github.com/apache/flink-shaded/pull/62
 
 
   In this PR will add ` dependency management` entries for `flink-shaded-hadoop`. 
   
   Process of consideration:
    1. DependencyList -> list all the dependency for `flink-shaded-hadoop` by `mvn dependency:tree`
    2.  TargetDependencyList -> find out the `artifactId` both in `DependencyList` and `flink-parent's dependencyManagement`. 
    3. add the `TargetDependencyList` into ` dependency management` of `flink-shaded-hadoop-2` in `flink-shaded`.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/May/19 08:35;githubbot;600","zentol commented on pull request #62:  [FLINK-12515][Build System, BuildSystem / Shaded] Sync flink-shaded-hadoop with dependency management entries
URL: https://github.com/apache/flink-shaded/pull/62
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/May/19 11:39;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 17 11:39:17 UTC 2019,,,,,,,,,,"0|z02pgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/19 11:39;chesnay;master: ee8c3158d03a3ae64cbdb408edad52a648f0b420;;;",,,,,,,,,,,,,,,,,,,,,,,
TableSourceTest#testNestedProject test failed,FLINK-12512,13233357,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,yanghua,yanghua,15/May/19 01:37,20/May/19 08:20,13/Jul/23 08:05,20/May/19 08:20,1.9.0,,,,,,,,1.9.0,,,,Table SQL / API,,,1,pull-request-available,,," 
{code:java}
20:41:59.128 [ERROR] testNestedProject(org.apache.flink.table.api.stream.table.TableSourceTest)  Time elapsed: 0.047 s  <<< FAILURE!
org.junit.ComparisonFailure: 
null expected:<...deepNested.nested2.f[lag AS nestedFlag, deepNested.nested2.num AS nestedNum])
StreamTableSourceScan(table=[[T]], fields=[id, deepNested, nested], source=[TestSource(read nested fields: id.*, deepNested.nested2.num, deepNested.nested2.flag], deepNested.nested1...> but was:<...deepNested.nested2.f[1 AS nestedFlag, deepNested.nested2.f0 AS nestedNum])
StreamTableSourceScan(table=[[T]], fields=[id, deepNested, nested], source=[TestSource(read nested fields: id.*, deepNested.nested2.f1, deepNested.nested2.f0], deepNested.nested1...>
	at org.apache.flink.table.api.stream.table.TableSourceTest.testNestedProject(TableSourceTest.scala:375)
{code}
log details : [https://api.travis-ci.org/v3/job/532319575/log.txt]

 ",,dwysakowicz,jark,pnowojski,yanghua,,,,,,,,,,,,,,,"dawidwys commented on pull request #8475: [FLINK-12512][table-planner] Create a new instance of CostFactory, FlinkTypeSystem & FlinkTypeFactory per TableEnvironment
URL: https://github.com/apache/flink/pull/8475
 
 
   ## What is the purpose of the change
   
   `CostFactory`, `FlinkTypeSystem` and `FlinkTypeFactory` maintain internal state, so we cannot reuse them for multiple TableEnvironments.
   
   ## Brief change log
   
   - Make `CostFactory`, `FlinkTypeSystem` and `FlinkTypeFactory` non static in `PlanningConfigurationBuilder`
   
   
   ## Verifying this change
   
   This should fix the random failures of `org.apache.flink.table.api.stream.table.TableSourceTest#testNestedProject`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/May/19 12:02;githubbot;600","dawidwys commented on pull request #8475: [FLINK-12512][table-planner] Create a new instance of CostFactory, FlinkTypeSystem & FlinkTypeFactory per TableEnvironment
URL: https://github.com/apache/flink/pull/8475
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/May/19 22:44;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-12545,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 20 08:20:56 UTC 2019,,,,,,,,,,"0|z02ozk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/19 07:38;chesnay;Could someone start working on this issue? This test has been failing for 3 days now, heavily impeding our CI.;;;","20/May/19 08:20;dwysakowicz;Fixed in de03b1de77503ddea79108eee25579465eabe6f6;;;",,,,,,,,,,,,,,,,,,,,,,
Deadlock when reading from InputGates,FLINK-12510,13233273,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,14/May/19 15:44,30/Aug/19 08:47,13/Jul/23 08:05,15/May/19 12:05,1.9.0,,,,,,,,1.9.0,,,,Runtime / Network,,,0,pull-request-available,,,"One refactor in https://issues.apache.org/jira/browse/FLINK-12434 caused a potential deadlock as visible here (from UnionStaticDynamicIterationITCase):

 
{noformat}
""CHAIN Union -> Pipe (4/4)"":
	at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.createSubpartitionView(ResultPartitionManager.java:63)
	- waiting to lock <0x00000000818edc90> (a java.util.HashMap)
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.requestSubpartition(LocalInputChannel.java:111)
	- locked <0x00000000890001d8> (a java.lang.Object)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:500)
	- locked <0x00000000890001e8> (a java.lang.Object)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.requestPartitions(UnionInputGate.java:160)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:183)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:169)
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:86)
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47)
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59)
	at org.apache.flink.runtime.operators.UnionWithTempOperator.run(UnionWithTempOperator.java:71)
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:504)
	at org.apache.flink.runtime.iterative.task.AbstractIterativeTask.run(AbstractIterativeTask.java:157)
	at org.apache.flink.runtime.iterative.task.IterationTailTask.run(IterationTailTask.java:122)
	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:689)
	at java.lang.Thread.run(Thread.java:748)
""flink-akka.actor.default-dispatcher-4"":
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.queueInputGate(UnionInputGate.java:296)
	- waiting to lock <0x00000000890019a8> (a java.util.LinkedHashSet)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.lambda$new$0(UnionInputGate.java:119)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate$$Lambda$224/305821273.run(Unknown Source)
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705)
	at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:687)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.queueChannel(SingleInputGate.java:672)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.notifyChannelNonEmpty(SingleInputGate.java:643)
	at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.notifyChannelNonEmpty(InputChannel.java:125)
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.notifyDataAvailable(LocalInputChannel.java:203)
	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.createReadView(BoundedBlockingSubpartition.java:216)
	- locked <0x0000000084c068a8> (a java.lang.Object)
	at org.apache.flink.runtime.io.network.partition.ResultPartition.createSubpartitionView(ResultPartition.java:350)
	at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.createSubpartitionView(ResultPartitionManager.java:71)
	- locked <0x00000000818edc90> (a java.util.HashMap)
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.requestSubpartition(LocalInputChannel.java:111)
	- locked <0x0000000089001fa8> (a java.lang.Object)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.updateInputChannel(SingleInputGate.java:382)
	- locked <0x0000000084c06948> (a java.lang.Object)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$updatePartitions$1(TaskExecutor.java:626)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$$Lambda$222/1536847067.run(Unknown Source)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
""CHAIN Union -> Pipe (3/4)"":
	at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.onConsumedPartition(ResultPartitionManager.java:114)
	- waiting to lock <0x00000000818edc90> (a java.util.HashMap)
	at org.apache.flink.runtime.io.network.partition.ResultPartition.onConsumedSubpartition(ResultPartition.java:438)
	at org.apache.flink.runtime.io.network.partition.ResultSubpartition.onConsumedSubpartition(ResultSubpartition.java:58)
	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionReader.notifySubpartitionConsumed(BoundedBlockingSubpartitionReader.java:99)
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.notifySubpartitionConsumed(LocalInputChannel.java:243)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.transformToBufferOrEvent(SingleInputGate.java:617)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:538)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNextBufferOrEvent(SingleInputGate.java:519)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.waitAndGetNextData(UnionInputGate.java:217)
	- locked <0x00000000890019a8> (a java.util.LinkedHashSet)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:185)
	at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:169)
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:86)
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:47)
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59)
	at org.apache.flink.runtime.operators.resettable.SpillingResettableMutableObjectIterator.next(SpillingResettableMutableObjectIterator.java:149)
	at org.apache.flink.runtime.operators.UnionWithTempOperator.run(UnionWithTempOperator.java:77)
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:504)
	at org.apache.flink.runtime.iterative.task.AbstractIterativeTask.run(AbstractIterativeTask.java:157)
	at org.apache.flink.runtime.iterative.task.IterationTailTask.run(IterationTailTask.java:122)
	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:689)
	at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.{noformat}
https://api.travis-ci.org/v3/job/531956581/log.txt",,aitozi,pnowojski,,,,,,,,,,,,,,,,,"pnowojski commented on pull request #8443: [FLINK-12510][network] Fix deadlock in InputGates
URL: https://github.com/apache/flink/pull/8443
 
 
   Because recursive calls reading from SingleInputGate or InputChannel can potentially trigger some
   notifications to happen, it's better to not execute those calls under the locks in SingleInputGate
   and UnionInputGate.
   
   This change is covered by various existing ITCases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable /** docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/May/19 15:47;githubbot;600","pnowojski commented on pull request #8443: [FLINK-12510][network] Fix deadlock in InputGates
URL: https://github.com/apache/flink/pull/8443
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 12:04;githubbot;600","aljoscha commented on pull request #9565: [FLINK-12510] Use SpecificRecord.getSchema in AvroFactory
URL: https://github.com/apache/flink/pull/9565
 
 
   ## What is the purpose of the change
   
   Before, we were using SpecificData.getSchema(type) which was not working
   for types that were generated using Avrohugger (for Scala) because
   the SCHEMA was generated in the companion object. Now we use a method
   that must be available on all SpecificRecord(s).
   
   We still use the old method as a fallback if we cannot instantiate or
   call getSchema() on the instance.
   
   ## Brief change log
   
     - We create an instance of the `SpecificRecord` using `Class.newInstance` and then call `SpecificRecord.getSchema()`. We use the old method as a fallback
   
   ## Verifying this change
   
   This is covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - The serializers: yes, the avro serializer
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 08:47;githubbot;600",,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 15 12:05:19 UTC 2019,,,,,,,,,,"0|z02ogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/19 12:05;pnowojski;merged commit eb8fff8 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix AsyncLookupJoin doesn't close all generated ResultFutures,FLINK-12507,13233143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,14/May/19 05:25,14/May/19 10:53,13/Jul/23 08:05,14/May/19 10:52,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,0,pull-request-available,,,"There is a fragile test in AsyncLookupJoinITCase, that not all the udfs are closed at the end.

{code:java}
02:40:48.787 [ERROR] Tests run: 22, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 47.098 s <<< FAILURE! - in org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase
02:40:48.791 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=HEAP](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase)  Time elapsed: 1.266 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<2>
	at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)

02:40:48.794 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=ROCKSDB](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase)  Time elapsed: 1.033 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<2>
	at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)
{code}
",,jark,,,,,,,,,,,,,,,,,,"wuchong commented on pull request #8436: [FLINK-12507][table-runtime-blink] Fix AsyncLookupJoin doesn't close all generated ResultFutures
URL: https://github.com/apache/flink/pull/8436
 
 
   
   
   
   ## What is the purpose of the change
   
   The `AsyncLookupJoin` doesn't close all the generated ResultFutures (which contains UDFs in it). Because currently we iterate on the BlockingQueue to close. But when the job is failing (we are using `failingSource` to trigger restore),  some ResultFutures may not be callback and not in the BlockingQueue.
   
   ## Brief change log
   
     - Put all the ResultFutures to an ArrayList, and iterate on the list to close ResultFutures.
   
   ## Verifying this change
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/May/19 05:30;githubbot;600","asfgit commented on pull request #8436: [FLINK-12507][table-runtime-blink] Fix AsyncLookupJoin doesn't close all generated ResultFutures
URL: https://github.com/apache/flink/pull/8436
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/May/19 10:53;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 10:52:18 UTC 2019,,,,,,,,,,"0|z02no0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/19 10:52;jark;Fixed in 1.9.0: a42b65c1ea8850ebc16dcd8d9913651e0837cb36;;;",,,,,,,,,,,,,,,,,,,,,,,
Harden JobMasterTest#testRequestNextInputSplitWithDataSourceFailover,FLINK-12502,13232997,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xleesf,trohrmann,trohrmann,13/May/19 14:21,13/Jun/19 07:10,13/Jul/23 08:05,13/Jun/19 07:10,1.9.0,,,,,,,,1.9.0,,,,Runtime / Coordination,Tests,,0,pull-request-available,,,"The {{JobMasterTest#testRequestNextInputSplitWithDataSourceFailover}} relies on how many files you have in your working directory. This assumption is quite brittle. Instead we should explicitly instantiate an {{InputSplitAssigner}} with a defined number of input splits. 

Moreover, we should make the assertions more explicit: Input split comparisons should not rely solely on the length of the input split data.

Maybe it is also not necessary to capture the full {{TaskDeploymentDescriptor}} because we could already know the producer's and consumer's {{JobVertexID}} when we create the {{JobGraph}}.",,gjy,trohrmann,wind_ljy,xleesf,,,,,,,,,,,,,,,"leesf commented on pull request #8667: [FLINK-12502] Harden JobMasterTest#testRequestNextInputSplitWithDataSourceFailover
URL: https://github.com/apache/flink/pull/8667
 
 
   
   ## What is the purpose of the change
   
   Harden JobMasterTest#testRequestNextInputSplitWithDataSourceFailover
   
   
   ## Brief change log
   
    *Explicitly instantiate an InputSplitAssigner with a defined number of input splits*
    *Remove TaskDeploymentDescriptor*
   
   
   
   ## Verifying this change
   
   
   This change is already covered by existing tests, such as *JobMasterTest#testRequestNextInputSplitWithDataSourceFailover*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
   cc @tillrohrmann @GJL 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jun/19 14:15;githubbot;600","tillrohrmann commented on pull request #8667: [FLINK-12502] Harden JobMasterTest#testRequestNextInputSplitWithDataSourceFailover
URL: https://github.com/apache/flink/pull/8667
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jun/19 07:10;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 13 07:10:09 UTC 2019,,,,,,,,,,"0|z02mrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/19 09:17;gjy;Are you working on this [~xleesf]?;;;","27/May/19 05:25;xleesf;Yes [~gjy];;;","13/Jun/19 07:10;trohrmann;Fixed via

484915bce2e88bb108e34921a0efb6d89fc030e9
8be60e6f2a06d3783c744749bb76aac14496e3aa;;;",,,,,,,,,,,,,,,,,,,,,
AvroTypeSerializer does not work with types generated by avrohugger,FLINK-12501,13232993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,13/May/19 14:06,09/Jul/20 14:50,13/Jul/23 08:05,06/Sep/19 11:22,,,,,,,,,1.10.0,1.9.1,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,1,pull-request-available,,,"The main problem is that the code in {{SpecificData.createSchema()}} tries to reflectively read the {{SCHEMA$}} field, that is normally there in Avro generated classes. However, avrohugger generates this field in a companion object, which the reflective Java code will therefore not find.

This is also described in these ML threads:
 * [https://lists.apache.org/thread.html/5db58c7d15e4e9aaa515f935be3b342fe036e97d32e1fb0f0d1797ee@%3Cuser.flink.apache.org%3E]
 * [https://lists.apache.org/thread.html/cf1c5b8fa7f095739438807de9f2497e04ffe55237c5dea83355112d@%3Cuser.flink.apache.org%3E]",,aljoscha,debasishg,georg.kf.heiler@gmail.com,gjy,jark,nkruber,tzulitai,,,,,,,,,,,,"aljoscha commented on pull request #9565: [FLINK-12501] Use SpecificRecord.getSchema in AvroFactory
URL: https://github.com/apache/flink/pull/9565
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 11:21;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,FLINK-18478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 14:50:57 UTC 2020,,,,,,,,,,"0|z02mqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/19 08:09;tzulitai;+1 to fix this.
This was also reported elsewhere by this blog post: https://medium.com/wbaa/making-sense-of-apache-flink-state-migration-with-scala-and-avro-69091c232646;;;","14/May/19 08:46;tzulitai;This is really a problem with how avrohugger is implemented.

Either this gets fixed in avrohugger, Avro's SpecificData class, or we workaround this specifically for avrohugger in {{AvroSerializerSnapshot#tryExtractAvroSchema}} and {{AvroSerializer#intializeAvro}}.

The problem with working around this on our side is that we don't really know what other popular non-Avro official libraries there are out there. Avrohugger seems to be a popular one for Scala + Avro, though.;;;","15/May/19 14:11;aljoscha;By the way, [~igalshilman] pointed out to me how Scio solves this: https://github.com/spotify/scio/blob/master/scio-macros/src/main/scala/com/spotify/scio/coders/AvroCoderMacros.scala

They create an instance of the Avro type (using the zero-argument constructor) and then call {{getSchema()}} on that, which should work for all avro-generated types, I think.
;;;","06/Sep/19 11:20;aljoscha;Fixed on master in
f43138e0fe52a66f23225cbf5f72eae110bd3841;;;","09/Sep/19 10:04;ykt836;Should we also fix it in 1.9 branch?;;;","09/Sep/19 10:43;debasishg;That would be very helpful Kurt;;;","11/Sep/19 06:56;gjy;What's the state here with respect to 1.9?;;;","11/Sep/19 10:17;aljoscha;I was sceptical about merging this on the 1.9 branch because it's technically a new feature. Though it could be viewed as fixing a bug, i.e. it was crashing before with Avrohugger-generated types.

I wouldn't be against it if someone wants to cherry-pick this on the 1.9 branch.;;;","24/Sep/19 06:26;jark;I'm fine to cherry-pick it to 1.9 as this is a minor fix and some users can benefit from this. What do you think [~tzulitai]?;;;","29/Sep/19 08:04;jark;As there are some users requesting this fixup. I cherry-picked it to 1.9 branch. 

Fixed in 1.9.1:
1196887407c9ef3fad7901f1c592c022b890b227;;;","20/Jan/20 14:59;gjy;I removed _""The `AvroTypeSerializer` now works with types that are generated from Avrohugger, an Avro plugin for Scala.""_ from the release notes. We do not include all bug fixes in the release notes, and I do not see how this one differs from other bug fixes.;;;","29/Jun/20 20:47;georg.kf.heiler@gmail.com;I am new to flink, but using 1.10.1 - I would assume this problem is fixed. However [https://stackoverflow.com/questions/62637009/flink-use-confluent-schema-registry-for-avro-serde] I still stumble upon the same problem of:  `

 

{{AvroRuntimeException: Not a Specific class}}`. What is still wrong?;;;","02/Jul/20 18:56;aljoscha;Thanks for noticing this, [~georg.kf.heiler@gmail.com]! I created FLINK-18478 to track this. Are you interested in contributing the quick fix for this or should I do it?;;;","06/Jul/20 10:02;georg.kf.heiler@gmail.com;I am very new to Flink so if you already know what needs to be changed and do not want to offer quite some guidance it would be awesome if you could provide the fix.

 

As I am lookint at the code now: Why is only the already fixed AvroSerializer using `AvroFactory`?;;;","09/Jul/20 08:00;aljoscha;I don't actually know, that's what I was also wondering about while writing the fix. I'll talk to some people.;;;","09/Jul/20 14:50;aljoscha;I fixed FLINK-18478. 👌
;;;",,,,,,,,
AWS EMR instructions lead to ClassNotFoundException,FLINK-12493,13232877,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aromero,aromero,12/May/19 18:32,13/May/19 10:05,13/Jul/23 08:05,13/May/19 10:05,,,,,,,,,1.9.0,,,,Documentation,,,1,pull-request-available,,,"Running jobs as described on the AWS EMR section ([https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/aws.html|[https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/aws.html)] leads to ClassNotFound error:
{code:java}
$ HADOOP_CONF_DIR=/etc/hadoop/conf ./bin/flink run -m yarn-cluster -yn 1 examples/streaming/WordCount.jar
2019-05-12 18:14:15,386 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - Found Yarn properties file under /tmp/.yarn-properties-hadoop.
2019-05-12 18:14:15,386 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - Found Yarn properties file under /tmp/.yarn-properties-hadoop.
java.lang.NoClassDefFoundError: javax/ws/rs/ext/MessageBodyReader
 at java.lang.ClassLoader.defineClass1(Native Method)
 at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
 at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
 at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
 at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at java.lang.ClassLoader.defineClass1(Native Method)
 at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
 at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
 at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
 at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at java.lang.ClassLoader.defineClass1(Native Method)
 at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
 at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
 at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
 at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at org.apache.hadoop.yarn.util.timeline.TimelineUtils.<clinit>(TimelineUtils.java:50)
 at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:179)
 at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
 at org.apache.flink.yarn.cli.FlinkYarnSessionCli.getClusterDescriptor(FlinkYarnSessionCli.java:1012)
 at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createDescriptor(FlinkYarnSessionCli.java:274)
 at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createClusterDescriptor(FlinkYarnSessionCli.java:454)
 at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createClusterDescriptor(FlinkYarnSessionCli.java:97)
 at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:224)
 at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
 at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
 at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)
Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ext.MessageBodyReader
 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 ... 52 more
{code}
 

This is due to missing Hadoop classpaths, and requires exporting the relevant variable for it to work, ie:
{code:java}
$ export HADOOP_CLASSPATH=`hadoop classpath`
$ HADOOP_CONF_DIR=/etc/hadoop/conf ./bin/flink run -m yarn-cluster -yn 1 examples/streaming/WordCount.jar
...
2019-05-12 18:19:19,607 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1543518955410_85464
2019-05-12 18:19:19,608 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Waiting for the cluster to be allocated
2019-05-12 18:19:19,611 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Deploying cluster, current state ACCEPTED
2019-05-12 18:19:24,130 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - YARN application has been deployed successfully.
Starting execution of program
Executing WordCount example with default input data set.
Use --input to specify file input.
Printing result to stdout. Use --output to specify output path.
Program execution finished
Job with JobID xxxxxxx has finished.
Job Runtime: 9640 ms
{code}
 

A reference to that should be added on the documentation, even if it's just a link to: [https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/hadoop.html]",AWS EMR 5.19.0,aljoscha,aromero,,,,,,,,,,,,,,,,,"a-romero commented on pull request #8422: FLINK-12493 [docs] Add step to export Hadoop classpath
URL: https://github.com/apache/flink/pull/8422
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Add reference to ensure Hadoop classpath is present before submitting jobs to EMR.
   
   
   ## Brief change log
   
   Added step to export hadoop classpath
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/May/19 22:08;githubbot;600","aljoscha commented on pull request #8422: FLINK-12493 [docs] Add step to export Hadoop classpath
URL: https://github.com/apache/flink/pull/8422
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/May/19 10:04;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 13 10:05:26 UTC 2019,,,,,,,,,,"0|z02m14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/19 10:05;aljoscha;Fixed on master in

518616e9f3527b2564a640b38c80db3eb54bef37;;;",,,,,,,,,,,,,,,,,,,,,,,
Scope of MemorySegment metrics has changed,FLINK-12488,13232655,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/May/19 12:54,15/May/19 09:16,13/Jul/23 08:05,15/May/19 09:16,1.9.0,,,,,,,,1.9.0,,,,Runtime / Metrics,Runtime / Network,,0,pull-request-available,,,"The scope of {{TotalMemorySegments}} and {{AvailableMemorySegments}} has changed since 1.8.

Previously it was {{Status.Network}}, whereas now it is just {{Network}}.",,zjwang,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8413: [FLINK-12488][metrics] Pass Status group to NetworkEnvironment
URL: https://github.com/apache/flink/pull/8413
 
 
   ## What is the purpose of the change
   
   Pass the TaskManagers `Status` metric group to the NetworkEnvironment to retain API compatibility.
   
   ## Verifying this change
   
   Run `MetricsAvailabilityITCase`.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/19 13:01;githubbot;600","zentol commented on pull request #8413: [FLINK-12488][metrics] Pass Status group to NetworkEnvironment
URL: https://github.com/apache/flink/pull/8413
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 09:16;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-12465,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 15 09:16:34 UTC 2019,,,,,,,,,,"0|z02ko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/19 09:16;chesnay;master: 753a11ea2c31c2cf2a24d18c5442b133004acba0;;;",,,,,,,,,,,,,,,,,,,,,,,
Simplify constructor of AggsHandlerCodeGenerator to explicitly tell which methods need to be generated,FLINK-12453,13232367,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,lzljs3620320,lzljs3620320,09/May/19 03:26,09/May/19 09:38,13/Jul/23 08:05,09/May/19 09:38,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,0,pull-request-available,,,"In generateAggsHandler: 

val generator = new AggsHandlerCodeGenerator(
   CodeGeneratorContext(config),
   relBuilder,
   FlinkTypeFactory.toInternalRowType(inputRowType).getFieldTypes,
   needRetract = false,
   config.getNullCheck,
   inputFieldCopy)

but AggsHandlerCodeGenerator args is:

class AggsHandlerCodeGenerator(
   ctx: CodeGeneratorContext,
   relBuilder: RelBuilder,
   inputFieldTypes: Seq[InternalType],
   needRetract: Boolean,
   copyInputField: Boolean,
   needAccumulate: Boolean = true)

Same issue to StreamExecIncrementalGroupAggregate",,jark,lzljs3620320,wind_ljy,,,,,,,,,,,,,,,,"wuchong commented on pull request #8378: [FLINK-12453][table-planner-blink] Simplify constructor of AggsHandlerCodeGenerator
URL: https://github.com/apache/flink/pull/8378
 
 
   
   ## What is the purpose of the change
   
   Currently, the constructor of `AggsHandlerCodeGenerator` contains several boolean flag to indicate which methods should be generated. It is error-prone that it's easy to pass wrong parameters.
   
   This pull request simplify the constructor of `AggsHandlerCodeGenerator` to avoid this problem.
   
   
   ## Brief change log
   
     - add a `needAccumulate` and `needRetract` and `needMerge` methods into `AggsHandlerCodeGenerator`.
   
   
   ## Verifying this change
   
   All the existing integration tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 06:21;githubbot;600","asfgit commented on pull request #8378: [FLINK-12453][table-planner-blink] Simplify constructor of AggsHandlerCodeGenerator
URL: https://github.com/apache/flink/pull/8378
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 09:38;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 09 09:38:00 UTC 2019,,,,,,,,,,"0|z02iwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/19 09:38;jark;Fixed in 1.9.0: 504e7be7dcb2cad4184f9ffd1ef20d8b13f64f40;;;",,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase#testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots does not stop application on failure,FLINK-12445,13232186,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/May/19 09:38,06/Nov/20 14:35,13/Jul/23 08:05,06/Nov/20 14:35,1.9.0,,,,,,,,,,,,Deployment / YARN,Tests,,0,pull-request-available,,,https://travis-ci.org/apache/flink/jobs/529632257,,dwysakowicz,mapohl,,,,,,,,,,,,,,,,,"zentol commented on pull request #8370: [FLINK-12445][yarn] Cancel application on failure
URL: https://github.com/apache/flink/pull/8370
 
 
   Fixes an issue in the `YARNSessionCapacitySchedulerITCase` where the application was not explicitly stopped if an assertion fails, resulting in misleading surefire output.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/May/19 09:41;githubbot;600","zentol commented on pull request #8370: [FLINK-12445][yarn] Cancel application on failure
URL: https://github.com/apache/flink/pull/8370
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/19 11:26;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 14:35:05 UTC 2020,,,,,,,,,,"0|z02hsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/19 11:26;chesnay;master: 71138456ddf82425572cb411bed4f5ab6a76bb2c;;;","23/May/19 06:46;dwysakowicz;Another occurence: https://api.travis-ci.org/v3/job/535929361/log.txt;;;","06/Nov/20 14:35;mapohl;I'm closing this issue again due to inactivity. Looks like the issue didn't happen again. Maybe, it's been a hickup?!;;;",,,,,,,,,,,,,,,,,,,,,
Large number of broken links,FLINK-12444,13232183,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,chesnay,chesnay,08/May/19 09:20,10/May/19 06:59,13/Jul/23 08:05,10/May/19 06:59,1.9.0,,,,,,,,1.9.0,,,,chinese-translation,Documentation,,0,pull-request-available,,,"{code}
[2019-05-07 11:35:10] ERROR `/zh/dev/stream/side_output.html' not found.
[2019-05-07 11:35:11] ERROR `/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.8.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.7.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.6.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.5.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/levels_of_abstraction.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/table_api.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/program_dataflow.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/parallel_dataflow.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/windows.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/event_ingestion_processing_time.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/state_partitioning.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/tasks_chains.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/processes.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/tasks_slots.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/slot_sharing.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/checkpoints.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/linking_with_flink.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/linking.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/apis/streaming/event_timestamps_watermarks.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/apis/streaming/event_timestamp_extractors.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/apis/streaming/event_time.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/checkpoint_tuning.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/local_recovery.png' not found.
{code}",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8394: [FLINK-12444][docs] Fix broken links in documentation to make CRON travis job work
URL: https://github.com/apache/flink/pull/8394
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/19 06:58;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,FLINK-12471,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 10 06:59:12 UTC 2019,,,,,,,,,,"0|z02hs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/19 06:59;chesnay;master: cc19aef90ca94a397905497086914b13f9048a19;;;",,,,,,,,,,,,,,,,,,,,,,,
Docker build script fails on symlink creation ln -s,FLINK-12416,13231788,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,slav4ik,slav4ik,06/May/19 15:17,24/May/19 17:01,13/Jul/23 08:05,24/May/19 17:01,1.8.0,,,,,,,,1.8.1,1.9.0,,,flink-docker,,,0,pull-request-available,,,"When using script 'build.sh' from 'flink-container/docker' it fails on 
{code:java}
+ ln -s /opt/flink-1.8.0-bin-hadoop28-scala_2.12.tgz /opt/flink
+ ln -s /opt/job.jar /opt/flink/lib
ln: /opt/flink/lib: Not a directory
{code}",,rmetzger,slav4ik,trohrmann,yunta,,,,,,,,,,,,,,,"Myasuka commented on pull request #8391: [FLINK-12416][FLINK-12375] Fix docker build scripts on Flink-1.8
URL: https://github.com/apache/flink/pull/8391
 
 
   ## What is the purpose of the change
   
   This PR fix docker build script error from `Flink-1.8` and also fix [FLINK-12375](https://issues.apache.org/jira/browse/FLINK-12375) to give proper persimmon to job jar package. 
   
   ## Brief change log
   
     - Change docker build script and related `Dockerfile`.
     - Change related `README`
   
   ## Verifying this change
   I verify this change manually.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 18:49;githubbot;600","tillrohrmann commented on pull request #8391: [FLINK-12416][FLINK-12375] Fix docker build scripts on Flink-1.8
URL: https://github.com/apache/flink/pull/8391
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/May/19 17:01;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-12569,,,,,,,FLINK-12546,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 24 17:01:39 UTC 2019,,,,,,,,,,"0|z02fco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/19 03:20;yunta;[~slav4ik], would you please share the full command to run the 'build.sh'?;;;","07/May/19 09:13;slav4ik;[~yunta], sure
{noformat}
/build.sh --job-jar /Users/slavad/.m2/repository/my/organization/my-project/0.1/my-project-0.1.jar --from-release --flink-version 1.8.0 --hadoop-version 2.8 --scala-version 2.12
{noformat};;;","07/May/19 17:58;yunta;[~slav4ik] Thanks for your reply, I could reproduce this problem and then figure out the root cause. From Flink-1.8, we had change the name of pre-built release binary tar package and the 'build.sh' would not download any effective release flink package from the url. You could take a look at [https://archive.apache.org/dist/flink/flink-1.8.0/] and [https://archive.apache.org/dist/flink/flink-1.7.2/] to find the difference.

One way to quick fix for you is just change the content of 'build.sh' to replace
{code:java}
curl -# ""https://archive.apache.org/dist/flink/flink-${FLINK_VERSION}/${FLINK_DIST_FILE_NAME}"" --output ${CURL_OUTPUT}
{code}
to
{code:java}
curl -# ""https://archive.apache.org/dist/flink/flink-1.8.0/flink-1.8.0-bin-scala_2.12.tgz"" --output ${CURL_OUTPUT}{code}
If you want to use Flink with Hadoop{color:#333333}, please refer to [https://flink.apache.org/downloads.html#apache-flink-180] .{color}

 ;;;","08/May/19 07:09;rmetzger;[~yunta] are you going to provide a PR for fixing the issue?;;;","08/May/19 08:05;yunta;[~rmetzger] Sure, after figure out why this problem happened, I plan to fix this issue.;;;","08/May/19 08:09;rmetzger;Great, thank you.
Maybe it makes sense to fix https://issues.apache.org/jira/browse/FLINK-12375 as well, while looking at the docker image.;;;","10/May/19 09:24;chesnay;The script just wasn't adjusted for the removal of the hadoop convenience binaries. They should download the flink binary along with with a ` flink-shaded-hadoop2-uber jar from maven.;;;","24/May/19 17:01;trohrmann;Fixed via
1.9.0: f1c3ac47b67a8940fcfdfb96ce3de5a32b34901d
1.8.1: 0094df9dc284d8748c80db7b5c7993f995dc59b0;;;",,,,,,,,,,,,,,,,
FilterableTableSource does not use filters on job run,FLINK-12399,13231514,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rongr,josh.bradt,josh.bradt,03/May/19 19:43,30/Mar/20 07:54,13/Jul/23 08:05,17/Oct/19 16:25,1.8.0,,,,,,,,1.10.0,1.9.2,,,Table SQL / API,,,1,pull-request-available,,,"As discussed [on the mailing list|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Filter-push-down-not-working-for-a-custom-BatchTableSource-tp27654.html], there appears to be a bug where a job that uses a custom FilterableTableSource does not keep the filters that were pushed down into the table source. More specifically, the table source does receive filters via applyPredicates, and a new table source with those filters is returned, but the final job graph appears to use the original table source, which does not contain any filters.

I attached a minimal example program to this ticket. The custom table source is as follows: 
{code:java}
public class CustomTableSource implements BatchTableSource<Model>, FilterableTableSource<Model> {

    private static final Logger LOG = LoggerFactory.getLogger(CustomTableSource.class);

    private final Filter[] filters;

    private final FilterConverter converter = new FilterConverter();

    public CustomTableSource() {
        this(null);
    }

    private CustomTableSource(Filter[] filters) {
        this.filters = filters;
    }

    @Override
    public DataSet<Model> getDataSet(ExecutionEnvironment execEnv) {
        if (filters == null) {
           LOG.info(""==== No filters defined ===="");
        } else {
            LOG.info(""==== Found filters ===="");
            for (Filter filter : filters) {
                LOG.info(""FILTER: {}"", filter);
            }
        }

        return execEnv.fromCollection(allModels());
    }

    @Override
    public TableSource<Model> applyPredicate(List<Expression> predicates) {
        LOG.info(""Applying predicates"");

        List<Filter> acceptedFilters = new ArrayList<>();
        for (final Expression predicate : predicates) {
            converter.convert(predicate).ifPresent(acceptedFilters::add);
        }

        return new CustomTableSource(acceptedFilters.toArray(new Filter[0]));
    }

    @Override
    public boolean isFilterPushedDown() {
        return filters != null;
    }

    @Override
    public TypeInformation<Model> getReturnType() {
        return TypeInformation.of(Model.class);
    }

    @Override
    public TableSchema getTableSchema() {
        return TableSchema.fromTypeInfo(getReturnType());
    }

    private List<Model> allModels() {
        List<Model> models = new ArrayList<>();

        models.add(new Model(1, 2, 3, 4));
        models.add(new Model(10, 11, 12, 13));
        models.add(new Model(20, 21, 22, 23));

        return models;
    }
}
{code}
 

When run, it logs
{noformat}
15:24:54,888 INFO  com.klaviyo.filterbug.CustomTableSource                       - Applying predicates
15:24:54,901 INFO  com.klaviyo.filterbug.CustomTableSource                       - Applying predicates
15:24:54,910 INFO  com.klaviyo.filterbug.CustomTableSource                       - Applying predicates
15:24:54,977 INFO  com.klaviyo.filterbug.CustomTableSource                       - ==== No filters defined ===={noformat}
which appears to indicate that although filters are getting pushed down, the final job does not use them.",,fhueske,josh.bradt,leonard,ningshi,rongr,,,,,,,,,,,,,,"walterddr commented on pull request #8389: [FLINK-12399][table] Fix FilterableTableSource does not change after applyPredicate
URL: https://github.com/apache/flink/pull/8389
 
 
   ## What is the purpose of the change
   
   This PR fixes the problem: FilterableTableSource does not generate a new digest after the predicate push down, unless `explainSource()` API is explicitly override. 
   
   ## Brief change log
   
   
     - Changed the `explainTerm` API from `FlinkLogicalTableSourceScan` to include auxiliary source description.
     - Changed the `TestFilterableTableSource` to include both the none explainSource and with explainSource version.
     - Added test to both stream and batch cases.
   
   ## Verifying this change
   
     - Tests for both stream and batch cases for predicate push down without explainSource override.
     - Others are covered for current tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? n/a
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 16:01;githubbot;600","walterddr commented on pull request #8389: [FLINK-12399][table] Fix FilterableTableSource does not change after applyPredicate
URL: https://github.com/apache/flink/pull/8389
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 21:20;githubbot;600","walterddr commented on pull request #8468: [FLINK-12399][table] Adding PushDownTableSource interface to fix FilterableTableSource applyPredicate problem
URL: https://github.com/apache/flink/pull/8468
 
 
   ## What is the purpose of the change
   
   This PR fixes the problem: FilterableTableSource does not generate a new digest after the predicate push down unless `explainSource()` API is explicitly override. 
   
   ## Brief change log
   
     - Adding a PushDownTableSource API that all predicate pushdown implementation should extend and an override for the `explainPushDown` is required.
     - Changed the `explainTerm` API from `FlinkLogicalTableSourceScan` and `PhysicalTableSourceScan` to include both `explainSource` and `explainPushDown`
     - Changed the `TestFilterableTableSource` accordingly 
     - Changed the corresponding dependent APIs such as `ProjectableTableSource` and `NestedProjectableTableSource`, `OrcTableSource` and `HBaseTableSource`
   
   ## Verifying this change
     - Covered for current tests.
   
   ## Does this pull request potentially affect one of the following parts:
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? n/a
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/May/19 17:13;githubbot;600","KurtYoung commented on pull request #8468: [FLINK-12399][table][table-planner] Fix FilterableTableSource does not change after applyPredicate
URL: https://github.com/apache/flink/pull/8468
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 16:24;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,FLINK-16860,,,,,,,,,,,"03/May/19 19:42;josh.bradt;flink-filter-bug.tar.gz;https://issues.apache.org/jira/secure/attachment/12967804/flink-filter-bug.tar.gz",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 19 18:05:06 UTC 2019,,,,,,,,,,"0|z02dns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/19 16:54;rongr;Hi [~josh.bradt]. I think I found the root cause of this issue.

Apparently you have to override the method {{explainSource}} in order to let calcite know that the new created TableSource with filter pushedDown is different from the original created CustomeTableSource (where you have not applyPredicates).
I think this might be related to the #4 changelog point https://github.com/apache/flink/pull/8324 when I try upgrading to CALCITE 1.19.0 I also encounter some weird issues where calcite tries to find the correct tablesource from the digest strings. 

I will assigned to myself and start looking into this issue. Please let me know if adding the override resolves your issue at this moment.;;;","06/May/19 18:01;josh.bradt;[~walterddr]: Thanks, that workaround does solve my problem for now!;;;","07/May/19 19:55;fhueske;Thanks for looking into this [~walterddr]!;;;","03/Sep/19 16:35;rongr;Hi [~fhueske]. would you please kindly take a look at the approach to address this issue? The problem has been created some problems for us and also some multiple threads in the mailing list.
It would be nice to address this before the next release. Much appreciated. ;;;","04/Sep/19 01:36;ykt836;Hi [~walterddr], sorry for the delay, I will take a look at your solution. ;;;","17/Oct/19 16:25;ykt836;merged to 1.10: cbf7b76923feec5ce8c9cb45a3082339be5ff17e;;;","19/Oct/19 18:05;rongr;merged to 1.9: c1019105c22455c554ab91b9fc2ef8512873bee8;;;",,,,,,,,,,,,,,,,,
KafkaITCase.testOneSourceMultiplePartitions doesn't fail properly,FLINK-12396,13231488,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,phoenixjiangnan,phoenixjiangnan,03/May/19 17:52,25/Apr/21 02:24,13/Jul/23 08:05,25/Apr/21 02:24,1.9.0,,,,,,,,,,,,Connectors / Kafka,,,0,stale-major,,,"https://api.travis-ci.org/v3/job/527599974/log.txt

In the log, we can see that KafkaITCase.testOneSourceMultiplePartitions failed, but it kept running and doing all the snapshot, which caused the built to timeout.

{code:java}
05:00:38,896 INFO  org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink  - Snapshot of counter 4800 at checkpoint 115
05:00:39,050 ERROR org.apache.flink.streaming.connectors.kafka.KafkaITCase       - 
--------------------------------------------------------------------------------
Test testOneSourceMultiplePartitions(org.apache.flink.streaming.connectors.kafka.KafkaITCase) failed with:
org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:621)
	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:79)
	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:35)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runOneSourceMultiplePartitionsExactlyOnceTest(KafkaConsumerTestBase.java:924)
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testOneSourceMultiplePartitions(KafkaITCase.java:102)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

================================================================================
05:00:39,057 INFO  org.apache.flink.streaming.connectors.kafka.KafkaITCase       - 
================================================================================
Test testCancelingFullTopic(org.apache.flink.streaming.connectors.kafka.KafkaITCase) is running.
--------------------------------------------------------------------------------
05:00:39,396 INFO  org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink  - Snapshot of counter 4800 at checkpoint 116
05:00:39,896 INFO  org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink  - Snapshot of counter 4800 at checkpoint 117
05:00:40,396 INFO  org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink  - Snapshot of counter 4800 at checkpoint 118
05:00:40,896 INFO  org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink  - Snapshot of counter 4800 at checkpoint 119
05:00:41,396 INFO  org.apache.flink.streaming.connectors.kafka.testutils.ValidatingExactlyOnceSink  - Snapshot of counter 4800 at checkpoint 120
{code}

",,becket_qin,phoenixjiangnan,SleePy,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 25 02:24:42 UTC 2021,,,,,,,,,,"0|z02di0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/19 16:38;becket_qin;This is probably related to FLINK-13226, which has a PR to fix at this point.;;;","01/Aug/19 02:14;SleePy;Another instance, https://api.travis-ci.com/v3/job/221185816/log.txt.;;;","22/Apr/21 11:51;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","25/Apr/21 02:24;becket_qin;Closing the ticket as we haven't seen this issue for over a year.;;;",,,,,,,,,,,,,,,,,,,,
FlinkRelMetadataQuery does not compile with Scala 2.12,FLINK-12392,13231247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,chesnay,chesnay,02/May/19 12:23,09/May/19 09:42,13/Jul/23 08:05,09/May/19 09:40,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Planner,,,0,pull-request-available,,,"{code}
10:57:51.770 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/plan/metadata/FlinkRelMetadataQuery.scala:52: error: value EMPTY in class RelMetadataQuery cannot be accessed in object org.apache.calcite.rel.metadata.RelMetadataQuery
10:57:51.770 [ERROR]  Access to protected value EMPTY not permitted because
10:57:51.770 [ERROR]  enclosing package metadata in package plan is not a subclass of
10:57:51.770 [ERROR]  class RelMetadataQuery in package metadata where target is defined
10:57:51.770 [ERROR]     this(RelMetadataQuery.THREAD_PROVIDERS.get, RelMetadataQuery.EMPTY)
{code}",,fan_li_ya,godfreyhe,jark,martijnvisser,twalthr,xsunsmile,,,,,,,,,,,,,"godfreyhe commented on pull request #8376: [FLINK-12392] [table-planner-blink] Port FlinkRelMetadataQuery into Java to avoid compiling error with Scala 2.12
URL: https://github.com/apache/flink/pull/8376
 
 
   
   ## What is the purpose of the change
   
   *Port FlinkRelMetadataQuery into Java to avoid compiling error with Scala 2.12*
   
   
   ## Brief change log
   
     - *Port FlinkRelMetadataQuery from Scala intto Java*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *FlinkRelMdXXXTest*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 03:48;githubbot;600","asfgit commented on pull request #8376: [FLINK-12392] [table-planner-blink] Port FlinkRelMetadataQuery into Java to avoid compiling error with Scala 2.12
URL: https://github.com/apache/flink/pull/8376
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 09:42;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 09 09:40:50 UTC 2019,,,,,,,,,,"0|z02c08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/19 09:41;jark;cc [~godfreyhe];;;","08/May/19 13:24;twalthr;[~jark] [~ykt836] can we fix this rather soon to make the builds pass again.;;;","09/May/19 01:48;jark;Sure [~twalthr], we will figure it out today.;;;","09/May/19 09:40;jark;Fixed in 1.9.0: ca8d9cab87fa082a0939ce51b8369b75691df3a4;;;",,,,,,,,,,,,,,,,,,,,
flink codegen set String type for ByteBuffer fields,FLINK-12389,13231193,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yuyang08,yuyang08,02/May/19 06:23,19/Nov/21 15:12,13/Jul/23 08:05,05/Sep/19 18:53,1.8.0,,,,,,,,,,,,Table SQL / Legacy Planner,,,0,,,,"We try to write a simple flink sql program using ""select  .. from "" statement, and encounter  a compile exception. 

*Caused by: org.codehaus.commons.compiler.CompileException: Line 110, Column 38: Cannot cast ""java.nio.ByteBuffer"" to ""java.lang.String""*

Further debugging shows that the following flink-generated code snippet caused problem: 

{code}
  final java.lang.reflect.Field field_com$pinterest$utzv2$thrift$RealtimeSpendValue_segmentIds =
      org.apache.flink.api.java.typeutils.TypeExtractor.getDeclaredField(
        com.pinterest.utzv2.thrift.RealtimeSpendValue.class, ""segmentIds"");
...

    boolean isNull$5 = (java.nio.ByteBuffer) field_com$pinterest$utzv2$thrift$RealtimeSpendValue_segmentIds.get(in1) == null;
    java.lang.String result$4;
    if (isNull$5) {
      result$4 = """";
    }
    else {
      result$4 = (java.lang.String) (java.nio.ByteBuffer) field_com$pinterest$utzv2$thrift$RealtimeSpendValue_segmentIds.get(in1);
    }
   
{code}

The following is the stack track:

Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:265)
	... 17 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36)
	at org.apache.flink.table.runtime.CRowOutputProcessRunner.compile(CRowOutputProcessRunner.scala:36)
	at org.apache.flink.table.runtime.CRowOutputProcessRunner.open(CRowOutputProcessRunner.scala:50)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:425)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:291)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.codehaus.commons.compiler.CompileException: Line 110, Column 38: Cannot cast ""java.nio.ByteBuffer"" to ""java.lang.String""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5049)
	at org.codehaus.janino.UnitCompiler.access$8600(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4416)
	at org.codehaus.janino.UnitCompiler$16.visitCast(UnitCompiler.java:4394)
	at org.codehaus.janino.Java$Cast.accept(Java.java:4887)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3752)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3732)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4466)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2776)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:33)
	... 9 more

",,fan_li_ya,fhueske,hongyu.bi,twalthr,yuyang08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 05 18:55:12 UTC 2019,,,,,,,,,,"0|z02bo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/19 06:30;yuyang08;[~fhueske@gmail.com]  not sure whom we should contact on this. could you share any pointers on how we should resolve this issue? ;;;","02/May/19 08:24;fhueske;Hi [~yuyang08], thanks for reporting this bug!

Can you maybe isolate the problem to a specific SQL function and share the query?
[~twalthr] might also be able to help here.

Thank you,
Fabian
;;;","02/May/19 08:28;twalthr;[~yuyang08] as far as I can see, you are accessing the field of a POJO which is a ByteBuffer and cast it to a string? A little reproducible example would be great. It definitely looks like a bug.;;;","02/May/19 16:36;yuyang08;[~twalthr], [~fhueske] thanks for the reply!  given that this issue is with legacy planner, shall we try with flink-table-planner-blink? any pointers on the current status and the community's plan on flink-table-planner-blink and flink-table-runtime-blink? 

[~ykt836], [~qingru.zhang]  could you share any insights on this? ^^;;;","03/May/19 07:48;ykt836;Hi [~yuyang08], as Fabian and Timo pointed out earlier, I also believe a reproducible example will be a great help. My gut feeling about this issue is related to your usage, and it's highly possible you will encounter to the same issue even if trying with blink planner. ;;;","03/May/19 08:37;twalthr;[~yuyang08] don't be confused by the name ""legacy planner"" so far this is still the official main planner for now. The Blink planner is work in progress.;;;","05/Sep/19 18:55;yuyang08;Resolving this issue as `not a bug`  as we had been able to resolve the problem by passing the right type info. ;;;",,,,,,,,,,,,,,,,,
GCS runtime exn: Request payload size exceeds the limit,FLINK-12376,13230991,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,,haf,haf,30/Apr/19 20:41,19/Jan/22 10:46,13/Jul/23 08:05,19/Jan/22 10:46,1.7.2,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,0,auto-deprioritized-major,auto-deprioritized-minor,auto-unassigned,"I'm trying to use the google cloud storage file system, but it would seem that the FLINK / GCS client libs are creating too-large requests far down in the GCS Java client.

The Java client is added to the lib folder with this command in Dockerfile (probably [hadoop2-1.9.16|https://search.maven.org/artifact/com.google.cloud.bigdataoss/gcs-connector/hadoop2-1.9.16/jar] at the time of writing):

 
{code:java}
ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar /opt/flink/lib{code}
This is the crash output. Focus lines:
{code:java}
java.lang.RuntimeException: Error while confirming checkpoint{code}
and
{code:java}
 Caused by: com.google.api.gax.rpc.InvalidArgumentException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: Request payload size exceeds the limit: 524288 bytes.{code}
Full stacktrace:

 
{code:java}
[analytics-867c867ff6-l622h taskmanager] 2019-04-30 20:23:14,532 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -> Process -> Timestamps/Watermarks -> app_events (1/1) (9a01e96c0271025d5ba73b735847cd4c) switched from RUNNING to FAILED.
[analytics-867c867ff6-l622h taskmanager] java.lang.RuntimeException: Error while confirming checkpoint
[analytics-867c867ff6-l622h taskmanager]     at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1211)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[analytics-867c867ff6-l622h taskmanager]     at java.lang.Thread.run(Thread.java:748)
[analytics-867c867ff6-l622h taskmanager] Caused by: com.google.api.gax.rpc.InvalidArgumentException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: Request payload size exceeds the limit: 524288 bytes.
[analytics-867c867ff6-l622h taskmanager]     at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:49)
[analytics-867c867ff6-l622h taskmanager]     at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:72)
[analytics-867c867ff6-l622h taskmanager]     at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:60)
[analytics-867c867ff6-l622h taskmanager]     at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97)
[analytics-867c867ff6-l622h taskmanager]     at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:68)
[analytics-867c867ff6-l622h taskmanager]     at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1056)
[analytics-867c867ff6-l622h taskmanager]     at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
[analytics-867c867ff6-l622h taskmanager]     at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1138)
[analytics-867c867ff6-l622h taskmanager]     at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:958)
[analytics-867c867ff6-l622h taskmanager]     at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:748)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:507)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:482)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:694)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:397)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
[analytics-867c867ff6-l622h taskmanager]     at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
[analytics-867c867ff6-l622h taskmanager]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
[analytics-867c867ff6-l622h taskmanager]     ... 3 more
[analytics-867c867ff6-l622h taskmanager]     Suppressed: com.google.api.gax.rpc.AsyncTaskException: Asynchronous task failed
[analytics-867c867ff6-l622h taskmanager]         at com.google.api.gax.rpc.ApiExceptions.callAndTranslateApiException(ApiExceptions.java:57)
[analytics-867c867ff6-l622h taskmanager]         at com.google.api.gax.rpc.UnaryCallable.call(UnaryCallable.java:112)
[analytics-867c867ff6-l622h taskmanager]         at okr.sources.PubSubSource.acknowledgeSessionIDs(PubSubSource.java:122)
[analytics-867c867ff6-l622h taskmanager]         at org.apache.flink.streaming.api.functions.source.MultipleIdsMessageAcknowledgingSourceBase.acknowledgeIDs(MultipleIdsMessageAcknowledgingSourceBase.java:122)
[analytics-867c867ff6-l622h taskmanager]         at org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.notifyCheckpointComplete(MessageAcknowledgingSourceBase.java:231)
[analytics-867c867ff6-l622h taskmanager]         at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
[analytics-867c867ff6-l622h taskmanager]         at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:684)
[analytics-867c867ff6-l622h taskmanager]         at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1206)
[analytics-867c867ff6-l622h taskmanager]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[analytics-867c867ff6-l622h taskmanager]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[analytics-867c867ff6-l622h taskmanager]         ... 3 more
{code}
The file system is configured as such in `conf/flink-conf.yaml`:

 
{code:java}
state.backend: rocksdb
state.checkpoints.num-retained: 3
state.checkpoints.dir: gs://example_bucket/flink/checkpoints
state.savepoints.dir: gs://example_bucket/flink/savepoints
state.backend.incremental: true
{code}
...and the checkpoints that are created before the crash are small in size:

 

!Screenshot 2019-04-30 at 22.32.34.png! I'll be testing with Flink 1.8.0 as well.

The pom.xml config:
{code:java}
<!-- https://stackoverflow.com/questions/51860988/flink-checkpoints-to-google-cloud-storage -->
<!-- https://search.maven.org/search?q=a:flink-statebackend-rocksdb_2.11 -->
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-statebackend-rocksdb_${scala.binary.version}</artifactId>
  <version>${flink.version}</version>
</dependency>

<!-- https://search.maven.org/search?q=g:com.google.cloud.bigdataoss -->
<!-- https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/pubsub/README.md -->
<!-- Cloud Storage: -->
<dependency>
  <groupId>com.google.cloud.bigdataoss</groupId>
  <artifactId>gcs-connector</artifactId>
  <version>hadoop2-1.9.16</version>
</dependency>

<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-connector-filesystem_2.11</artifactId>
  <version>${flink.version}</version>
</dependency>
{code}
 ","FROM flink:1.8.0-scala_2.11
ARG version=0.17
ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar /opt/flink/lib
COPY target/analytics-${version}.jar /opt/flink/lib/analytics.jar",haf,rmetzger,sewen,Xeli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/19 20:33;haf;Screenshot 2019-04-30 at 22.32.34.png;https://issues.apache.org/jira/secure/attachment/12967524/Screenshot+2019-04-30+at+22.32.34.png","08/May/19 10:42;haf;Screenshot 2019-05-08 at 12.41.07.png;https://issues.apache.org/jira/secure/attachment/12968182/Screenshot+2019-05-08+at+12.41.07.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Jan 19 10:46:58 UTC 2022,,,,,,,,,,"0|z02afk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/19 06:51;sewen;This is actually not a GCS / checkpoint issue, it is an issue in the source (probably PubSub connector?)

The checkpoint completes, then Flink notifies the source that the checkpoint is complete and the source task acks some IDs back. That ack message is too large for the PubSub client's RPC service.

I think we need to rethink how the PubSub source works. Seems that keeping the IDS and acknowledging a large number or records is not feasible in a stable way.

I am not a PubSub expert, but is there a way to keep something like a sequence number (or vector of sequence numbers), similar to Kafka's offsets? ;;;","08/May/19 08:36;Xeli;[~sewen] PubSub has no concept of ordering. For better or worse this is by design: [https://cloud.google.com/pubsub/docs/ordering] 

 

So what is happening is that grpc has limits that are server side and are not exposed to the clients.

I've actually added code to counter the exact exact issue [~haf] sees: [https://github.com/apache/flink/pull/6594/files#diff-ea875742509cef8c6f26e1b488447130R125] This code has been inspired/copied from the go pubsub client here: [https://code-review.googlesource.com/c/gocloud/+/9758/2/pubsub/service.go]

In short: Instead of acknowledging all id's at once, it tries to split it up in chunks of <500kb and does multiple requests. 

 

So one of two things might've happened:
 * Either [~haf] used a version of the connector that did not have this fix (could you confirm [~haf]?)
 * The way the connector splits up acknowledgement ids is off, adjusting for overhead isn't the easiest in java :(

I found this issue: [https://github.com/GoogleCloudPlatform/pubsub/pull/194] and they propose making ids per request configurable incase pubsub changes this limit on their side.

 

 

I could make the connector to split into smaller chunks and/or add it as a configuration option but it's quite a technical and hard to tune option.

Any thought on how to best approach this?;;;","08/May/19 10:42;haf;> Either [~haf] used a version of the connector that did not have this fix (could you confirm [~haf]?)

I'm using the latest version; the one you pinged me and said that you had a fix for some exceptions in (the one that you said would work with 1.7.x)

This is the code that was running in this issue.

!Screenshot 2019-05-08 at 12.41.07.png!;;;","16/Apr/21 11:10;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 23:05;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","07/Jun/21 22:53;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Jun/21 08:06;haf;It's messed up that you let this linger for two years before acting on it and when you act on it, you let a bot do it.;;;","16/Jun/21 10:41;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","14/Dec/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/Dec/21 22:38;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/Jan/22 10:46;haf;Seens PR has been made: in any case, this is a stale issue.;;;",,,,,,,,,,,,,
flink-container job jar does not have read permissions,FLINK-12375,13230980,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,adamonduty,adamonduty,30/Apr/19 18:52,02/Oct/19 17:47,13/Jul/23 08:05,24/May/19 17:01,,,,,,,,,1.8.1,1.9.0,,,flink-docker,,,0,,,,"When building a custom job container using flink-container, the job can't be launched if the provided job jar does not have world-readable permission.

This is because the job jar in the container is owned by root:root, but the docker container executes as the flink user.

In environments with restrictive umasks (e.g. company laptops) that create files without group and other read permissions by default, this causes the instructions to fail.

To reproduce on master:
{code:java}
cd flink-container/docker
cp ../../flink-examples/flink-examples-streaming/target/WordCount.jar .
chmod go-r WordCount.jar  # still maintain user read permission
./build.sh --job-jar WordCount.jar --from-archive flink-1.8.0-bin-scala_2.11.tgz --image-name flink-job:latest
FLINK_DOCKER_IMAGE_NAME=flink-job FLINK_JOB=org.apache.flink.streaming.examples.wordcount.WordCount docker-compose up{code}
which results in the following error:
{code:java}
job-cluster_1 | 2019-04-30 18:40:57,787 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Could not start cluster entrypoint StandaloneJobClusterEntryPoint.
job-cluster_1 | org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneJobClusterEntryPoint.
job-cluster_1 | at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:190)
job-cluster_1 | at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)
job-cluster_1 | at org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.main(StandaloneJobClusterEntryPoint.java:105)
job-cluster_1 | Caused by: org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
job-cluster_1 | at org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.create(AbstractDispatcherResourceManagerComponentFactory.java:257)
job-cluster_1 | at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224)
job-cluster_1 | at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)
job-cluster_1 | at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
job-cluster_1 | at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)
job-cluster_1 | ... 2 more
job-cluster_1 | Caused by: org.apache.flink.util.FlinkException: Could not load the provided entrypoint class.
job-cluster_1 | at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.createPackagedProgram(ClassPathJobGraphRetriever.java:119)
job-cluster_1 | at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.retrieveJobGraph(ClassPathJobGraphRetriever.java:96)
job-cluster_1 | at org.apache.flink.runtime.dispatcher.JobDispatcherFactory.createDispatcher(JobDispatcherFactory.java:62)
job-cluster_1 | at org.apache.flink.runtime.dispatcher.JobDispatcherFactory.createDispatcher(JobDispatcherFactory.java:41)
job-cluster_1 | at org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.create(AbstractDispatcherResourceManagerComponentFactory.java:184)
job-cluster_1 | ... 6 more
job-cluster_1 | Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.examples.wordcount.WordCount
job-cluster_1 | at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
job-cluster_1 | at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
job-cluster_1 | at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
job-cluster_1 | at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
job-cluster_1 | at org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.createPackagedProgram(ClassPathJobGraphRetriever.java:116)
job-cluster_1 | ... 10 more{code}
This issue can be fixed by chown'ing the job.jar file to flink:flink in the Dockerfile.",,adamonduty,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12546,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 24 17:01:34 UTC 2019,,,,,,,,,,"0|z02ad4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/19 09:57;chesnay;[~yunta] What's the state here?;;;","24/May/19 10:02;yunta;[~chesnay], I fix this in PR [https://github.com/apache/flink/pull/8391] two weeks ago. However, sine that PR fix both FLINK-12416 and FLINK-12375. I just found only FLINK-12416 attach the PR url automatically while this not. I have attach the url here and please take a look at the PR.;;;","24/May/19 17:01;trohrmann;Fixed via
1.9.0: f1c3ac47b67a8940fcfdfb96ce3de5a32b34901d
1.8.1: 0094df9dc284d8748c80db7b5c7993f995dc59b0;;;",,,,,,,,,,,,,,,,,,,,,
SystemResourcesMetricsITCase unstable,FLINK-12359,13230558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,28/Apr/19 15:29,03/May/19 09:25,13/Jul/23 08:05,03/May/19 09:25,1.9.0,,,,,,,,1.9.0,,,,Runtime / Metrics,Tests,,0,pull-request-available,test-stability,,"The {{SystemResourcesMetricsITCase}} checks that task managers register specific set of metrics if configured to do so. The test assumes that the TM is already started completely when the test starts, but this may not be the case.",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8307: [FLINK-12359][metrics][tests] Harden SystemResourcesMetricsITCase
URL: https://github.com/apache/flink/pull/8307
 
 
   ## What is the purpose of the change
   
   Hardens the `SystemResourcesMetricsITCase` against
   * slow starts of a TaskManager, in which case metrics could be registered later than the test expects
   * the host portion of the metric identifier not being `localhost`
   
   We now exclude the host from the identifier, and generate a future in the reporter that we wait on instead.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Apr/19 16:09;githubbot;600","zentol commented on pull request #8307: [FLINK-12359][metrics][tests] Harden SystemResourcesMetricsITCase
URL: https://github.com/apache/flink/pull/8307
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/May/19 09:25;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 03 09:25:33 UTC 2019,,,,,,,,,,"0|z027rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/19 09:25;chesnay;master: e72c9d8881faf4681c1c1aa229e79e77a89ec15f
;;;",,,,,,,,,,,,,,,,,,,,,,,
Remove useless code in TableConfig,FLINK-12357,13230540,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,28/Apr/19 10:16,03/May/19 12:03,13/Jul/23 08:05,03/May/19 12:02,,,,,,,,,1.9.0,,,,Table SQL / API,,,0,pull-request-available,,,,,fan_li_ya,hequn8128,jark,twalthr,,,,,,,,,,,,,,,"hequn8128 commented on pull request #8301: [FLINK-12357][api-java][hotfix] Remove useless code in TableConfig
URL: https://github.com/apache/flink/pull/8301
 
 
   
   ## What is the purpose of the change
   
   Remove useless code in TableConfig. It was added accidentally in [FLINK-11067](https://issues.apache.org/jira/browse/FLINK-11067).
   
   
   ## Brief change log
   
     - Remove `userDefinedConfig` in `TableConfig`
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Apr/19 12:07;githubbot;600","asfgit commented on pull request #8301: [FLINK-12357][api-java][hotfix] Remove useless code in TableConfig
URL: https://github.com/apache/flink/pull/8301
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/May/19 12:03;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 03 12:02:52 UTC 2019,,,,,,,,,,"0|z027nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/19 12:02;twalthr;Fixed in 1.9.0: 4c8bd42474cd0c5ef41783770a23816f973c9064;;;",,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperator should deep copy StreamElement when object reuse is enabled,FLINK-12351,13230519,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,28/Apr/19 03:39,28/May/21 09:13,13/Jul/23 08:05,24/Apr/21 13:58,,,,,,,,,1.13.1,1.14.0,,,API / DataStream,,,0,pull-request-available,stale-assigned,,"Currently, AsyncWaitOperator directly put the input StreamElement into {{StreamElementQueue}}. But when object reuse is enabled, the StreamElement is reused, which means the element in {{StreamElementQueue}} will be modified. As a result, the output of AsyncWaitOperator might be wrong.

An easy way to fix this might be deep copy the input StreamElement when object reuse is enabled, like this: https://github.com/apache/flink/blob/blink/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/async/AsyncWaitOperator.java#L209",,aitozi,hailong wang,jark,kisimple,leonard,pnowojski,rmetzger,tartarus,trohrmann,wenlong.lwl,yunta,,,,,,,,"wuchong commented on pull request #8321: [FLINK-12351][DataStream] Fix AsyncWaitOperator to deep copy StreamElement when object reuse is enabled
URL: https://github.com/apache/flink/pull/8321
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, AsyncWaitOperator directly put the input StreamElement into StreamElementQueue. But when object reuse is enabled, the StreamElement is reused, which means the element in StreamElementQueue will be modified. As a result, the output of AsyncWaitOperator might be wrong.
   
   This pull request is aiming to fix this problem.
   
   ## Brief change log
   
   - initialize `isObjectReuseEnabled` flag in `AsyncWaitOperator#open()` from `ExecutionConfig`.
   - deep copy input `StreamRecord` using serializer if `isObjectReuseEnabled` is true
   
   
   ## Verifying this change
   
   add a test in `AsyncWaitOperatorTest` to verify state snapshot and restore is correct even if object reuse is enabled and the input record is mutable.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable )
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Apr/19 12:39;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 24 13:58:42 UTC 2021,,,,,,,,,,"0|z027io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/19 08:44;aitozi;Hi, [~jark]

I checked the objectuse config in other operator just now. Found that it's mostly been checked in batch related operator. Is only AsyncWaitOperator affected?  But i think other operator may be affected too. If user use the object reuse feature without paying attention to the side effect of the changes on the object as the doc of ExecutionConfig#enableObjectReuse say, the result may be unpredictable. Or we have to enable objectReuse to operator level to let user config the behaviour of the operator individually, what's your idea?;;;","29/Apr/19 01:28;jark;Hi [~aitozi], I think fix the bug in AsyncWaitOperator and enable objectReuse on operator level are two orthogonal problems. We can create another JIRA to discuss the operator level object reuse problem.

Currently, I only find the AsyncWaitOperator is affected, because it doesn't deep copy input record before put it into heap buffer (Java ArrayDeque).

IMO, no matter object reuse is enabled or not, the AsyncWaitOperator should output the same result, because it's the framework code not user code.

Hi [~till.rohrmann], what do you think about this? If you don't object, I can create a PR for this.;;;","29/Apr/19 09:42;trohrmann;Go ahead with fixing this problem [~jark].;;;","29/Apr/19 13:09;rmetzger;Hey [~jark], thanks a lot for opening a ticket in the FLINK bug tracker. I just manually assigned the ticket to a component.

For future tickets, please remember to always assign a new issue to a component, so that the component owner can pick it up.;;;","29/Apr/19 14:22;jark;[~rmetzger] Thanks for the reminder. ;;;","19/Aug/20 02:10;wenlong.lwl;Hi, [~jark][~trohrmann] I think we may need to fix this issue in 1.11, it may be  a regression for 1.11. 

Before 1.11, AsyncWaitOperator is not chainnable because of [FLINK-13063|https://issues.apache.org/jira/browse/FLINK-13063], all of input records are new created from network inputs, so this bug would not be triggerred.

In 1.11, AsyncWaitOperator is chainnable again([FLINK-16219|https://issues.apache.org/jira/browse/FLINK-16219]), this bug would affect the result when object reuse is enabled. ;;;","19/Aug/20 06:51;trohrmann;Thanks for reporting this issue [~wenlong.lwl]. Pulling in [~AHeise] and [~pnowojski] who worked on FLINK-16219 and who might be able to tell more about the implications of this change.;;;","19/Aug/20 07:49;pnowojski;I think this is a valid concern. I'm not sure how much performance impact matters here (probably not much judging by the common AsyncWaitOperator usecases). We we could try to avoid the deepcopy overhead when the operator is the head of the chain, but I'm not sure how elegant would it be to depend on such behaviour.;;;","20/Aug/20 09:56;wenlong.lwl;[~pnowojski] As I known, it is easy to get whether the op is the head of chain by: StreamConfig#isChainStart;;;","20/Aug/20 10:30;pnowojski;Yes that's true. I was more worried about relaying on an assumption that network stack is not re using the records in any way. But maybe this is not big of an issue and could be guarded by some unit test for `AsyncWaitOperator`.;;;","16/Apr/21 11:10;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","24/Apr/21 13:58;jark;Fixed in 
 - master: a4dcd91455d94c7500ce761aa6aa4337cb552b67
 - release-1.13: 253553062a6c944bac174af0804fd192d550b97f
;;;",,,,,,,,,,,,
flink-table-runtime-blink is missing scala suffix,FLINK-12347,13230479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fan_li_ya,chesnay,chesnay,27/Apr/19 15:09,28/Apr/19 09:30,13/Jul/23 08:05,28/Apr/19 09:30,1.9.0,,,,,,,,1.9.0,,,,Build System,Table SQL / Runtime,,0,pull-request-available,,,{{flink-table-runtime-blink}} has a dependency on {{flink-streaming-java}} and thus requires a scala suffix.,,,,,,,,,,,,,,,,,,,,"liyafan82 commented on pull request #8293: [FLINK-12347][Table SQL/Runtime]add scala suffix for flink-table-runt…
URL: https://github.com/apache/flink/pull/8293
 
 
   …ime-blink
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Add scala suffix to project flink-table-runtime-blink
   
   
   ## Brief change log
   
     - Change pom.xml for project flink-table-runtime-blink, add scala suffix to artifact ID
     - Change artifact ID for flink-table-runtime-blink, in pom.xml files dependent on flink-table-runtime-blink
   
   
   ## Verifying this change
   
   *Manual build passed in my computer.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Apr/19 02:03;githubbot;600","zentol commented on pull request #8293: [FLINK-12347][Table SQL/Runtime]add scala suffix for flink-table-runt…
URL: https://github.com/apache/flink/pull/8293
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Apr/19 09:27;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 28 09:30:11 UTC 2019,,,,,,,,,,"0|z0279s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/19 09:30;chesnay;master: 6bc25c2c3691d9bd2c12fcb0b3c123cf5ab70b7b;;;",,,,,,,,,,,,,,,,,,,,,,,
Scala-suffix check broken on Travis,FLINK-12346,13230478,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/Apr/19 15:09,28/Apr/19 11:08,13/Jul/23 08:05,28/Apr/19 11:08,1.9.0,,,,,,,,1.9.0,,,,Build System,Travis,,0,pull-request-available,,,"the scala-suffix check currently does not work on travis since the maven output is not what the script expects. On travis we have timestamps in the maven output, which breaks the parsing.",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8289: [FLINK-12346][travis][build] Account for timestampts in scala-suffix check
URL: https://github.com/apache/flink/pull/8289
 
 
   ## What is the purpose of the change
   
   The scala-suffix check currently does not work on travis since the maven output is not what the script expects. On travis we have timestamps in the maven output, which breaks the parsing.
   
   ## Brief change log
   
   The travis run of this PR will fail since `flink-table-runtime-blink` is missing a scala suffix.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Apr/19 15:12;githubbot;600","zentol commented on pull request #8289: [FLINK-12346][travis][build] Account for timestampts in scala-suffix check
URL: https://github.com/apache/flink/pull/8289
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Apr/19 11:08;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 28 11:08:49 UTC 2019,,,,,,,,,,"0|z0279k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/19 11:08;chesnay;master: 8f02746ec7316ccdc6d25e915f3b58595438cc44;;;",,,,,,,,,,,,,,,,,,,,,,,
Yarn Resource Manager Acquires Too Many Containers,FLINK-12342,13230359,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,ZhenqiuHuang,ZhenqiuHuang,26/Apr/19 17:39,07/Nov/19 10:31,13/Jul/23 08:05,06/Nov/19 22:53,1.6.4,1.7.2,1.8.0,,,,,,1.10.0,1.8.3,1.9.2,,Deployment / YARN,,,0,pull-request-available,,,"In currently implementation of YarnFlinkResourceManager, it starts to acquire new container one by one when get request from SlotManager. The mechanism works when job is still, say less than 32 containers. If the job has 256 container, containers can't be immediately allocated and appending requests in AMRMClient will be not removed accordingly. We observe the situation that AMRMClient ask for current pending request + 1 (the new request from slot manager) containers. In this way, during the start time of such job, it asked for 4000+ containers. If there is an external dependency issue happens, for example hdfs access is slow. Then, the whole job will be blocked without getting enough resource and finally killed with SlotManager request timeout.

Thus, we should use the total number of container asked rather than pending request in AMRMClient as threshold to make decision whether we need to add one more resource request.



",We runs job in Flink release 1.6.3. ,kisimple,Paul Lin,sanath.mv,trohrmann,yanyan300300,ZhenqiuHuang,,,,,,,,,,,,,"HuangZhenQiu commented on pull request #8306: [FLINK-12342] Use total requested pending containers to bound the maximum containers to…
URL: https://github.com/apache/flink/pull/8306
 
 
   ## What is the purpose of the change
   
   Fix the issue of requesting too many containers issue in FlinkResourceManager. Original the new request issue is bounded by the number of pending containers in AMRMClientAsync. As every time AMRMClientAsync will issue request of pendingContainers  + 1 containers, it always ask more than we need. Especially when an Flink application needs 100+ containers, the number total containers will be in thousands. The issue will be enlarged when we do cluster maintenance and restart 1000+ applications as soon as possible. Too many request will overload yarn resource manager and introduce extra delay for restarting applications.
   
   ## Brief change log
   
     - record the total number of containers are requested and still in pending status
     - use the totalRequestedPendingContainers as upper bound fore new container acquirement
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Apr/19 15:26;githubbot;600","tillrohrmann commented on pull request #8306: [FLINK-12342] Add fast-heartbeat-delay yarn config for  jobs with large number of containers
URL: https://github.com/apache/flink/pull/8306
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/May/19 15:24;githubbot;600","tillrohrmann commented on pull request #10089: [FLINK-12342][yarn] Remove container requests in order to reduce excess containers
URL: https://github.com/apache/flink/pull/10089
 
 
   ## What is the purpose of the change
   
   This commit changes the order in which the container requests are removed when
   onContainersAllocated is being called. The idea is to remove the container requests
   as fast as possible in order to avoid allocating excess containers as described in
   YARN-1902.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Nov/19 14:34;githubbot;600","tillrohrmann commented on pull request #10089: [FLINK-12342][yarn] Remove container requests in order to reduce excess containers
URL: https://github.com/apache/flink/pull/10089
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 22:51;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,FLINK-13184,,,,,FLINK-14582,,,,,,,"29/Apr/19 18:41;ZhenqiuHuang;Screen Shot 2019-04-29 at 12.06.23 AM.png;https://issues.apache.org/jira/secure/attachment/12967398/Screen+Shot+2019-04-29+at+12.06.23+AM.png","29/Apr/19 08:09;ZhenqiuHuang;container.log;https://issues.apache.org/jira/secure/attachment/12967352/container.log","29/Apr/19 18:42;ZhenqiuHuang;flink-1.4.png;https://issues.apache.org/jira/secure/attachment/12967399/flink-1.4.png","29/Apr/19 18:42;ZhenqiuHuang;flink-1.6.png;https://issues.apache.org/jira/secure/attachment/12967400/flink-1.6.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 10:30:38 UTC 2019,,,,,,,,,,"0|z026iw:",9223372036854775807,With Flink 1.9.0 the Yarn heartbeat configuration parameter has been renamed from `yarn.heartbeat-delay` to `yarn.heartbeat.interval`.,,,,,,,,,,,,,,,,,,,"28/Apr/19 11:52;trohrmann;Thanks for reporting this issue [~hpeter]. For my understanding, will the previous container requests be added up with every call to {{AMRMClientAsync#addContainerRequest}} to the total number of container requests? So to say, the system would request 1 + 2 + 3 + 4 + 5 + ... + n if calling n times {{addContainerRequest}} and if no container request could be fulfilled in the meantime? Or how does it happen that we add 4000+ container requests to the {{AMRMClient}}? ;;;","28/Apr/19 13:14;ZhenqiuHuang;[~till.rohrmann]
The job that has the issue tries to acquire 256 containers. At the same time it starts, hdfs that serves the jar is very relatively slow. You can image the sum of 1 + 2 + 3  +  ....  T. It stops to acquire new when 256  - T < the appending request in  AMRMClientAsync.;;;","29/Apr/19 06:42;trohrmann;Before diving into the implementation, I would first like to fully understand the problem. Concretely a Yarn reference would be good which explains the behaviour. Otherwise we might simply fix a symptom or not the problem at all.;;;","29/Apr/19 08:10;ZhenqiuHuang; [^container.log] 

[~till.rohrmann]
As the full log is too big. I copied the container allocation part for you to diagnosis.;;;","29/Apr/19 18:46;ZhenqiuHuang; !flink-1.4.png!  !flink-1.6.png! 

[~till.rohrmann]
This is the same job needs (256 containers). If runs on Flink 1.4, it only totally acquired 257 containers. But if run on Flink 1.6, it will acquire much large number of containers. The allocation of each container will goes to very slow, thus cause issue of deployment job with more than 10 minutes which is unacceptable for streaming job use cases.;;;","01/May/19 05:46;ZhenqiuHuang;[~till.rohrmann]
After reading the AMRMAsynclient, I just find the resource request is actually sent in each of heartbeat. If the existing pending request N is not removed yet, the new added request will be added as N +1 and be sent to RM. For the issue we observe, I think it is caused by  FAST_YARN_HEARTBEAT_INTERVAL_MS = 500 is set during the resource allocation triggered by SlotManager.  Somehow the number of container allocated is always less than pending request within 500 millisecond. So each of fast heartbeat will ask for extra number of containers. If we change FAST_YARN_HEARTBEAT_INTERVAL_MS to 2000 ms, and wait for more containers be returned before sending another heartbeat, we can definitely reduce the total number of requested containers.

Thus, the solution I would like to propose is to make the FAST_YARN_HEARTBEAT_INTERVAL_MS as one of YarnConfigOptions. So that the parameter can be tuned according to the size of job/cluster. How do you think?



;;;","02/May/19 06:14;ZhenqiuHuang;As using the config and set it to 3000 milliseconds, the job with 256 containers can be successfully launched with only 1000+ total requested containers. The number can be further reduced by using larger number, such as 5000 or even higher. So, for small jobs with 32 containers, user should just default value for sending out request as soon as possible. For large jobs, user need to tune the parameter to trade-off the fast request and negative impact of repetitively as more containers.;;;","02/May/19 09:23;trohrmann;Thanks for the investigation of this problem [~hpeter]. I think you are right that our aggressive {{FAST_YARN_HEARTBEAT_INTERVAL}} plus this YARN-1902 bug are the cause for the problem. If YARN-1902 were properly resolved, then it wouldn't be a problem. Until this is the case, a way to mitigate the problem would be to make {{FAST_YARN_HEARTBEAT_INTERVAL}} configurable as you've suggested.;;;","07/May/19 15:27;trohrmann;Fixed via
1.9.0: 3871d4d2bf19d904252ed2fe8fe7cbad9af2c634
1.8.1: 1e25889796f1fb5e857b51158005e89d7a462595
1.7.3: f221542031a4725040c952c09a5c012b8fed2efb;;;","05/Nov/19 14:19;trohrmann;The problem seems to be not entirely resolved: https://lists.apache.org/thread.html/173a678c02246af6b6a7b3dcfac0d548f5be27496fb5b53145b853e5@%3Cuser.flink.apache.org%3E;;;","05/Nov/19 15:05;trohrmann;One problem seems to be that we remove the container request too slowly since we remove one request and then start a {{TaskExecutor}} in the container. Start a {{TaskExecutor}} can take some time since we upload files to HDFS. I propose to change the {{YarnResourceManager#onContainerAllocated}} method to first remove all container requests and only then to start the {{TaskExecutors}}.;;;","06/Nov/19 22:53;trohrmann;Fixed via

1.10.0: b8500691c6b1e23c61c9408b5562d67bb2fc2336
1.9.2: 5c9a013e5bc2f7f309b75eb6d4310d16d5585d90
1.8.3: cb319547268aa282bf413d1ed9a1cac8999ae80d;;;","07/Nov/19 10:30;trohrmann;I fear that this issue has not been fully fixed with the latest commits but only mitigated as we are still blocking the main thread when starting the {{TaskExecutors}}. This could lead to a delay of processing incoming {{YarnResourceManager#onContainersAllocated}} messages. For more details see FLINK-13184. ;;;",,,,,,,,,,,
Statsd reporter gives wrong metrics when using negative numbers,FLINK-12325,13229937,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Xeli,Xeli,Xeli,24/Apr/19 18:18,09/May/19 07:01,13/Jul/23 08:05,09/May/19 07:01,1.6.4,1.7.2,1.8.0,,,,,,1.9.0,,,,Runtime / Metrics,,,0,pull-request-available,,,"The statsd reporter has a bug I believe when using negative numbers.

 

This is because when a metric is sent it is first converted to a string value. This means 100 becomes ""100"" and -100 becomes ""-100"". This value is then sent to statsd as a gauge value.

See this line for the conversion to string: [https://github.com/apache/flink/blob/master/flink-metrics/flink-metrics-statsd/src/main/java/org/apache/flink/metrics/statsd/StatsDReporter.java#L130]

And this line for sending it as a gauge:

[https://github.com/apache/flink/blob/master/flink-metrics/flink-metrics-statsd/src/main/java/org/apache/flink/metrics/statsd/StatsDReporter.java#L184]

 

This means a value of -100 will be sent like this:
{code:java}
<metric_name>:-100|g{code}
 

The statsd protocol how ever states the following ([https://github.com/statsd/statsd/blob/master/docs/metric_types.md#gauges]):
{noformat}
Adding a sign to the gauge value will change the value, rather than setting it.
{noformat}
 

 

So sending -100 multiple times means the gauge in statsd will be decremented multiple times, rather than set to -100.

 

I believe this isn't how flink expects it to work, is it?",,chesnay,Xeli,,,,,,,,,,,,,,,,,"Xeli commented on pull request #8259: [FLINK-12325] [metrics-statsd] Fix bug in statsd exporter when using negative values
URL: https://github.com/apache/flink/pull/8259
 
 
   ## What is the purpose of the change
   
   Fix a bug in the statsd reporter. With this PR flink is able to correctly send negative metric values to statsd. See FLINK-12325 for the exact details what is going wrong.
   
   ## Brief change log
   - * Change in StatsdReporter class. When sending negative values to statsd first reset the metric to 0
   
   ## Verifying this change
   
   This change added an unit tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/19 20:05;githubbot;600","zentol commented on pull request #8259: [FLINK-12325][metrics] Fix bug in statsd exporter when using negative values
URL: https://github.com/apache/flink/pull/8259
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 06:57;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 09 07:01:29 UTC 2019,,,,,,,,,,"0|z023xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/19 07:01;chesnay;master: 92b1a66a63aa10fc58c9f2ac4baca859437db40c;;;",,,,,,,,,,,,,,,,,,,,,,,
StackOverFlowError in cep.nfa.sharedbuffer.SharedBuffer,FLINK-12319,13229862,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fan_li_ya,mpf,mpf,24/Apr/19 12:42,01/Oct/19 15:39,13/Jul/23 08:05,05/Jul/19 11:02,1.6.4,1.7.2,1.8.0,,,,,,1.7.3,1.8.2,1.9.0,,Library / CEP,,,0,pull-request-available,,," 

I wrote a simple SourceFunction that creats Events in a loop.

The CEP pattern is very simple:
{code:java}
      final Pattern<Event, ?> failurePattern =
              Pattern.<Event>begin(""5 or more failures"", AfterMatchSkipStrategy.skipPastLastEvent())
                      .subtype(LoginEvent.class)
                      .where(
                              new IterativeCondition<LoginEvent>() {
                                  @Override
                                  public boolean filter(LoginEvent value, Context<LoginEvent> ctx) throws Exception {
                                      return value.get(""type"").equals(""failed"");
                                  }
                              })
                      .times(5)
                      .next(""1 or more successes"")
                      .subtype(LoginEvent.class)
                      .where(
                              new IterativeCondition<LoginEvent>() {
                                  @Override
                                  public boolean filter(LoginEvent value, Context<LoginEvent> ctx) throws Exception {
                                      return value.get(""type"").equals(""success"");
                                  }
                              })
                      .times(1)
                      .within(Time.milliseconds(20));
{code}
 

After about 100k Events, Flink aborts with this stacktrace:

 
{noformat}
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
    at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:630)
    at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:123)
    at org.classdump.alerts.FlinkCep.brute_force_login(FlinkCep.java:263)
    at org.classdump.alerts.FlinkCep.main(FlinkCep.java:41)
Caused by: java.lang.StackOverflowError
    at org.apache.flink.runtime.state.heap.HeapMapState.get(HeapMapState.java:85)
    at org.apache.flink.runtime.state.UserFacingMapState.get(UserFacingMapState.java:47)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:339)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.releaseNode(SharedBuffer.java:342)
    at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.removeNode(SharedBuffer.java:355)
[...]{noformat}
 

This happens with version 1.8.0, 1.7.2, 1.6.4

Version 1.5.6 does not have this issue.

Seems to be related to FLINK-9418

 

 ","Ubuntu 18.04

openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.18.04.1-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)",dwysakowicz,fan_li_ya,knaufk,mpf,rmetzger,,,,,,,,,,,,,,"liyafan82 commented on pull request #8511: [FLINK-12319][Library/CEP]Change the logic of releasing node from recursive to non-recursive
URL: https://github.com/apache/flink/pull/8511
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Resolve issue FlINK-12319. The CEP job throws StackOverFlowError when releasing the nodes in the SharedBuffer. 
   
   After investigation, it shows that the reason is that the logic for releasing nodes in the SharedBuffer is a recursive algorithm. The depth of the recursion depends on the diameter of the graph, which is proportional the number of buffered events (the graph is a line graph). 
   In the experiments, the number of buffered events can be as many as 100,000, which will easily cause stack overflow.
   
   Therefore, in this PR we change the logic from recursive to non-recursive. 
   
   
   ## Brief change log
   
     - Change the SharedAccessBuffer#releaseNode to a non-recursive algorithm.
     
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as SharedBufferTest#testSharedBuffer and SharedBufferTest#testClearingSharedBufferWithMultipleEdgesBetweenEntries.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/May/19 10:22;githubbot;600","dawidwys commented on pull request #8511: [FLINK-12319][Library/CEP]Change the logic of releasing node from recursive to non-recursive
URL: https://github.com/apache/flink/pull/8511
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jul/19 06:44;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 11:02:12 UTC 2019,,,,,,,,,,"0|z023gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/19 01:39;fan_li_ya;Hi [~mpf], thank you for opening this issue.

It seems like a bug related to the ending condition of the recursive calls.

However, the related code no longer exists in the latest code base, so it may not exist in the next release. ;;;","25/Apr/19 08:05;mpf; 

Hi [~fan_li_ya],

the code still exists IMHO:

[https://github.com/apache/flink/blob/master/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/sharedbuffer/SharedBufferAccessor.java#L256]

 ;;;","26/Apr/19 03:18;fan_li_ya;Hi [~mpf], thanks again for the additional information. 

It seems like a real bug of Flink. Would you please provide more information to reproduce the error? Can we reproduce the problem in local host? What is the implementation of class LoginEvent?;;;","10/May/19 12:39;dwysakowicz;Hi [~mpf]
Could you share with us a reproducible example, as I could not reproduce it.

It is true that this method can create a few nested calls, but it should not exceed the length of the longest match. (So in your case with {{times(5)}} it should be no longer than 5x2(remove,release) calls). This should not result in a {{StackOverflowError}}.;;;","14/May/19 12:45;mpf;Hello [~fan_li_ya], Hello [~dwysakowicz],

 

sorry for the delay. I've created a small example here:  [https://github.com/mpfz0r/flink-cep-issue]

 ;;;","22/May/19 06:16;fan_li_ya;Hi [~mpf], thank you for your feedback. I am looking at this issue.;;;","22/May/19 10:35;fan_li_ya;[~mpf] The problem has been fixed by our [PR|[https://github.com/apache/flink/pull/8511].] Would you please take a look?;;;","05/Jul/19 11:02;dwysakowicz;Fixed in:
master: f130e4bc259746b1542a2f4d8907d3b35195feb8
1.8: 755ab6f859c06c8c2955e5fc916ed412db52b125
1.7: 5d6ff74c51d3065e0428d2641e938cc65cf31833;;;",,,,,,,,,,,,,,,,
SynchronousCheckpointITCase.taskCachedThreadPoolAllowsForSynchronousCheckpoints is unstable,FLINK-12313,13229804,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,1u0,uce,uce,24/Apr/19 08:31,14/Jun/19 06:34,13/Jul/23 08:05,14/Jun/19 06:34,,,,,,,,,1.9.0,,,,Runtime / Checkpointing,,,0,pull-request-available,test-stability,,"{{SynchronousCheckpointITCase.taskCachedThreadPoolAllowsForSynchronousCheckpoints}} fails and prints the Thread stack traces due to no output on Travis occasionally.

{code}
==============================================================================
Printing stack trace of Java process 10071
==============================================================================
2019-04-24 07:55:29
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode):

""Attach Listener"" #17 daemon prio=9 os_prio=0 tid=0x00007f2948920000 nid=0x2cf5 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Async calls on Test Task (1/1)"" #15 daemon prio=5 os_prio=0 tid=0x00007f2948dd1800 nid=0x27a9 waiting on condition [0x00007f292cea9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008bb5e558> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""Async calls on Test Task (1/1)"" #14 daemon prio=5 os_prio=0 tid=0x00007f2948dce800 nid=0x27a8 in Object.wait() [0x00007f292cfaa000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008bac58f8> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.streaming.runtime.tasks.SynchronousSavepointLatch.blockUntilCheckpointIsAcknowledged(SynchronousSavepointLatch.java:66)
	- locked <0x000000008bac58f8> (a java.lang.Object)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:726)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:604)
	at org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase$SynchronousCheckpointTestingTask.triggerCheckpoint(SynchronousCheckpointITCase.java:174)
	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1182)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""CloseableReaperThread"" #13 daemon prio=5 os_prio=0 tid=0x00007f2948d9b800 nid=0x27a7 in Object.wait() [0x00007f292d0ab000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008bbe3990> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x000000008bbe3990> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:193)

""Test Task (1/1)"" #12 prio=5 os_prio=0 tid=0x00007f2948d97000 nid=0x27a6 in Object.wait() [0x00007f292d1ac000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008e63f7d8> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:63)
	- locked <0x000000008e63f7d8> (a java.lang.Object)
	at org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase$SynchronousCheckpointTestingTask.run(SynchronousCheckpointITCase.java:161)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:335)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:724)
	at java.lang.Thread.run(Thread.java:748)

""process reaper"" #11 daemon prio=10 os_prio=0 tid=0x00007f294885e000 nid=0x2793 waiting on condition [0x00007f292d7e5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080089928> (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""surefire-forkedjvm-ping-30s"" #10 daemon prio=5 os_prio=0 tid=0x00007f2948433000 nid=0x2775 waiting on condition [0x00007f292dd20000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080089b88> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

""surefire-forkedjvm-command-thread"" #9 daemon prio=5 os_prio=0 tid=0x00007f2948416000 nid=0x276e runnable [0x00007f292e02a000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	- locked <0x000000008008be38> (a java.io.BufferedInputStream)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
	at java.lang.Thread.run(Thread.java:748)

""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007f2948214800 nid=0x276a runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f2948207000 nid=0x2769 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f2948205000 nid=0x2768 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f2948203000 nid=0x2767 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f2948201800 nid=0x2766 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f29481c9000 nid=0x2765 in Object.wait() [0x00007f292f346000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008008c7b8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x000000008008c7b8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f29481c4800 nid=0x2764 in Object.wait() [0x00007f292f447000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008008c970> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x000000008008c970> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

""main"" #1 prio=5 os_prio=0 tid=0x00007f294800a800 nid=0x2759 in Object.wait() [0x00007f294df6f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008e63f4a0> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.core.testutils.MultiShotLatch.await(MultiShotLatch.java:50)
	- locked <0x000000008e63f4a0> (a java.lang.Object)
	at org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.taskCachedThreadPoolAllowsForSynchronousCheckpoints(SynchronousCheckpointITCase.java:140)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

Full log: https://api.travis-ci.org/v3/job/523820519/log.txt

The changes of the commit that caused this are in an independent module ({{flink-container}}) and they succeeded on Travis before (https://travis-ci.org/apache/flink/builds/520877649).",,aitozi,dwysakowicz,fan_li_ya,pnowojski,trohrmann,uce,,,,,,,,,,,,,"1u0 commented on pull request #8602: [FLINK-12313] Add workaround to avoid race condition in SynchronousCheckpointITCase test
URL: https://github.com/apache/flink/pull/8602
 
 
   ## What is the purpose of the change
   
   The `SynchronousCheckpointITCase` has a race condition and with some chance may fail on random tests on CI. This PR adds an additional synchronization point as a workaround to avoid the issue.
   
   Additionally, the PR simplifies (refactors) the test, although the race condition is present in both refactored and non-refactored versions.
   
   ## Brief change log
   
     - The test is simplified to not use dedicated locks for general flow checks. The test run set to limited by a timeout.
     - Added a hacky synchronization point in `advanceToEndOfEventTime` method, to make sure that `triggerCheckpoint` thread has progressed far enough, before triggering `notifyCheckpointComplete`.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jun/19 20:54;githubbot;600","1u0 commented on pull request #8602: [FLINK-12313] Add workaround to avoid race condition in SynchronousCheckpointITCase test
URL: https://github.com/apache/flink/pull/8602
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jun/19 07:56;githubbot;600","1u0 commented on pull request #8602: [FLINK-12313] Add workaround to avoid race condition in SynchronousCheckpointITCase test
URL: https://github.com/apache/flink/pull/8602
 
 
   ## What is the purpose of the change
   
   The `SynchronousCheckpointITCase` has a race condition and with some chance may fail on random tests on CI. This PR adds an additional synchronization point as a workaround to avoid the issue.
   
   Additionally, the PR simplifies (refactors) the test, although the race condition is present in both refactored and non-refactored versions.
   
   ## Brief change log
   
     - The test is simplified to not use dedicated locks for general flow checks. The test run set to limited by a timeout.
     - Added a hacky synchronization point in `advanceToEndOfEventTime` method, to make sure that `triggerCheckpoint` thread has progressed far enough, before triggering `notifyCheckpointComplete`.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jun/19 07:56;githubbot;600","pnowojski commented on pull request #8602: [FLINK-12313] Add workaround to avoid race condition in SynchronousCheckpointITCase test
URL: https://github.com/apache/flink/pull/8602
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jun/19 06:33;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,FLINK-12315,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 14 06:34:33 UTC 2019,,,,,,,,,,"0|z02348:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/19 13:59;pnowojski;Another instance: [https://api.travis-ci.org/v3/job/538336222/log.txt];;;","07/Jun/19 06:57;dwysakowicz;Another instance: https://api.travis-ci.org/v3/job/542529201/log.txt;;;","14/Jun/19 06:34;pnowojski;merged commit c539333 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,
Scala value classes inside case classes cannot be serialized anymore in Flink 1.8.0,FLINK-12301,13229566,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,hiddenbit,hiddenbit,23/Apr/19 09:54,13/May/19 15:06,13/Jul/23 08:05,13/May/19 14:32,1.8.0,,,,,,,,1.8.1,1.9.0,,,API / Scala,,,0,pull-request-available,,,"
There is a regression in Flink 1.8.0 compared to 1.7.2: Scala [value classes|https://docs.scala-lang.org/overviews/core/value-classes.html] cannot be serialized anymore as a case class attribute.

Some short example code:

{code:scala}
package com.example.valueclassissue

import org.apache.flink.streaming.api.scala._

object ValueClassExample extends App {
  val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

  val measurements = env.fromCollection(Seq(Measurement(1, new DegreeCelsius(32.5f))))
  measurements.print()

  env.execute()
}

class DegreeCelsius(val value: Float) extends AnyVal {
  override def toString: String = s""$value °C""
}

case class Measurement(i: Int, temperature: DegreeCelsius)
{code}

While with Flink 1.7.2 the program outputs _{{3> Measurement(1,32.5 °C)}}_ as expected, in Flink 1.8.0 an exception is thrown:

{noformat}
java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

Full log:

{noformat}
2019-04-23T09:33:48.485Z [main] INFO org.apache.flink.api.java.typeutils.TypeExtractor - class com.example.valueclassissue.DegreeCelsius does not contain a setter for field value
2019-04-23T09:33:48.487Z [main] INFO org.apache.flink.api.java.typeutils.TypeExtractor - Class class com.example.valueclassissue.DegreeCelsius cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.
2019-04-23T09:33:49.594Z [main] INFO org.apache.flink.streaming.api.environment.LocalStreamEnvironment - Running job on local embedded Flink mini cluster
2019-04-23T09:33:49.616Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Starting Flink Mini Cluster
2019-04-23T09:33:49.618Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Starting Metrics Registry
2019-04-23T09:33:49.665Z [main] INFO org.apache.flink.runtime.metrics.MetricRegistryImpl - No metrics reporter configured, no metrics will be exposed/reported.
2019-04-23T09:33:49.665Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Starting RPC Service(s)
2019-04-23T09:33:49.896Z [flink-akka.actor.default-dispatcher-3] INFO akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2019-04-23T09:33:49.913Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Trying to start actor system at :0
2019-04-23T09:33:49.952Z [flink-metrics-2] INFO akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2019-04-23T09:33:50.001Z [flink-metrics-2] INFO akka.remote.Remoting - Starting remoting
2019-04-23T09:33:50.139Z [flink-metrics-2] INFO akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://flink-metrics@127.0.1.1:36651]
2019-04-23T09:33:50.144Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Actor system started at akka.tcp://flink-metrics@127.0.1.1:36651
2019-04-23T09:33:50.146Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Starting high-availability services
2019-04-23T09:33:50.155Z [main] INFO org.apache.flink.runtime.blob.BlobServer - Created BLOB server storage directory /tmp/blobStore-0e1c3305-1e6e-47a0-885a-8c82bb5ae09f
2019-04-23T09:33:50.158Z [main] INFO org.apache.flink.runtime.blob.BlobServer - Started BLOB server at 0.0.0.0:40817 - max concurrent requests: 50 - max backlog: 1000
2019-04-23T09:33:50.161Z [main] INFO org.apache.flink.runtime.blob.PermanentBlobCache - Created BLOB cache storage directory /tmp/blobStore-e3046023-c398-4a42-88f4-20949f7876ce
2019-04-23T09:33:50.162Z [main] INFO org.apache.flink.runtime.blob.TransientBlobCache - Created BLOB cache storage directory /tmp/blobStore-ef6f2625-afcc-4937-90ba-a981af59bce4
2019-04-23T09:33:50.162Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Starting 1 TaskManger(s)
2019-04-23T09:33:50.164Z [main] INFO org.apache.flink.runtime.taskexecutor.TaskManagerRunner - Starting TaskManager with ResourceID: ce7b0b20-a06b-4135-aa6b-7a051cfc3672
2019-04-23T09:33:50.217Z [main] INFO org.apache.flink.runtime.taskexecutor.TaskManagerServices - Temporary file directory '/tmp': total 97 GB, usable 60 GB (61.86% usable)
2019-04-23T09:33:50.325Z [main] INFO org.apache.flink.runtime.io.network.buffer.NetworkBufferPool - Allocated 246 MB for network buffer pool (number of memory segments: 7882, bytes per segment: 32768).
2019-04-23T09:33:50.328Z [main] INFO org.apache.flink.runtime.io.network.NetworkEnvironment - Starting the network environment and its components.
2019-04-23T09:33:50.329Z [main] INFO org.apache.flink.runtime.taskexecutor.TaskManagerServices - Limiting managed memory to 0.7 of the currently free heap space (1543 MB), memory will be allocated lazily.
2019-04-23T09:33:50.331Z [main] INFO org.apache.flink.runtime.io.disk.iomanager.IOManager - I/O manager uses directory /tmp/flink-io-f952881a-df88-4b6f-84fe-1e83412f120b for spill files.
2019-04-23T09:33:50.377Z [main] INFO org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration - Messages have a max timeout of 10000 ms
2019-04-23T09:33:50.387Z [main] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/taskmanager_0 .
2019-04-23T09:33:50.400Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Start job leader service.
2019-04-23T09:33:50.401Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.filecache.FileCache - User file cache uses directory /tmp/flink-dist-cache-da57d8de-8359-4f8b-838d-88094398b7cd
2019-04-23T09:33:50.430Z [main] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Starting rest endpoint.
2019-04-23T09:33:50.573Z [main] WARN org.apache.flink.runtime.webmonitor.WebMonitorUtils - Log file environment variable 'log.file' is not set.
2019-04-23T09:33:50.573Z [main] WARN org.apache.flink.runtime.webmonitor.WebMonitorUtils - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'Key: 'web.log.path' , default: null (fallback keys: [{key=jobmanager.web.log.path, isDeprecated=true}])'.
2019-04-23T09:33:50.580Z [main] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2019-04-23T09:33:50.715Z [main] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Rest endpoint listening at localhost:39621
2019-04-23T09:33:50.716Z [main] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint@25f723b0 @ http://localhost:39621
2019-04-23T09:33:50.718Z [mini-cluster-io-thread-1] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - http://localhost:39621 was granted leadership with leaderSessionID=848a2169-0638-4284-b5b1-c74561a5016d
2019-04-23T09:33:50.718Z [mini-cluster-io-thread-1] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader http://localhost:39621 , session=848a2169-0638-4284-b5b1-c74561a5016d
2019-04-23T09:33:50.728Z [main] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/resourcemanager .
2019-04-23T09:33:50.737Z [main] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/dispatcher .
2019-04-23T09:33:50.745Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.resourcemanager.StandaloneResourceManager@586de9c6 @ akka://flink/user/resourcemanager
2019-04-23T09:33:50.745Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.dispatcher.StandaloneDispatcher@171bdca8 @ akka://flink/user/dispatcher
2019-04-23T09:33:50.748Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - ResourceManager akka://flink/user/resourcemanager was granted leadership with fencing token 8d0000744239b25d92a24a84384e421b
2019-04-23T09:33:50.748Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager - Starting the SlotManager.
2019-04-23T09:33:50.750Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/resourcemanager , session=92a24a84-384e-421b-8d00-00744239b25d
2019-04-23T09:33:50.751Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Dispatcher akka://flink/user/dispatcher was granted leadership with fencing token 3be1cf58-f256-464c-a503-0130f1d6653a
2019-04-23T09:33:50.755Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Flink Mini Cluster started successfully
2019-04-23T09:33:50.759Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Connecting to ResourceManager akka://flink/user/resourcemanager(8d0000744239b25d92a24a84384e421b).
2019-04-23T09:33:50.760Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Recovering all persisted jobs.
2019-04-23T09:33:50.761Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/dispatcher , session=3be1cf58-f256-464c-a503-0130f1d6653a
2019-04-23T09:33:50.767Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Resolved ResourceManager address, beginning registration
2019-04-23T09:33:50.768Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Registration at ResourceManager attempt 1 (timeout=100ms)
2019-04-23T09:33:50.771Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registering TaskManager with ResourceID ce7b0b20-a06b-4135-aa6b-7a051cfc3672 (akka://flink/user/taskmanager_0) at ResourceManager
2019-04-23T09:33:50.772Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Successful registration at resource manager akka://flink/user/resourcemanager under registration id 57ee60b35a4b387afe62844e5dd075e5.
2019-04-23T09:33:50.775Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Received JobGraph submission 0934b4e32657180f004df5cf4e003ab4 (Flink Streaming Job).
2019-04-23T09:33:50.775Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Submitting job 0934b4e32657180f004df5cf4e003ab4 (Flink Streaming Job).
2019-04-23T09:33:50.788Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_1 .
2019-04-23T09:33:50.796Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.jobmaster.JobMaster - Initializing job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4).
2019-04-23T09:33:50.800Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.jobmaster.JobMaster - Using restart strategy NoRestartStrategy for Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4).
2019-04-23T09:33:50.827Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job recovers via failover strategy: full graph restart
2019-04-23T09:33:50.847Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.jobmaster.JobMaster - Running initialization on master for job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4).
2019-04-23T09:33:50.847Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.jobmaster.JobMaster - Successfully ran initialization on master in 0 ms.
2019-04-23T09:33:50.866Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.jobmaster.JobMaster - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2019-04-23T09:33:50.876Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.jobmaster.JobManagerRunner@6667cd6 @ akka://flink/user/jobmanager_1
2019-04-23T09:33:50.877Z [mini-cluster-io-thread-4] INFO org.apache.flink.runtime.jobmaster.JobManagerRunner - JobManager runner for job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) was granted leadership with session id ed72f40e-a416-4efc-856d-27e694574e2f at akka://flink/user/jobmanager_1.
2019-04-23T09:33:50.879Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.JobMaster - Starting execution of job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) under job master id 856d27e694574e2fed72f40ea4164efc.
2019-04-23T09:33:50.879Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) switched from state CREATED to RUNNING.
2019-04-23T09:33:50.882Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from CREATED to SCHEDULED.
2019-04-23T09:33:50.890Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{6a6e14d6bca90867458577365cb9721e}]
2019-04-23T09:33:50.894Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from CREATED to SCHEDULED.
2019-04-23T09:33:50.895Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from CREATED to SCHEDULED.
2019-04-23T09:33:50.895Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from CREATED to SCHEDULED.
2019-04-23T09:33:50.895Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from CREATED to SCHEDULED.
2019-04-23T09:33:50.898Z [jobmanager-future-thread-1] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/jobmanager_1 , session=ed72f40e-a416-4efc-856d-27e694574e2f
2019-04-23T09:33:50.898Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.JobMaster - Connecting to ResourceManager akka://flink/user/resourcemanager(8d0000744239b25d92a24a84384e421b)
2019-04-23T09:33:50.900Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.JobMaster - Resolved ResourceManager address, beginning registration
2019-04-23T09:33:50.900Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.JobMaster - Registration at ResourceManager attempt 1 (timeout=100ms)
2019-04-23T09:33:50.902Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registering job manager 856d27e694574e2fed72f40ea4164efc@akka://flink/user/jobmanager_1 for job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.908Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registered job manager 856d27e694574e2fed72f40ea4164efc@akka://flink/user/jobmanager_1 for job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.909Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.JobMaster - JobManager successfully registered at ResourceManager, leader id: 8d0000744239b25d92a24a84384e421b.
2019-04-23T09:33:50.910Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{6a6e14d6bca90867458577365cb9721e}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource manager.
2019-04-23T09:33:50.911Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} for job 0934b4e32657180f004df5cf4e003ab4 with allocation id 16865b3b201cc25ae128095d117c2e7d.
2019-04-23T09:33:50.911Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 16865b3b201cc25ae128095d117c2e7d for job 0934b4e32657180f004df5cf4e003ab4 from resource manager with leader id 8d0000744239b25d92a24a84384e421b.
2019-04-23T09:33:50.912Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 16865b3b201cc25ae128095d117c2e7d.
2019-04-23T09:33:50.912Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Add job 0934b4e32657180f004df5cf4e003ab4 for job leader monitoring.
2019-04-23T09:33:50.914Z [mini-cluster-io-thread-3] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id ed72f40e-a416-4efc-856d-27e694574e2f.
2019-04-23T09:33:50.915Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Resolved JobManager address, beginning registration
2019-04-23T09:33:50.915Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Registration at JobManager attempt 1 (timeout=100ms)
2019-04-23T09:33:50.918Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Successful registration at job manager akka://flink/user/jobmanager_1 for job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.918Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Establish JobManager connection for job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.920Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.923Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{0889151458281fff281d1f40b8949362}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource manager.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{6048f2d28bc988068b4f3c09c96e5bf4}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource manager.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} for job 0934b4e32657180f004df5cf4e003ab4 with allocation id 04c413ac1d7681b769798f365e4a8565.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{af9271e58daaf7760d2bad798c764943}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource manager.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} for job 0934b4e32657180f004df5cf4e003ab4 with allocation id 4c11ede5a7f087d8653710365e1af854.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 04c413ac1d7681b769798f365e4a8565 for job 0934b4e32657180f004df5cf4e003ab4 from resource manager with leader id 8d0000744239b25d92a24a84384e421b.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} for job 0934b4e32657180f004df5cf4e003ab4 with allocation id e31bbfd59504d04ff607286275900171.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 04c413ac1d7681b769798f365e4a8565.
2019-04-23T09:33:50.924Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.925Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Received repeated offer for slot [16865b3b201cc25ae128095d117c2e7d]. Ignoring.
2019-04-23T09:33:50.925Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 4c11ede5a7f087d8653710365e1af854 for job 0934b4e32657180f004df5cf4e003ab4 from resource manager with leader id 8d0000744239b25d92a24a84384e421b.
2019-04-23T09:33:50.925Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 4c11ede5a7f087d8653710365e1af854.
2019-04-23T09:33:50.925Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.925Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Received repeated offer for slot [04c413ac1d7681b769798f365e4a8565]. Ignoring.
2019-04-23T09:33:50.925Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 16865b3b201cc25ae128095d117c2e7d.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Received repeated offer for slot [16865b3b201cc25ae128095d117c2e7d]. Ignoring.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request e31bbfd59504d04ff607286275900171 for job 0934b4e32657180f004df5cf4e003ab4 from resource manager with leader id 8d0000744239b25d92a24a84384e421b.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for e31bbfd59504d04ff607286275900171.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 04c413ac1d7681b769798f365e4a8565.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Received repeated offer for slot [04c413ac1d7681b769798f365e4a8565]. Ignoring.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 16865b3b201cc25ae128095d117c2e7d.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 04c413ac1d7681b769798f365e4a8565.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from SCHEDULED to DEPLOYING.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 16865b3b201cc25ae128095d117c2e7d.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 4c11ede5a7f087d8653710365e1af854.
2019-04-23T09:33:50.926Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Collection Source (1/1) (attempt #0) to ce7b0b20-a06b-4135-aa6b-7a051cfc3672 @ localhost (dataPort=-1)
2019-04-23T09:33:50.930Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from SCHEDULED to DEPLOYING.
2019-04-23T09:33:50.930Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Sink: Print to Std. Out (1/4) (attempt #0) to ce7b0b20-a06b-4135-aa6b-7a051cfc3672 @ localhost (dataPort=-1)
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from SCHEDULED to DEPLOYING.
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Sink: Print to Std. Out (2/4) (attempt #0) to ce7b0b20-a06b-4135-aa6b-7a051cfc3672 @ localhost (dataPort=-1)
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from SCHEDULED to DEPLOYING.
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Sink: Print to Std. Out (3/4) (attempt #0) to ce7b0b20-a06b-4135-aa6b-7a051cfc3672 @ localhost (dataPort=-1)
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from SCHEDULED to DEPLOYING.
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Sink: Print to Std. Out (4/4) (attempt #0) to ce7b0b20-a06b-4135-aa6b-7a051cfc3672 @ localhost (dataPort=-1)
2019-04-23T09:33:50.932Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Received repeated offer for slot [4c11ede5a7f087d8653710365e1af854]. Ignoring.
2019-04-23T09:33:50.943Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Collection Source (1/1).
2019-04-23T09:33:50.943Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 04c413ac1d7681b769798f365e4a8565.
2019-04-23T09:33:50.943Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from CREATED to DEPLOYING.
2019-04-23T09:33:50.944Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot e31bbfd59504d04ff607286275900171.
2019-04-23T09:33:50.944Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable - Activate slot 4c11ede5a7f087d8653710365e1af854.
2019-04-23T09:33:50.944Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) [DEPLOYING]
2019-04-23T09:33:50.948Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) [DEPLOYING].
2019-04-23T09:33:50.949Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Registering task at network: Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) [DEPLOYING].
2019-04-23T09:33:50.949Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Sink: Print to Std. Out (1/4).
2019-04-23T09:33:50.951Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from CREATED to DEPLOYING.
2019-04-23T09:33:50.951Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) [DEPLOYING]
2019-04-23T09:33:50.951Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Sink: Print to Std. Out (2/4).
2019-04-23T09:33:50.951Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) [DEPLOYING].
2019-04-23T09:33:50.952Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Registering task at network: Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) [DEPLOYING].
2019-04-23T09:33:50.953Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Sink: Print to Std. Out (3/4).
2019-04-23T09:33:50.953Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from CREATED to DEPLOYING.
2019-04-23T09:33:50.953Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) [DEPLOYING]
2019-04-23T09:33:50.953Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) [DEPLOYING].
2019-04-23T09:33:50.954Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Registering task at network: Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) [DEPLOYING].
2019-04-23T09:33:50.955Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Sink: Print to Std. Out (4/4).
2019-04-23T09:33:50.955Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from CREATED to DEPLOYING.
2019-04-23T09:33:50.955Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) [DEPLOYING]
2019-04-23T09:33:50.955Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) [DEPLOYING].
2019-04-23T09:33:50.956Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Registering task at network: Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) [DEPLOYING].
2019-04-23T09:33:50.963Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from CREATED to DEPLOYING.
2019-04-23T09:33:50.963Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.964Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.963Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.964Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) [DEPLOYING]
2019-04-23T09:33:50.965Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.967Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) [DEPLOYING].
2019-04-23T09:33:50.965Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2019-04-23T09:33:50.965Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.967Z [Source: Collection Source (1/1)] INFO org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2019-04-23T09:33:50.967Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.965Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2019-04-23T09:33:50.966Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.967Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2019-04-23T09:33:50.967Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.968Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Registering task at network: Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) [DEPLOYING].
2019-04-23T09:33:50.969Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.969Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from DEPLOYING to RUNNING.
2019-04-23T09:33:50.969Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2019-04-23T09:33:51.028Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from RUNNING to FAILED.
java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
2019-04-23T09:33:51.029Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources for Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2).
2019-04-23T09:33:51.045Z [Source: Collection Source (1/1)] INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem streams are closed for task Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) [FAILED]
2019-04-23T09:33:51.059Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Un-registering task and sending final execution state FAILED to JobManager for task Source: Collection Source 833716fb73726ad792a24c184d5312b2.
2019-04-23T09:33:51.064Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Collection Source (1/1) (833716fb73726ad792a24c184d5312b2) switched from RUNNING to FAILED.
java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
2019-04-23T09:33:51.064Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) switched from state RUNNING to FAILING.
java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
2019-04-23T09:33:51.065Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.066Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Discarding the results produced by task execution 833716fb73726ad792a24c184d5312b2.
2019-04-23T09:33:51.066Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Attempting to cancel task Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9).
2019-04-23T09:33:51.066Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.066Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Triggering cancellation of task code Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9).
2019-04-23T09:33:51.068Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.069Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.072Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.073Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources for Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9).
2019-04-23T09:33:51.073Z [Sink: Print to Std. Out (1/4)] INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem streams are closed for task Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) [CANCELED]
2019-04-23T09:33:51.075Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.081Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Attempting to cancel task Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125).
2019-04-23T09:33:51.082Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.082Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Triggering cancellation of task code Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125).
2019-04-23T09:33:51.084Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.084Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources for Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125).
2019-04-23T09:33:51.085Z [Sink: Print to Std. Out (2/4)] INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem streams are closed for task Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) [CANCELED]
2019-04-23T09:33:51.084Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Un-registering task and sending final execution state CANCELED to JobManager for task Sink: Print to Std. Out 6982b1cd1147229d264344ec41df12a9.
2019-04-23T09:33:51.087Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Attempting to cancel task Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93).
2019-04-23T09:33:51.088Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.088Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Triggering cancellation of task code Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93).
2019-04-23T09:33:51.089Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (1/4) (6982b1cd1147229d264344ec41df12a9) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.099Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Attempting to cancel task Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9).
2019-04-23T09:33:51.099Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from RUNNING to CANCELING.
2019-04-23T09:33:51.100Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskmanager.Task - Triggering cancellation of task code Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9).
2019-04-23T09:33:51.100Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.100Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources for Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93).
2019-04-23T09:33:51.100Z [Sink: Print to Std. Out (3/4)] INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem streams are closed for task Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) [CANCELED]
2019-04-23T09:33:51.103Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.103Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources for Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9).
2019-04-23T09:33:51.104Z [Sink: Print to Std. Out (4/4)] INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem streams are closed for task Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) [CANCELED]
2019-04-23T09:33:51.106Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Un-registering task and sending final execution state CANCELED to JobManager for task Sink: Print to Std. Out fcccdf96f9411000517ecc55b620f125.
2019-04-23T09:33:51.107Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Un-registering task and sending final execution state CANCELED to JobManager for task Sink: Print to Std. Out 09fdc88b7fe9efe5eeb0d30327eb8c93.
2019-04-23T09:33:51.108Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (2/4) (fcccdf96f9411000517ecc55b620f125) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.109Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (3/4) (09fdc88b7fe9efe5eeb0d30327eb8c93) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.112Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Un-registering task and sending final execution state CANCELED to JobManager for task Sink: Print to Std. Out 27df5777eb4ebd374f003634a00a7cd9.
2019-04-23T09:33:51.113Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Print to Std. Out (4/4) (27df5777eb4ebd374f003634a00a7cd9) switched from CANCELING to CANCELED.
2019-04-23T09:33:51.114Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Try to restart or fail the job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) if no longer possible.
2019-04-23T09:33:51.114Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) switched from state FAILING to FAILED.
java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
2019-04-23T09:33:51.114Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Could not restart the job Flink Streaming Job (0934b4e32657180f004df5cf4e003ab4) because the restart strategy prevented it.
java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
2019-04-23T09:33:51.114Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Stopping checkpoint coordinator for job 0934b4e32657180f004df5cf4e003ab4.
2019-04-23T09:33:51.114Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore - Shutting down
2019-04-23T09:33:51.124Z [main] INFO org.apache.flink.runtime.minicluster.MiniCluster - Shutting down Flink Mini Cluster
2019-04-23T09:33:51.124Z [main] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Shutting down rest endpoint.
2019-04-23T09:33:51.124Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Job 0934b4e32657180f004df5cf4e003ab4 reached globally terminal state FAILED.
2019-04-23T09:33:51.124Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Stopping TaskExecutor akka://flink/user/taskmanager_0.
2019-04-23T09:33:51.125Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Stop job leader service.
2019-04-23T09:33:51.128Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager - Shutting down TaskExecutorLocalStateStoresManager.
2019-04-23T09:33:51.136Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.JobMaster - Stopping the JobMaster for job Flink Streaming Job(0934b4e32657180f004df5cf4e003ab4).
2019-04-23T09:33:51.137Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Suspending SlotPool.
2019-04-23T09:33:51.137Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.JobMaster - Close ResourceManager connection a7d3b0f5395dbabb2f7c22f3c62765c1: JobManager is shutting down..
2019-04-23T09:33:51.137Z [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Stopping SlotPool.
2019-04-23T09:33:51.137Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Disconnect job manager 856d27e694574e2fed72f40ea4164efc@akka://flink/user/jobmanager_1 for job 0934b4e32657180f004df5cf4e003ab4 from the resource manager.
2019-04-23T09:33:51.149Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.io.disk.iomanager.IOManager - I/O manager removed spill file directory /tmp/flink-io-f952881a-df88-4b6f-84fe-1e83412f120b
2019-04-23T09:33:51.150Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.io.network.NetworkEnvironment - Shutting down the network environment and its components.
2019-04-23T09:33:51.154Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.taskexecutor.JobLeaderService - Stop job leader service.
2019-04-23T09:33:51.154Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.filecache.FileCache - removed file cache directory /tmp/flink-dist-cache-da57d8de-8359-4f8b-838d-88094398b7cd
2019-04-23T09:33:51.154Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Stopped TaskExecutor akka://flink/user/taskmanager_0.
2019-04-23T09:33:51.156Z [ForkJoinPool.commonPool-worker-1] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Removing cache directory /tmp/flink-web-ui
2019-04-23T09:33:51.156Z [ForkJoinPool.commonPool-worker-1] INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Shut down complete.
2019-04-23T09:33:51.159Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
2019-04-23T09:33:51.160Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Stopping dispatcher akka://flink/user/dispatcher.
2019-04-23T09:33:51.160Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager - Closing the SlotManager.
2019-04-23T09:33:51.160Z [flink-akka.actor.default-dispatcher-4] INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager - Suspending the SlotManager.
2019-04-23T09:33:51.160Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Stopping all currently running jobs of dispatcher akka://flink/user/dispatcher.
2019-04-23T09:33:51.161Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.rest.handler.legacy.backpressure.StackTraceSampleCoordinator - Shutting down stack trace sample coordinator.
2019-04-23T09:33:51.161Z [flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Stopped dispatcher akka://flink/user/dispatcher.
2019-04-23T09:33:51.171Z [flink-metrics-2] INFO akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2019-04-23T09:33:51.172Z [flink-metrics-2] INFO akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
2019-04-23T09:33:51.191Z [flink-metrics-2] INFO akka.remote.RemoteActorRefProvider$RemotingTerminator - Remoting shut down.
2019-04-23T09:33:51.206Z [flink-metrics-2] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - Stopping Akka RPC service.
2019-04-23T09:33:51.223Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.blob.PermanentBlobCache - Shutting down BLOB cache
2019-04-23T09:33:51.223Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.blob.TransientBlobCache - Shutting down BLOB cache
2019-04-23T09:33:51.223Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.blob.BlobServer - Stopped BLOB server at 0.0.0.0:40817
2019-04-23T09:33:51.223Z [flink-akka.actor.default-dispatcher-3] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - Stopped Akka RPC service.
Exception in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:638)
	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:123)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1509)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:645)
	at com.example.valueclassissue.ValueClassExample$.delayedEndpoint$com$gotomeeting$xqm$streamprocessor$ValueClassExample$1(ValueClassExample.scala:11)
	at com.example.valueclassissue.ValueClassExample$delayedInit$body.apply(ValueClassExample.scala:5)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.App$class.main(App.scala:76)
	at com.example.valueclassissue.ValueClassExample$.main(ValueClassExample.scala:5)
	at com.example.valueclassissue.ValueClassExample.main(ValueClassExample.scala)
Caused by: java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@466b6f83
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,aljoscha,dwysakowicz,elevy,hiddenbit,kisimple,martijnvisser,uce,,,,,,,,,,,,"aljoscha commented on pull request #8426: [FLINK-12301] Fix ScalaCaseClassSerializer to support value types
URL: https://github.com/apache/flink/pull/8426
 
 
   We now use Scala reflection because it correctly deals with Scala
   langauge features.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/May/19 09:06;githubbot;600","aljoscha commented on pull request #8426: [FLINK-12301] Fix ScalaCaseClassSerializer to support value types
URL: https://github.com/apache/flink/pull/8426
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/May/19 14:33;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 13 15:06:39 UTC 2019,,,,,,,,,,"0|z021nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/19 10:15;aljoscha;The swallowed exception is this:
{code}
Caused by: java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions.
Serializer is org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer@51d7fc56
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:158)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:95)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:59)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:102)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:335)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:727)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: org.apache.flink.streaming.scala.examples.wordcount.DegreeCelsius cannot be cast to java.lang.Number
	at sun.invoke.util.ValueConversions.primitiveConversion(ValueConversions.java:242)
	at sun.invoke.util.ValueConversions.unboxFloat(ValueConversions.java:130)
	at org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer.createInstance(ScalaCaseClassSerializer.scala:50)
	at org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializer.createInstance(ScalaCaseClassSerializer.scala:40)
	at org.apache.flink.api.scala.typeutils.CaseClassSerializer.deserialize(CaseClassSerializer.scala:124)
	at org.apache.flink.api.scala.typeutils.CaseClassSerializer.deserialize(CaseClassSerializer.scala:32)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:155)
	... 6 more
{code}

The root of the problem is that Flink does not have good support for Scala Value Types, i.e. it doesn't understand them and they are analyzed as a generic type. In your example the type would be {{CaseClassTypeInfo<Measurement>(IntegerTypeInfo, GenericTypeInfo<DegreeCelsius>)}}.

This worked in Flink 1.7.x because we were generating Scala code for instantiating a {{Measurement}} in the serializer that was (macro-)generated for {{Measurement}}. It seems this code was able to deal with the dual nature of Scala Value types. The new code (since Flink 1.8) uses Java reflection to find the constructor for {{Measurement}}, which expects a {{float}}, not a {{DegreeCelsius}}.

We don't have support for value types because the Scala compiler does too much ""magic"" around them, i.e. in the generated Java code a value is sometimes the value type and sometimes the underlying type. See also FLINK-2747.;;;","08/May/19 14:25;aljoscha;One (very ugly) solution for this is changing {{ScalaCaseClassSerializer.createInstance(...)}} from
{code}
  override def createInstance(fields: Array[AnyRef]): T = {
    constructor.invoke(fields).asInstanceOf[T]
  }
{code}

to

{code}
override def createInstance(fields: Array[Object]): T = {
    val deValuedFields = fields map {
      f =>
        if (f.getClass.getSuperclass.equals(classOf[AnyVal])) {
          val fields = f.getClass.getDeclaredFields
          require(fields.length == 1)
          val field = fields(0)
          field.setAccessible(true)
          field.get(f)
        } else {
          f
        }
    }
    constructor.invoke(deValuedFields).asInstanceOf[T]
  }
{code}

This is basically what the Scala compiler does, but done reflectively.

To see the Scala compiler in action we can look at how this example code is compiled:
{code}
object ValueClassExample extends App {
  val m = Measurement(0, new DegreeCelsius(0.0f))
}{code}

Bytecode:
{code}
public final void delayedEndpoint$org$apache$flink$streaming$scala$examples$wordcount$ValueClassExample$1();
    Code:
       0: aload_0
       1: new           #63                 // class org/apache/flink/streaming/scala/examples/wordcount/Measurement
       4: dup
       5: iconst_0
       6: fconst_0
       7: invokespecial #66                 // Method org/apache/flink/streaming/scala/examples/wordcount/Measurement.""<init>"":(IF)V
      10: putfield      #60                 // Field m:Lorg/apache/flink/streaming/scala/examples/wordcount/Measurement;
      13: return
{code}

I.e. there is no {{DegreeCelsius}} in the compiled code.;;;","13/May/19 14:32;aljoscha;Fixed on release-1.8 in
ecc6639053cb36672ce552bb7626f75ff98b8293

Fixed on master in
9caf2c4355f851c7a8ca2b1fe9a1c6dab7bd95e3;;;","13/May/19 15:06;hiddenbit;Thank you a lot [~aljoscha] for the fast fix!;;;",,,,,,,,,,,,,,,,,,,,
Make ClosureCleaner recursive,FLINK-12297,13229519,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aitozi,dwysakowicz,dwysakowicz,23/Apr/19 06:54,01/Oct/19 15:39,13/Jul/23 08:05,15/Jun/19 06:57,1.8.0,,,,,,,,1.8.1,1.9.0,,,API / DataSet,API / DataStream,,0,pull-request-available,,,"Right now we do not invoke closure cleaner on output tags. Therefore such code:

{code}
	@Test
	public void testFlatSelectSerialization() throws Exception {
		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		DataStreamSource<Integer> elements = env.fromElements(1, 2, 3);
		OutputTag<Integer> outputTag = new OutputTag<Integer>(""AAA"") {};
		CEP.pattern(elements, Pattern.begin(""A"")).flatSelect(
			outputTag,
			new PatternFlatTimeoutFunction<Integer, Integer>() {
				@Override
				public void timeout(
					Map<String, List<Integer>> pattern,
					long timeoutTimestamp,
					Collector<Integer> out) throws Exception {

				}
			},
			new PatternFlatSelectFunction<Integer, Object>() {
				@Override
				public void flatSelect(Map<String, List<Integer>> pattern, Collector<Object> out) throws Exception {

				}
			}
		);

		env.execute();
	}
{code}

will fail with {{The implementation of the PatternFlatSelectAdapter is not serializable. }} exception",,aitozi,aljoscha,andrew_lin,dwysakowicz,elevy,SleePy,windpicker,,,,,,,,,,,,"Aitozi commented on pull request #8258: [FLINK-12297]Harden ClosureCleaner to handle the wrapped function
URL: https://github.com/apache/flink/pull/8258
 
 
   ## What is the purpose of the change
   
   ClosureCleaner is used to clean the implicit `this$x` point to the outer class in the user defined function to reduce the case of serialization function failed.
   
   But if the function is wrapped by user or the flink framework, it will escape the clean operation. This pr harden the ClosureCleaner to handle this. And I think it is also a feasible approach for [[FLINK-12113]](https://issues.apache.org/jira/browse/FLINK-12113)
   
   ## Brief change log
   
   check the field if need to further clean and apply
   
   ## Verifying this change
   
   1. Add the case in the issue to confirm
   2. Add the wrapFunction test case in ClosuerCleanerTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/19 17:57;githubbot;600","Aitozi commented on pull request #8258: [FLINK-12297]Harden ClosureCleaner to handle the wrapped function
URL: https://github.com/apache/flink/pull/8258
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Apr/19 05:24;githubbot;600","Aitozi commented on pull request #8280: [FLINK-12297]Harden ClosureCleaner to handle the wrapped function
URL: https://github.com/apache/flink/pull/8280
 
 
   ## What is the purpose of the change
   
   ClosureCleaner is used to clean the implicit `this$x` point to the outer class in the user defined function to reduce the case of serialization function failed.
   
   But if the function is wrapped by user or the flink framework, it will escape the clean operation. This pr harden the ClosureCleaner to handle this. And I think it is also a feasible approach for [[FLINK-12113]](https://issues.apache.org/jira/browse/FLINK-12113)
   
   ## Brief change log
   
   check the field if need to further clean and apply
   
   ## Verifying this change
   
   1. Add the case in the issue to confirm
   2. Add the wrapFunction test case in ClosuerCleanerTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Apr/19 12:11;githubbot;600","aljoscha commented on pull request #8744: [FLINK-12297]Harden ClosureCleaner to handle the wrapped function
URL: https://github.com/apache/flink/pull/8744
 
 
   Copy of #8280 for running CI
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jun/19 15:06;githubbot;600","aljoscha commented on pull request #8744: [FLINK-12297]Harden ClosureCleaner to handle the wrapped function
URL: https://github.com/apache/flink/pull/8744
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jun/19 06:47;githubbot;600","aljoscha commented on pull request #8280: [FLINK-12297]Harden ClosureCleaner to handle the wrapped function
URL: https://github.com/apache/flink/pull/8280
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jun/19 06:50;githubbot;600","xiaoguichao commented on pull request #9069: [FLINK-12297] Fixes StackOverflowException of ClosureCleaner when cle…
URL: https://github.com/apache/flink/pull/9069
 
 
   @aljoscha
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 12:27;githubbot;600","xiaoguichao commented on pull request #9069: [FLINK-12297] Fixes StackOverflowException of ClosureCleaner when cle…
URL: https://github.com/apache/flink/pull/9069
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Aug/19 06:52;githubbot;600",,,,,,0,4800,,,0,4800,,,,,,,,,,,,,FLINK-13586,,,,,,,,,,,"08/Jul/19 11:54;windpicker;image-2019-07-08-19-54-04-416.png;https://issues.apache.org/jira/secure/attachment/12973921/image-2019-07-08-19-54-04-416.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 10 12:30:27 UTC 2019,,,,,,,,,,"0|z021cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/19 08:25;SleePy;It seems that there are a lot of non-cleaned components passed, another issue, [FLINK-12113|https://issues.apache.org/jira/browse/FLINK-12113].

We can easily fix these issues case by case. However I'm not sure is there any clean way to solve these problems.;;;","23/Apr/19 09:11;SleePy;BTW, the {{OutputTag}} is not an interface, in this case, I can't find the reason why extends it.
Is there any possible that the original author thought {{OutputTag}} should not be extended?;;;","24/Apr/19 08:13;aitozi;Hi, [~SleePy][~dwysakowicz]
As I mentioned in [FLINK-12113|https://issues.apache.org/jira/browse/FLINK-12113] .  _Can we enable the recursive clean in ClosureCleaner#clean by check the class fields recursively. In this way we don't have to take care of all the wrapper functions, we just have to clean up to all userfunction register entrance._;;;","24/May/19 14:26;aljoscha;{{OutputTag}} needs to be an anonymous inner class (or a concrete subclass, at least) so that the Java compiler will put the information about the generic parameters in the generated code. Thats why you see {{new OutputTag<T>()}}, notice the {{{}}}. ;;;","25/May/19 01:37;aitozi;Hi，[~aljoscha]
Since you have noticed this issue, can you take a look on this PR to give me some suggestion since you are the author for the ClosureCleaner . 3Q.;;;","15/Jun/19 06:48;aljoscha;Merged on master in 68cc21e4af71505efa142110e35a1f8b1c25fe6e;;;","15/Jun/19 06:57;aljoscha;Merge on release-1.8 in 159c52769045853bc95a0efaae8ab2c7675e1d42;;;","08/Jul/19 11:56;windpicker;Did this recursive closure cleaner import a bug when cleaning an object with recursive references? [~aitozi] [~aljoscha]

I got an stackoverflow exception when clean a function with a JsonRowDeserializationSchema object. It seems the 

objectMapper field has recursive references. So I added a set to avoid this. Is this right?

  !image-2019-07-08-19-54-04-416.png!;;;","08/Jul/19 13:48;aljoscha;That seems like a good approach. Could you create a new Jira issue for that and open a PR, along with a test?;;;","10/Jul/19 12:30;windpicker;[~aljoscha] Sure, [https://github.com/apache/flink/pull/9069];;;",,,,,,,,,,,,,,
Data loss silently in RocksDBStateBackend when more than one operator(has states) chained in a single task ,FLINK-12296,13229518,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,klion26,klion26,klion26,23/Apr/19 06:53,29/May/19 04:31,13/Jul/23 08:05,29/May/19 02:42,1.6.3,1.6.4,1.7.2,1.8.0,,,,,1.7.3,1.8.1,1.9.0,,Runtime / State Backends,,,3,pull-request-available,,,"As the mail list said[1], there may be a problem when more than one operator chained in a single task, and all the operators have states, we'll encounter data loss silently problem.

Currently, the local directory we used is like below

../local_state_root_1/allocation_id/job_id/vertex_id_subtask_idx/chk_1/(state),

 

if more than one operator chained in a single task, and all the operators have states, then all the operators will share the same local directory(because the vertext_id is the same), this will lead a data loss problem. 

 

The path generation logic is below:
{code:java}
// LocalRecoveryDirectoryProviderImpl.java

@Override
public File subtaskSpecificCheckpointDirectory(long checkpointId) {
   return new File(subtaskBaseDirectory(checkpointId), checkpointDirString(checkpointId));
}


@VisibleForTesting
String subtaskDirString() {
   return Paths.get(""jid_"" + jobID, ""vtx_"" + jobVertexID + ""_sti_"" + subtaskIndex).toString();
}

@VisibleForTesting
String checkpointDirString(long checkpointId) {
   return ""chk_"" + checkpointId;
}
{code}
[1] [http://mail-archives.apache.org/mod_mbox/flink-user/201904.mbox/%3Cm2ef5tpfwy.wl-ningshi2@gmail.com%3E]",,aitozi,elevy,facboy,gyfora,kezhuw,kisimple,klion26,liyu,ningshi,Paul Lin,Seed Z,shixg,srichter,sunjincheng121,trohrmann,,,,"klion26 commented on pull request #8263: St[FLINK-12296][StateBackend]Data loss silently in RocksDBStateBackend because of local directory collision
URL: https://github.com/apache/flink/pull/8263
 
 
   ## What is the purpose of the change
   Fix the data loss silently problem in RocksDBStateBackend when more than one stateful operators chained in a single task.
   
   When preparing the local directory we'll delete the local directory if it already exists.
   
   Currently, if more than one stateful operators chained in a single task, they'll share the same local directory path, then the local directory will be deleted unexpected, and we'll get data loss silently.
   
   ## Brief change log
   
   Change the local data path from
   `/local_state_root/allocatio_id/job_id/jobvertext_id_subtask_id/chk_id`
   to
   `/local_state_root/allocatio_id/job_id/jobvertext_id_subtask_id/operator_id_chk_id`
   
   The reason to use operator instead of operatorIdentifier is that we'll look up operator owned data path in `TaskLocalStateStoreImpl#discardLocalStateForCheckpoint`.
   
   The reason to put operator_id in the last dir instread of the last but one dir because we'll look up the task owned base directory in `TaskLocalStateStoreImpl#dispose`.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - `LocalRecoveryDirectoryProviderImplTest#differentDirectoryForDifferentOperator()`
   - `OperatorSubtaskDescriptionTextTest#testExtractOperatorId`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**no**)
     - The serializers: (**no**)
     - The runtime per-record code paths (performance sensitive): (**no**)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes**)
     - The S3 file system connector: (**no**)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**no**)
     - If yes, how is the feature documented? (**not applicable**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Apr/19 05:46;githubbot;600","asfgit commented on pull request #8263: [FLINK-12296][StateBackend]Data loss silently in RocksDBStateBackend because of local directory collision
URL: https://github.com/apache/flink/pull/8263
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Apr/19 07:57;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 29 04:31:43 UTC 2019,,,,,,,,,,"0|z021co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/19 11:29;liyu;Thanks for filing the JIRA [~klion26]. Is this some regression issue or long existing one? Please fill the affected version field after confirmation. And since data loss is a critical issue, I'd suggest to escalate the priority from Major to Critical. Thanks.;;;","23/Apr/19 11:44;ningshi;We observed the issue on Flink 1.6.1. We haven’t tested on later versions yet.;;;","23/Apr/19 11:46;klion26;I think this is a long existing issue from release-1.5,

the relative issue is [#8360|https://issues.apache.org/jira/browse/FLINK-8360];;;","23/Apr/19 14:28;ningshi;Unchaining the two stateful operators seems to have worked around the issue, meaning that the state for both operators are in the checkpoint.;;;","23/Apr/19 20:15;Seed Z;Unchaining the operator makes the new job be able to save the state via checkpoint.

It is unfortunately not able to resume from the old checkpoint (RocksDB backed incremental checkpoint) likely due to the way the path is composed;;;","24/Apr/19 09:13;srichter;I guess this is one remaining relic from how backends were created, because previously only the head operator of a chain could have keyed state and a backend. This was partially changed now, every op can have a keyed state backend. The old assumption was probably made because of the expectation that keyed state only appears right after keyBy, which also always starts a new chain. So how is your job even getting keyed state on a non-head operator? Using {{reinterpretAsKeyedStream}}?;;;","24/Apr/19 11:14;klion26;[~srichter]  from what [~ningshi] have posted in the ml[1], the two states are user state and timer state
{noformat}
There are two stateful operators in the chain, one is a

CoBroadcastWithKeyedOperator, the other one is a StreamMapper. The

CoBroadcastWithKeyedOperator creates timer states in RocksDB, the latter

doesn’t. Because of the checkpoint directory collision bug, we always end up

saving the states for CoBroadcastWithKeyedOperator.{noformat}
 

Currently, I want to add {{operatorId}} in the path, so the path will look like

{{../local_state_root_1/allocation_id/job_id/vertex_id_subtask_idx/chk_1_operator_id/(state)}}

 

Why use {{operatorId}} instead of the {{operatorIdentifier}} because we need to find the local data path for a specific checkpoint ( {{TaskLocalStateStoreImpl#discardLocalStateForCheckpoint(long, TaskStateSnapshot)}}).

 

Will change  as following:
{code:java}
//LocalRecoveryDirectoryProviderImpl.java
@Override
public File subtaskSpecificCheckpointDirectory(long checkpointId, String operatorIdentifier) {
   return new File(subtaskBaseDirectory(checkpointId), checkpointDirString(operatorIdentifier, checkpointId));
}
@VisibleForTesting
String checkpointDirString(String operatorIdentifier, long checkpointId) {
   return ""opid_"" + OperatorSubtaskDescriptionText.getOperatorIdByOperatorIdentifier(operatorIdentifier) + ""_chk_"" + checkpointId;
}

//OperatorSubtaskDescriptionText.java
public static String getOperatorIdByOperatorIdentifier(String operatorIdentifier) { ... }
{code}
 

What do you think about this?

 

[1][http://mail-archives.apache.org/mod_mbox/flink-user/201904.mbox/%3Cm2imv4i2nd.wl-ningshi2@gmail.com%3E];;;","24/Apr/19 12:26;ningshi;[~srichter] You are right, we use reinterpretAsKeyedStream between the two stateful operators.;;;","24/Apr/19 13:21;srichter;[~klion26] In general I think it is a good idea to change this and to support chained operators with keyed state. 

I wonder why you would build the paths as {{../local_state_root_1/allocation_id/job_id/vertex_id_subtask_idx/chk_1*_operator_id*/(state)}} and not better use {{../local_state_root_1/allocation_id/job_id/*operator_id*_subtask_idx/chk_1/(state)}}.

[~ningshi] I see, so that reinterpretation was intially introduced by me as kind of a hidden feature for a very specific case, but seems like it is now getting used in many different contexts and this also leads to  a case that is breaking the old assumption that only a head operator can have keyed state. I don't see a downside in just changing the path construction to also make it work properly with a non-head operators. Until then, workaround should be to avoid getting the second operator chained.;;;","24/Apr/19 14:16;klion26;[~srichter] I first implemented as you suggest[1], but in {{TaskLocalStateStoreImpl#dispose()}} I can't get {{operatorId}} which needed to find the data path(I added an {{UUID}} as a placeholder in the patch temporary), so I proposed the path as {{../local_state_root_1/allocation_id/job_id/vertex_id_subtask_idx/chk_1*_operator_id*/(state)}}

 

[1] https://github.com/klion26/flink/blob/9efd8907dd08a0f50ff81e04b028cb87945da0ca/flink-runtime/src/main/java/org/apache/flink/runtime/state/TaskLocalStateStoreImpl.java#L258;;;","25/Apr/19 02:25;ningshi;[~srichter] The assumption totally made sense in that context. I agree that the reinterpretation is now a widely adopted feature as it is in the documentation, albeit under experiment feature, and is often recommended on the mailing list and in blog posts. It will be very nice to have this bug fixed, or at the very least prevent from using reinterpretation and RocksDB checkpointing together. The documentation should also warn against using them together. The challenge of the workaround for a user is that sometimes it's not obvious what operator have internal states, e.g. KeyedBroadcastProcessFunction may have internal timers stored in RocksDB. So it's not always easy to avoid getting two stateful operators chained unless the job disallows chaining altogether.;;;","26/Apr/19 08:59;srichter;[~klion26] I was just taking a look into the code and in my opinion the paths should be already good as they are, each operator goes into a separate directory already inside the {{chk_x}} directory. However, the problem is with {{RocksIncrementalSnapshotStrategy#prepareLocalSnapshotDirectory}}, which a bit too ambitiously does things like {{if (directory.exists()) FileUtils.deleteDirectory(directory)}}. What about just not deleting the existing directory here, just checking this for the sub-directory of the operator itself, but not the chk-root? If I am not missing anything here, that would should be a more appropriate fix. What do you think?;;;","28/Apr/19 03:32;klion26;After discussed offline with [~srichter], will change the directory to following

{{../local_state_root_1/allocation_id/job_id/vertex_id_subtask_idx/chk_1/operatorIdentifier}} ;;;","23/May/19 02:37;klion26;merged
 * master         ee60846dc588b1a832a497ff9522d7a3a282c350
 * release-1.8  531d727f9b32c310d8d63b253019b8cc4a23a3eb
 * release-1.7  1ce2efd7a38d091fc004a8dba034ece0bcc42385
 * release-1.6  0dda6fe9dff4f667b110cda39bfe9738ba615b24;;;","29/May/19 02:42;klion26;Since the commits have been merged in master, release-1.8, release-1.7 and release-1.6(find the commits' info in the previous comments), resolve this issue.;;;","29/May/19 03:36;sunjincheng121;Thanks for double check this JIRA, and close it. :);;;","29/May/19 04:31;klion26;Thanks for the reminder [~sunjincheng121];;;",,,,,,,
Fix java doc inconsistencies and style issues in Memory manager,FLINK-12289,13229361,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fan_li_ya,fan_li_ya,fan_li_ya,22/Apr/19 11:41,29/Oct/19 13:12,13/Jul/23 08:05,29/Oct/19 13:12,,,,,,,,,1.10.0,,,,Runtime / Task,,,0,pull-request-available,,,"According to the JavaDoc,  MemoryManager.release method should throw an NPE if the input argument is null. 

In addition, there are some typos in class MemoryManager.",,azagrebin,fan_li_ya,,,,,,,,,,,,,,,,,"liyafan82 commented on pull request #8236: [FLINK-12289][flink-runtime]Fix bugs and typos in Memory manager
URL: https://github.com/apache/flink/pull/8236
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix some bugs and typos in memory manager
   
   
   ## Brief change log
   
     - Fix the bugs and typos in class MemoryManager
     - Add a test case to verify the change in MemoryManagerTest.testReleaseNull
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - When the input argument is null, the method should throw a NullPointerException.
     
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes)
     
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Apr/19 11:58;githubbot;600","azagrebin commented on pull request #8236: [FLINK-12289][Runtime] Fix java doc inconsistencies and style issues in Memory manager
URL: https://github.com/apache/flink/pull/8236
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/19 13:10;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,FLINK-12189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 13:12:29 UTC 2019,,,,,,,,,,"0|z020e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/19 13:12;azagrebin;Merged into master by 11ef25319484baf8e00e5ab3bafc7e4c95416891;;;",,,,,,,,,,,,,,,,,,,,,,,
Memory leak in SavepointITCase and SavepointMigrationTestBase,FLINK-12285,13229328,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,SleePy,shixg,shixg,22/Apr/19 06:50,15/May/19 08:33,13/Jul/23 08:05,15/May/19 08:33,,,,,,,,,1.9.0,,,,Runtime / Coordination,Tests,,0,pull-request-available,test-stability,,"The tests in {{SavepointITCase}} and {{SavepointMigrationTestBase}} do not cancel running jobs before exit. It will cause exceptions in {{TaskExecutor}}s and unreleased memory segments. Succeeding tests may fail due to insufficient amount of memory.

The problem is caused by cancelling {{TaskExecutor}}s with running tasks. Another issue caused by the reason can be seen in FLINK-11343. Maybe we can find a more dedicated method to cancel those {{TaskExecutor}}s still having running tasks.",,shixg,SleePy,,,,,,,,,,,,,,,,,"ifndef-SleePy commented on pull request #8269: [FLINK-12285][test] Harden savepoint relevant test cases by explicitly canceling test job
URL: https://github.com/apache/flink/pull/8269
 
 
   ## What is the purpose of the change
   
   * Harden `SavepointITCase` and `SavepointMigrationTestBase`
   
   ## Brief change log
   
   * Cancel test job explicitly before shutting down cluster
   
   ## Verifying this change
   
   * This change is about test cases
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Apr/19 10:22;githubbot;600","zentol commented on pull request #8269: [FLINK-12285][test] Harden savepoint relevant test cases by explicitly canceling test job
URL: https://github.com/apache/flink/pull/8269
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 08:32;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-11630,,,,,FLINK-12332,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 15 08:33:00 UTC 2019,,,,,,,,,,"0|z0206o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/19 10:02;SleePy;Hi [~xiaogang.shi],
Thank you for reporting.
I think we should improvement this both in test case and Flink components sides. Test case should wait the state of job to be terminal. And Flink components (like {{TaskExecutor}}) should support an elegant way to exit.;;;","15/May/19 08:33;chesnay;master: 2d76e0a2809a093fa3aca97779dac305ba1509de;;;",,,,,,,,,,,,,,,,,,,,,,
InputBufferPoolUsage is incorrect in credit-based network control flow,FLINK-12284,13229326,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,shixg,shixg,22/Apr/19 06:36,03/Jul/19 09:48,13/Jul/23 08:05,03/Jul/19 09:48,1.6.3,1.6.4,1.7.2,1.8.0,,,,,1.9.0,,,,Runtime / Metrics,Runtime / Network,,0,pull-request-available,,,"When using credit-based network control flow, exclusive buffers are directly assigned to {{RemoteInputChannel}} and are not counted in {{LocalBufferPool}}, leading to incorrect InputBufferPoolUsage.",,aitozi,kisimple,pnowojski,shixg,,,,,,,,,,,,,,,"Aitozi commented on pull request #8455: [FLINK-12284]Fix the incorrect inputBufferUsage metric in credit-based netowork mode
URL: https://github.com/apache/flink/pull/8455
 
 
   ## What is the purpose of the change
   
   This PR is to fix the bug in the calculation of  inputBufferUsage. It does't now take the exclusive buffer which is assigned from global buffer pool into account. 
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 16:14;githubbot;600","pnowojski commented on pull request #8455: [FLINK-12284][Network,Metrics]Fix the incorrect inputBufferUsage metric in credit-based network mode
URL: https://github.com/apache/flink/pull/8455
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 09:47;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 03 09:48:45 UTC 2019,,,,,,,,,,"0|z02068:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/19 15:18;aitozi;It's truly a bug, because the exclusive buffers are assigned in advance from networkbufferpool not occupy the number buffer in LocalBufferPool;;;","07/Jun/19 07:41;pnowojski;In the scope of this ticket, we decided to rework pool usage metrics for credit based flow control for which we will provide 3 metrics:
 * floatingBuffersUsage
 * exclusiveBuffersUsage
 * inPoolUsage - combined floating + exclusive usage;;;","03/Jul/19 09:48;pnowojski;merged to master as commit 36a938a;;;",,,,,,,,,,,,,,,,,,,,,
Table API does not allow non-static inner class as UDF,FLINK-12283,13229322,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,zjffdu,zjffdu,22/Apr/19 05:01,18/Dec/19 10:00,13/Jul/23 08:05,18/Dec/19 10:00,1.8.0,,,,,,,,1.10.0,,,,Table SQL / API,,,0,pull-request-available,,,See details here [https://lists.apache.org/thread.html/9ecec89ba1225dbd6b3ea2466a910ad9685a42a4672b449f6ee13565@%3Cuser.flink.apache.org%3E],,aljoscha,phoenixjiangnan,twalthr,zjffdu,,,,,,,,,,,,,,,"aljoscha commented on pull request #10461: [FLINK-12283] In Table API, allow non-static inner class as UDF
URL: https://github.com/apache/flink/pull/10461
 
 
   ## What is the purpose of the change
   
   This relaxes a check in the Table API to allow more types of functions. Specifically, this allows to use UDFs as defined in the Scala Shell.
   
   This might lead to cases where a user specifies a function that then is not serializable, but I think it's worth it to enable the above use case.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 10:58;githubbot;600","aljoscha commented on pull request #10461: [FLINK-12283] In Table API, allow non-static inner class as UDF
URL: https://github.com/apache/flink/pull/10461
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 08:12;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 10:00:10 UTC 2019,,,,,,,,,,"0|z0205c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/19 19:15;phoenixjiangnan;Should the title be ""Allow non-static *inner class *as UDF"";;;","23/Apr/19 01:08;zjffdu;Thanks [~phoenixjiangnan], I have updated the title;;;","04/Dec/19 09:26;aljoscha;I think this change would be good because it allows specifying UDFs in the Scala shell. Even though it might let through some UDFs that don't work because of dependencies in their closure.

[~zjffdu] I guess we also need to add {{-Yrepl-class-based}} to the Scala shell, then?;;;","09/Dec/19 16:08;twalthr;[~zjffdu] I have a working prototype for this issue. It requires quite a lot of changes but I think it would be nice for users. The only problem right now is that we don't have access to the Scala ClosureCleaner. I opened FLINK-15162 and hope somebody could pick it up. Could you give me some UDF examples to test my implementation?;;;","09/Dec/19 16:35;twalthr;In particular I'm interested if you are also trying to implement \{{object}} or only \{{class}}. What is the outer class? Because I fear that we might serialize the outer class as well if no closure cleaner is in pace.;;;","18/Dec/19 10:00;twalthr;Fixed in 1.11: 098af17d8f84068569e96c13fcea5713eec44407
Fixed in 1.10: ec40d8528621d7533565261b8e46de91a14c40ee;;;",,,,,,,,,,,,,,,,,,
SortDistinctAggregateITCase fails on Travis,FLINK-12270,13229192,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,chesnay,chesnay,20/Apr/19 07:01,20/Dec/19 08:54,13/Jul/23 08:05,20/Dec/19 08:54,1.9.0,,,,,,,,,,,,Table SQL / Runtime,,,0,,,,"https://travis-ci.org/apache/flink/jobs/522247106

{code}
20:47:38.852 [ERROR] Tests run: 23, Failures: 0, Errors: 1, Skipped: 2, Time elapsed: 75.246 s <<< FAILURE! - in org.apache.flink.table.runtime.batch.sql.agg.SortDistinctAggregateITCase
20:47:38.857 [ERROR] testSomeColumnsBothInDistinctAggAndGroupBy(org.apache.flink.table.runtime.batch.sql.agg.SortDistinctAggregateITCase)  Time elapsed: 5.705 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 64 pages. Only 0 pages are remaining.
Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 64 pages. Only 0 pages are remaining.

{code}",,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12265,FLINK-12281,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 22 07:48:55 UTC 2019,,,,,,,,,,"0|z01zcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/19 12:08;ykt836;[~lzljs3620320] Could you take a look at this?;;;","20/Apr/19 12:43;jark;I have also encountered this issue in pre-commit of FLINK-12133. After discussed with [~godfreyhe] offline, the reason might be the default SQL_RESOURCE_HASH_AGG_TABLE_MEM is too big. So I decrease this value to 4M in test base class in the [latest merged commit|https://github.com/apache/flink/commit/fefdd08b849d486234cb38ff0c0a12359eae24d2#diff-cb1826ace95100e2eacf8989309e076c]. But it might be other reasons.;;;","22/Apr/19 07:48;lzljs3620320;I set SQL_RESOURCE_HASH_AGG_TABLE_MEM to 2M and change size of MemoryManager to 100m in (Default in MiniClusterResource is 80m):

[https://github.com/apache/flink/pull/8221/commits/040cdb3916c2c1a3f5dc5eb12d66e7a201836c17#diff-420c670b7ea21e4abb618006ed6b5b9aR50]

I think it can solve this error.;;;",,,,,,,,,,,,,,,,,,,,,
Slot allocation failure by taskmanager registration timeout and race,FLINK-12260,13228978,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hwanju,hwanju,hwanju,19/Apr/19 00:02,14/May/19 09:19,13/Jul/23 08:05,14/May/19 09:19,1.6.3,,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / Coordination,,,0,pull-request-available,,," 

In 1.6.2., we have seen slot allocation failure keep happening for long time. Having looked at the log, I see the following behavior:
 # TM sends a registration request R1 to resource manager.
 # R1 times out after 100ms, which is initial timeout.
 # TM retries a registration request R2 to resource manager (with timeout 200ms).
 # R2 arrives first at resource manager and registered, and then TM gets successful response moving onto step 5 below.
 # On successful registration, R2's instance is put to taskManagerRegistrations
 # Then R1 arrives at resource manager and realizes the same TM resource ID is already registered, which then unregisters R2's instance ID from taskManagerRegistrations. A new instance ID for R1 is registered to workerRegistration.
 # R1's response is not handled though since it already timed out (see akka temp actor resolve failure below), hence no registration to taskManagerRegistrations.
 # TM keeps heartbeating to the resource manager with slot status.
 # Resource manager ignores this slot status, since taskManagerRegistrations contains R2, not R1, which replaced R2 in workerRegistration at step 6.
 # Slot request can never be fulfilled, timing out.

The following is the debug logs for the above steps:

 
{code:java}
JM log:

2019-04-11 22:39:40.000,Registering TaskManager with ResourceID 46c8e0d0fcf2c306f11954a1040d5677 (akka.ssl.tcp://flink@flink-taskmanager:6122/user/taskmanager_0) at ResourceManager

2019-04-11 22:39:40.000,Registering TaskManager 46c8e0d0fcf2c306f11954a1040d5677 under deade132e2c41c52019cdc27977266cf at the SlotManager.

2019-04-11 22:39:40.000,Replacing old registration of TaskExecutor 46c8e0d0fcf2c306f11954a1040d5677.

2019-04-11 22:39:40.000,Unregister TaskManager deade132e2c41c52019cdc27977266cf from the SlotManager.

2019-04-11 22:39:40.000,Registering TaskManager with ResourceID 46c8e0d0fcf2c306f11954a1040d5677 (akka.ssl.tcp://flink@flink-taskmanager:6122/user/taskmanager_0) at ResourceManager

TM log:

2019-04-11 22:39:40.000,Registration at ResourceManager attempt 1 (timeout=100ms)

2019-04-11 22:39:40.000,Registration at ResourceManager (akka.ssl.tcp://flink@flink-jobmanager:6123/user/resourcemanager) attempt 1 timed out after 100 ms

2019-04-11 22:39:40.000,Registration at ResourceManager attempt 2 (timeout=200ms)

2019-04-11 22:39:40.000,Successful registration at resource manager akka.ssl.tcp://flink@flink-jobmanager:6123/user/resourcemanager under registration id deade132e2c41c52019cdc27977266cf.

2019-04-11 22:39:41.000,resolve of path sequence [/temp/$c] failed{code}
 

As RPC calls seem to use akka ask, which creates temporary source actor, I think multiple RPC calls could've arrived out or order by different actor pairs and the symptom above seems to be due to that. If so, it could have attempt account in the call argument to prevent unexpected unregistration? At this point, what I have done is only log analysis, so I could do further analysis, but before that wanted to check if it's a known issue. I also searched with some relevant terms and log pieces, but couldn't find the duplicate. Please deduplicate if any.",,hongyu.bi,hwanju,jiaxl,kisimple,lamber-ken,rmetzger,trohrmann,,,,,,,,,,,,"tillrohrmann commented on pull request #8415: [FLINK-12260] [Runtime/Coordination] Slot allocation failure by taskmanager registration tim…
URL: https://github.com/apache/flink/pull/8415
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/May/19 09:16;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/19 23:28;hwanju;FLINK-12260-repro.diff;https://issues.apache.org/jira/secure/attachment/12967185/FLINK-12260-repro.diff",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 09:19:09 UTC 2019,,,,,,,,,,"0|z01y2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/19 16:46;trohrmann;Thanks for reporting this issue [~hwanju]. This sounds like a Flink problem to me. Are you able to reproduce the problem? If yes, then it would be helpful to modify the logging statement in line {{ResourceManager.java:727}} into {{log.info(""Registering TaskManager with ResourceID {} ({}) at ResourceManager under instance id {}"", taskExecutorResourceId, taskExecutorAddress, registration.getInstanceID());}}.

Something which looks a bit odd is that all logging statement happened at the same time even though there should be a timeout of {{100}} ms in between. 

At the moment I'm not sure whether I can fully backtrack the problem to its cause. Maybe the problem is caused by the {{RetryingRegistration}} which uses a thread pool of multiple threads to send the registration requests. The additional logs would be tremendously helpful for further debugging this problem.;;;","23/Apr/19 16:49;trohrmann;I think introducing an attempt counter should solve the problem. But before we fix it that way I would like to fully understand how this out of orderness happens.;;;","23/Apr/19 17:14;hwanju;Thanks Till for the comment. I haven't attempted to reproduce the issue locally, and I will try reproducing the issue with the better logging and will update this thread once I get more info.

Regarding odd timestamps of the logs, this is our logging issue we plan to fix, which loses millisecond granularity when sending flink logs to long-term log archive. As this was the postmortem analysis, I couldn't gather the live flink logs, that was why the logs zeroed out milliseconds part.;;;","26/Apr/19 23:29;hwanju;I got repro but in somewhat tricky way, since it's definitely rarely happening race. But as mentioned, once it falls into this state, it can't get out of the state (by assuming that we're not using active resource manager).

In the repro, I injected artificial delay to RM->TM connection on task executor registration, which can timeout the first registration request resulting in 2nd try. Since RM->TM connection is carried out in a separate thread via akka ask call, delaying here can't block the resource manager endpoint mailbox processing, so any further request can be processed during the delay. I initially added the delay in handling registerTaskExecutorInternal, but as it uses RPC's executor, the delay blocks all the further retries, hence not reproducing the race. With the delay in TM connection, 2nd task registration attempt can overtake the 1st one going ahead with TM registration, and then the resumed 1st request unregisters the TM registration. Although I mimicked the race on RM side, I think still sender side can also have potential delay (like by network) during tell part in akka ask causing timeout and leading to 2nd try racing 1st one. The latter was trickier to mimic, so I tried the first approach. 

The following is the JM/TM logs.

JM log:
{code:java}
2019-04-26 17:14:44,921 DEBUG org.apache.flink.runtime.rpc.akka.AkkaRpcService - Try to connect to remote RPC endpoint with address akka.ssl.tcp://flink@192.168.69.15:6122/user/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway. 
2019-04-26 17:14:44,924 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - [REPRO] thread 19 attempt 1
2019-04-26 17:14:44,996 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - [REPRO] thread 22 sleep...
2019-04-26 17:14:45,021 DEBUG org.apache.flink.runtime.rpc.akka.AkkaRpcService - Try to connect to remote RPC endpoint with address akka.ssl.tcp://flink@192.168.69.15:6122/user/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
2019-04-26 17:14:45,022 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService - [REPRO] thread 19 attempt 2
2019-04-26 17:14:45,038 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Registering TaskManager with ResourceID d0a410ee9060e62e0c7ef9e46f6418da (akka.ssl.tcp://flink@192.168.69.15:6122/user/taskmanager_0) at ResourceManager under instance id fa4408b5412bb8c18a6a7e58fdc8ff18
2019-04-26 17:14:45,093 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager  - Registering TaskManager d0a410ee9060e62e0c7ef9e46f6418da under fa4408b5412bb8c18a6a7e58fdc8ff18 at the SlotManager.
2019-04-26 17:14:45,997 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService              - [REPRO] thread 22 done
2019-04-26 17:14:45,998 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Replacing old registration of TaskExecutor d0a410ee9060e62e0c7ef9e46f6418da.
2019-04-26 17:14:45,998 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager  - Unregister TaskManager fa4408b5412bb8c18a6a7e58fdc8ff18 from the SlotManager.
2019-04-26 17:14:46,000 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - Registering TaskManager with ResourceID d0a410ee9060e62e0c7ef9e46f6418da (akka.ssl.tcp://flink@192.168.69.15:6122/user/taskmanager_0) at ResourceManager under instance id ebad00b418637d2774b8f131d49cc79e
2019-04-26 17:14:46,000 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager  - The target with resource ID d0a410ee9060e62e0c7ef9e46f6418da is already been monitored.
2019-04-26 17:14:47,387 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager  - Received slot report from instance ebad00b418637d2774b8f131d49cc79e.
2019-04-26 17:14:47,387 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager  - Received slot report for unknown task manager with instance id ebad00b418637d2774b8f131d49cc79e. Ignoring this report.
2019-04-26 17:19:48,045 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job  (ed0fbfff272391d1f2a98de45fda6453) switched from state RUNNING to FAILING.
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate all requires slots within timeout of 300000 ms. Slots required: 1, slots allocated: 0
...
{code}
TM log:

 
{code:java}
2019-04-26 17:14:44,897 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Resolved ResourceManager address, beginning registration
2019-04-26 17:14:44,897 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Registration at ResourceManager attempt 1 (timeout=100ms)
2019-04-26 17:14:45,017 DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor            - Registration at ResourceManager (akka.ssl.tcp://flink@flink-jobmanager:6123/user/resourcemanager) attempt 1 timed out after 100 ms
2019-04-26 17:14:45,017 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Registration at ResourceManager attempt 2 (timeout=200ms)
2019-04-26 17:14:45,047 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Successful registration at resource manager akka.ssl.tcp://flink@flink-jobmanager:6123/user/resourcemanager under registration id fa4408b5412bb8c18a6a7e58fdc8ff18.
2019-04-26 17:14:46,006 DEBUG akka.actor.LocalActorRefProvider(akka://flink)                - resolve of path sequence [/temp/$c] failed

{code}
 

 

This is the diff for log and reproduce (against 1.6.2) – [^FLINK-12260-repro.diff]

(The current repro method is not systematic rather a tweak due to tricky race)

 

I may be testing with attempt count approach.;;;","28/Apr/19 10:58;jiaxl;Hi,

If this issue is still open, I'd like to fix this problem with an attempt count, as [~till.rohrmann] suggested.  

Thanks.;;;","29/Apr/19 09:56;trohrmann;[~jiaxl] I think [~hwanju] is already working on a fix for it.

[~hwanju] if I understood you correctly, then I think the solution could be keeping a reference to the {{CompletableFuture<TaskExecutorGateway> taskExecutorGatewayFuture}} future returned in {{ResourceManager.java:373}} and then check in the callback for referential equality {{ResourceManager.java:376}}. Then we would not have to add another field to the RPC call.

This is a really good discovery. Thanks a lot for investigating this problem!;;;","30/Apr/19 09:21;hwanju;Thanks Till. I had tested with attempt count addition to RPC before your comment. I worked as expected.

At any rate, I may need some clarification on your idea about reference equality, regarding how it could address this issue. Although the repro uses delay in task executor connection, as mentioned, the race may happen on sender side. Even for dealing with race in the task executor connection, I am curious how reference check alone would work. If receiver-only approach can work, it would be better compared to attempt count one.;;;","30/Apr/19 10:33;trohrmann;I'm still a bit skeptical that there is a race condition on the sender side. We only trigger the second registration call after the first one has produced a timeout. A timeout can only be produced if we have sent a request to the receiver side.

My proposal would be the following:

We have a {{Map<ResourceID, CompletableFuture<TaskExecutorGateway>> taskExecutorGatewayFutures}} of pending {{TaskExecutor}} connections. When we enter the method {{ResourceManager#registerTaskExecutor}} we create a new future and update {{taskExecutorGatewayFutures}}.

Inside the {{handleAsync}} call on the future we check whether we are still the pending {{TaskExecutor}} connection:

{code}
return taskExecutorGatewayFuture.handleAsync(
	(TaskExecutorGateway taskExecutorGateway, Throwable throwable) -> {
		if (taskExecutorGatewayFutures.get(taskExecutorResourceId) == taskExecutorGatewayFuture) {
			taskExecutorGatewayFutures.remove(taskExecutorResourceId);
			if (throwable != null) {
				return new RegistrationResponse.Decline(throwable.getMessage());
			} else {
				return registerTaskExecutorInternal(
					taskExecutorGateway,
					taskExecutorAddress,
					taskExecutorResourceId,
					dataPort,
					hardwareDescription);
			}
		} else {
			log.debug(""Ignoring outdated TaskExecutorGateway connection."");
		}
	},
	getMainThreadExecutor());
{code}

Given that the {{registerTaskExecutor}} calls come in order, this should prevent that an earlier register call overrides a later one.;;;","01/May/19 04:57;hwanju;Thanks for the clarification. I thought you meant it without introducing any additional map, but now it seems clear.

I had tried thinking conservative approach as I couldn't 100% rule out the possibility of sender-side race. As we may have a potential simpler solution, I looked at the code again a little further. Initially what led me to any possibility of race is this part:

 
{code:java}
val a = PromiseActorRef(ref.provider, timeout, targetName = actorRef, message.getClass.getName, sender)
actorRef.tell(message, a)
a.result.future
{code}
This is internalAsk from invokeRpc and PromiseActorRef internally does scheduler.scheduleOnce(timeout.duration) for the timer. The tell of actorRef is sending a message to RM through RemoteActorRef and EndpointManager where the message is passed to Dispatcher, which enqueues the message to mbox and executes mbox via executor thread. My impression was that as tell is asynchronous via executor service, the timer of PromiseActorRef set up before can fire before the message hit the road off the sender. Although that'd be possible, the message at least seems to be enqueued to mbox for RM endpoint and thus the order can be preserved against the next attempt after timeout. So, the ordering seems fine. In addition I was also concerned the case where two different ask calls might happen to use two different TCP connections leading any possible out-of-order delivery. Although not 100% exercising the relevant code, it seems to use a single connection associated by akka endpoints and I checked that's true by packet capture. 

So, based on the code inspection and no successful repro on sender-side, we can currently conclude that the race is likely happening in task executor connection/handshake on the receiver-side (as repro does). I will test it out with the Till's proposal. On our side, once this fix ends up being applied, we can keep eyes on our test apps, which intermittently hit this issue, to see if there's any other race issue.

 

 ;;;","02/May/19 10:38;trohrmann;Thanks a lot for your detailed analysis of the ask code path in Akka [~hwanju]. Do you wanna open a PR for this fix? You can ping me for the review.;;;","03/May/19 07:36;hwanju;Sure. I will ping you once I will have tested the fix.;;;","14/May/19 09:19;trohrmann;Merged via

1.9.0:
07773d0d9251d6ad8c1770de985d33be8e72b032
2284f777ecd3b62b412bd0fdb9dbcf492314c589

1.8.1:
28b539da749949c656259c68e4a0a98e081551cf
a043e41fe14113d2bf3b9b25438680759619e418

1.7.3:
bcd35b9b51e96b231bc64d3b45583bfcf47c3d18
ca85285cda0f0cb6f82ed55a25aa4c439be1c2b2;;;",,,,,,,,,,,,
Type equivalence check fails for Window Aggregates,FLINK-12249,13228769,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hequn8128,dwysakowicz,dwysakowicz,18/Apr/19 07:08,08/May/20 03:29,13/Jul/23 08:05,30/Jul/19 13:21,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Legacy Planner,Tests,,0,pull-request-available,,,"Creating Aggregate node fails in rules: {{LogicalWindowAggregateRule}} and {{ExtendedAggregateExtractProjectRule}} if the only grouping expression is a window and
we compute aggregation on NON NULLABLE field.

The root cause for that, is how return type inference strategies in calcite work and how we handle window aggregates. Take {{org.apache.calcite.sql.type.ReturnTypes#AGG_SUM}} as an example, based on {{groupCount}} it adjusts type nullability based on groupCount.

Though we pass a false information as we strip down window aggregation from groupSet (in {{LogicalWindowAggregateRule}}).

One can reproduce this problem also with a unit test like this:

{code}
@Test
  def testTumbleFunction2() = {
 
    val innerQuery =
      """"""
        |SELECT
        | CASE a WHEN 1 THEN 1 ELSE 99 END AS correct,
        | rowtime
        |FROM MyTable
      """""".stripMargin

    val sql =
      ""SELECT "" +
        ""  SUM(correct) as cnt, "" +
        ""  TUMBLE_START(rowtime, INTERVAL '15' MINUTE) as wStart "" +
        s""FROM ($innerQuery) "" +
        ""GROUP BY TUMBLE(rowtime, INTERVAL '15' MINUTE)""
    val expected = """"
    streamUtil.verifySql(sql, expected)
  }
{code}

This causes e2e tests to fail: https://travis-ci.org/apache/flink/builds/521183361?utm_source=slack&utm_medium=notificationhttps://travis-ci.org/apache/flink/builds/521183361?utm_source=slack&utm_medium=notification",,dwysakowicz,godfreyhe,hequn8128,jark,sunjincheng121,,,,,,,,,,,,,,"dawidwys commented on pull request #9141: [FLINK-12249][table] Fix type equivalence check problems for Window Aggregates
URL: https://github.com/apache/flink/pull/9141
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 12:38;githubbot;600",,,,,,,,,,,,,0,600,,,0,1800,,,,,,,,,,,,,FLINK-12589,,FLINK-13392,FLINK-17553,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 30 13:21:27 UTC 2019,,,,,,,,,,"0|z01ws8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/19 07:23;hequn8128;[~dwysakowicz] Thank you for providing the valuable information and the test case. It is very helpful. I will take a look at the problem.

Best, Hequn;;;","15/Jul/19 10:12;sunjincheng121;Hi [~hequn8128] [~dwysakowicz] this issue looks LogicalWindowAggregateRule's problem, i.e. in this rule remove the group key(window). and in comment database, such as: mysql, we usually output the NULL, when data is empty when non group key. and if with a groupkey will output the emptyset as the result, So, I think we should fix this issue in `LogicalWindowAggregateRule`, what do you think?;;;","16/Jul/19 07:34;hequn8128;[~sunjincheng121] Thanks for your suggestions. I think you make a good point here. 

The behavior of Calcite is similar to the common databases thus I think we should fix in our side. One way to fix this problem is to change the current sum/avg {{SqlAggFunction}} to a self-defined one which avoids turning the ""not null"" to ""nullable"" for window aggregate. We can simply use a Rule to archive this.

Blink-planner also has the same problem. ;;;","17/Jul/19 08:35;sunjincheng121;Right, we always have to be consistent with the database semantics! and we can open the PR by add the rule to solve the groupkey (window) issue. :);;;","17/Jul/19 12:23;dwysakowicz;[~sunjincheng121] I agree that Calcite behaves correctly in cases where group set is empty. The problem though is we produce that false information intentionally (in a way). The reason is that there is no bit-like field in the input of `LogicalWindowAggregate` that we can group on. Calcite creates a {{LogicalAggregate}} that groups on value of function like {{TUMBLE(f0, ...)}}, we strip that information in {{LogicalWindowAggregateRule}} and perform the window assignment within the physical node. Therefore it is not as simple as just leave the idx there.

[~hequn8128] What do you mean by: ""We can simply use a Rule to achieve this""? I think the problem lies in the return type inference strategies, which cannot be adjusted in a Rule, or am I wrong?;;;","17/Jul/19 12:29;dwysakowicz;[~sunjincheng121] I agree that Calcite behaves correctly in cases where group set is empty. The problem though is we produce that false information intentionally (in a way). The reason is that there is no bit-like field in the input of `LogicalWindowAggregate` that we can group on. Calcite creates a {{LogicalAggregate}} that groups on value of function like {{TUMBLE(f0, ...)}}, we strip that information in {{LogicalWindowAggregateRule}} and perform the window assignment within the physical node. Therefore it is not as simple as just leave the idx there.

[~hequn8128] What do you mean by: ""We can simply use a Rule to achieve this""? I think the problem lies in the return type inference strategies, which cannot be adjusted in a Rule, or am I wrong?;;;","17/Jul/19 15:37;hequn8128;Hi [~dwysakowicz],

You are right. I think you have pointed out the most important, i.e., the default SqlReturnTypeInference(ReturnTypes.AGG_SUM) in SqlSumAggFunction does not fit window aggregate well(same for avg). 

To solve this, we can create a new SqlReturnTypeInference in a new SqlSumAggFunction(SqlWindowSumAggFunction). A rule can be used to replace the SqlSumAggFunction to a SqlWindowSumAggFunction for window aggregate. After this, there will be no check fails in LogicalWindowAggregateRule.

I have opened a PR and welcome to have your suggestions. Thank you very much.

Best, Hequn;;;","18/Jul/19 10:10;godfreyhe;there is another big issue: is {{WindowAggregate}} inherited from {{Aggregate}} correct? My answer is NO.

for {{WindowAggregate}}, the group keys are window group and normal fields (may be empty), while {{Aggregate}} only has normal group keys part, and know nothing about window group key. currently, many planner rules match and apply transformation on {{Aggregate}}, however some of them does not applicable to {{WindowAggregate}}, e.g. {{AggregateJoinTransposeRule}}, {{AggregateProjectMergeRule}}, etc. I think the design violates the Liskov Substitution Principle. 

there are three solutions: 
1. make {{Aggregate}}'s group key supports expressions(such as RexCall), not field reference only. and then the window group expression could be as a part of {{Aggregate}}'s group key. the disadvantage is we must update all existing aggregate rules, metadata handlers, etc.
2. make {{WindowAggregate}} extends from {{SingleRel}}, not from {{Aggregate}}. the disadvantage is we must implement related planner rules about WindowAggregate. 
3. in logical phase, we does not merge {{Aggregate}} and {{Project}} (with window group) into {{WindowAggregate}}, and convert the {{Project}} to a new kind of node named {{WindowAssigner}}, which could prevent {{Project}} from being pushed down/merged. and in physical phase, we merge them into {{WindowAggregate}}. the advantage is we could reuse current aggregate rules, and the disadvantage is we should add new rules about {{WindowAssigner}}.

i think solution3 is a more easier approach, which could make sure all rules are correct.

if this refactor is finished, i think the above bug is fixed too.

thank~;;;","19/Jul/19 03:51;jark;What would be the effort if we make `WindowAggregate` doesn't extend from `Aggregate` ? 
I mean we don't have much optimization rules for window aggregate, it might be not a big effort and can aim to 1.9 if possible. ;;;","19/Jul/19 06:22;dwysakowicz;I think this is not a prominent bug. It was present since the introduction of window groupings and no users spotted this bug for a couple of releases. I would be against rushing a relatively involving solution just before releasing 1.9. I think we can safely postpone fixing this bug to the beginning of 1.10 and do it with a proper care.

I think we should carefully analyze which Aggregate rules we want to preserve for WindowAggregate and which we want to drop.;;;","19/Jul/19 06:31;godfreyhe;i and [~hequn8128] found a minimal change way to fix this bug: add a {{Project}} on {{WindowAggregate}} in {{LogicalWindowAggregateRule}}. the {{Project}} will correct the output type, and make sure type equivalence with original aggregate. He will validate this approach;;;","19/Jul/19 06:51;jark;I'm fine with postpone this fixing. But if the fixing is small, it will be nice to have it in 1.9. 
Let's wait for the new approach [~hequn8128] is trying out. ;;;","19/Jul/19 08:02;hequn8128;Same with the comments from [~godfreyhe]. I have verified the approach. It works fine. :) I will update the PR soon. Thank you for every one! ;;;","22/Jul/19 02:33;sunjincheng121;And I like the new update about workaround patch, in this way we can solve this issue for user in 1.9, and create a new JIRA for the follow-up discussion. just like [~dwysakowicz] and all of you mentioned this issue is not the blocker of 1.9 release. ;;;","24/Jul/19 01:50;hequn8128;[~sunjincheng121] Good suggestion! I have created FLINK-13392 to further improve the WindowAggregate. ;;;","30/Jul/19 13:21;dwysakowicz;Fixed in:
master: 305051cc6e03d7e0ed9a9c3afe9e90b56bab0dda
1.9: 1de005392022404adbce4cd8b9de90f157052bd0;;;",,,,,,,,
fix NPE when writing an archive file to a FileSystem,FLINK-12247,13228763,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lamber-ken,lamber-ken,lamber-ken,18/Apr/19 06:35,26/Apr/19 13:56,13/Jul/23 08:05,24/Apr/19 14:15,1.6.3,1.8.0,,,,,,,1.7.3,1.8.1,1.9.0,,Deployment / YARN,,18/Apr/19 00:00,0,pull-request-available,,,"h3. *Issue detail info*

In our hadoop product env, we use fixed-delay restart-strategy.
{code:java}
restart-strategy: fixed-delay
restart-strategy.fixed-delay.attempts: 20
restart-strategy.fixed-delay.delay: 2 s
{code}
if a flink-job reaches the max attempt count, the flink job will write an archive file to +FileSystem+ and shut down.

but when +SubtaskExecutionAttemptDetailsHandler+ handle the detail attempt info of subtask, met NEP.
h3. *Detailed reasons are as follows:*

0. Assume a scenario, a flink job {color:#ff0000}reaches the max attempt count, ( 20 ){color}

1. +ExecutionVertex+ is a parallel subtask of the execution. Each +ExecutionVertex+ was created with {color:#660e7a}MAX_ATTEMPTS_HISTORY_SIZE( default value: 16 ).{color}

2. when +SubtaskExecutionAttemptDetailsHandler+ hand will get the attempt from ++priorExecutions,

   but priorExecutions just retained {color:#660e7a}MAX_ATTEMPTS_HISTORY_SIZE{color} elemets, so some element

   was dropped from the head of the list(FIFO). so may return null.
h3. *Detailed StackTrace*
{code:java}
java.lang.NullPointerException
   at org.apache.flink.runtime.rest.handler.util.MutableIOMetrics.addIOMetrics(MutableIOMetrics.java:88)
   at org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.createDetailsInfo(SubtaskExecutionAttemptDetailsHandler.java:140)
   at org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.archiveJsonWithPath(SubtaskExecutionAttemptDetailsHandler.java:120)
   at org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.archiveJsonWithPath(WebMonitorEndpoint.java:780)
   at org.apache.flink.runtime.dispatcher.JsonResponseHistoryServerArchivist.archiveExecutionGraph(JsonResponseHistoryServerArchivist.java:57)
   at org.apache.flink.runtime.dispatcher.Dispatcher.archiveExecutionGraph(Dispatcher.java:758)
   at org.apache.flink.runtime.dispatcher.Dispatcher.jobReachedGloballyTerminalState(Dispatcher.java:730)
   at org.apache.flink.runtime.dispatcher.MiniDispatcher.jobReachedGloballyTerminalState(MiniDispatcher.java:138)
   at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$startJobManagerRunner$6(Dispatcher.java:341)
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
   at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
   at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
   at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
   at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
   at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
   at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
   at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
   at akka.actor.ActorCell.invoke(ActorCell.scala:495)
   at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
   at akka.dispatch.Mailbox.run(Mailbox.scala:224)
   at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


{code}
h3. *Minimal reproducible example*
{code:java}
public static void main(String[] args) throws Exception {

    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    DataStream<String> text = env.addSource(new SourceFunction<String>() {
        @Override
        public void run(SourceContext<String> ctx) throws Exception {
            while (true) {
                ctx.collect(""aaaa"");
                Thread.sleep(100);
            }
        }

        @Override
        public void cancel() {

        }
    });

    text.addSink(new SinkFunction<String>() {
        @Override
        public void invoke(String value, Context context) throws Exception {
            System.out.println(1 / 0);
        }
    });

    env.execute();

}
{code}
 ",,lamber-ken,trohrmann,,,,,,,,,,,,,,,,,"asfgit commented on pull request #8250: [FLINK-12247][runtime] fix NPE when writing the archive json file to FileSystem
URL: https://github.com/apache/flink/pull/8250
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/19 14:16;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,FLINK-12183,,,,,,,,,,,,,,,,"19/Apr/19 09:39;lamber-ken;fix-nep.patch;https://issues.apache.org/jira/secure/attachment/12966469/fix-nep.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 24 14:15:07 UTC 2019,,,,,,,,,,"0|z01wqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/19 12:41;trohrmann;Have you figured out why {{attempt}} was {{null}} [~lamber-ken]?;;;","18/Apr/19 15:59;lamber-ken;[~till.rohrmann], yes, I had figured out, and I will update the issue later.;;;","19/Apr/19 09:40;lamber-ken;[~till.rohrmann], I updated the issue and upload a patch. ;;;","23/Apr/19 14:09;trohrmann;Thanks for investigating the problem [~lamber-ken]. The patch also looks good. Would it be possible to open a PR against Flink's Github repository? This would make the review process easier. Moreover, we should add some tests for this fix.;;;","24/Apr/19 08:50;lamber-ken;ok.;;;","24/Apr/19 14:15;trohrmann;Fixed via 

1.9.0: 191b9dff2f3faf281a77e211c6ef47243d6a9e8d
1.8.1: 767fe152cb69a204261a0770412c8b28d037614d
1.7.3: 814039588128b1f0f546573339cb218caa32ee9a;;;",,,,,,,,,,,,,,,,,,
Fixed can't get MAX_ATTEMPTS_HISTORY_SIZE value when create ExecutionVertex,FLINK-12246,13228760,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lamber-ken,lamber-ken,lamber-ken,18/Apr/19 06:29,03/May/19 12:00,13/Jul/23 08:05,03/May/19 12:00,1.6.3,,,,,,,,1.9.0,,,,Runtime / Configuration,,18/Apr/19 00:00,0,pull-request-available,,,"the jobConfiguration was defined as a final variable, so the {{MAX_ATTEMPTS_HISTORY_SIZE}} read from the jobManagerConfig in the ExecutionGraph and then passed to the ExecutionJobVertex.
{code:java}
/** The job configuration attached to this job. */

private final Configuration jobConfiguration = new Configuration();
{code}",,fan_li_ya,lamber-ken,trohrmann,zhuzh,,,,,,,,,,,,,,,"lamber-ken commented on pull request #8268: [FLINK-12246][runtime] Fixed can't get MAX_ATTEMPTS_HISTORY_SIZE value when create ExecutionVertex
URL: https://github.com/apache/flink/pull/8268
 
 
   ## What is the purpose of the change
   
   fix can't get `MAX_ATTEMPTS_HISTORY_SIZE` value when create ExecutionVertex.
   
   ## Brief change log
   
   the `MAX_ATTEMPTS_HISTORY_SIZE` read from the jobManagerConfig in the ExecutionGraph and then passed to the ExecutionJobVertex.
   
   ## Verifying this change
   
   This change is already covered by existing tests, `ExecutionGraphDeploymentTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable )
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Apr/19 10:20;githubbot;600","tillrohrmann commented on pull request #8268: [FLINK-12246][runtime] Fix can't get max_attempts_history_size value when create ExecutionVertex
URL: https://github.com/apache/flink/pull/8268
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/May/19 12:00;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 03 12:00:05 UTC 2019,,,,,,,,,,"0|z01wq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/19 06:36;fan_li_ya;Thanks for opening this issue. I do not see this is a problem. The configuration is empty initially. However, values can be inserted to it later. ;;;","18/Apr/19 07:14;zhuzh;Currently most of the _*ConfigOption*_(s) are cluster configs. They are in the jobMangerConfig or taskExecutorConfig.

The jobConfiguration in *_JobGraph_* is for internal use currently and is only used to record user artifacts currently. It's not open to users.;;;","18/Apr/19 09:19;lamber-ken;[~fan_li_ya], [~zhuzh] thanks for concern about this issue. I think it's better to define +jobConfiguration+ as a private variable and init it through `set` method.

and because of this problem, it affetc get +MAX_ATTEMPTS_HISTORY_SIZE+ option, it always return default value.

[option MAX_ATTEMPTS_HISTORY_SIZE |https://github.com/apache/flink/blob/92ae67d788050f2e2d457692bc0c638bc142a265/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionJobVertex.java#L218]

 ;;;","18/Apr/19 12:48;trohrmann;I think the {{MAX_ATTEMPTS_HISTORY_SIZE}} needs to be read from the {{jobManagerConfig}} in the {{ExecutionGraphBuilder#buildGraph}} instead from the {{jobConfiguration}} and then passed to the {{ExecutionJobVertex}}.;;;","18/Apr/19 17:41;lamber-ken;[~till.rohrmann], I see.;;;","03/May/19 12:00;trohrmann;Fixed via 9a345633dab41906239f21bc2c27ad3c5b6f16df;;;",,,,,,,,,,,,,,,,,,
HeapMemorySegment.getArray should return null after being freed,FLINK-12223,13228545,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fan_li_ya,fan_li_ya,fan_li_ya,17/Apr/19 08:36,29/Oct/19 13:11,13/Jul/23 08:05,29/Oct/19 13:06,1.6.3,,,,,,,,1.10.0,,,,Runtime / Task,,,0,pull-request-available,,,"According to the JavaDoc, HeapMemorySegment.getArray should return null after free is called, but it does not. ",,azagrebin,fan_li_ya,,,,,,,,,,,,,,,,,"liyafan82 commented on pull request #8197: [FLINK-12223][Runtime]HeapMemorySegment.getArray should return null a…
URL: https://github.com/apache/flink/pull/8197
 
 
   …rs in HybridMemorySegment
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix a bug in class HeapMemorySegment. The getArray method should return null after the segment is freed. But it does not.
   
   
   ## Brief change log
   
     - Fix the bug in class HeapMemorySegment
     - Add a UT in HeapMemorySegmentTest.
     
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Test HeapMemorySegmentTest.testGetArrayAfterFree will fail without this fix, and it runs successfully with this patch.
     
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/19 08:54;githubbot;600","liyafan82 commented on pull request #8408: [FLINK-12223][Runtime]HeapMemorySegment.getArray should return null a…
URL: https://github.com/apache/flink/pull/8408
 
 
   …rs in HybridMemorySegment
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix a bug in class HeapMemorySegment. The getArray method should return null after the segment is freed. But it does not.
   
   
   ## Brief change log
   
     - Fix the bug in class HeapMemorySegment
     - Add a UT in HeapMemorySegmentTest.
     
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Test HeapMemorySegmentTest.testGetArrayAfterFree will fail without this fix, and it runs successfully with this patch.
     
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/19 10:31;githubbot;600","liyafan82 commented on pull request #8197: [FLINK-12223][Runtime]HeapMemorySegment.getArray should return null a…
URL: https://github.com/apache/flink/pull/8197
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/19 10:35;githubbot;600","azagrebin commented on pull request #8408: [FLINK-12223][Runtime] HeapMemorySegment.getArray should return null after being freed
URL: https://github.com/apache/flink/pull/8408
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/19 13:05;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,FLINK-12189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 13:11:43 UTC 2019,,,,,,,,,,"0|z01veg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/19 13:11;azagrebin;Merged into master by f71ce33c2e6de60a2b3ab671e121a851b7b07b59;;;",,,,,,,,,,,,,,,,,,,,,,,
Yarn application can't stop when flink job failed in per-job yarn cluster mode,FLINK-12219,13228513,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lamber-ken,lamber-ken,lamber-ken,17/Apr/19 07:03,07/May/19 15:27,13/Jul/23 08:05,07/May/19 15:27,1.6.3,1.8.0,,,,,,,1.7.3,1.8.1,1.9.0,,Deployment / YARN,Runtime / REST,17/Apr/19 00:00,0,pull-request-available,,,"h3. *Issue detail info*

In our flink(1.6.3) product env, I often encounter a scene that yarn application can't stop when flink job failed in per-job yarn cluste mode, so I deeply analyzed the reason why it happened.

When a flink job fail, system will write an archive file to a FileSystem through +MiniDispatcher#archiveExecutionGraph+ method, then notify YarnJobClusterEntrypoint to shutDown. But, if +MiniDispatcher#archiveExecutionGraph+ throw exceptions during execution, it affect the following calls.

So I open [FLINK-12247|https://issues.apache.org/jira/projects/FLINK/issues/FLINK-12247] to solve NEP bug when system write archive to FileSystem. But We still need to consider other exceptions, so we should catch Exception / Throwable not just IOExcetion.
h3. *Flink yarn job fail flow*

!image-2019-04-23-17-37-00-081.png!
h3. *Flink yarn job fail on yarn*

!image-2019-04-17-15-00-40-687.png!

 
h3. *Flink yarn application can't stop*

!image-2019-04-17-15-02-49-513.png!

 

 ",,lamber-ken,trohrmann,txhsj,,,,,,,,,,,,,,,,"lamber-ken commented on pull request #8254: [FLINK-12219][runtime] Yarn application can't stop when flink job failed in per-job yarn cluste mode
URL: https://github.com/apache/flink/pull/8254
 
 
   ## What is the purpose of the change
   
   In our flink(1.6.3) product env, I often encounter a scene that yarn application can't stop when flink job failed in per-job yarn cluste mode.
   
   When a flink job fail, system will write an archive file to a FileSystem through `MiniDispatcher#archiveExecutionGraph` method, then notify `YarnJobClusterEntrypoint` to shutDown. But, if `MiniDispatcher#archiveExecutionGraph` throw exceptions during execution, it affect the following calls.
   
   So I open [FLINK-12247](https://issues.apache.org/jira/browse/FLINK-12247) to solve NEP bug when system write archive to FileSystem. But We still need to consider other exceptions, so we should catch Exception / Throwable not just IOExcetion.
   
   for more detail, please see [FLINK-12219-JIRA](https://issues.apache.org/jira/browse/FLINK-12219).
   
   
   ## Brief change log
   
   `JsonResponseHistoryServerArchivist#archiveExecutionGraph` change Exception to IOException.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `TestingDispatcher`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/19 14:50;githubbot;600","lamber-ken commented on pull request #8254: [FLINK-12219][runtime] Yarn application can't stop when flink job failed in per-job yarn cluste mode
URL: https://github.com/apache/flink/pull/8254
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Apr/19 10:11;githubbot;600","lamber-ken commented on pull request #8254: [FLINK-12219][runtime] Yarn application can't stop when flink job failed in per-job yarn cluste mode
URL: https://github.com/apache/flink/pull/8254
 
 
   ## What is the purpose of the change
   
   In our flink(1.6.3) product env, I often encounter a scene that yarn application can't stop when flink job failed in per-job yarn cluste mode.
   
   When a flink job fail, system will write an archive file to a FileSystem through `MiniDispatcher#archiveExecutionGraph` method, then notify `YarnJobClusterEntrypoint` to shutDown. But, if `MiniDispatcher#archiveExecutionGraph` throw exceptions during execution, it affect the following calls.
   
   So I open [FLINK-12247](https://issues.apache.org/jira/browse/FLINK-12247) to solve NEP bug when system write archive to FileSystem. But We still need to consider other exceptions, so we should catch Exception / Throwable not just IOExcetion.
   
   for more detail, please see [FLINK-12219-JIRA](https://issues.apache.org/jira/browse/FLINK-12219).
   
   
   ## Brief change log
   
   `JsonResponseHistoryServerArchivist#archiveExecutionGraph` change Exception to IOException.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `TestingDispatcher`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Apr/19 10:11;githubbot;600","tillrohrmann commented on pull request #8254: [FLINK-12219][runtime] Yarn application can't stop when flink job failed in per-job yarn cluste mode
URL: https://github.com/apache/flink/pull/8254
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Apr/19 08:53;githubbot;600","tillrohrmann commented on pull request #8334: [FLINK-12219] Add utility to check for normal future completion
URL: https://github.com/apache/flink/pull/8334
 
 
   ## What is the purpose of the change
   
   FutureUtils#assertNoException will assert that the given future has not been completed
   exceptionally. If it has been completed exceptionally, then it will call the
   FatalExitExceptionHandler.
   
   ## Verifying this change
   
   Added `FutureUtilsTest#testAssertNoExceptionWithoutExceptions` and `#testAssertNoExceptionWithExceptions`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/May/19 12:53;githubbot;600","tillrohrmann commented on pull request #8334: [FLINK-12219] Log uncaught exceptions and terminate in case Dispatcher#jobReachedGloballyTerminalState fails
URL: https://github.com/apache/flink/pull/8334
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/May/19 15:24;githubbot;600",,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/19 12:20;lamber-ken;fix-bug.patch;https://issues.apache.org/jira/secure/attachment/12966234/fix-bug.patch","17/Apr/19 07:00;lamber-ken;image-2019-04-17-15-00-40-687.png;https://issues.apache.org/jira/secure/attachment/12966189/image-2019-04-17-15-00-40-687.png","17/Apr/19 07:02;lamber-ken;image-2019-04-17-15-02-49-513.png;https://issues.apache.org/jira/secure/attachment/12966188/image-2019-04-17-15-02-49-513.png","23/Apr/19 09:37;lamber-ken;image-2019-04-23-17-37-00-081.png;https://issues.apache.org/jira/secure/attachment/12966722/image-2019-04-23-17-37-00-081.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 07 15:27:31 UTC 2019,,,,,,,,,,"0|z01v7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/19 15:14;trohrmann;In order to better understand the problem I would need a bit more context [~lamber-ken]. How did you deploy the Flink job (job mode detached or attached)? How did the job fail? What exception is being thrown in {{FsJobArchivist}} to prevent the cluster from shutting down?;;;","23/Apr/19 09:48;lamber-ken;hi, [~till.rohrmann], I updated the issue. help to review or offer your comments or suggestions. thanks.;;;","23/Apr/19 14:11;trohrmann;I guess we could add an {{ArchivedExecutionGraph}} which contains the failure cause why we couldn't archive the job. What is the exception you are seeing in your deployment [~lamber-ken]?;;;","23/Apr/19 14:12;trohrmann;You are running the Flink per-job cluster in attached mode [~lamber-ken], right?;;;","24/Apr/19 08:49;lamber-ken;no, in detach mode.;;;","30/Apr/19 08:55;trohrmann;Problem has been resolved with FLINK-12247;;;","07/May/19 15:27;trohrmann;Fixed via
1.9.0: 417d6d2070e7ff82eb73a605f12f50ca13acce15
1.8.1: a956a49876bb1733bb9354372c25c05d0d96d7be
1.7.3: cfbac8f024e818e9b9f816d93a684c4f8b721c2a;;;",,,,,,,,,,,,,,,,,
OperationTreeBuilder.map() should perform ExpressionResolver.resolve(),FLINK-12217,13228487,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,17/Apr/19 04:23,17/Apr/19 07:26,13/Jul/23 08:05,17/Apr/19 07:26,,,,,,,,,1.9.0,,,,Table SQL / API,,,0,pull-request-available,,,"In {{OperationTreeBuilder.map()}}, we should resolve all LookupCallExpression for the case of java, otherwise, exceptions will be thrown.

The following test case can reproduce the problem.
{code:java}
  @Test
  def testMap(): Unit = {
    val util = streamTestUtil()
    val t = util.addTable[(Int, Long, String)](""Table3"",'a, 'b, 'c)
    util.tableEnv.registerFunction(""func"", Func23)

    val t1 = t.map(""func(a, b, c)"")
    val t2 = t.map(Func23('a, 'b, 'c))

    verifyTableEquals(t1, t2)
  }
{code}

{code:java}
org.apache.flink.table.api.ValidationException: Only ScalarFunction can be used in the map operator.

	at org.apache.flink.table.operations.OperationTreeBuilder.map(OperationTreeBuilder.scala:355)
	at org.apache.flink.table.api.TableImpl.map(tableImpl.scala:461)
	at org.apache.flink.table.api.TableImpl.map(tableImpl.scala:457)
	at org.apache.flink.table.api.stream.table.stringexpr.CalcStringExpressionTest.testMap(CalcStringExpressionTest.scala:178)
{code}

",,dwysakowicz,hequn8128,,,,,,,,,,,,,,,,,"hequn8128 commented on pull request #8192: [FLINK-12217][table] OperationTreeBuilder.map() should perform ExpressionResolver.resolve()
URL: https://github.com/apache/flink/pull/8192
 
 
   
   ## What is the purpose of the change
   
   This pull request fixes the problem for OperationTreeBuilder.map(). In OperationTreeBuilder.map(), we should resolve all LookupCallExpression for the case of java, otherwise, exceptions will be thrown.
   
   ## Brief change log
   
     - Add resolver logic for OperationTreeBuilder.map().
     - Add test.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Added tests to cover the case of java in CalcStringExpressionTest.testMap().
    
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/19 04:33;githubbot;600","dawidwys commented on pull request #8192: [FLINK-12217][table] OperationTreeBuilder.map() should perform ExpressionResolver.resolve()
URL: https://github.com/apache/flink/pull/8192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/19 07:25;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 17 07:26:20 UTC 2019,,,,,,,,,,"0|z01v1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/19 07:26;dwysakowicz;Fixed in 7fba616857a6a71c6a818c008e59af94bc933375;;;",,,,,,,,,,,,,,,,,,,,,,,
Respect the number of bytes from input parameters in HybridMemorySegment,FLINK-12216,13228483,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fan_li_ya,fan_li_ya,fan_li_ya,17/Apr/19 04:08,07/Nov/19 17:41,13/Jul/23 08:05,07/Nov/19 17:41,,,,,,,,,1.10.0,,,,Runtime / Network,Runtime / Task,,0,pull-request-available,,,"For the following two methods in HybridMemorySegment class,

public final void get(int offset, ByteBuffer target, int numBytes)

public final void put(int offset, ByteBuffer source, int numBytes) 

the actual number of bytes read/written should be specified by the input parameter numBytes, but it does not for some types of ByteBuffer. Instead, it simply read/write until the end.

So this is a bug and I am going to fix it.",,fan_li_ya,sewen,,,,,,,,,,,,,,,,,"liyafan82 commented on pull request #8194: [FLINK-12216][Runtime]Respect the number of bytes from input paramete…
URL: https://github.com/apache/flink/pull/8194
 
 
   …rs in HybridMemorySegment
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix some bugs in class HybridMemorySegment. In particular, when reading from or writing to a ByteBuffer, the number of bytes actually read/written should be specified by the input parameter numBytes. However, the current logic reads or writes data till the ByteBuffer is exhausted.
   
   
   ## Brief change log
   
     - Fix the bug in HybridMemorySegment.java
     - Add a UT in HybridOnHeapMemorySegmentTest.
     
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Test HybridMemorySegment.testReadOnlyByteBufferPut tests the case when the ByteBuffer is neither a DirectByteBuffer, nor a HeapByteBuffer. This test will fail without this patch.
     
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/19 06:41;githubbot;600","asfgit commented on pull request #8194: [FLINK-12216][Runtime]Respect the number of bytes from input parameters in HybridMemorySegment
URL: https://github.com/apache/flink/pull/8194
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 17:33;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,FLINK-12189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 17:41:28 UTC 2019,,,,,,,,,,"0|z01v0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/19 17:41;sewen;Fixed via 534835b0386c767116708f7986dfb759e377dcaf;;;",,,,,,,,,,,,,,,,,,,,,,,
"Flink SVGs on ""Material"" page broken, render incorrectly on Firefox",FLINK-12191,13228023,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,plucas,plucas,plucas,15/Apr/19 07:53,17/Apr/19 16:19,13/Jul/23 08:05,17/Apr/19 16:19,,,,,,,,,,,,,Project Website,,,0,pull-request-available,,,"Like FLINK-11043, the Flink SVGs on the [Material page of the Flink website|https://flink.apache.org/material.html] are invalid and do not render correctly on Firefox.

I'm not sure if there is an additional source-of-truth for these images, or if these hosted on the website are canonical, but I can fix them nonetheless.

I also noticed that one of the squirrels in both {{color_black.svg}} and {{color_white.svg}} is missing the eye gradient, which can also be easily fixed.

 !Screen Shot 2019-04-15 at 09.48.15.png|thumbnail!",,plucas,,,,,,,,,,,,,,,,,,"patricklucas commented on pull request #199: [FLINK-12191] Fix SVG logos
URL: https://github.com/apache/flink-web/pull/199
 
 
   * Gradients don't appear correctly in Firefox since gradient definitions
     not in <defs/> tag
   * One squirrel missing eye gradient
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/19 07:59;githubbot;600","asfgit commented on pull request #199: [FLINK-12191] Fix SVG logos
URL: https://github.com/apache/flink-web/pull/199
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/19 15:57;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,FLINK-11043,,,,,,,"15/Apr/19 07:50;plucas;Screen Shot 2019-04-15 at 09.48.15.png;https://issues.apache.org/jira/secure/attachment/12965910/Screen+Shot+2019-04-15+at+09.48.15.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-15 07:53:20.0,,,,,,,,,,"0|z01s6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix IllegalArgumentException thrown by FlinkKinesisConsumerMigrationTest#writeSnapshot,FLINK-12190,13228022,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yanghua,yanghua,yanghua,15/Apr/19 07:21,26/Jun/19 03:35,13/Jul/23 08:05,26/Jun/19 03:35,,,,,,,,,1.9.0,,,,Connectors / Kinesis,Tests,,0,pull-request-available,,,"Currently, {{FlinkKinesisConsumerMigrationTest#writeSnapshot}} throws an exception : 
{code:java}
java.lang.IllegalArgumentException: Cannot create enum from null value!

at com.amazonaws.regions.Regions.fromName(Regions.java:79)
at org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.createKinesisClient(AWSUtil.java:93)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.createKinesisClient(KinesisProxy.java:203)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.<init>(KinesisProxy.java:138)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.create(KinesisProxy.java:213)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.<init>(KinesisDataFetcher.java:275)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.<init>(KinesisDataFetcher.java:237)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest$TestFetcher.<init>(FlinkKinesisConsumerMigrationTest.java:422)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.writeSnapshot(FlinkKinesisConsumerMigrationTest.java:332)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.writeSnapshot(FlinkKinesisConsumerMigrationTest.java:113)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runners.Suite.runChild(Suite.java:128)
at org.junit.runners.Suite.runChild(Suite.java:27)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}
This exception may make the upgrader confused.

The exception is because the exists code did not initialize TestFetcher correctly. More details see the commits : https://issues.apache.org/jira/browse/FLINK-10785?focusedCommentId=16778899&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16778899 under FLINK-10785",,tzulitai,yanghua,,,,,,,,,,,,,,,,,"yanghua commented on pull request #8174: [FLINK-12190] Fix IllegalArgumentException throwed by FlinkKinesisConsumerMigrationTest#writeSnapshot
URL: https://github.com/apache/flink/pull/8174
 
 
   ## What is the purpose of the change
   
   *This pull request fixes IllegalArgumentException throwed by FlinkKinesisConsumerMigrationTest#writeSnapshot*
   
   ## Brief change log
   
     - *Fix IllegalArgumentException throwed by FlinkKinesisConsumerMigrationTest#writeSnapshot*
   
   ## Verifying this change
   
   This change is already verified by commits [86e33fe36ed29b6e01dd7fe4151a8650ad25612c](86e33fe36ed29b6e01dd7fe4151a8650ad25612c) under PR #8168 *.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/19 07:41;githubbot;600","asfgit commented on pull request #8174: [FLINK-12190] Fix IllegalArgumentException thrown by FlinkKinesisConsumerMigrationTest#writeSnapshot
URL: https://github.com/apache/flink/pull/8174
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jun/19 03:35;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,FLINK-10785,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 26 03:35:32 UTC 2019,,,,,,,,,,"0|z01s6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/19 03:35;tzulitai;Merged for 1.9.0: f9681c0d1d12b9120c5f6e8cbc3e81265d4b403a;;;",,,,,,,,,,,,,,,,,,,,,,,
wrap FileWriter with BufferedWriter for better performance,FLINK-12187,13227916,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,bd2019us,bd2019us,14/Apr/19 04:32,30/Apr/19 11:59,13/Jul/23 08:05,30/Apr/19 11:59,,,,,,,,,1.9.0,,,,Examples,,,0,patch-available,pull-request-available,pull-requests-available,"Location: src/main/java/org/apache/flink/examples/java/relational/util/WebLogDataGenerator.java

The FileWriter.write() method is invoked multiple times in loops, which is bad to the performance of program. The decorator class BufferedWriter can be used to alleviate the impact of frequent IO operation with caches. Therefore, when the write() method is intensively used, the BufferedWriter is highly recommended and should be preferred.",,bd2019us,fan_li_ya,rmetzger,,,,,,,,,,,,,,,,"bd2019us commented on pull request #8167: [FLINK-12187][Examples] Bug fixes, Use BufferedWriter in a loop instead of FileWriter
URL: https://github.com/apache/flink/pull/8167
 
 
   ## What is the purpose of the change
   
   This pull request changes FileWriter to BufferedWriter when the write() method is intensively invoked in loops. This can improve the performance by reducing frequent IO operations.
   
   ## Brief change log
   
   Wrap the FileWriter with BufferedWriter.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: ( no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Apr/19 04:43;githubbot;600","rmetzger commented on pull request #8167: [FLINK-12187][Examples] Bug fixes, Use BufferedWriter in a loop instead of FileWriter
URL: https://github.com/apache/flink/pull/8167
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Apr/19 11:58;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/19 04:47;bd2019us;1.patch;https://issues.apache.org/jira/secure/attachment/12965851/1.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 30 11:59:19 UTC 2019,,,,,,,,,,"0|z01riw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/19 07:34;fan_li_ya;[~bd2019us], thanks for opening this issue. Could you please give some references to show that the performance of BufferedWriter is better?;;;","30/Apr/19 11:59;rmetzger;Merged in https://github.com/apache/flink/commit/ca8145e54607aa46b275d09e785e7c1653f0181c;;;",,,,,,,,,,,,,,,,,,,,,,
HistoryServerArchiveFetcher isn't compatible with old version,FLINK-12184,13227867,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,klion26,yumeng,yumeng,13/Apr/19 10:52,02/May/19 11:54,13/Jul/23 08:05,02/May/19 11:54,1.6.4,1.7.2,1.8.0,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / Coordination,,,0,pull-request-available,,,"If we have old verison json files in history server, the HistoryServerArchiveFetcher can't convert legacy job overview. It will throw the NullPointerException when trying to convert legacy job overview because the tasks don't have the ""pending"" field.",,trohrmann,yumeng,,,,,,,,,,,,,,,,,"yumengz5 commented on pull request #8164: [FLINK-12184] history server compatible with old version
URL: https://github.com/apache/flink/pull/8164
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fixes FLINK-12184. The problem is that the HistoryServerArchiveFetcher can't convert legacy job overview. 
   
   Get the ""scheduled"" field instead of getting the ""pending"" filed which doesn't exist in legacy job overview json.
   
   ## Brief change log
   
     - *The tasks get ""scheduled"" field instead of getting ""pending"" field.*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Apr/19 11:03;githubbot;600","tillrohrmann commented on pull request #8164: [FLINK-12184] history server compatible with old version
URL: https://github.com/apache/flink/pull/8164
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Apr/19 09:22;githubbot;600","klion26 commented on pull request #8313: [FLINK-12184][coordinator]HistoryServerArchieFetcher incompatible with old version
URL: https://github.com/apache/flink/pull/8313
 
 
   ## What is the purpose of the change
   
   If we have old verison json files in history server, the HistoryServerArchiveFetcher can't convert legacy job overview. It will throw the NullPointerException when trying to convert legacy job overview because the tasks don't have the ""pending"" field.
   
   There are actually two paths for the legacy job overview to consider, one is coming from Flink < 1.4 where we actually wrote the pending field and the other is for Flink >= 1.4 (legacy mode) which split the pending field up into scheduled, deployed and created.
   
   
   ## Brief change log
   Add a condition check and add the right state.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   `HistoryServerTest#testHistoryServerIntegration`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**no**)
     - The serializers: (**no**)
     - The runtime per-record code paths (performance sensitive): (**no**)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**no**)
     - The S3 file system connector: (**no**)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**no**)
     - If yes, how is the feature documented? (**not applicable**)
   @tillrohrmann @yumengz5
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Apr/19 16:01;githubbot;600","tillrohrmann commented on pull request #8313: [FLINK-12184][Coordination]HistoryServerArchieFetcher incompatible with old version
URL: https://github.com/apache/flink/pull/8313
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/May/19 11:53;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 02 11:54:01 UTC 2019,,,,,,,,,,"0|z01r80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/19 11:54;trohrmann;Fixed via
1.9.0: b561a5eb8fdb19af77d0a3ca3d115b11fbeaba9b
1.8.1: c1835d4abb727bc2d0d989f7bdbc6dc01bb40f92
1.7.3: 070679124406c6223064956fad05b778a0fa9f47;;;",,,,,,,,,,,,,,,,,,,,,,,
TypeExtractor.getMapReturnTypes produces different TypeInformation than createTypeInformation for classes with parameterized ancestors,FLINK-12175,13227781,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dadams,dadams,12/Apr/19 19:14,30/Jul/20 14:13,13/Jul/23 08:05,30/Jul/20 14:12,1.7.2,1.9.0,,,,,,,1.12.0,,,,API / Type Serialization System,,,0,pull-request-available,,,"I expect that the {{TypeMapper}} {{createTypeInformation}} and {{getMapReturnTypes}} would produce equivalent type information for the same type. But when there is a parameterized superclass, this does not appear to be the case.

Here's a test case that could be added to {{PojoTypeExtractorTest.java}} that demonstrates the issue:
{code}
public static class Pojo implements Serializable {
	public int digits;
	public String letters;
}

public static class ParameterizedParent<T extends Serializable> implements Serializable {
	public T pojoField;
}

public static class ConcreteImpl extends ParameterizedParent<Pojo> {
	public double precise;
}

public static class ConcreteMapFunction implements MapFunction<ConcreteImpl, ConcreteImpl> {
	@Override
	public ConcreteImpl map(ConcreteImpl value) throws Exception {
		return null;
	}
}

@Test
public void testMapReturnType() {
	final TypeInformation<ConcreteImpl> directTypeInfo = TypeExtractor.createTypeInfo(ConcreteImpl.class);

	Assert.assertTrue(directTypeInfo instanceof PojoTypeInfo);
	TypeInformation<?> directPojoFieldTypeInfo = ((PojoTypeInfo) directTypeInfo).getPojoFieldAt(0).getTypeInformation();
	Assert.assertTrue(directPojoFieldTypeInfo instanceof PojoTypeInfo);

	final TypeInformation<ConcreteImpl> mapReturnTypeInfo
		= TypeExtractor.getMapReturnTypes(new ConcreteMapFunction(), directTypeInfo);
	Assert.assertTrue(mapReturnTypeInfo instanceof PojoTypeInfo);
	TypeInformation<?> mapReturnPojoFieldTypeInfo = ((PojoTypeInfo) mapReturnTypeInfo).getPojoFieldAt(0).getTypeInformation();
	Assert.assertTrue(mapReturnPojoFieldTypeInfo instanceof PojoTypeInfo);

	Assert.assertEquals(directTypeInfo, mapReturnTypeInfo);
}
{code}

This test case will fail on the last two asserts because {{getMapReturnTypes}} produces a {{TypeInformation}} for {{ConcreteImpl}} with a {{GenericTypeInfo}} for the {{pojoField}}, whereas {{createTypeInformation}} correctly produces a {{PojoTypeInfo}}.",,andbul,dadams,dwysakowicz,yangfei,,,,,,,,,,,,,,,"andbul commented on pull request #8357: [FLINK-12175] Change filling of typeHierarchy in analyzePojo, for cor…
URL: https://github.com/apache/flink/pull/8357
 
 
   ## What is the purpose of the change
   
   This pull request fixes bug with creating TypeInfo for POJO Classes with generic superclasses which has fields of that genric type. The problem is when we building typeHierarchy for Pojo with generic superclass we already have values in typeHierarchy array, we stopped building typehierahy and coud not extract type for fields later.
   
   
   ## Brief change log
   
     - Removed condition on if typeHierarchy <= 1. Changed to simple else statement cause we shoudn't stop building type hierarchy.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Added *PojoParametrizedTypeExtractionTest* test that validates that TypeInfo for generic field correctlly exctracted by TypeExtractor
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: yes 
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:  no 
   
   ## Documentation
   
     - Does this pull request introduce a new feature?  no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/May/19 09:20;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 14:12:32 UTC 2020,,,,,,,,,,"0|z01qow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/19 12:17;andbul;Can I work on this ticket?;;;","16/Apr/19 14:56;dadams;It's fine with me, I'm not working on a fix.;;;","28/Apr/19 10:04;yangfei;[~andbul] are you working on this fix? Can I work on this ticket?;;;","28/Apr/19 19:19;andbul;Yes I am working on this. I have found a solution and working on pull request.;;;","06/May/19 11:51;yangfei;[~andbul] can you show your PR to me? thanks ,I want to learn how you solve this issue;;;","07/May/19 10:04;andbul;[~yangfei] you can find it [here |http://example.com].;;;","30/Jul/20 14:12;dwysakowicz;Fixed in a8847061c40bf8ca17e22e6e412a378f53b8b82d;;;",,,,,,,,,,,,,,,,,
The network buffer memory size should not be checked against the heap size on the TM side,FLINK-12171,13227691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,12/Apr/19 11:55,30/Jul/19 10:10,13/Jul/23 08:05,30/Jul/19 10:10,1.7.2,1.8.0,1.9.0,,,,,,1.10.0,,,,Runtime / Network,,,0,pull-request-available,,,"Currently when computing the network buffer memory size on the TM side in _TaskManagerService#calculateNetworkBufferMemory_`(version 1.8 or 1.7) or _NetworkEnvironmentConfiguration#calculateNewNetworkBufferMemory_(master), the computed network buffer memory size is checked to be less than `maxJvmHeapMemory`. However, in TM side, _maxJvmHeapMemory_ stores the maximum heap memory (namely -Xmx) .

 

With the above process, when TM starts, -Xmx is computed in RM or in _taskmanager.sh_ with (container memory - network buffer memory - managed memory),  thus the above checking implies that the heap memory of the TM must be larger than the network memory, which seems to be not necessary.

 

This may cause TM to use more memory than expected. For example, for a job who has a large network throughput, uses may configure network memory to 2G. However, if users want to assign 1G to heap memory, the TM will fail to start, and user has to allocate at least 2G heap memory (in other words, 4G in total for the TM instead of 3G) to make the TM runnable. This may cause resource inefficiency.

 

Therefore, I think the network buffer memory size also need to be checked against the total memory instead of the heap memory on the TM  side:
 # Checks that networkBufFraction < 1.0.
 # Compute the total memory by ( jvmHeapNoNet / (1 - networkBufFraction)).
 # Compare the network buffer memory with the total memory.

This checking is also consistent with the similar one done on the RM side.","Flink-1.7.2, and Flink-1.8 seems have not modified the logic here.

 ",gaoyunhaii,maguowei,nkruber,,,,,,,,,,,,,,,,"gaoyunhaii commented on pull request #8556: [FLINK-12171][Network] Do not limit the network buffer memory by heap size on the TM side
URL: https://github.com/apache/flink/pull/8556
 
 
   ## What is the purpose of the change
   
   This pull request fixes the bug that limits the network buffer size with the heap size on the TM side. In fact, network buffer occupies a part of direct memory and is independent with the heap.
   
   To fix this problem, The limitation on the TM side is removed. Although we may want to compare the network memory size with the total memory size on the TM side, currently we can only compute the total memory with heap + computed network memory and the computed total memory should be always larger than the computed network memory.
   
   To remove the limitation, the max allowed memory used to check the network memory size on TM side is changed to Long.MAX_VALUE. Another option is to move the checking to the caller function on the RM side. however, it is not easy to achieve since the checking relies on the configured values of MIN and MAX, and it is not accessible outside of the current function.
   
   ## Brief change log
     - *Change the maximum allow memory on TM side to Long.MAX_VALUE.*
   
   ## Verifying this change
   This change added tests and can be verified as follows:
   
     - *Manually verified the change by running a cluster with two task managers for both standalone and YARN mode, and test the configuration with heap = 3G/network = 2G and heap = 5G/network = 2G*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/May/19 05:18;githubbot;600","NicoK commented on pull request #8556: [FLINK-12171][Network] Do not limit the network buffer memory by heap size on the TM side
URL: https://github.com/apache/flink/pull/8556
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 10:09;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 30 10:10:39 UTC 2019,,,,,,,,,,"0|z01q4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/19 03:34;gaoyunhaii;After further analyze this problem, now I think we do not need to check the maximum allowed memory on TM side.

For RM side, we compute the network memory size from the total memory size, there may be cases that the configured MIN and MAX is too large that the resulted network memory is larger than the total memory size, we need to check against that.

However, on TM side, we do not know the total memory size, instead we only know the heap size. We can only deduce the total memory size by heap size + computed network memory, which is always larger than the computed network memory. 

Therefore, unless we ensure the total memory size is available on the TM side and we also compute the network memory size from the total memory size on TM side, we can not check the network memory size.

According to the above analysis, I think we can first remove the comparison of the network memory size and heap memory size directly. This comparison is not right since the network memory is not part of the heap memory, and it may raise error when the configuration is in fact reasonable. 

 

 ;;;","13/Jun/19 10:58;gaoyunhaii;[~pnowojski]  very sorry to disturb, but I would like to know how do you think about this issue? ;;;","30/Jul/19 10:10;nkruber;fixed via 8dec21f;;;",,,,,,,,,,,,,,,,,,,,,
ClusterClient doesn't unset the context class loader after program run,FLINK-12167,13227591,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zorro,zorro,zorro,12/Apr/19 01:17,27/May/19 08:47,13/Jul/23 08:05,27/May/19 08:47,1.8.0,,,,,,,,1.9.0,,,,Command Line Client,,,0,pull-request-available,,," 
{code:java}
public JobSubmissionResult run(PackagedProgram prog, int parallelism)
{code}
This method doesn't restore the thread's original class loader after program is run. This could lead to several class loading issues.",,trohrmann,yanghua,zorro,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #8154: [FLINK-12167] Reset context classloader in run and getOptimizedPlan methods
URL: https://github.com/apache/flink/pull/8154
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/May/19 05:38;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 27 08:47:08 UTC 2019,,,,,,,,,,"0|z01pio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/19 01:30;zorro;[~yanghua] Can you please assign it to me? I have a PR ready.;;;","12/Apr/19 01:40;yanghua;[~zorro] Feel free to open a pr for this issue. I can not assign it to you because you do not have Jira contributor permission. You should send a mail to flink dev mailing list to request a Jira contribute permission and attach your Jira id information.;;;","24/May/19 09:56;trohrmann;[~zorro] I've given you contributor permissions and assigned you to this ticket. Please open the PR if you have the fix already ready. Thanks a lot.;;;","24/May/19 11:19;zorro;[~till.rohrmann] Thank you for that. I have it open here [https://github.com/apache/flink/pull/8154]
Please review and let me know if I need to do anything else.;;;","27/May/19 08:47;trohrmann;Fixed via 6af47370be952e4dca3b18d71f5206269ac78fc8;;;",,,,,,,,,,,,,,,,,,,
JobMasterTest.testJobFailureWhenTaskExecutorHeartbeatTimeout is unstable,FLINK-12164,13227413,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,SleePy,aljoscha,aljoscha,11/Apr/19 09:15,03/Sep/19 05:04,13/Jul/23 08:05,03/Sep/19 05:04,1.9.0,,,,,,,,1.10.0,1.9.1,,,Runtime / Coordination,,,0,pull-request-available,test-stability,,"{code}
07:28:23.957 [ERROR] Tests run: 24, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.968 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterTest
07:28:23.957 [ERROR] testJobFailureWhenTaskExecutorHeartbeatTimeout(org.apache.flink.runtime.jobmaster.JobMasterTest)  Time elapsed: 0.177 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.lang.Exception: Unknown TaskManager 69a7c8c18a36069ff90a1eae8ec41066
	at org.apache.flink.runtime.jobmaster.JobMasterTest.registerSlotsAtJobMaster(JobMasterTest.java:1746)
	at org.apache.flink.runtime.jobmaster.JobMasterTest.runJobFailureWhenTaskExecutorTerminatesTest(JobMasterTest.java:1670)
	at org.apache.flink.runtime.jobmaster.JobMasterTest.testJobFailureWhenTaskExecutorHeartbeatTimeout(JobMasterTest.java:1630)
Caused by: java.lang.Exception: Unknown TaskManager 69a7c8c18a36069ff90a1eae8ec41066
{code}",,aljoscha,jark,SleePy,trohrmann,yunta,,,,,,,,,,,,,,"zentol commented on pull request #8388: [FLINK-12164][runtime] Harden JobMasterTest against timeouts
URL: https://github.com/apache/flink/pull/8388
 
 
   Hardens `JobMasterTest#testJobFailureWhenTaskExecutorHeartbeatTimeout`
   
   The reported failure occurred when offering slots, indicating that at this time the TM was no longer registered at the JM. However, the TM is being registered right before the slot offer.
   
   The only explanation I could find is that due so some freak timing thing a heartbeat times out in between these 2 calls. `testJobFailureWhenTaskExecutorHeartbeatTimeout` uses a very small heartbeat interval (1ms) and timeout (5ms).
   I was only able to reproduce the issue locally after reducing the timeout to 2ms.
   
   This PR doubles the timeout to reduce the likely-hood of this happening again.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/May/19 15:09;githubbot;600","tillrohrmann commented on pull request #8388: [FLINK-12164][runtime] Harden JobMasterTest against timeouts
URL: https://github.com/apache/flink/pull/8388
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/May/19 14:27;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 05:04:43 UTC 2019,,,,,,,,,,"0|z01ofc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/19 09:15;aljoscha;Travis run: https://travis-ci.org/apache/flink/jobs/518617259;;;","13/May/19 14:26;trohrmann;Increased the timeout to make heartbeat timeouts less likely.

9c2bcae735cd00336af3284e4f631afe061552ba;;;","29/Jun/19 03:30;yunta;This problem happened again, here is the instance: [https://api.travis-ci.org/v3/job/551302724/log.txt] .;;;","26/Jul/19 08:43;SleePy;Another instance, [https://travis-ci.com/flink-ci/flink/jobs/218954532].

If nobody is following this, I would like to take over it.

 ;;;","29/Jul/19 08:47;trohrmann;Another instance: https://api.travis-ci.com/v3/job/220250300/log.txt;;;","30/Jul/19 15:14;SleePy;It seems that the PR does not be linked here somehow. So add the link manually.;;;","30/Aug/19 02:06;jark;Another instance in release-1.9: https://api.travis-ci.org/v3/job/578428491/log.txt;;;","30/Aug/19 07:59;aljoscha;Is anyone working on this? It affects our build stability so should be worked on ASAP.;;;","30/Aug/19 08:16;SleePy;Hi [~aljoscha],
Sorry I forget to update the progress. I'm not working on it for now. I have abandoned my prior PR. I was planning to implement a better one (not mocking so much) however recently I didn't find time to do so :(
If it becomes a blocker, I could postpone other things and get back to this issue first.
;;;","30/Aug/19 12:27;SleePy;I just finished the PR.
[~till.rohrmann], could you take a look at it?;;;","03/Sep/19 05:04;trohrmann;Fixed via

1.10.0:
c0454ef64e71095cd38e6ba958ecce26f934c610
30f73c1787d5bb735edf12c5f65382e6aee340ca
0e75425e61274ba1591172021868678dee357bb5

1.9.1:
66bfe3fbb1ee8233cea8c9bbadc22e36bb3bed9b
96800c94a52c58a86d000d9846c2ff8956732c76
e0a95d4a1d536b479c7788c941dc5e8a74d7997c;;;",,,,,,,,,,,,,
Use correct ClassLoader for Hadoop Writable TypeInfo,FLINK-12163,13227406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arganzheng,morvenhuang,morvenhuang,11/Apr/19 09:03,24/Mar/22 04:54,13/Jul/23 08:05,13/Jun/19 12:25,1.7.2,1.8.0,,,,,,,1.9.0,,,,Connectors / Hadoop Compatibility,,,0,,,,"For Flink 1.5.6, 1.7.2, I keep getting error when using Hadoop Compatibility, 
{code:java}
Caused by: java.lang.RuntimeException: Could not load the TypeInformation for the class 'org.apache.hadoop.io.Writable'. You may be missing the 'flink-hadoop-compatibility' dependency.
at org.apache.flink.api.java.typeutils.TypeExtractor.createHadoopWritableTypeInfo(TypeExtractor.java:2140)
at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1759)
at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1701)
at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:956)
at org.apache.flink.api.java.typeutils.TypeExtractor.createSubTypesInfo(TypeExtractor.java:1176)
at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:889)
at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:839)
at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:805)
at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:798)
at org.apache.flink.api.common.typeinfo.TypeHint.<init>(TypeHint.java:50)

{code}
Packaging the flink-hadoop-compatibility dependency with my code into a fat jar doesn't help.

The error won't go until I copy the flink-hadoop-compatibility jar to FLINK_HOME/lib.

This seems to be a classloader issue when looking into the TypeExtractor#createHadoopWritableTypeInfo
{code:java}
Class<?> typeInfoClass;
try {
typeInfoClass = Class.forName(HADOOP_WRITABLE_TYPEINFO_CLASS, false, TypeExtractor.class.getClassLoader());
}
catch (ClassNotFoundException e) {
throw new RuntimeException(""Could not load the TypeInformation for the class '""
+ HADOOP_WRITABLE_CLASS + ""'. You may be missing the 'flink-hadoop-compatibility' dependency."");
}
{code}
 ","Flink 1.5.6 standalone, Flink 1.7.2 standalone, 

Hadoop 2.9.1 standalone",aljoscha,andyhoang,morvenhuang,yuchuanchen,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12725,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 13 12:25:47 UTC 2019,,,,,,,,,,"0|z01ods:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/19 09:05;morvenhuang;Discussion: [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Is-copying-flink-hadoop-compatibility-jar-to-FLINK-HOME-lib-the-only-way-to-make-it-work-td27181.html];;;","13/Jun/19 12:25;aljoscha;Merged on master in 79379909c7af42ac201b1b47033e9f3de4ec411b;;;",,,,,,,,,,,,,,,,,,,,,,
The example in /docs/ops/deployment/yarn_setup.md should be updated due to the change FLINK-2021,FLINK-12132,13226866,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,biggeng,biggeng,biggeng,09/Apr/19 06:56,13/Apr/21 20:40,13/Jul/23 08:05,16/Apr/19 10:17,,,,,,,,,1.8.1,1.9.0,,,Deployment / YARN,Documentation,,0,pull-request-available,,,"In  FLINK-2021, the example has been changed to ParameterTool, and the parameters of WordCount example should use prefix of ""--input"" and ""--output"" . However, the document of /docs/ops/deployment/yarn_setup.md (https://github.com/apache/flink/blob/master/docs/ops/deployment/yarn_setup.md) has not been changed when describing the example of WordCount on YARN.",,aljoscha,biggeng,fhueske,,,,,,,,,,,,,,,,"biggeng commented on pull request #8126: Fix of FLINK-12132
URL: https://github.com/apache/flink/pull/8126
 
 
   Update the doc of yarn setup
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 07:18;githubbot;600","biggeng commented on pull request #8126: FLINK-12132
URL: https://github.com/apache/flink/pull/8126
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 07:46;githubbot;600","biggeng commented on pull request #8129: FLINK-12132
URL: https://github.com/apache/flink/pull/8129
 
 
   The example in yarn setup should specify a prefix for input and output
   files, due to the change in FLINK-2021
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 08:49;githubbot;600","asfgit commented on pull request #8129: [FLINK-12132]The example in yarn setup should specify a prefix for input and output files
URL: https://github.com/apache/flink/pull/8129
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/19 09:54;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 16 10:17:31 UTC 2019,,,,,,,,,,"0|z01l34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/19 07:22;biggeng;I have committed a pull request(https://github.com/apache/flink/pull/8129) to address this issue.

Thanks.;;;","16/Apr/19 03:07;biggeng;Thanks for assigning this issue to me. I'm working on this.;;;","16/Apr/19 10:17;fhueske;Fixed for 1.9.0 with b4e271bb9101b337b23b192fcbd4f2070d78ef15
Fixed for 1.8.1 with a1b574cc67b7f417c1ab6b698b05cb6fd1ea51c0;;;",,,,,,,,,,,,,,,,,,,,,
There is a typo on the website,FLINK-12128,13226831,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kennethlnnn,kennethlnnn,kennethlnnn,09/Apr/19 01:38,09/Apr/19 12:55,13/Jul/23 08:05,09/Apr/19 12:55,,,,,,,,,,,,,Project Website,,,0,pull-request-available,,,"[https://flink.apache.org/roadmap.html]

""Various of these enhancements can be taken _*form*_ the contributed code from the [Blink fork|https://github.com/apache/flink/tree/blink].""

I think this sentence has a typo, should change the *form* to _from_

 ",,aljoscha,fhueske,kennethlnnn,tianchen92,,,,,,,,,,,,,,,"asfgit commented on pull request #195: [FLINK-12128][website]Fix a typo in roadmap.md
URL: https://github.com/apache/flink-web/pull/195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 12:53;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 09 12:55:56 UTC 2019,,,,,,,,,,"0|z01kvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/19 07:46;fhueske;Thanks for reporting this issue!
Would you like to provide a fix?;;;","09/Apr/19 10:32;tianchen92;[~fhueske] [~kennethlnnn] If nobody is working on this, I would like to provide a quick fix.;;;","09/Apr/19 10:32;kennethlnnn;[~fhueske]  Thank you. I've fixed it here  [https://github.com/apache/flink-web/pull/195];;;","09/Apr/19 12:55;fhueske;Fixed with 9e789e1ce158d6de7c96b70e8356ad8b58d4e4a9;;;",,,,,,,,,,,,,,,,,,,,
AbstractTaskManagerProcessFailureRecoveryTest process output logging does not work properly,FLINK-12112,13226135,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Apr/19 14:03,05/Apr/19 08:29,13/Jul/23 08:05,05/Apr/19 08:29,1.8.0,1.9.0,,,,,,,1.8.1,1.9.0,,,Runtime / Coordination,Tests,,0,pull-request-available,,,"The {{AbstractTaskManagerProcessFailureRecoveryTest}} starts multiple taskmanagers and prints their output if the test fails.

However, due to a recent refactoring the logs of the first TM is printed multiple times instead, and the code is prone to NullPointerExceptions.",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8113: [FLINK-12112][tests] Properly print process output
URL: https://github.com/apache/flink/pull/8113
 
 
   ## What is the purpose of the change
   
   Fixes an issue in the `AbstractTaskManagerProcessFailureRecoveryTest` where the test, during the process output dumping,
   a) prints the logs of the first TM 3 times
   b) is very prone to NullPointerExceptions.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/19 14:07;githubbot;600","zentol commented on pull request #8113: [FLINK-12112][tests] Properly print process output
URL: https://github.com/apache/flink/pull/8113
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Apr/19 08:28;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 08:29:18 UTC 2019,,,,,,,,,,"0|z01gls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/19 08:29;chesnay;master: bf62427cc13cdb427172d22d427d3563694fcbc4
1.8: 46934b39cbe2de7d608a901d4c8caec5a84ec59f ;;;",,,,,,,,,,,,,,,,,,,,,,,
TaskManagerProcessFailureBatchRecoveryITCase fails due to removed Slot,FLINK-12111,13226133,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,04/Apr/19 13:51,15/May/19 11:50,13/Jul/23 08:05,15/May/19 11:50,1.8.1,,,,,,,,1.9.0,,,,Runtime / Coordination,,,0,pull-request-available,test-stability,,"https://travis-ci.org/apache/flink/jobs/515636826

{code}
org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: 4f32093d9c7554c3de832d20f0a06eb5)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:268)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:483)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:471)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:446)
	at org.apache.flink.client.RemoteExecutor.executePlanWithJars(RemoteExecutor.java:210)
	at org.apache.flink.client.RemoteExecutor.executePlan(RemoteExecutor.java:187)
	at org.apache.flink.api.java.RemoteEnvironment.execute(RemoteEnvironment.java:173)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:817)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:413)
	at org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:115)
	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest$1.run(AbstractTaskManagerProcessFailureRecoveryTest.java:143)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:265)
	... 10 more
Caused by: org.apache.flink.util.FlinkException: The assigned slot 786ec47f893240315bb01291aab680ec_1 was removed.
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlot(SlotManager.java:893)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlots(SlotManager.java:863)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.internalUnregisterTaskManager(SlotManager.java:1058)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.unregisterTaskManager(SlotManager.java:385)
	at org.apache.flink.runtime.resourcemanager.ResourceManager.closeTaskManagerConnection(ResourceManager.java:847)
	at org.apache.flink.runtime.resourcemanager.ResourceManager$TaskManagerHeartbeatListener$1.run(ResourceManager.java:1161)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:392)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:185)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: 4f32093d9c7554c3de832d20f0a06eb5)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:268)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:483)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:471)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:446)
	at org.apache.flink.client.RemoteExecutor.executePlanWithJars(RemoteExecutor.java:210)
	at org.apache.flink.client.RemoteExecutor.executePlan(RemoteExecutor.java:187)
	at org.apache.flink.api.java.RemoteEnvironment.execute(RemoteEnvironment.java:173)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:817)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:413)
	at org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:115)
	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest$1.run(AbstractTaskManagerProcessFailureRecoveryTest.java:143)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:265)
	... 10 more
Caused by: org.apache.flink.util.FlinkException: The assigned slot 786ec47f893240315bb01291aab680ec_1 was removed.
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlot(SlotManager.java:893)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlots(SlotManager.java:863)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.internalUnregisterTaskManager(SlotManager.java:1058)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.unregisterTaskManager(SlotManager.java:385)
	at org.apache.flink.runtime.resourcemanager.ResourceManager.closeTaskManagerConnection(ResourceManager.java:847)
	at org.apache.flink.runtime.resourcemanager.ResourceManager$TaskManagerHeartbeatListener$1.run(ResourceManager.java:1161)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:392)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:185)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
java.lang.AssertionError: The program encountered a ProgramInvocationException : Job failed. (JobID: 4f32093d9c7554c3de832d20f0a06eb5)
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:190)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,trohrmann,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8412: [FLINK-12111][tests] Harden AbstractTaskManagerProcessFailureRecoveryTest
URL: https://github.com/apache/flink/pull/8412
 
 
   ## What is the purpose of the change
   
   Assortment of changes to improve/harden the `AbstractTaskManagerProcessFailureRecoveryTest`
   
   ## Brief change log
   
   * removed unused field
   * no longer sets `taskManagerProcess1` to null so that the process output is printed on failure
   * wait until destroyed process has actually shut down
   Prevents theoretical scenarios where the job can finish because the destroy() command takes a while to take effect.
   * reduce number of initial TMs to 1,
   The batch test could still succeed (if ExecutionMode == BATCH) even if the new TM was never used.
   Reduce the number of initial TMs to 1 so that once that TM crashes all tasks MUST be moved to the new TM.
   Doubled number of slots to compensate the loss of a TM.
   * allow 2 restarts
   For some reason this test could fail multiple times, instead of just once.
   
   ## Verifying this change
   
   The issue with the BATCH execution mode could be reproduced easily (just skip the start of the third TM), and the change should fix this in an obvious way.
   
   The restart fix is basically a shot in the dark + band-aid; ideally we would find the underlying cause.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/19 12:13;githubbot;600","zentol commented on pull request #8412: [FLINK-12111][tests] Harden AbstractTaskManagerProcessFailureRecoveryTest
URL: https://github.com/apache/flink/pull/8412
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/May/19 11:49;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 15 11:50:09 UTC 2019,,,,,,,,,,"0|z01glc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/19 14:04;chesnay;Unfortunately the process output could not be printed due to FLINK-12112.;;;","02/May/19 11:59;trohrmann;Might be another instance: https://api.travis-ci.org/v3/job/526781858/log.txt. However in this instance, the heartbeat to the TaskManager timed out.;;;","08/May/19 09:25;chesnay;Another instance with a heartbeat timeout: https://travis-ci.org/apache/flink/jobs/529353235;;;","15/May/19 11:50;chesnay;master:
294915e3e057886fef39528c5e1e72582ea1f4b8
c10e18c0eb497588f0f2e27590a6f6c6a9d23563;;;",,,,,,,,,,,,,,,,,,,,
Flink Kafka fails with Incompatible KafkaProducer version / NoSuchFieldException sequenceNumbers,FLINK-12104,13225862,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,vicTTim,vicTTim,03/Apr/19 16:59,05/Apr/19 21:04,13/Jul/23 08:05,05/Apr/19 21:04,1.7.2,,,,,,,,,,,,Connectors / Kafka,,,0,,,,"FlinkKafkaProducer (in flink-connector-kafka-0.11) tries to access a field named `sequenceNumbers` from the KafkaProducer's TransactionManager.  You can find this line on the [master branch here|[https://github.com/apache/flink/blob/d6be68670e661091d94a3c65a2704d52fc0e827c/flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/FlinkKafkaProducer.java#L197].]

 
{code:java}
Object transactionManager = getValue(kafkaProducer, ""transactionManager"");
...
Object sequenceNumbers = getValue(transactionManager, ""sequenceNumbers"");
{code}
 

However, the Kafka TransactionManager no longer has a ""sequenceNumbers"" field.  This was changed back on 9/14/2017 (KAFKA-5494) in an effort to support multiple inflight requests while still guaranteeing idempotence.  See [commit diff here|[https://github.com/apache/kafka/commit/5d2422258cb975a137a42a4e08f03573c49a387e#diff-f4ef1afd8792cd2a2e9069cd7ddea630].]

Subsequently when Flink tries to ""recoverAndCommit"" (see FlinkKafkaProducer011) it fails with a ""NoSuchFieldException: sequenceNumbers"", followed by a ""Incompatible KafkaProducer version"".

Given that the KafkaProducer used is so old (this change was made almost two years ago) are there any plans of upgrading?   Or - are there some known compatibility issues that prevent Flink/Kafka connector from doing so?

 ",,elevy,vicTTim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 21:04:28 UTC 2019,,,,,,,,,,"0|z01f74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/19 18:03;elevy;The {{flink-connector-kafka-0.11}} depends on the Kafka 0.11.0.2 client, which does have that field. The field was only removed in 1.0.0. Are you overriding the Kafka client dependency? If so, that is your problem.


 If you want to use a newer Kafka client, use the universal Kafka connection ({{flink-connector-kafka_2.11}}), which tracks the latest version of the Kafka client.;;;","03/Apr/19 20:22;vicTTim;I am sure I am, not intentionally - but due to other libs in the dependency chain.   I will try with flink-connector-kafka_2.11

Side note:  I'm guessing we use reflection (Class.getDeclaredField) to access these because they are private in the Kafka codebase, and accessing them through the public API was not feasible given what FlinkKafkaProducer was trying to do?   Asking because in that sense the build would have failed to compile.;;;","05/Apr/19 21:04;vicTTim;I can confirm that this fix works.;;;",,,,,,,,,,,,,,,,,,,,,
Race condition when concurrently running uploaded jars via REST,FLINK-12101,13225821,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xleesf,mxm,mxm,03/Apr/19 13:47,06/Jun/19 14:17,13/Jul/23 08:05,06/Jun/19 14:17,1.6.4,1.7.2,,,,,,,1.9.0,,,,Command Line Client,,,0,pull-request-available,,,"Flink enables to upload and run Jars via REST. When multiple uploaded jars are invoked interactively to generate the JobGraph, the static initialization of the {{ContextEnvironment}}, when calls are interleaved, will override each other and produce a local execution of the jar. The local execution uses an incorrect class loader and throws an exception like this:

{noformat}
2019-04-02 14:25:05,549 ERROR <pipeline class>   - Failed to create job graph
java.lang.RuntimeException: Pipeline execution failed
    at org.apache.beam.runners.flink.FlinkRunner.run(FlinkRunner.java:117)
    at org.apache.beam.sdk.Pipeline.run(Pipeline.java:313)
    at org.apache.beam.sdk.Pipeline.run(Pipeline.java:299)
    at <pipeline class run>
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
    at org.apache.flink.client.program.OptimizerPlanEnvironment.getOptimizedPlan(OptimizerPlanEnvironment.java:83)
    at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:78)
    at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:120)
    at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:117)
    at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$getJobGraphAsync$7(JarRunHandler.java:151)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
    at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:647)
    at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:123)
    at org.apache.beam.runners.flink.FlinkPipelineExecutionEnvironment.executePipeline(FlinkPipelineExecutionEnvironment.java:125)
    at org.apache.beam.runners.flink.FlinkRunner.run(FlinkRunner.java:114)
    ... 18 more
Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Could not instantiate outputs in order.
    at org.apache.flink.streaming.api.graph.StreamConfig.getOutEdgesInOrder(StreamConfig.java:398)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.createStreamRecordWriters(StreamTask.java:1164)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.(StreamTask.java:212)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.(StreamTask.java:190)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.(SourceStreamTask.java:51)
    at org.apache.flink.streaming.runtime.tasks.StoppableSourceStreamTask.(StoppableSourceStreamTask.java:39)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.flink.runtime.taskmanager.Task.loadAndInstantiateInvokable(Task.java:1398)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:682)
    ... 1 more
Caused by: java.lang.ClassNotFoundException: org.apache.beam.runners.flink.translation.wrappers.streaming.WorkItemKeySelector
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$ChildFirstClassLoader.loadClass(FlinkUserCodeClassLoaders.java:129)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
    at java.util.ArrayList.readObject(ArrayList.java:797)
    at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1170)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2178)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:566)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:552)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:540)
    at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:501)
    at org.apache.flink.streaming.api.graph.StreamConfig.getOutEdgesInOrder(StreamConfig.java:395)
    ... 12 more
{noformat}",,aljoscha,mxm,tison,trohrmann,xleesf,,,,,,,,,,,,,,"leesf commented on pull request #8543: [FLINK-12101] Race condition when concurrently running uploaded jars via REST
URL: https://github.com/apache/flink/pull/8543
 
 
   ## What is the purpose of the change
   
   Fix race condition when concurrently running uploaded jars via REST.
   
   
   ## Brief change log
   
   * Introduce a _ThreadLocal_ field to store _ExecutionEnvironmentFactory_ 
   * When _contextEnvironmentFactory_ is null, use _ThreadLocal_ to get Factory.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
   cc @tillrohrmann 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/May/19 02:48;githubbot;600","tillrohrmann commented on pull request #8543: [FLINK-12101] Race condition when concurrently running uploaded jars via REST
URL: https://github.com/apache/flink/pull/8543
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jun/19 14:17;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 06 14:17:54 UTC 2019,,,,,,,,,,"0|z01ey0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/19 12:28;trohrmann;Thanks for reporting this issue [~mxm]. Before starting the implementation work [~xleesf], I would like to first understand what the problem is. From the stack trace it looks as if the {{JarRunHandler}} actually starts a {{MiniCluster}} to execute the program. This is clearly wrong and should not happen. It looks as if the {{FlinkPipelineExecutionEnvironment}} is responsible for this. Thus, I'm actually wondering whether the {{FlinkRunner}} might be resetting the {{ExecutionEnvironmentFactory}} which was set by the {{JarRunHandler}} to instantiate an {{OptimizerPlanEnvironment}}.;;;","08/Apr/19 12:47;trohrmann;Ok, I think I understand the problem now. The following sequence produces the problem:

1. Job1 enter getOptimizedPlan[thread-1]: setAsContext(OptimizerPlanEnvironment) --> ctxEnvFactory = Optimizer
2. Job2 enter getOptimizedPlan[thread-2]: setAsContext(OptimizerPlanEnvironment) --> ctxEnvFactory = Optimizer
3. Job1 finish getOptimizedPlan[thread-1]: unsetContext --> ctxEnvFactory = null
4. Job2 enter invokeInteractiveModeForExecution [thread-2]: start execution with ctxEnvFactory = null --> Instantiate a {{LocalEnvironment}}

The problem seems to be the static field {{ExecutionEnvironment#contextEnvironmentFactory}} which is shared by all jobs. A quick fix could be to use thread local variables to set the {{ContextEnvironmentFactory}}. In order to not break existing setups we could only respect the thread local variable if {{ExecutionEnvironment#contextEnvironmentFactory}} is {{null}}.
;;;","08/Apr/19 13:16;trohrmann;The problem is more related to Flink's {{flink-clients}} module because it can also happen if you try to submit multiple jobs from the same process when using multiple threads.;;;","08/Apr/19 14:25;mxm;Thanks for taking the time to look into this [~till.rohrmann]. Your analysis is spot-on. The problem is that the static context environment variable can be accessed by multiple threads. The ThreadLocal static variable could be a simple and backwards-compatible fix.;;;","24/May/19 09:58;trohrmann;[~xleesf] did you have time to work on a fix for this issue?;;;","26/May/19 01:46;xleesf;[~till.rohrmann] Sorry for lately participating in, will provide a PR soon.;;;","06/Jun/19 14:17;trohrmann;Fixed via
ad801b9be6e6485c57e426d6fde0c499e3f9be1d
ac84e0c84dee39da7339722aad5ed2dcf26e2d27;;;",,,,,,,,,,,,,,,,,
Not able to submit jobs on YARN when there's a firewall,FLINK-12075,13225190,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,Xeli,Xeli,01/Apr/19 07:28,02/Apr/19 17:22,13/Jul/23 08:05,02/Apr/19 17:22,1.7.2,1.8.0,,,,,,,1.7.3,1.8.0,,,Deployment / YARN,,,0,pull-request-available,,,"If there is a firewall around the YARN cluster and the machine, submitting flink job it is unpractical because new flink clusters start up with random ports for REST communication.

 

FLINK-5758 should've fixed this. But it seems FLINK-11081 either undid the changes or did not implement this. The relevant code is changed in FLINK-11081 ([https://github.com/apache/flink/commit/730eed71ef3f718d61f85d5e94b1060844ca56db#diff-487838863ab693af7008f04cb3359be3R102])

 

 ",,aljoscha,liyu,trohrmann,Xeli,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #8096: [FLINK-12075][yarn] Set RestOptions.BIND_PORT only to 0 if not specified
URL: https://github.com/apache/flink/pull/8096
 
 
   ## What is the purpose of the change
   
   This PR changes the YarnEntrypointUtils#loadConfiguration so that it only
   sets RestOptions.BIND_PORT to 0 if it has not been specified. This allows to
   explicitly set a port range for Yarn applications which are running behind a
   firewall, for example.
   
   Moreover, this PR updates the flink-conf.yaml to contain the new rest options and comments
   out the `rest.port` per default so that it will be set to `0` in the Yarn case.
   
   ## Verifying this change
   
   * Added `YarnEntrypointUtilsTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/19 20:19;githubbot;600","asfgit commented on pull request #8096: [FLINK-12075][yarn] Set RestOptions.BIND_PORT only to 0 if not specified
URL: https://github.com/apache/flink/pull/8096
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/19 17:13;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-5758,,,,,FLINK-11081,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 02 17:22:53 UTC 2019,,,,,,,,,,"0|z01b2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/19 14:52;aljoscha;I'd say this is a regression and a blocker if it keeps people from using Flink on a setup where they previously could.;;;","01/Apr/19 14:53;aljoscha;Relevant ML thread: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Submitting-job-to-Flink-on-yarn-timesout-on-flip-6-1-5-x-td26199.html#a26383;;;","01/Apr/19 19:24;trohrmann;I agree that we should fix this problem for the {{1.8.0}} release. I suggest the following as a solution: 

1. only set the Yarn bind-port to {{0}} if it has not been specified ({{rest.bind-port}} and all of its fallback keys have not been set).
2. Remove the explicit setting of {{rest.port}} from the default config

That way, it should be possible to define a port range for {{rest.bind-port}} and keep the behaviour that you only need to specify a single option {{rest.port}} if you want to bind and connect to a single port which existing setups might depend on. Moreover, with 2. the Yarn case should work out of the box with random port binding.;;;","02/Apr/19 09:21;Xeli;I've just tested the PR of [~till.rohrmann] on our cluster and this works perfectly for us.

 

Thank you [~aljoscha] and [~till.rohrmann] for fixing and giving this priority for 1.8.0!;;;","02/Apr/19 14:42;aljoscha;Thanks for testing this on short notice, [~Xeli]! 😃;;;","02/Apr/19 17:22;trohrmann;Fixed via
master:
03499035cc3e0c3e5a5307cadd9c8159f668047e
065306d85e693759af36aaccc91df97c90965da8
5210948a2666084e0834cd47c695e696722f426f

1.8.0:
a1c568ba62b70f43ca07eeed754d34266fd94bd8
1b502f77fb8eb79c26d61dbd980013f30b65b0aa
e12b64c64eaca89cdede48138a3d5fee8eac336b

1.7.3:
b75463a2456f40df51b0cc5eb2ff712642b2629e
440f8b7b5f6ea334f466bbfbf9569e9a67391d28
;;;",,,,,,,,,,,,,,,,,,
Fix typo in `flinkDev/building.md`,FLINK-12074,13225151,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunjincheng121,sunjincheng121,sunjincheng121,01/Apr/19 03:02,01/Apr/19 03:27,13/Jul/23 08:05,01/Apr/19 03:27,1.9.0,,,,,,,,,,,,Documentation,,,0,,,,"Liquid Exception: Liquid syntax error (line 120): Unknown tag 'hightlight' in flinkDev/building.md

`hightlight` -> `highlight`
",,sunjincheng121,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 01 03:27:34 UTC 2019,,,,,,,,,,"0|z01au8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/19 03:27;sunjincheng121;Already hotfix by aafc881.;;;",,,,,,,,,,,,,,,,,,,,,,,
RocksDBKeyedStateBackend snapshot uses incorrect key serializer if reconfigure happens during restore,FLINK-12064,13224784,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,liyu,liyu,29/Mar/19 10:23,01/Apr/19 04:28,13/Jul/23 08:05,01/Apr/19 04:28,,,,,,,,,1.8.0,,,,,,,0,pull-request-available,,,"As titled, in current {{RocksDBKeyedStateBackend}} we use {{keySerializer}} rather than {{keySerializerProvider.currentSchemaSerializer()}}, which is incorrect. The issue is not revealed in existing UT since current cases didn't check snapshot after state schema migration.

This is a regression issue caused by the FLINK-10043 refactoring work.",,liyu,tzulitai,,,,,,,,,,,,,,,,,"carp84 commented on pull request #8076: [FLINK-12064] [core, State Backends] RocksDBKeyedStateBackend uses incorrect key serializer if reconfigure happens during restore
URL: https://github.com/apache/flink/pull/8076
 
 
   ## What is the purpose of the change
   
   This PR fixes the issue of using incorrect key serializer if reconfigured during restore in `RocksDBKeyedStateBackendBuilder`, which is a regression caused by FLINK-10043. We also reenforce the UT cases to cover the issue scenario.
   
   
   ## Brief change log
   
   Remove `keySerializer` from `AbstractKeyedStateBackendBuilder` and use `keySerializerProvider.currentSchemaSerializer()` instead in `RocksDBKeyedStateBackendBuilder`. For the test case part, perform an additional snapshot after state migration.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   * Reenforced `StateBackendMigrationTestBase`, adding an additional snapshot after state migration to verify the snapshot part.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (**yes**)
        * This change assures to use correct serializer in snapshot
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes**)
        * This change prevents possible corruption of snapshot data
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Mar/19 10:36;githubbot;600","asfgit commented on pull request #8076: [FLINK-12064] [core, State Backends] RocksDBKeyedStateBackend snapshot uses incorrect key serializer if reconfigure happens during restore
URL: https://github.com/apache/flink/pull/8076
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/19 04:24;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-10043,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 01 04:28:14 UTC 2019,,,,,,,,,,"0|z018kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/19 11:58;liyu;[~tzulitai] [~srichter] [~aljoscha] FYI.;;;","29/Mar/19 16:20;tzulitai;Just to clarify:
the problem is that the state backend uses the correct key serializer for state access, but the wrong one is snapshotted in checkpoints, is that correct?
Otherwise, the title implies that it is using the wrong key serializer for runtime state access.;;;","30/Mar/19 09:28;liyu;bq. but the wrong one is snapshotted in checkpoints, is that correct?
Correct, have updated the JIRA title to reflect this.;;;","01/Apr/19 04:28;tzulitai;Thanks for the fix [~carp84].

Merged with the following commits:
master (1.9.0) - c5e4accffd47eca269b25970127048258e33cb45
release-1.8 (1.8.0) - 9795457a0228e055b82049977ac84d5bc70e6a23;;;",,,,,,,,,,,,,,,,,,,,
TaskExecutorTest.testFilterOutDuplicateJobMasterRegistrations() does not wait for TaskExecutor to be started,FLINK-12051,13224574,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,kkl0u,kkl0u,28/Mar/19 12:12,05/Apr/19 11:56,13/Jul/23 08:05,29/Mar/19 10:59,1.8.0,,,,,,,,1.8.0,1.9.0,,,Tests,,,0,pull-request-available,test-stability,,"The test failed locally with:

{code}

Wanted but not invoked:
 jobLeaderService.start(
 <any string>,
 <any org.apache.flink.runtime.rpc.RpcService>,
 <any org.apache.flink.runtime.highavailability.HighAvailabilityServices>,
 <Capturing argument>
 );
 -> at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testFilterOutDuplicateJobMasterRegistrations(TaskExecutorTest.java:1171)
 Actually, there were zero interactions with this mock.

Wanted but not invoked:
 jobLeaderService.start(
 <any string>,
 <any org.apache.flink.runtime.rpc.RpcService>,
 <any org.apache.flink.runtime.highavailability.HighAvailabilityServices>,
 <Capturing argument>
 );
 -> at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testFilterOutDuplicateJobMasterRegistrations(TaskExecutorTest.java:1171)
 Actually, there were zero interactions with this mock.

at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testFilterOutDuplicateJobMasterRegistrations(TaskExecutorTest.java:1171)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runners.Suite.runChild(Suite.java:128)
 at org.junit.runners.Suite.runChild(Suite.java:27)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
 at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
 at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
 at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)

{code}",,kkl0u,,,,,,,,,,,,,,,,,,"zentol commented on pull request #8071: [FLINK-12051][runtime][tests] Wait for TaskExecutor to be started
URL: https://github.com/apache/flink/pull/8071
 
 
   ## What is the purpose of the change
   
   Fixes an issue in the `TaskExecutorTest` where we did not wait for the `TaskExecutor` to finishing starting before verifying behavior. Since the introduction of `TaskExecutor#onStart` `TaskExecutor#start` only kicks off the asynchronous startup of the `TaskExecutor`, whereas previously this asynchronous was done within `start` itself.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/19 13:04;githubbot;600","zentol commented on pull request #8071: [FLINK-12051][runtime][tests] Wait for TaskExecutor to be started
URL: https://github.com/apache/flink/pull/8071
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Mar/19 10:57;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 29 10:59:14 UTC 2019,,,,,,,,,,"0|z017b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/19 10:59;chesnay;master: 600be0097661fc3c45ef50d40f8e831421ab1d5c
1.8: d68c45e05b6a9ec1cf3fc49e1d82a863795f7341 ;;;",,,,,,,,,,,,,,,,,,,,,,,
RocksDBStateBackend mistakenly uses default filesystem,FLINK-12042,13224485,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Paul Lin,Paul Lin,Paul Lin,28/Mar/19 05:31,27/Apr/19 09:43,13/Jul/23 08:05,27/Apr/19 09:43,1.6.4,1.7.2,1.8.0,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / State Backends,,,0,pull-request-available,,,"The scheme of SnapshotDirectory is not specified when RocksDBStateBackend is performing incremental checkpoints with local recovery disabled, and this will lead to IllegalStateException when the async task tries to check the file existence using the default system if the default filesystem scheme is specified.",,aljoscha,Paul Lin,trohrmann,,,,,,,,,,,,,,,,"link3280 commented on pull request #8068: [FLINK-12042] Fix RocksDBStateBackend's mistaken usage of default filesystem
URL: https://github.com/apache/flink/pull/8068
 
 
   
   ## What is the purpose of the change
   
   The scheme of SnapshotDirectory is not specified when RocksDBStateBackend is performing incremental checkpoints with local recovery disabled, and this will lead to IllegalStateException (for 1.6.4) when the async task tries to check the file existence using the default system if the default filesystem scheme is specified.
   
   ## Brief change log
   
   - *Use URI instead of the absolute path of local path to initialize SnapshotDirectory*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/19 06:15;githubbot;600","tillrohrmann commented on pull request #8068: [FLINK-12042][StateBackends] Fix RocksDBStateBackend's mistaken usage of default filesystem
URL: https://github.com/apache/flink/pull/8068
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Apr/19 09:42;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 27 09:43:44 UTC 2019,,,,,,,,,,"0|z016rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/19 12:24;Paul Lin;[~aljoscha] Please take a look at the PR. Should be a quick one. Thanks!;;;","27/Apr/19 09:43;trohrmann;Fixed via

1.9.0: 
fa3f761af050791158e175f034575d7d16398d78
aa9a4520add35feb8d29551ebd993e0ac6afd1e2

1.8.1:
d0013c735308244037757ef6710a555843343156
a63e03f69a6950502f70b49ea088a0632778d70b

1.7.3:
e6e2677ba25050380fa6f73b8730261afdac43f5
df89c1c1b15a125e9072b47e32da21588a162947;;;",,,,,,,,,,,,,,,,,,,,,,
YARNITCase stalls on travis,FLINK-12038,13224311,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tiemsn,chesnay,chesnay,27/Mar/19 13:45,30/Jul/19 11:52,13/Jul/23 08:05,30/Jul/19 11:52,1.9.0,,,,,,,,1.9.0,,,,Deployment / YARN,Tests,,0,pull-request-available,test-stability,,https://travis-ci.org/apache/flink/jobs/511932978,,azagrebin,tiemsn,trohrmann,,,,,,,,,,,,,,,,"shuai-xu commented on pull request #9175: [FLINK-12038] [test] fix YARNITCase random fail
URL: https://github.com/apache/flink/pull/9175
 
 
   
   ## What is the purpose of the change
   
   This pr fix that the YARNITCase may random fail due to [YARN-2853](https://issues.apache.org/jira/browse/YARN-2853). 
   In fact, the killApplication when case finished is not needed as in tearDown it will stop the YARN mini cluster.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 04:42;githubbot;600","tillrohrmann commented on pull request #9175: [FLINK-12038] [test] fix YARNITCase random fail
URL: https://github.com/apache/flink/pull/9175
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 11:49;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 30 11:52:25 UTC 2019,,,,,,,,,,"0|z015p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/19 11:01;tiemsn;[~till.rohrmann], I think this is caused by https://issues.apache.org/jira/browse/YARN-2853 of yarn 2.4. In the end of the case, it try to kill the application while YarnResourceManager will unregisterAM to yarn. I think we could fix it by adding a check for the application state before killing it, what do you think?;;;","18/Jul/19 15:17;azagrebin;I tried to loop the test on my travis. It is re-produceable: [https://travis-ci.org/azagrebin/flink/builds/557947981]

[~tiemsn] Have you tried this approach with waiting of the application state and looping it on Travis?
It does not look like this test actually tries to unregisterAM (found it only in YarnResourceManager#internalDeregisterApplication).
At least the debugger does not stop there.
I guess it was the original reason to kill app because it is blocking instead of shutting down the cluster asynchronously over REST and wait somehow for the shutdown which might be an option.;;;","19/Jul/19 03:30;tiemsn;This failure can be easily re-produced in my local machine. I enabled the logs of YARN, and found the reason. You can find the log of unregisterAM in jobmanager.log. When the job is finished, it will try to unregisterAM to YARN. In fact, it is not necessary to call killApplication, as the whole YARN mini cluster will be closed in the tearDown of test case. 

The bellowing is part of logs of job master:

2019-07-16 18:20:34,376 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source (1/2) (e13567c7f2d7a389c74f4583a67e34e8) switched from SCHEDULED to DEPLOYING.
2019-07-16 18:20:34,376 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source (1/2) (attempt #0) to container_1563272405568_0001_01_000002 @ e011239174096.et15sqa (dataPort=42072)
2019-07-16 18:20:34,404 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source (2/2) (fc3d9d65a75eabaf00d7d9372d2b9884) switched from SCHEDULED to DEPLOYING.
2019-07-16 18:20:34,405 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source (2/2) (attempt #0) to container_1563272405568_0001_01_000003 @ e011239174096.et15sqa (dataPort=41793)
2019-07-16 18:20:34,405 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Unnamed (1/2) (65db57ac7166e0a96a3c5318bb262fb0) switched from SCHEDULED to DEPLOYING.
2019-07-16 18:20:34,414 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Sink: Unnamed (1/2) (attempt #0) to container_1563272405568_0001_01_000003 @ e011239174096.et15sqa (dataPort=41793)
2019-07-16 18:20:34,447 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Unnamed (2/2) (22c3e0c0fd37dd00e75fcf855e2a6ca4) switched from SCHEDULED to DEPLOYING.
2019-07-16 18:20:34,447 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Sink: Unnamed (2/2) (attempt #0) to container_1563272405568_0001_01_000002 @ e011239174096.et15sqa (dataPort=42072)
2019-07-16 18:20:34,897 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source (1/2) (e13567c7f2d7a389c74f4583a67e34e8) switched from DEPLOYING to RUNNING.
2019-07-16 18:20:34,949 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source (2/2) (fc3d9d65a75eabaf00d7d9372d2b9884) switched from DEPLOYING to RUNNING.
2019-07-16 18:20:35,056 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Unnamed (1/2) (65db57ac7166e0a96a3c5318bb262fb0) switched from DEPLOYING to RUNNING.
2019-07-16 18:20:35,067 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Unnamed (2/2) (22c3e0c0fd37dd00e75fcf855e2a6ca4) switched from DEPLOYING to RUNNING.
2019-07-16 18:20:35,450 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source (2/2) (fc3d9d65a75eabaf00d7d9372d2b9884) switched from RUNNING to FINISHED.
2019-07-16 18:20:35,480 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source (1/2) (e13567c7f2d7a389c74f4583a67e34e8) switched from RUNNING to FINISHED.
2019-07-16 18:20:35,494 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Unnamed (2/2) (22c3e0c0fd37dd00e75fcf855e2a6ca4) switched from RUNNING to FINISHED.
2019-07-16 18:20:35,508 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Sink: Unnamed (1/2) (65db57ac7166e0a96a3c5318bb262fb0) switched from RUNNING to FINISHED.
2019-07-16 18:20:35,513 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job Flink Streaming Job (2f9313ea4fd33bef68111ed380a2ae1b) switched from state RUNNING to FINISHED.
2019-07-16 18:20:35,513 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Stopping checkpoint coordinator for job 2f9313ea4fd33bef68111ed380a2ae1b.
2019-07-16 18:20:35,513 INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore - Shutting down
2019-07-16 18:20:35,564 INFO org.apache.flink.runtime.dispatcher.MiniDispatcher - Job 2f9313ea4fd33bef68111ed380a2ae1b reached globally terminal state FINISHED.
2019-07-16 18:20:35,573 INFO org.apache.flink.runtime.jobmaster.JobMaster - Stopping the JobMaster for job Flink Streaming Job(2f9313ea4fd33bef68111ed380a2ae1b).
2019-07-16 18:20:35,664 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Suspending SlotPool.
2019-07-16 18:20:35,666 INFO org.apache.flink.runtime.jobmaster.JobMaster - Close ResourceManager connection 165d22977dc31b3b410489789fdc1050: JobManager is shutting down..
2019-07-16 18:20:35,668 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl - Stopping SlotPool.
2019-07-16 18:20:35,668 INFO org.apache.flink.yarn.YarnResourceManager - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@e011239174096.et15sqa:50088/user/jobmanager_0 for job 2f9313ea4fd33bef68111ed380a2ae1b from the resource manager.
2019-07-16 18:20:35,681 INFO org.apache.flink.runtime.jobmaster.JobManagerRunner - JobManagerRunner already shutdown.
2019-07-16 18:20:36,844 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Shutting YarnJobClusterEntrypoint down with application status SUCCEEDED. Diagnostics null.
2019-07-16 18:20:36,844 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint - Shutting down rest endpoint.
2019-07-16 18:20:36,889 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint - Removing cache directory /tmp/flink-web-0b120d31-7130-4fcb-bfa8-699abdd5b81e/flink-web-ui
2019-07-16 18:20:36,890 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint - http://e011239174096.et15sqa:38342 lost leadership
2019-07-16 18:20:36,892 INFO org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint - Shut down complete.
2019-07-16 18:20:36,895 INFO org.apache.flink.yarn.YarnResourceManager - Shut down cluster because application is in SUCCEEDED, diagnostics null.
2019-07-16 18:20:36,896 INFO org.apache.flink.yarn.YarnResourceManager - {color:#FF0000}Unregister application from the YARN Resource Manager with final status SUCCEEDED.{color}
2019-07-16 18:20:36,981 INFO org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl - Waiting for application to be successfully unregistered.;;;","19/Jul/19 14:54;azagrebin;True, I see it also in JM logs.

I looped again the test on Travis with the previously suggested fix where the test waits for yarn app FINISHED state with a timeout.
 [https://travis-ci.org/azagrebin/flink/builds/560969859]
The CI fails due to overall timeout because loop has too many iterations but the test does not fail as previously after couple of iterations. I think the fix works.

The waiting does not take long. Then there is no need to kill the app in this case, only if normal shutdown does not reach FINISHED within a timeout which would again signal that there is some problem. I think it is a cleaner approach. I would see the yarn mini cluster shutdown in the @AfterClass test method as an emergency cleanup.;;;","23/Jul/19 09:37;azagrebin;Thanks for adjusting the PR [~tiemsn]. I will review it.;;;","30/Jul/19 11:52;trohrmann;Fixed via

1.10.0: 36d9a756343b41f7fd3123dff8bd2bb5737b6e26
1.9.0: 420f6cac29bf795f524a40bca60adb46edfa6e3a;;;",,,,,,,,,,,,,,,,,,
KafkaITCase.testMultipleSourcesOnePartition is unstable: This server does not host this topic-partition,FLINK-12030,13224248,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,aljoscha,aljoscha,27/Mar/19 09:57,19/Mar/21 09:47,13/Jul/23 08:05,20/May/20 04:06,1.11.0,,,,,,,,1.11.0,,,,Connectors / Kafka,Tests,,0,pull-request-available,test-stability,,"This is a relevant part from the log:
{code}
14:11:45,305 INFO  org.apache.flink.streaming.connectors.kafka.KafkaITCase       - 
================================================================================
Test testMetricsAndEndOfStream(org.apache.flink.streaming.connectors.kafka.KafkaITCase) is running.
--------------------------------------------------------------------------------
14:11:45,310 INFO  org.apache.flink.streaming.connectors.kafka.KafkaTestBase     - 
===================================
== Writing sequence of 300 into testEndOfStream with p=1
===================================
14:11:45,311 INFO  org.apache.flink.streaming.connectors.kafka.KafkaTestBase     - Writing attempt #1
14:11:45,316 INFO  org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl  - Creating topic testEndOfStream-1
14:11:45,863 WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
14:11:45,910 WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Using AT_LEAST_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
14:11:45,921 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer  - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic testEndOfStream-1
14:11:46,006 ERROR org.apache.flink.streaming.connectors.kafka.KafkaTestBase     - Write attempt failed, trying again
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:638)
	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:79)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.writeSequence(KafkaConsumerTestBase.java:1918)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runEndOfStreamTest(KafkaConsumerTestBase.java:1537)
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testMetricsAndEndOfStream(KafkaITCase.java:136)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: This server does not host this topic-partition.
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1002)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:787)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:658)
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:443)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:318)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	... 1 more
{code}

Travis run: https://travis-ci.org/apache/flink/jobs/510953235",,becket_qin,dwysakowicz,limbo,rmetzger,trohrmann,wanglijie,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17821,,,,,,,FLINK-18444,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 09:47:33 UTC 2021,,,,,,,,,,"0|z015b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/20 06:02;zjwang;Another failure instance [https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6288/logs/83];;;","13/May/20 07:27;rmetzger;Same exception, different test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1121&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20

{code}
2020-05-12T19:15:16.8004940Z [ERROR] Failures: 
2020-05-12T19:15:16.8006368Z [ERROR]   KafkaProducerExactlyOnceITCase.testMultipleSinkOperators:55->KafkaProducerTestBase.testExactlyOnce:370 Test failed: Job execution failed.


2020-05-12T19:08:06.8513168Z 19:08:06,850 [   Time-limited test] INFO  org.apache.flink.streaming.connectors.kafka.KafkaTestBase    [] - Writing attempt #1
2020-05-12T19:08:06.8513971Z 19:08:06,850 [   Time-limited test] INFO  org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl [] - Creating topic testEndOfStream-1
2020-05-12T19:08:06.8761388Z 19:08:06,875 [   Time-limited test] WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
2020-05-12T19:08:06.8987875Z 19:08:06,896 [Source: Custom Source -> Sink: Unnamed (1/1)] WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Using AT_LEAST_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-05-12T19:08:06.8998360Z 19:08:06,899 [Source: Custom Source -> Sink: Unnamed (1/1)] INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Starting FlinkKafkaInternalProducer (1/1) to produce into default topic testEndOfStream-1
2020-05-12T19:08:07.0247487Z 19:08:07,019 [   Time-limited test] ERROR org.apache.flink.streaming.connectors.kafka.KafkaTestBase    [] - Write attempt failed, trying again
2020-05-12T19:08:07.0248442Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-12T19:08:07.0250051Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0251647Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:659) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0253275Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81) ~[flink-test-utils_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0255042Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1645) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0257769Z 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.writeSequence(KafkaConsumerTestBase.java:1994) [flink-connector-kafka-base_2.11-1.11-SNAPSHOT-tests.jar:?]
2020-05-12T19:08:07.0259661Z 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runEndOfStreamTest(KafkaConsumerTestBase.java:1517) [flink-connector-kafka-base_2.11-1.11-SNAPSHOT-tests.jar:?]
2020-05-12T19:08:07.0261292Z 	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testMetricsAndEndOfStream(KafkaITCase.java:141) [test-classes/:?]
2020-05-12T19:08:07.0262096Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
2020-05-12T19:08:07.0262851Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
2020-05-12T19:08:07.0263723Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
2020-05-12T19:08:07.0264508Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
2020-05-12T19:08:07.0265652Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]
2020-05-12T19:08:07.0267210Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]
2020-05-12T19:08:07.0268544Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]
2020-05-12T19:08:07.0269775Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]
2020-05-12T19:08:07.0271042Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) [junit-4.12.jar:4.12]
2020-05-12T19:08:07.0272388Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) [junit-4.12.jar:4.12]
2020-05-12T19:08:07.0273203Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242]
2020-05-12T19:08:07.0273823Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
2020-05-12T19:08:07.0274505Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-05-12T19:08:07.0276095Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:112) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0278249Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0279962Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:189) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0281576Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0283556Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0285401Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:497) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0287227Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:384) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0288103Z 	at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source) ~[?:?]
2020-05-12T19:08:07.0288831Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
2020-05-12T19:08:07.0289538Z 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
2020-05-12T19:08:07.0290912Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0292717Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0294302Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0295831Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0297425Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0298638Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0299842Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
2020-05-12T19:08:07.0301079Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0302318Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
2020-05-12T19:08:07.0303535Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
2020-05-12T19:08:07.0304733Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
2020-05-12T19:08:07.0305961Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0307394Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0308628Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0309767Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0310939Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0312127Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0313463Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0314815Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0316247Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0317742Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0319297Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
2020-05-12T19:08:07.0321225Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Failed to send data to Kafka: This server does not host this topic-partition.
2020-05-12T19:08:07.0322367Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_242]
2020-05-12T19:08:07.0323427Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) ~[?:1.8.0_242]
2020-05-12T19:08:07.0325246Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:144) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0327515Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0328743Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0329762Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0330991Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:300) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0332025Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:553) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0332952Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:532) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0333799Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0334610Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0335043Z 	... 1 more
2020-05-12T19:08:07.0335779Z Caused by: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Failed to send data to Kafka: This server does not host this topic-partition.
2020-05-12T19:08:07.0336481Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1224) ~[classes/:?]
2020-05-12T19:08:07.0337417Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:865) ~[classes/:?]
2020-05-12T19:08:07.0338279Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) ~[flink-core-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0339308Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0340601Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0341733Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:186) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0342826Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:160) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0343978Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0345062Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0346241Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0347602Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:138) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0348685Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0349687Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0350657Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0351648Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:300) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0352733Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:553) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0353635Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:532) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0354484Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0355355Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0355721Z 	... 1 more
2020-05-12T19:08:07.0356380Z Caused by: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: This server does not host this topic-partition.
2020-05-12T19:08:07.0357181Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1224) ~[classes/:?]
2020-05-12T19:08:07.0357769Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:977) ~[classes/:?]
2020-05-12T19:08:07.0358327Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:838) ~[classes/:?]
2020-05-12T19:08:07.0359163Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) ~[flink-core-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0360140Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0361252Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0362367Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:186) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0363468Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:160) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0364675Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0365699Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0366652Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:79) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0378098Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:138) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0379166Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:113) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0380169Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:117) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0381140Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:78) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0382124Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:300) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0383075Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:553) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0384132Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:532) ~[flink-streaming-java_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0385049Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0385853Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539) ~[flink-runtime_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-05-12T19:08:07.0386222Z 	... 1 more
2020-05-12T19:08:07.0386948Z Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
{code};;;","14/May/20 06:06;rmetzger;This time its ""Kafka011ProducerExactlyOnceITCase>KafkaProducerTestBase.testExactlyOnceRegularSink:309->KafkaProducerTestBase.testExactlyOnce:370"" https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1165&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","14/May/20 14:32;rmetzger;Another case https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1216&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","18/May/20 06:13;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1653&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8

{code}
2020-05-17T20:56:20.3972904Z 20:56:20,386 [                main] ERROR org.apache.flink.streaming.connectors.kafka.table.Kafka010TableITCase [] - 
2020-05-17T20:56:20.3974368Z --------------------------------------------------------------------------------
2020-05-17T20:56:20.3975255Z Test testKafkaSourceSink[legacy = false, topicId = 1](org.apache.flink.streaming.connectors.kafka.table.Kafka010TableITCase) failed with:
2020-05-17T20:56:20.3976273Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-17T20:56:20.3977113Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-17T20:56:20.3977826Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-05-17T20:56:20.3978648Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil$.execInsertSqlAndWaitResult(TableEnvUtil.scala:31)
2020-05-17T20:56:20.3979513Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil.execInsertSqlAndWaitResult(TableEnvUtil.scala)
2020-05-17T20:56:20.3980379Z 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.testKafkaSourceSink(KafkaTableTestBase.java:145)
2020-05-17T20:56:20.3981313Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-17T20:56:20.3981940Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-17T20:56:20.3982863Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-17T20:56:20.3983605Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-17T20:56:20.3984754Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-17T20:56:20.3985595Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-17T20:56:20.3986382Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-17T20:56:20.3987139Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-17T20:56:20.3987839Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-17T20:56:20.3988434Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-17T20:56:20.3989040Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-17T20:56:20.3989713Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-17T20:56:20.3990472Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-17T20:56:20.3991285Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T20:56:20.3992133Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T20:56:20.3992815Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T20:56:20.3993538Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T20:56:20.3994536Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T20:56:20.3995182Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T20:56:20.3995742Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-17T20:56:20.3996307Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-17T20:56:20.3996875Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-17T20:56:20.3997522Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-17T20:56:20.3998184Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-17T20:56:20.3998822Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-17T20:56:20.3999444Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-17T20:56:20.4000318Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-17T20:56:20.4001173Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-17T20:56:20.4001836Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-17T20:56:20.4002420Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-17T20:56:20.4002773Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-17T20:56:20.4003258Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-17T20:56:20.4003742Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-17T20:56:20.4004211Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-17T20:56:20.4004685Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-17T20:56:20.4005197Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-17T20:56:20.4005686Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-17T20:56:20.4006160Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-17T20:56:20.4006581Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-17T20:56:20.4007022Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-05-17T20:56:20.4007477Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-05-17T20:56:20.4008048Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:186)
2020-05-17T20:56:20.4008631Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-05-17T20:56:20.4009077Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-05-17T20:56:20.4009538Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-17T20:56:20.4009984Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-17T20:56:20.4010466Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-05-17T20:56:20.4010972Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-17T20:56:20.4011631Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-17T20:56:20.4012187Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-17T20:56:20.4012634Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-05-17T20:56:20.4013124Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:890)
2020-05-17T20:56:20.4013535Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-05-17T20:56:20.4013865Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-05-17T20:56:20.4014226Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-05-17T20:56:20.4014762Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-05-17T20:56:20.4015133Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-17T20:56:20.4015579Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-05-17T20:56:20.4016016Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-05-17T20:56:20.4016449Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-05-17T20:56:20.4016832Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-05-17T20:56:20.4017271Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-05-17T20:56:20.4017864Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-05-17T20:56:20.4018306Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-05-17T20:56:20.4018714Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-05-17T20:56:20.4019085Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-05-17T20:56:20.4019509Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-05-17T20:56:20.4019990Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-05-17T20:56:20.4020507Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-17T20:56:20.4021006Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-05-17T20:56:20.4021591Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-05-17T20:56:20.4022108Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-05-17T20:56:20.4022502Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-05-17T20:56:20.4022969Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-05-17T20:56:20.4023489Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-17T20:56:20.4023892Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-17T20:56:20.4024317Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-17T20:56:20.4024724Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-17T20:56:20.4025192Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-05-17T20:56:20.4025761Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-05-17T20:56:20.4026398Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-05-17T20:56:20.4026993Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-05-17T20:56:20.4027495Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-05-17T20:56:20.4028044Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-05-17T20:56:20.4028589Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:501)
2020-05-17T20:56:20.4029070Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-05-17T20:56:20.4029481Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-17T20:56:20.4029849Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-17T20:56:20.4030317Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-17T20:56:20.4030723Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-17T20:56:20.4031365Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-05-17T20:56:20.4031851Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-05-17T20:56:20.4032406Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-17T20:56:20.4032903Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-17T20:56:20.4033378Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-17T20:56:20.4033761Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-17T20:56:20.4034148Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-17T20:56:20.4034635Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-17T20:56:20.4035045Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-17T20:56:20.4035433Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-17T20:56:20.4035832Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-17T20:56:20.4036207Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-17T20:56:20.4036560Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-17T20:56:20.4036935Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-17T20:56:20.4037267Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-17T20:56:20.4037619Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-17T20:56:20.4037941Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-17T20:56:20.4038262Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-17T20:56:20.4038494Z 	... 4 more
2020-05-17T20:56:20.4039533Z Caused by: java.lang.Exception: Failed to send data to Kafka: This server does not host this topic-partition.
2020-05-17T20:56:20.4040089Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.checkErroneous(FlinkKafkaProducerBase.java:376)
2020-05-17T20:56:20.4040641Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.invoke(FlinkKafkaProducer010.java:287)
2020-05-17T20:56:20.4041350Z 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:86)
2020-05-17T20:56:20.4041960Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:715)
2020-05-17T20:56:20.4042610Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:690)
2020-05-17T20:56:20.4043251Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:670)
2020-05-17T20:56:20.4043754Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
2020-05-17T20:56:20.4044236Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
2020-05-17T20:56:20.4044610Z 	at StreamExecCalc$126.processElement(Unknown Source)
2020-05-17T20:56:20.4045049Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:715)
2020-05-17T20:56:20.4045616Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:690)
2020-05-17T20:56:20.4046146Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:670)
2020-05-17T20:56:20.4046665Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
2020-05-17T20:56:20.4047126Z 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
2020-05-17T20:56:20.4047669Z 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
2020-05-17T20:56:20.4048259Z 	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:93)
2020-05-17T20:56:20.4048914Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
2020-05-17T20:56:20.4049372Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
2020-05-17T20:56:20.4049870Z 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)
2020-05-17T20:56:20.4050709Z Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.

2020-05-17T20:59:01.6386784Z [ERROR]   Kafka010TableITCase>KafkaTableTestBase.testKafkaSourceSink:145 Â» Execution org...

{code};;;","18/May/20 06:21;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1650&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","18/May/20 06:29;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1654&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","18/May/20 18:04;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1712&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","19/May/20 07:06;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1798&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","19/May/20 07:07;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1788&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","19/May/20 07:23;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1768&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","19/May/20 16:16;wanglijie;I met the same problem. [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/1860/logs/79];;;","20/May/20 04:06;becket_qin;Patch merged.
master: 51a0d42ade8ee3789036ac1ee7c121133b58212a
release-1.11: 0f072234d5cd30879b4e4845e69bee1a03cf1817;;;","01/Jun/20 17:54;rmetzger;Is this still the same problem? https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2497&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=e4f347ab-2a29-5d7c-3685-b0fcd2b6b051;;;","02/Jun/20 03:20;becket_qin;[~rmetzger] Technically speaking this is caused by the same problem of asynchronous communication between the controller and the brokers. I explained the potential failure case in the PR.

{quote}

Theoretically speaking, using {{KafkaAdminClient}} to create topic does not 100% guarantee that a producer will not see the ""does not host this topic-partition"" error. This is because when the {{AdminClient}} can only guarantee the topic metadata information has existed in the broker to which it sent the {{CreateTopicRequest}}. When a producer comes at a later point, it might send {{TopicMetdataRequest}} to a different broker and that broker may have not received the updated topic metadata yet. But this is much unlikely to happen given the broker usually receives the metadata update at the same time. Having retries configured on the producer side should be sufficient to handle such cases. We can also do that for 0.10 and 0.11 producers. But given that we have the producer properties scattered over the places (which is something we probably should avoid to begin with), it would be simpler to just make sure the topic has been created successfully before we start the tests.

{quote}

I think we are hitting the issue here. To be absolutely sure no such exception is thrown, we need to check each broker to ensure they are aware of the topic.

Given that we have a bunch of other KafkaITCase stability tickets to handle at this point. I'd suggest to do this a little later unless we see this problem comes up frequently.;;;","02/Jun/20 07:13;rmetzger;Thanks a lot for for your explanation. I'm okay with waiting and seeing how frequent this error is.;;;","11/Jun/20 13:14;rmetzger;Another case (just adding it to measure frequency) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3268&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","12/Jun/20 06:00;rmetzger;{code}
2020-06-11T22:22:31.4964111Z Test testKafkaDebeziumChangelogSource[legacy = false, format = csv](org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase) failed with:
2020-06-11T22:22:31.4965146Z java.lang.Exception: Failed to write debezium data to Kafka.
2020-06-11T22:22:31.4965971Z 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testKafkaDebeziumChangelogSource(KafkaTableITCase.java:83)
2020-06-11T22:22:31.4966755Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-11T22:22:31.4967404Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-11T22:22:31.4968255Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-11T22:22:31.4968920Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-11T22:22:31.4969606Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-11T22:22:31.4970360Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-11T22:22:31.4971261Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-11T22:22:31.4972012Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-11T22:22:31.4972736Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-06-11T22:22:31.4973440Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-11T22:22:31.4974035Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-11T22:22:31.4974856Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-11T22:22:31.4975531Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-11T22:22:31.4976286Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-11T22:22:31.4977310Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-11T22:22:31.4978082Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-11T22:22:31.4978744Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-11T22:22:31.4979373Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-11T22:22:31.4980033Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-11T22:22:31.4980658Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-11T22:22:31.4981368Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-06-11T22:22:31.4981916Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-06-11T22:22:31.4982487Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-11T22:22:31.4983113Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-11T22:22:31.4983714Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-11T22:22:31.4984326Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-11T22:22:31.4985118Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-11T22:22:31.4985789Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-06-11T22:22:31.4986460Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-06-11T22:22:31.4987159Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-11T22:22:31.4987957Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-11T22:22:31.4988691Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-11T22:22:31.4989058Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-11T22:22:31.4989459Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-11T22:22:31.4989941Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-11T22:22:31.4990438Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-11T22:22:31.4990894Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-11T22:22:31.4991499Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-11T22:22:31.4991995Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-11T22:22:31.4992507Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-11T22:22:31.4993036Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-11T22:22:31.4993474Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-06-11T22:22:31.4993937Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-06-11T22:22:31.4994407Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:677)
2020-06-11T22:22:31.4995096Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-06-11T22:22:31.4995629Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1686)
2020-06-11T22:22:31.4996244Z 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testKafkaDebeziumChangelogSource(KafkaTableITCase.java:80)
2020-06-11T22:22:31.4996645Z 	... 41 more
2020-06-11T22:22:31.4996968Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-06-11T22:22:31.4997622Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-06-11T22:22:31.4998255Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-06-11T22:22:31.4999122Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
2020-06-11T22:22:31.4999653Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-06-11T22:22:31.5000186Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-06-11T22:22:31.5000734Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-06-11T22:22:31.5001325Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)
2020-06-11T22:22:31.5001729Z 	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
2020-06-11T22:22:31.5002131Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-11T22:22:31.5002525Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-11T22:22:31.5002946Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-06-11T22:22:31.5003400Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-06-11T22:22:31.5003906Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-06-11T22:22:31.5004407Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-06-11T22:22:31.5004970Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-06-11T22:22:31.5005359Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-06-11T22:22:31.5005725Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2020-06-11T22:22:31.5006388Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2020-06-11T22:22:31.5006966Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-06-11T22:22:31.5007366Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-06-11T22:22:31.5007858Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2020-06-11T22:22:31.5008248Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2020-06-11T22:22:31.5008613Z 	at akka.actor.Actor.aroundReceive(Actor.scala:517)
2020-06-11T22:22:31.5008938Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
2020-06-11T22:22:31.5009299Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-06-11T22:22:31.5009679Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-06-11T22:22:31.5010016Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-06-11T22:22:31.5010362Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-06-11T22:22:31.5010694Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-06-11T22:22:31.5011089Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-06-11T22:22:31.5011438Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-06-11T22:22:31.5011863Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-06-11T22:22:31.5012289Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-11T22:22:31.5012702Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-11T22:22:31.5014110Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Failed to send data to Kafka: This server does not host this topic-partition.
2020-06-11T22:22:31.5014960Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-06-11T22:22:31.5015408Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-06-11T22:22:31.5015957Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:161)
2020-06-11T22:22:31.5016528Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:130)
2020-06-11T22:22:31.5017172Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:134)
2020-06-11T22:22:31.5017908Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:80)
2020-06-11T22:22:31.5018408Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.closeOperators(OperatorChain.java:302)
2020-06-11T22:22:31.5018889Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:567)
2020-06-11T22:22:31.5019334Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:538)
2020-06-11T22:22:31.5019759Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-11T22:22:31.5020140Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-11T22:22:31.5020445Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-11T22:22:31.5021350Z Caused by: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Failed to send data to Kafka: This server does not host this topic-partition.
2020-06-11T22:22:31.5022021Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1257)
2020-06-11T22:22:31.5022557Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:898)
2020-06-11T22:22:31.5023077Z 	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)
2020-06-11T22:22:31.5023591Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109)
2020-06-11T22:22:31.5024175Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$closeOperator$5(StreamOperatorWrapper.java:205)
2020-06-11T22:22:31.5025085Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
2020-06-11T22:22:31.5025715Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.closeOperator(StreamOperatorWrapper.java:203)
2020-06-11T22:22:31.5026324Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$deferCloseOperatorToMailbox$3(StreamOperatorWrapper.java:177)
2020-06-11T22:22:31.5026999Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
2020-06-11T22:22:31.5027633Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
2020-06-11T22:22:31.5028126Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.tryYield(MailboxExecutorImpl.java:91)
2020-06-11T22:22:31.5028718Z 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.quiesceTimeServiceAndCloseOperator(StreamOperatorWrapper.java:155)
2020-06-11T22:22:31.5029108Z 	... 9 more
2020-06-11T22:22:31.5029804Z Caused by: org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: This server does not host this topic-partition.
2020-06-11T22:22:31.5030416Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1257)
2020-06-11T22:22:31.5030952Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:1010)
2020-06-11T22:22:31.5031556Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:871)
2020-06-11T22:22:31.5031887Z 	... 19 more
2020-06-11T22:22:31.5032465Z Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3343&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f09203c9-1af8-53a6-da0c-2e60f5418512;;;","16/Mar/21 07:03;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14738&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=eb5f4d19-2d2d-5856-a4ce-acf5f904a994;;;","16/Mar/21 08:20;trohrmann;Should we reopen this issue [~dwysakowicz]?;;;","16/Mar/21 08:30;dwysakowicz;Not sure, If I understood the discussion correctly the consensus was ""Won't fix"", unless it is super frequent (which I guess is not the case yet).

I'd leave the decision up to [~becket_qin];;;","17/Mar/21 14:27;trohrmann;Ah ok, sorry I've missed this part about the won't fix.;;;","18/Mar/21 00:15;becket_qin;[~dwysakowicz] Hmm, this issue should actually already be fixed completely. I do not expect this to happen again.

Looking at the failure test case, it was from {{org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011ITCase, }}which seems not exist in the master branch anymore. Am I missing something?;;;","19/Mar/21 09:47;dwysakowicz;[~becket_qin] The test failures occur on 1.11 branch.;;;"
TaskManagerRunnerTest is unstable,FLINK-12015,13223954,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,aljoscha,aljoscha,26/Mar/19 06:08,05/Apr/19 12:04,13/Jul/23 08:05,30/Mar/19 12:29,,,,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,test-stability,,"I saw this failure:
{code:java}
17:34:16.833 [INFO] Running org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest
17:34:19.872 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.036 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest
17:34:19.880 [ERROR] testShouldShutdownOnFatalError(org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest)  Time elapsed: 0.353 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: is <1>
     but: was <0>
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.testShouldShutdownOnFatalError(TaskManagerRunnerTest.java:59)
{code}

Travis log: https://travis-ci.org/apache/flink/jobs/511042156",,aljoscha,trohrmann,,,,,,,,,,,,,,,,,"aljoscha commented on pull request #8053: [FLINK-12015] Fix TaskManagerRunnerTest instability
URL: https://github.com/apache/flink/pull/8053
 
 
   Before, the was a race condition between the termination future in
   TaskManagerRunner completing and the asynchronous shutdown part here:
   https://github.com/apache/flink/blob/70107c4647ecac3df9b2b8c7920e7cb99ad550f1/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java#L258
   
   The test would go out of the block that was waiting on the future but
   the shutdown code that is executed after the future completes is
   executed asynchronously, so is not guaranteed to have run at that point.
   
   This also refactors the code a bit to make it more obvious what is
   happening and removes the SecurityManagerContext because it was
   obscuring the problem.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/19 16:51;githubbot;600","aljoscha commented on pull request #8053: [FLINK-12015] Fix TaskManagerRunnerTest instability
URL: https://github.com/apache/flink/pull/8053
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/19 12:27;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 30 12:29:46 UTC 2019,,,,,,,,,,"0|z013i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/19 12:29;aljoscha;Fixed on master in
10bf3b0b754b5eded1f9a18089d2d0c0c053aafb

Fixed on release-1.8 in
c80fcb8b0f7e0e42536332ae35135d1fe5047672;;;",,,,,,,,,,,,,,,,,,,,,,,
"Flink CEP Doc missing ""SkipToNextStrategy""",FLINK-12014,13223953,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Tom Goong,Tom Goong,Tom Goong,26/Mar/19 06:00,02/Apr/19 15:10,13/Jul/23 08:05,02/Apr/19 15:10,,,,,,,,,1.9.0,,,,Documentation,Library / CEP,,0,pull-request-available,,,"in cep.md file，The following description misses *SkipToNextStrategy：*

 

*After Match Skip Strategy*

For a given pattern, the same event may be assigned to multiple successful matches. To control to how many matches an event will be assigned, you need to specify the skip strategy called `AfterMatchSkipStrategy`. There are four types of skip strategies, listed as follows:
 * <strong>*NO_SKIP*</strong>: Every possible match will be emitted.
 * <strong>*SKIP_PAST_LAST_EVENT*</strong>: Discards every partial match that started after the match started but before it ended.
 * <strong>*SKIP_TO_FIRST*</strong>: Discards every partial match that started after the match started but before the first event of *PatternName* occurred.
 * <strong>*SKIP_TO_LAST*</strong>: Discards every partial match that started after the match started but before the last event of *PatternName* occurred.",,dwysakowicz,Tom Goong,,,,,,,,,,,,,,,,,"Tom-Goong commented on pull request #8049: [FLINK-12014][Doc]Flink CEP Doc missing ""SkipToNextStrategy""
URL: https://github.com/apache/flink/pull/8049
 
 
   ## What is the purpose of the change
   
   Improve the inaccurate description of AfterMatchSkipStrategy in the documentation
   
   ## Brief change log
   
   modify cep.md file in the path: docs/dev/libs/cep.md
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency):  **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`:  **no**
     - The serializers:  **no**
     - The runtime per-record code paths (performance sensitive):  **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper:  **no**
     - The S3 file system connector:  **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature?  **no**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/19 06:15;githubbot;600","dawidwys commented on pull request #8049: [FLINK-12014][Doc]Flink CEP Doc missing ""SkipToNextStrategy""
URL: https://github.com/apache/flink/pull/8049
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/19 15:08;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 02 15:10:08 UTC 2019,,,,,,,,,,"0|z013i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/19 15:10;dwysakowicz;Fixed in 8a56b8943865d42fa6479e5f3cc5bbd1f210e502;;;",,,,,,,,,,,,,,,,,,,,,,,
BroadcastStateITCase does not use Test Environments,FLINK-12012,13223841,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,25/Mar/19 18:22,05/Apr/19 12:01,13/Jul/23 08:05,26/Mar/19 10:22,1.8.0,,,,,,,,1.8.0,,,,API / DataStream,Tests,,0,,,,"The test uses the default local environment (and thus embedded mini cluster), where the parallelism depends on the number of CPU cores.

That makes the network buffer consumption non-deterministic. It should run in a test environment with well defined parallelism and memory footprint.",,liyu,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 26 10:22:28 UTC 2019,,,,,,,,,,"0|z012ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/19 10:22;sewen;Fixed in
  - 1.9.0 (master) in d4a0ecce47ee2b2fdd579634ea23855c23164087
  - 1.8.0 in e620844ceed020e3d976aa4dc976b1ff8bf3fc80;;;",,,,,,,,,,,,,,,,,,,,,,,
Wrong check message about heartbeat interval for HeartbeatServices,FLINK-12009,13223770,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yanghua,baranda,baranda,25/Mar/19 14:08,13/Apr/21 20:40,13/Jul/23 08:05,09/Apr/19 02:53,1.7.2,,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / Coordination,,,0,pull-request-available,,,"I am seeing:
{code:java}
The heartbeat timeout should be larger or equal than the heartbeat timeout{code}
due to bad configuration. I guess it should be instead:
{code:java}
The heartbeat interval should be larger or equal than the heartbeat timeout{code}
at:

https://github.com/apache/flink/blob/1f0e036bbf6a37bb83623fb62d4900d7c28a5e1d/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatServices.java#L43",,baranda,sunjincheng121,,,,,,,,,,,,,,,,,"yanghua commented on pull request #8048: [FLINK-12009] Fix wrong check message about heartbeat interval for HeartbeatServices
URL: https://github.com/apache/flink/pull/8048
 
 
   ## What is the purpose of the change
   
   *This pull request fixes wrong check message about heartbeat interval for HeartbeatServices*
   
   ## Brief change log
   
     - *Fix wrong check message about heartbeat interval for HeartbeatServices*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/19 06:11;githubbot;600","asfgit commented on pull request #8048: [FLINK-12009] Fix wrong check message about heartbeat interval for HeartbeatServices
URL: https://github.com/apache/flink/pull/8048
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 02:48;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 09 02:53:43 UTC 2019,,,,,,,,,,"0|z012e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/19 02:53;sunjincheng121;Fixed in master: dca64c20d9524bdcd4d8d6d399639a7bbd1e5ccb
Fixed in release-1.8: 03d0a398e3402e7566c1a396586e7fff9414e2e9
Fixed in release-1.7: 5c4ed0e0a44017701229b6063ff8203b138757d5;;;",,,,,,,,,,,,,,,,,,,,,,,
Clarify scala 2.12 build instructions,FLINK-12007,13223730,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,lodo1995,lodo1995,25/Mar/19 10:11,27/Mar/19 12:55,13/Jul/23 08:05,27/Mar/19 12:55,1.7.2,1.9.0,,,,,,,1.9.0,,,,Build System,Documentation,,0,build,windows,,"I cannot build Flink on Windows 10, for Scala 2.12. I used the following command
{code:bash}
mvn clean install -Pscala-2.12 -Pinclude-kinesis -PskipTests{code}
I tried with both Flink 1.7.2 (source downloaded from the website) and 1.9.0-SNAPSHOT (clone of GitHub master branch as of 25.03.2019).

I get the following error:
{code:plain}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-runtime_2.12 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: com.typesafe.akka:akka-testkit_2.12:jar:2.4.20
Found Banned Dependency: org.apache.flink:flink-queryable-state-client-java_2.12:jar:1.9-SNAPSHOT
Found Banned Dependency: com.typesafe.akka:akka-remote_2.12:jar:2.4.20
Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.12:jar:0.8.0
Found Banned Dependency: com.typesafe:ssl-config-core_2.12:jar:0.2.1
Found Banned Dependency: org.clapper:grizzled-slf4j_2.12:jar:1.3.2
Found Banned Dependency: com.github.scopt:scopt_2.12:jar:3.5.0
Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.12:jar:2.4.20
Found Banned Dependency: com.twitter:chill_2.12:jar:0.7.6
Found Banned Dependency: org.scalatest:scalatest_2.12:jar:3.0.0
Found Banned Dependency: com.typesafe.akka:akka-actor_2.12:jar:2.4.20
Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.12:jar:2.4.20
Found Banned Dependency: org.scalactic:scalactic_2.12:jar:3.0.0
Found Banned Dependency: com.typesafe.akka:akka-stream_2.12:jar:2.4.20
Found Banned Dependency: org.scala-lang.modules:scala-xml_2.12:jar:1.0.5
Found Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.12:jar:1.0.4
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] force-shading 1.9-SNAPSHOT ......................... SUCCESS [ 1.334 s]
[INFO] flink 1.9-SNAPSHOT ................................. SUCCESS [ 4.135 s]
[INFO] flink-annotations 1.9-SNAPSHOT ..................... SUCCESS [ 3.553 s]
[INFO] flink-shaded-hadoop 1.9-SNAPSHOT ................... SUCCESS [ 0.223 s]
[INFO] flink-shaded-hadoop2 2.4.1-1.9-SNAPSHOT ............ SUCCESS [ 20.724 s]
[INFO] flink-shaded-hadoop2-uber 2.4.1-1.9-SNAPSHOT ....... SUCCESS [ 11.424 s]
[INFO] flink-shaded-yarn-tests 1.9-SNAPSHOT ............... SUCCESS [ 17.047 s]
[INFO] flink-shaded-curator 1.9-SNAPSHOT .................. SUCCESS [ 2.530 s]
[INFO] flink-metrics 1.9-SNAPSHOT ......................... SUCCESS [ 0.616 s]
[INFO] flink-metrics-core 1.9-SNAPSHOT .................... SUCCESS [ 3.845 s]
[INFO] flink-test-utils-parent 1.9-SNAPSHOT ............... SUCCESS [ 0.169 s]
[INFO] flink-test-utils-junit 1.9-SNAPSHOT ................ SUCCESS [ 0.774 s]
[INFO] flink-core 1.9-SNAPSHOT ............................ SUCCESS [ 26.409 s]
[INFO] flink-java 1.9-SNAPSHOT ............................ SUCCESS [ 5.840 s]
[INFO] flink-queryable-state 1.9-SNAPSHOT ................. SUCCESS [ 0.177 s]
[INFO] flink-queryable-state-client-java 1.9-SNAPSHOT ..... SUCCESS [ 2.904 s]
[INFO] flink-filesystems 1.9-SNAPSHOT ..................... SUCCESS [ 0.226 s]
[INFO] flink-hadoop-fs 1.9-SNAPSHOT ....................... SUCCESS [ 2.401 s]
[INFO] flink-runtime 1.9-SNAPSHOT ......................... FAILURE [ 57.654 s]
[INFO] flink-scala 1.9-SNAPSHOT ........................... SKIPPED
[INFO] flink-mapr-fs 1.9-SNAPSHOT ......................... SKIPPED
[INFO] flink-filesystems :: flink-fs-hadoop-shaded 1.9-SNAPSHOT SKIPPED
[INFO] flink-s3-fs-base 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-s3-fs-hadoop 1.9-SNAPSHOT .................... SKIPPED
[INFO] flink-s3-fs-presto 1.9-SNAPSHOT .................... SKIPPED
[INFO] flink-swift-fs-hadoop 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-oss-fs-hadoop 1.9-SNAPSHOT ................... SKIPPED
[INFO] flink-optimizer 1.9-SNAPSHOT ....................... SKIPPED
[INFO] flink-clients 1.9-SNAPSHOT ......................... SKIPPED
[INFO] flink-streaming-java 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-test-utils 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-runtime-web 1.9-SNAPSHOT ..................... SKIPPED
[INFO] flink-examples 1.9-SNAPSHOT ........................ SKIPPED
[INFO] flink-examples-batch 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-connectors 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-hadoop-compatibility 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-state-backends 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-statebackend-rocksdb 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-tests 1.9-SNAPSHOT ........................... SKIPPED
[INFO] flink-streaming-scala 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-table 1.9-SNAPSHOT ........................... SKIPPED
[INFO] flink-table-common 1.9-SNAPSHOT .................... SKIPPED
[INFO] flink-table-api-java 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-table-api-java-bridge 1.9-SNAPSHOT ........... SKIPPED
[INFO] flink-libraries 1.9-SNAPSHOT ....................... SKIPPED
[INFO] flink-cep 1.9-SNAPSHOT ............................. SKIPPED
[INFO] flink-table-planner 1.9-SNAPSHOT ................... SKIPPED
[INFO] flink-orc 1.9-SNAPSHOT ............................. SKIPPED
[INFO] flink-jdbc 1.9-SNAPSHOT ............................ SKIPPED
[INFO] flink-hbase 1.9-SNAPSHOT ........................... SKIPPED
[INFO] flink-hcatalog 1.9-SNAPSHOT ........................ SKIPPED
[INFO] flink-metrics-jmx 1.9-SNAPSHOT ..................... SKIPPED
[INFO] flink-connector-kafka-base 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-connector-kafka-0.9 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-connector-kafka-0.10 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-connector-kafka-0.11 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-formats 1.9-SNAPSHOT ......................... SKIPPED
[INFO] flink-json 1.9-SNAPSHOT ............................ SKIPPED
[INFO] flink-connector-elasticsearch-base 1.9-SNAPSHOT .... SKIPPED
[INFO] flink-connector-elasticsearch 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-connector-elasticsearch2 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-connector-elasticsearch5 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-connector-elasticsearch6 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-connector-rabbitmq 1.9-SNAPSHOT .............. SKIPPED
[INFO] flink-connector-twitter 1.9-SNAPSHOT ............... SKIPPED
[INFO] flink-connector-nifi 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-connector-cassandra 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-avro 1.9-SNAPSHOT ............................ SKIPPED
[INFO] flink-connector-filesystem 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-connector-kafka 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-sql-connector-elasticsearch6 1.9-SNAPSHOT .... SKIPPED
[INFO] flink-sql-connector-kafka-0.9 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-sql-connector-kafka-0.10 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-sql-connector-kafka-0.11 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-sql-connector-kafka 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-connector-kafka-0.8 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-connector-kinesis 1.9-SNAPSHOT ............... SKIPPED
[INFO] flink-avro-confluent-registry 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-parquet 1.9-SNAPSHOT ......................... SKIPPED
[INFO] flink-sequence-file 1.9-SNAPSHOT ................... SKIPPED
[INFO] flink-csv 1.9-SNAPSHOT ............................. SKIPPED
[INFO] flink-examples-streaming 1.9-SNAPSHOT .............. SKIPPED
[INFO] flink-table-api-scala 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-table-api-scala-bridge 1.9-SNAPSHOT .......... SKIPPED
[INFO] flink-examples-table 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-examples-build-helper 1.9-SNAPSHOT ........... SKIPPED
[INFO] flink-examples-streaming-twitter 1.9-SNAPSHOT ...... SKIPPED
[INFO] flink-examples-streaming-state-machine 1.9-SNAPSHOT SKIPPED
[INFO] flink-container 1.9-SNAPSHOT ....................... SKIPPED
[INFO] flink-queryable-state-runtime 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-end-to-end-tests 1.9-SNAPSHOT ................ SKIPPED
[INFO] flink-cli-test 1.9-SNAPSHOT ........................ SKIPPED
[INFO] flink-parent-child-classloading-test-program 1.9-SNAPSHOT SKIPPED
[INFO] flink-parent-child-classloading-test-lib-package 1.9-SNAPSHOT SKIPPED
[INFO] flink-dataset-allround-test 1.9-SNAPSHOT ........... SKIPPED
[INFO] flink-datastream-allround-test 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-stream-sql-test 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-bucketing-sink-test 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-distributed-cache-via-blob 1.9-SNAPSHOT ...... SKIPPED
[INFO] flink-high-parallelism-iterations-test 1.9-SNAPSHOT SKIPPED
[INFO] flink-stream-stateful-job-upgrade-test 1.9-SNAPSHOT SKIPPED
[INFO] flink-queryable-state-test 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-local-recovery-and-allocation-test 1.9-SNAPSHOT SKIPPED
[INFO] flink-elasticsearch1-test 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-elasticsearch2-test 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-elasticsearch5-test 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-elasticsearch6-test 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-quickstart 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-quickstart-java 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-quickstart-scala 1.9-SNAPSHOT ................ SKIPPED
[INFO] flink-quickstart-test 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-confluent-schema-registry 1.9-SNAPSHOT ....... SKIPPED
[INFO] flink-stream-state-ttl-test 1.9-SNAPSHOT ........... SKIPPED
[INFO] flink-sql-client-test 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-streaming-file-sink-test 1.9-SNAPSHOT ........ SKIPPED
[INFO] flink-state-evolution-test 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-e2e-test-utils 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-streaming-python 1.9-SNAPSHOT ................ SKIPPED
[INFO] flink-mesos 1.9-SNAPSHOT ........................... SKIPPED
[INFO] flink-yarn 1.9-SNAPSHOT ............................ SKIPPED
[INFO] flink-gelly 1.9-SNAPSHOT ........................... SKIPPED
[INFO] flink-gelly-scala 1.9-SNAPSHOT ..................... SKIPPED
[INFO] flink-gelly-examples 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-metrics-dropwizard 1.9-SNAPSHOT .............. SKIPPED
[INFO] flink-metrics-graphite 1.9-SNAPSHOT ................ SKIPPED
[INFO] flink-metrics-influxdb 1.9-SNAPSHOT ................ SKIPPED
[INFO] flink-metrics-prometheus 1.9-SNAPSHOT .............. SKIPPED
[INFO] flink-metrics-statsd 1.9-SNAPSHOT .................. SKIPPED
[INFO] flink-metrics-datadog 1.9-SNAPSHOT ................. SKIPPED
[INFO] flink-metrics-slf4j 1.9-SNAPSHOT ................... SKIPPED
[INFO] flink-python 1.9-SNAPSHOT .......................... SKIPPED
[INFO] flink-cep-scala 1.9-SNAPSHOT ....................... SKIPPED
[INFO] flink-ml 1.9-SNAPSHOT .............................. SKIPPED
[INFO] flink-ml-uber 1.9-SNAPSHOT ......................... SKIPPED
[INFO] flink-table-uber 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-sql-client 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-scala-shell 1.9-SNAPSHOT ..................... SKIPPED
[INFO] flink-dist 1.9-SNAPSHOT ............................ SKIPPED
[INFO] flink-end-to-end-tests-common 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-metrics-availability-test 1.9-SNAPSHOT ....... SKIPPED
[INFO] flink-metrics-reporter-prometheus-test 1.9-SNAPSHOT SKIPPED
[INFO] flink-heavy-deployment-stress-test 1.9-SNAPSHOT .... SKIPPED
[INFO] flink-streaming-kafka-test-base 1.9-SNAPSHOT ....... SKIPPED
[INFO] flink-streaming-kafka-test 1.9-SNAPSHOT ............ SKIPPED
[INFO] flink-streaming-kafka011-test 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-streaming-kafka010-test 1.9-SNAPSHOT ......... SKIPPED
[INFO] flink-streaming-kinesis-test 1.9-SNAPSHOT .......... SKIPPED
[INFO] flink-table-runtime-blink 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-table-planner-blink 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-contrib 1.9-SNAPSHOT ......................... SKIPPED
[INFO] flink-connector-wikiedits 1.9-SNAPSHOT ............. SKIPPED
[INFO] flink-yarn-tests 1.9-SNAPSHOT ...................... SKIPPED
[INFO] flink-fs-tests 1.9-SNAPSHOT ........................ SKIPPED
[INFO] flink-docs 1.9-SNAPSHOT ............................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:45 min
[INFO] Finished at: 2019-03-25T10:55:39+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) on project flink-runtime_2.12: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR] mvn <goals> -rf :flink-runtime_2.12{code}
I don't understand why these dependencies are being banned, as the {{pom.xml}}, inside the {{scala-2.12}} profile, the dependencies are banned in this way:
{code:xml}
<!-- make sure we don't have any _2.10 or _2.11 dependencies when building
for Scala 2.12 -->
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-enforcer-plugin</artifactId>
    <executions>
        <execution>
            <id>enforce-versions</id>
            <goals>
                <goal>enforce</goal>
            </goals>
            <configuration>
                <rules>
                    <bannedDependencies>
                        <excludes combine.children=""append"">
                            <exclude>*:*_2.11</exclude>
                            <exclude>*:*_2.10</exclude>
                        </excludes>
                    </bannedDependencies>
                </rules>
            </configuration>
        </execution>
    </executions>
</plugin>
{code}
Previously, I successfully compiled Flink for Scala 2.11. To make sure that my environment was not polluted from this previous compilation, I deleted and re-downloaded the Flink sources, and I completely wiped out the {{.m2}} Maven cache.

 

This issue was originally posted as question on Stackoverflow, but I moved it because I didn't get any answer and I could reproduce the issue also on the current master branch.","Maven 3.6.0

Windows 10, 64 bits",dwysakowicz,lodo1995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 27 12:55:21 UTC 2019,,,,,,,,,,"0|z01254:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/19 10:16;dwysakowicz;Try with:

{{mvn clean install -Pscala-2.12 }}{{*-Dscala-2.12*}} -Pinclude-kinesis -DskipTests

 ;;;","25/Mar/19 16:49;chesnay;You have to use {{-Dscala-2.12}}; activating the profile directly is not sufficient. If you use the property you can omit the profile activation.;;;","25/Mar/19 17:02;lodo1995;I will try. Thank you for the support.

Maybe it would be useful to specify this in [the relevant doc page|https://ci.apache.org/projects/flink/flink-docs-release-1.9/flinkDev/building.html#scala-versions].;;;","27/Mar/19 12:55;chesnay;master: 92f2d0d2a65fdbf3c2cb3abefb0a6276822271aa ;;;",,,,,,,,,,,,,,,,,,,,
ZooKeeperHaServicesTest.testSimpleCloseAndCleanupAllData is unstable,FLINK-12006,13223727,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tison,aljoscha,aljoscha,25/Mar/19 10:06,31/Jul/19 09:34,13/Jul/23 08:05,29/Mar/19 10:27,1.8.0,1.9.0,,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,test-stability,,"This is the log from a failed Travis run: https://travis-ci.org/apache/flink/jobs/510263371

The relevant section is:
{code}
11:15:11.201 [INFO] Running org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest
11:15:12.908 [INFO] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.255 s - in org.apache.flink.runtime.jobmaster.JobMasterTest
11:15:13.646 [INFO] Running org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRegistryTest
11:15:14.874 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.67 s <<< FAILURE! - in org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest
11:15:14.874 [ERROR] testSimpleCloseAndCleanupAllData(org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest)  Time elapsed: 0.132 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: is <[zookeeper]>
     but: was <[zookeeper, foo]>
	at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.testSimpleCloseAndCleanupAllData(ZooKeeperHaServicesTest.java:136)
{code}",,aljoscha,tison,trohrmann,,,,,,,,,,,,,,,,"TisonKun commented on pull request #8046: [FLINK-12006][coordination] Ensure owned znode deleted on ZooKeeperHaServices#deleteOwnedZNode
URL: https://github.com/apache/flink/pull/8046
 
 
   ## What is the purpose of the change
   
   Since the failing log doesn't show an exception thrown, the case should be we passed
   
   ```java
   client.delete().deletingChildrenIfNeeded().forPath(""/"");
   zNodeDeleted = true;
   ```
   
   but the znode ""/"" wasn't be deleted. For any reason we use `client.checkExists().forPath(""/"")` to ensure its deletion. Also add `#guaranteed` on `#deleted` to best effort delete the znode even if we failed by an retryable exception.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as ZooKeeperHaServiceTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers:(no)
     - The runtime per-record code paths (performance sensitive):(no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
   cc @tillrohrmann @GJL @aljoscha 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/19 10:37;githubbot;600","asfgit commented on pull request #8046:  [FLINK-12006][tests] Wait for Curator background operation finished
URL: https://github.com/apache/flink/pull/8046
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Mar/19 10:26;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-12019,,,,,,,,,FLINK-13417,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 29 10:27:37 UTC 2019,,,,,,,,,,"0|z0124g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/19 15:36;trohrmann;I think the problem is that we have a background task running which recreates the parent nodes. The background task originates from a {{NodeCache}} which we use for the {{ZooKeeperLeaderRetrievalService}}. I think it can happen that such a background task won't be immediately executed which can lead to the following interleaving:

1. {{ZooKeeperHaServices#deleteOwndZNode()}}
2. {{background-task}} (recreate the parent zNodes)
3. {{ZooKeeperHaServices#closeInternal()}};;;","27/Mar/19 15:38;tison;[~till.rohrmann] thanks for your analysis, I'll take a look now.;;;","27/Mar/19 15:56;tison;[~till.rohrmann] your analysis is reasonable. For this test we can extract the {{TestingListener}} and ensure that the leader is elected. Thus operation 2 happens before operation 1.

{code:java}
 import org.apache.flink.runtime.util.ZooKeeperUtils;
 import org.apache.flink.runtime.zookeeper.ZooKeeperResource;
@@ -191,11 +192,15 @@ public class ZooKeeperHaServicesTest extends TestLogger {
                        final LeaderElectionService resourceManagerLeaderElectionService = zooKeeperHaServices.getResourceManagerLeaderElectionService();
                        final RunningJobsRegistry runningJobsRegistry = zooKeeperHaServices.getRunningJobsRegistry();
 
-                       resourceManagerLeaderRetriever.start(new TestingListener());
+                       final TestingListener listener = new TestingListener();
+
+                       resourceManagerLeaderRetriever.start(listener);
                        resourceManagerLeaderElectionService.start(new TestingContender(""foobar"", resourceManagerLeaderElectionService));
                        final JobID jobId = new JobID();
                        runningJobsRegistry.setJobRunning(jobId);
 
+                       listener.waitForNewLeader(2000L);
+
                        resourceManagerLeaderRetriever.stop();
                        resourceManagerLeaderElectionService.stop();
                        runningJobsRegistry.clearJob(jobId);
{code}

However, in production code it is still buggy on this execute order. But if we bump ZK version to support {{CreateMode#CONTAINER}}, then the remain znode(path) should be all containers and thus we can say that they will finally get removed.;;;","27/Mar/19 16:19;trohrmann;Yes, atm I don't see a proper fix apart from using the {{CreateMode#CONTAINER}} for which we and all our users need to upgrade Zk. 

In the meantime, we should try to harden this test case so that this background operation does not occur. I think your proposal to explicitly wait on the new leader could work. Can we do the following: 

1. Try to reliably reproduce the problem on Travis by running this test in a loop (take a look at this commit https://github.com/tillrohrmann/flink/commit/a4a2b30f4306f3a768d95126b1345f3c12ad2a9f)
2. Add the proposed fix (waiting for a new leader) to see whether this will clear the background operation;;;","27/Mar/19 16:36;tison;Let's continue discussion on JIRA.

I think your idea make sense to harden the test. Even though running the test in a loop is just increase probability we reproduce the problem(not 100% anyway).

The fix is based on that in the test, at most one {{WatchEvent}} (i.e., write leader information) was triggered. ATM I think we preserve this condition.

I'd at first try to reliably reproduce the problem on Travis and if succeed, add the fix as described above.;;;","27/Mar/19 16:50;tison;It looks like such a repeat test a common request, we could take it into consideration as a follow up to emulate {{@RepeatedTest}} in Junit5. A related post is on https://turreta.com/2017/07/15/junit-4-run-test-method-more-than-once/;;;","28/Mar/19 10:39;trohrmann;Another instance: https://api.travis-ci.org/v3/job/512114781/log.txt;;;","29/Mar/19 10:27;trohrmann;Fixed via
1.9.0: 72a6f3f76040d9d594d184077ba7790fb8d6ec69
1.8.1: f2d4fdd9fc50bd0e71cab7442a2f26a3e4b94fb0;;;",,,,,,,,,,,,,,,,
"Test AvroExternalJarProgramITCase, get `JAR file does not exist`  Error.",FLINK-12001,13223687,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunjincheng121,sunjincheng121,sunjincheng121,25/Mar/19 07:37,05/Apr/19 12:02,13/Jul/23 08:05,27/Mar/19 12:57,1.8.0,,,,,,,,1.8.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,,"When I run `mvn clean verify`  I got the follows error:
{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.328 s <<< FAILURE! - in org.apache.flink.formats.avro.AvroExternalJarProgramITCase
[ERROR] org.apache.flink.formats.avro.AvroExternalJarProgramITCase  Time elapsed: 2.328 s  <<< ERROR!
{code}

And run `AvroExternalJarProgramITCase` in IDE, I also got the follows error: 

{code}

org.apache.flink.client.program.ProgramInvocationException: JAR file does not exist '/Users/jincheng/work/opensource/FLINK-MERGE/flink-formats/flink-avro/maven-test-jar.jar'

	at org.apache.flink.client.program.PackagedProgram.checkJarFile(PackagedProgram.java:777)
	at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:183)
	at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:108)
	at org.apache.flink.formats.avro.AvroExternalJarProgramITCase.testExternalProgram(AvroExternalJarProgramITCase.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}

And test JAR file is placed in `target` folder. So we should change the jar file path in test case file.
What do you think?",,aljoscha,sunjincheng121,,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #8047: [FLINK-12001][tests] fix the external jar path config error for AvroE…
URL: https://github.com/apache/flink/pull/8047
 
 
   ## What is the purpose of the change
   
   fix the external jar path config error for AvroExternalJarProgramITCase.
   
   ## Brief change log
     -  fix the external jar path config error for AvroExternalJarProgramITCase.
   
   ## Verifying this change
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? ( not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/19 12:29;githubbot;600","asfgit commented on pull request #8047: [FLINK-12001][tests] fix the external jar path config error for AvroE…
URL: https://github.com/apache/flink/pull/8047
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 12:47;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 27 12:57:30 UTC 2019,,,,,,,,,,"0|z011vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/19 12:57;sunjincheng121;Fixed in master: b60104494a0750635b5ed5afcb445d33020fc363
Fixed in release-1.8: ef9d0c2344108bf1d817c6d7364f0fb7322ad0fc;;;",,,,,,,,,,,,,,,,,,,,,,,
Kafka producer occasionally throws NullpointerException,FLINK-11987,13223005,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,Zor_X_L,Zor_X_L,21/Mar/19 06:52,19/Feb/21 07:23,13/Jul/23 08:05,02/Jun/19 05:49,1.6.3,1.6.4,1.7.2,,,,,,1.6.5,1.7.3,1.8.1,1.9.0,Connectors / Kafka,,,0,pull-request-available,,,"We are using Flink 1.6.2 in our production environment, and kafka producer occasionally throws NullpointerException.

We found in line 175 of flink/flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer011.java, NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR was created as a static variable.

Then in line 837, 
{code:java}
context.getOperatorStateStore().getUnionListState(NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR);
{code}
was called, and that leads to line 734 of 
 flink/flink-runtime/src/main/java/org/apache/flink/runtime/state/DefaultOperatorStateBackend.java: 
{code:java}
stateDescriptor.initializeSerializerUnlessSet(getExecutionConfig());
{code}

In function initializeSerializerUnlessSet(line 283 of flink/flink-core/src/main/java/org/apache/flink/api/common/state/StateDescriptor.java):

{code:java}
if (serializer == null) {
  checkState(typeInfo != null, ""no serializer and no type info"");
  // instantiate the serializer
  serializer = typeInfo.createSerializer(executionConfig);
  // we can drop the type info now, no longer needed
  typeInfo  = null;
}

""serializer = typeInfo.createSerializer(executionConfig);"" is the line which throws the exception.
{code}

We think that's because multiple subtasks of the same producer in a same TaskManager share a same NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR.","Flink 1.6.2 (Standalone Cluster)

Oracle JDK 1.8u151

Centos 7.4",chethanuk,guanghui,lamber-ken,liyu,sewen,tzulitai,Zor_X_L,,,,,,,,,,,,"Zor-X-L commented on pull request #8331: [FLINK-11987][flink-connector-kafka-0.11] convert NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR to a non-static variabl…
URL: https://github.com/apache/flink/pull/8331
 
 
   …e to avoid StateDescriptor#initializeSerializerUnlessSet throws NullPointerException
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   To fix a problem in flink-connector-kafka-0.11 which can cause StateDescriptor#initializeSerializerUnlessSet to throw NullPointerException
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   convert NEXT_TRANSACTONAL_ID_HINT_DESCRIPTOR to a non-static final variable
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   No.
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/May/19 07:53;githubbot;600","carp84 commented on pull request #8570: [FLINK-11987] [state] Make serializer lazy initialization thread safe in StateDescriptor
URL: https://github.com/apache/flink/pull/8570
 
 
   ## What is the purpose of the change
   
   This PR aims at making the lazy initialization of `StateDescriptor.serializer` thread safe, more details about what problem there will be w/o the fix please refer to [JIRA](https://issues.apache.org/jira/browse/FLINK-11987).
   
   This PR supersedes [PR#8331](https://github.com/apache/flink/pull/8331) as per discussed there.
   
   ## Brief change log
   
   Changes mainly inlcude:
   * Use CAS when lazily initializing `serializer` in `StateDescriptor#initializeSerializerUnlessSet`, to make sure only one serializer will be used.
   * Remove the nullify of `typeInfo` in `StateDescriptor#initializeSerializerUnlessSet` to prevent NPE as observed in `FlinkKafkaProducer011` from jira description.
   * Added a new UT case in `StateDescriptorTest` to reproduce the problem and verify the fix.
   
   
   ## Verifying this change
   
   Added a `testSerializerLazyInitializeInParallel` test case in `StateDescriptorTest` for verification.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
        - `StateDescriptor` is annotated with `@PublicEvolving` while changes here don't involve any modification on method/interface signatures.
     - The serializers: (yes)
        - The change here assures only one single serializer instance will be returned by `StateDescriptor.getSerializer` after lazy initialization while previously no such assurance.
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/May/19 08:10;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,FLINK-12688,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 02 05:49:30 UTC 2019,,,,,,,,,,"0|z00xow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/19 08:16;sewen;I think there is a deeper issue here, with {{StateDescriptors}} and the way they hold {{Serializers}}.

The easiest fix for now might be to remove the line that nulls out the {{TypeInformation}} in the {{StateDescriptor}} class.;;;","06/May/19 09:31;Zor_X_L;[~sewen] We don't have much knowledge on this StateDescriptor so we choose to fix kafka connector. Fixing StateDescriptor is indeed a better way. Should I change my PR to fix StateDescriptor?;;;","13/May/19 13:33;chethanuk;Yes, I also faced this issue. Out of nowhere some NullpointerException. 

Error: 

```
 java.lang.NullPointerException
 at java.io.StringReader.<init>(StringReader.java:50)
 at com.google.gson.JsonParser.parse(JsonParser.java:45)
 at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:579)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:718)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:696)
 at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:579)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:718)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:696)
 at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
 at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collectWithTimestamp(StreamSourceContexts.java:111)
 at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordWithTimestamp(AbstractFetcher.java:398)
 at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.emitRecord(KafkaFetcher.java:185)
 at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:150)
 at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:711)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:93)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
 at java.lang.Thread.run(Thread.java:748)
```;;;","31/May/19 04:51;liyu;Have created FLINK-12688 to fix the issue from inside {{StateDescriptor}} as suggested above and marked as blocker to this one.;;;","31/May/19 05:01;liyu;Regarding the issue mentioned by [~chethanuk], from the posted stack the NPE was thrown from inside the user-defined function (UDF) thus probably not a problem of Flink. Could you double check your UDF codes [~chethanuk]? Thanks.;;;","02/Jun/19 05:49;tzulitai;This is now fixed along with the resolution of FLINK-12688.;;;",,,,,,,,,,,,,,,,,,
StreamingFileSink docs do not mention S3 savepoint caveats.,FLINK-11984,13222833,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,kkl0u,kkl0u,20/Mar/19 12:51,25/Mar/19 16:03,13/Jul/23 08:05,25/Mar/19 16:03,1.7.2,,,,,,,,1.8.0,,,,Connectors / FileSystem,Documentation,,0,pull-request-available,,,,,jgrier,kkl0u,,,,,,,,,,,,,,,,,"kl0u commented on pull request #8021: [FLINK-11984][docs] MPU timeout implications on StreamingFileSink.
URL: https://github.com/apache/flink/pull/8021
 
 
   This is a trivial PR that improves documentation.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/19 12:54;githubbot;600","kl0u commented on pull request #8021: [FLINK-11984][docs] MPU timeout implications on StreamingFileSink.
URL: https://github.com/apache/flink/pull/8021
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/19 16:01;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 25 16:03:07 UTC 2019,,,,,,,,,,"0|z00wmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/19 16:03;kkl0u;Merged on master with fd318d8cef29cdbe86ba3882101d7251e92d3d52

and on release-1.8 with 839886e558c4c4c3d8a5f6639adb257a1bcdfbe3

and on release-1.7 with 11c3e6a8518673376187f06ab0773e0e8ca6a778;;;",,,,,,,,,,,,,,,,,,,,,,,
The classpath is missing the `flink-shaded-hadoop2-uber-2.8.3-1.8.0.jar` JAR during the end-to-end test.,FLINK-11972,13222738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyu,sunjincheng121,sunjincheng121,20/Mar/19 06:02,21/Mar/19 12:23,13/Jul/23 08:05,21/Mar/19 07:51,1.8.0,1.9.0,,,,,,,1.8.0,,,,Tests,,,0,pull-request-available,,,"Since the difference between 1.8.0 and 1.7.x is that 1.8.x does not put the `hadoop-shaded` JAR integrated into the dist.  It will cause an error when the end-to-end test cannot be found with `Hadoop` Related classes,  such as: `java.lang.NoClassDefFoundError: Lorg/apache/hadoop/fs/FileSystem`. So we need to improve the end-to-end test script, or explicitly stated in the README, i.e. end-to-end test need to add `flink-shaded-hadoop2-uber-XXXX.jar` to the classpath. So, we will get the exception something like:
{code:java}
[INFO] 3 instance(s) of taskexecutor are already running on jinchengsunjcs-iMac.local.

Starting taskexecutor daemon on host jinchengsunjcs-iMac.local.

java.lang.NoClassDefFoundError: Lorg/apache/hadoop/fs/FileSystem;

at java.lang.Class.getDeclaredFields0(Native Method)

at java.lang.Class.privateGetDeclaredFields(Class.java:2583)

at java.lang.Class.getDeclaredFields(Class.java:1916)

at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:72)

at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.clean(StreamExecutionEnvironment.java:1558)

at org.apache.flink.streaming.api.datastream.DataStream.clean(DataStream.java:185)

at org.apache.flink.streaming.api.datastream.DataStream.addSink(DataStream.java:1227)

at org.apache.flink.streaming.tests.BucketingSinkTestProgram.main(BucketingSinkTestProgram.java:80)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:498)

at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)

at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)

at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:423)

at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:813)

at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:287)

at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)

at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050)

at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FileSystem

at java.net.URLClassLoader.findClass(URLClassLoader.java:381)

at java.lang.ClassLoader.loadClass(ClassLoader.java:424)

at java.lang.ClassLoader.loadClass(ClassLoader.java:357)

... 22 more

Job () is running.{code}
So, I think we can import the test script or improve the README.

What do you think?",,aljoscha,kisimple,liyu,sunjincheng121,,,,,,,,,,,,,,,"carp84 commented on pull request #8027: [FLINK-11972] [docs] Add necessary notes about running streaming bucketing e2e test in README
URL: https://github.com/apache/flink/pull/8027
 
 
   ## What is the purpose of the change
   
   * Add necessary notes to make it easier for running streaming bucketing end to end test case
   
   
   ## Brief change log
   
   * Mainly two notes added, one for downloading hadoop bundles, the other for running mvn install to generated required jars.
   
   
   ## Verifying this change
   
   This change is a document improvement without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/19 22:45;githubbot;600","asfgit commented on pull request #8027: [FLINK-11972] [docs] Add necessary notes about running streaming bucketing e2e test in README
URL: https://github.com/apache/flink/pull/8027
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Mar/19 07:49;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/19 22:27;sunjincheng121;image-2019-03-21-06-26-49-787.png;https://issues.apache.org/jira/secure/attachment/12963185/image-2019-03-21-06-26-49-787.png","20/Mar/19 22:30;sunjincheng121;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12963187/screenshot-1.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 21 08:21:14 UTC 2019,,,,,,,,,,"0|z00w1k:",9223372036854775807,Add necessary notes about running streaming bucketing end-to-end test in README,,,,,,,,,,,,,,,,,,,"20/Mar/19 08:55;liyu;+1 on improving. Encountered the same issue in local Mac environment when verifying 1.8.0 rc1.;;;","20/Mar/19 20:01;liyu;There's another necessity to run the {{test_streaming_bucketing.sh}} case, that we must make sure to run \{{mvn install}} in the flink-end-to-end-tests directory, below are more detailed reason:

In {{test_streaming_bucketing.sh}} the command to submit job and get job id is like:
{noformat}
TEST_PROGRAM_JAR=${END_TO_END_DIR}/flink-bucketing-sink-test/target/BucketingSinkTestProgram.jar
...
JOB_ID=$($FLINK_DIR/bin/flink run -d -p 4 $TEST_PROGRAM_JAR -outputPath $TEST_DATA_DIR/out/result \
  | grep ""Job has been submitted with JobID"" | sed 's/.* //g')
{noformat}
And the {{TEST_PROGRAM_JAR}} need to be generated by install and won't be there by default. In this case, the result of the job submission command will be something like:
{noformat}
Could not build the program from JAR file.

Use the help option (-h or --help) to get help on the command.
{noformat}
Thus the grep will get nothing, so job id is empty and the script will hang at the {{wait_job_running}} phase, with some log like:
{noformat}
Job () is running.
Waiting for job () to have at least 5 completed checkpoints ...
{noformat}

Will add the notice to document, and also try to improve the script to do logging and fast fail if the target jar is missing.;;;","20/Mar/19 20:31;sunjincheng121;Hi [~carp84] Thanks for your feedback! and do the deep testing!  I appreciate if you want to take this ticket and fix all of them?;;;","20/Mar/19 20:36;liyu;bq. I appreciate if you want to take this ticket and fix all of them?
Sure, let me take this one. Find one more possible issue and locating the root cause now, will submit a PR once done. Thanks. [~sunjincheng121];;;","20/Mar/19 20:53;sunjincheng121;Sounds good!;;;","20/Mar/19 22:17;liyu;I tried the streaming bucket case on two environments, Mac 10.14.3 and Linux 3.10.0, both failed. I'm running the test with the shaded hadoop 2.8.3 bundle which could be downloaded [here|https://repository.apache.org/content/repositories/orgapacheflink-1213/org/apache/flink/flink-shaded-hadoop2-uber/2.8.3-1.8.0/flink-shaded-hadoop2-uber-2.8.3-1.8.0.jar]. It seems to me like a real issue instead of environment problem, and I think we need to further investigate into it. [~sunjincheng121] [~aljoscha]

From the output of the test script, we could see below messages:
{noformat}
Starting taskexecutor daemon on host z05f06378.sqa.zth.
Waiting for job (905ae10bae4b99031e724b9c29f0ca7b) to reach terminal state FINISHED ...
Truncating buckets
Truncating  to
{noformat}
And in standalonesession log, we could confirm the job is finished:
{noformat}
2019-03-21 05:59:59,512 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Job 905ae10bae4b99031e724b9c29f0ca7b reached globally terminal state FINISHED.
{noformat}

Checking the {{test_streaming_bucketing.sh}} script, we could see it runs to below lines:
{noformat}
LOG_LINES=$(grep -rnw $FLINK_DIR/log -e 'Writing valid-length file')

# perform truncate on every line
echo ""Truncating buckets""
while read -r LOG_LINE; do
  PART=$(echo ""$LOG_LINE"" | awk '{ print $10 }' FS="" "")
  LENGTH=$(echo ""$LOG_LINE"" | awk '{ print $15 }' FS="" "")

  echo ""Truncating $PART to $LENGTH""

  dd if=$PART of=""$PART.truncated"" bs=$LENGTH count=1
  rm $PART
  mv ""$PART.truncated"" $PART
done <<< ""$LOG_LINES""
{noformat}

However, when trying to grep the ""Writing valid-length file"" message in log dir, *nothing appeared*. Checking the task-executor log, observed something suspicious:
{noformat}
2019-03-21 05:59:59,486 DEBUG org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink  - Moving in-progress bucket /home/jueding.ly/flink_rc_check/flink-1.8.0-src/flink-end-to-end-tests/test-scripts/temp-test-directory-53249980906/out/result4/_part-0-1.in-progress to pending file /home/jueding.ly/flink_rc_check/flink-1.8.0-src/flink-end-to-end-tests/test-scripts/temp-test-directory-53249980906/out/result4/_part-0-1.pending
{noformat};;;","20/Mar/19 23:19;liyu;Checked and confirmed with hadoop 2.6.5 the same test passed in both environment, either the [shaded bundle |https://repository.apache.org/content/repositories/orgapacheflink-1213/org/apache/flink/flink-shaded-hadoop2-uber/2.6.5-1.8.0/flink-shaded-hadoop2-uber-2.6.5-1.8.0.jar] or [hadoop dist|http://archive.apache.org/dist/hadoop/core/hadoop-2.6.5/] way.

Output of the script in the passing state:
{noformat}
Truncating /home/jueding.ly/flink_rc_check/flink-1.8.0-src/flink-end-to-end-tests/test-scripts/temp-test-directory-06210353709/out/result3/part-3-0 to 51250
1+0 records in
1+0 records out
51250 bytes (51 kB) copied, 0.000377998 s, 136 MB/s
Truncating /home/jueding.ly/flink_rc_check/flink-1.8.0-src/flink-end-to-end-tests/test-scripts/temp-test-directory-06210353709/out/result7/part-3-0 to 51250
1+0 records in
1+0 records out
51250 bytes (51 kB) copied, 0.00033118 s, 155 MB/s
pass Bucketing Sink
{noformat};;;","21/Mar/19 07:51;sunjincheng121;Fixed in master: 40fe63c5057a9388559c90b75e45ac24a1a387d0
Fixed in release-1.8: a1a864765bc95f8ae0e4e75b575820b3fcc9a6db;;;","21/Mar/19 07:53;sunjincheng121;[~carp84] I have close this JIRA due to you already fix the current JIRA. So feel free to open new JIRA if you find some new bugs. ;;;","21/Mar/19 08:21;liyu;Sure, let me open a JIRA for the left over issue. Thanks. [~sunjincheng121];;;",,,,,,,,,,,,,,
Fix `Command: start_kubernetes_if_not_ruunning failed` error,FLINK-11971,13222737,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,sunjincheng121,sunjincheng121,20/Mar/19 05:56,21/Mar/19 10:45,13/Jul/23 08:05,20/Mar/19 20:22,1.8.0,1.9.0,,,,,,,1.8.0,,,,Tests,,,0,pull-request-available,,,"When I did the end-to-end test under Mac OS, I found the following two problems:
 1. The verification returned for different `minikube status` is not enough for the robustness. The strings returned by different versions of different platforms are different. the following misjudgment is caused:
 When the `Command: start_kubernetes_if_not_ruunning failed` error occurs, the `minikube` has actually started successfully. The core reason is that there is a bug in the `test_kubernetes_embedded_job.sh` script.  The error message as follows:
 !screenshot-1.png! 
{code:java}
Current check logic: echo ${status} | grep -q ""minikube: Running cluster: Running kubectl: Correctly Configured""

==== My local info====
jinchengsunjcs-iMac:flink-1.8.0 jincheng$ minikube status

host: Running

kubelet: Running

apiserver: Running

kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.101{code}
So, I think we should improve the check logic of `minikube status`, What do you think?",,aljoscha,hequn8128,sunjincheng121,,,,,,,,,,,,,,,,"aljoscha commented on pull request #8024: [FLINK-11971] Fix kubernetes check in end-to-end test
URL: https://github.com/apache/flink/pull/8024
 
 
   @dawidwys You originally added this. Do you think this simpler check would work as well?
   
   @hequn8128 Sorry for taking this issue, but I quickly wanted to fix this before creating a new RC because I was running the end-to-end tests on my Mac.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/19 15:23;githubbot;600","asfgit commented on pull request #8024: [FLINK-11971] Fix kubernetes check in end-to-end test
URL: https://github.com/apache/flink/pull/8024
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/19 20:18;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/19 06:22;sunjincheng121;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12963077/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 20:22:14 UTC 2019,,,,,,,,,,"0|z00w1c:",9223372036854775807,Fix kubernetes check in end-to-end test.,,,,,,,,,,,,,,,,,,,"20/Mar/19 15:41;hequn8128;[~sunjincheng121] Thanks for looking into the problem and providing valuable information. I see two options to solve the problem:
 - Making the match condition looser. For example, we can only match `Running` keyword. However, this way we would also make the checking very fragile.
 - Add another match condition for the new status message. For example, we can change the original match logic(in test_kubernetes_embedded_job.sh) from
{code:java}
    echo ${status} | grep -q ""minikube: Running cluster: Running kubectl: Correctly Configured""

{code}
to
{code:java}
    echo ${status} | grep -q ""minikube: Running cluster: Running kubectl: Correctly Configured"" \
        || echo ${status} | grep -q ""host: Running kubelet: Running apiserver: Running kubectl: Correctly Configured""
{code}
Use `||` to connect the two conditions so that either one success can lead to success.

Personally, I prefer the second option.
 [~sunjincheng121] [~aljoscha] What do you think?;;;","20/Mar/19 20:21;sunjincheng121;Thanks for taking this JIRA. [~hequn8128]!
Due to the release-1.8 new RC should be coming ASAP. So [~aljoscha] open the PR. I will merge it.
Anyway, Thanks for your efforts. 

Best,
Jincheng.;;;","20/Mar/19 20:22;sunjincheng121;Fixed in master: 56d81e0dcadd07d634f92ac464a49d3313f56621
Fixed in release-1.8: eb571567ccbf5cb663e91cbbe3d9d8685b6b52fa;;;",,,,,,,,,,,,,,,,,,,,,
Avoid hash collision of partition and bucket in HybridHashTable in Blink SQL,FLINK-11964,13222518,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,19/Mar/19 08:32,07/Jan/20 10:43,13/Jul/23 08:05,07/Jan/20 10:43,,,,,,,,,1.10.0,,,,Table SQL / Runtime,,,0,,,,"In HybridHashTable, first select the corresponding partition according to hashCode, and then select the bucket in the partition according to hashCode, using the same hashCode can easily cause hash collision.

Consider doing some mix to hashCode when choosing bucket.

Like JDK HashMap, we can just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds. (bucket use power-of-two masking).  Just like:  (hash ^ (hash >>> 16))

In some cases, if a lot of conflicts occurred, this will lead to job hang, because hash join will degenerate to nested loop join.",,fan_li_ya,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 10:43:17 UTC 2020,,,,,,,,,,"0|z00uoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/19 04:05;fan_li_ya;Hi [~lzljs3620320], this is a good point, because the selection of the hash function has great impact on the hash bucket collision, and hence the E2E query performance. However, selecting a proper hash function is tricky, because if the hash function is overly computing-intensive, the computing overhead will outweigh the reduction in hash collision. 

I am interested in this topic. Would you please assign this issue to me? Maybe I can do some systematic research :);;;","03/Jan/20 05:53;lzljs3620320;Hi [~fan_li_ya], sorry for late responding, thanks for involving. But ""(hash ^ (hash >>> 16))"" is very low overhead, I think it is OK to add.;;;","03/Jan/20 06:05;lzljs3620320;PR: [https://github.com/apache/flink/pull/10756];;;","07/Jan/20 10:43;jark;1.11.0: 69ed6feef09d36df48b2e849888f9faebdaa2981
1.10.0: 81b18957da8e35b414b6c6017d13720157340d59;;;",,,,,,,,,,,,,,,,,,,,
Add missing dependencies in NOTICE file of flink-dist.,FLINK-11950,13222253,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sunjincheng121,sunjincheng121,sunjincheng121,18/Mar/19 08:12,21/Mar/19 12:23,13/Jul/23 08:05,18/Mar/19 11:48,1.8.0,1.9.0,,,,,,,1.8.0,,,,Documentation,,,0,pull-request-available,,,Add Missing dependencies in NOTICE file of flink-dist. ,,sunjincheng121,,,,,,,,,,,,,,,,,,"sunjincheng121 commented on pull request #8004: [FLINK-11950][dist]Add missing dependencies in NOTICE file of flink-d…
URL: https://github.com/apache/flink/pull/8004
 
 
   ## What is the purpose of the change
   
   This pull request add the `frocksdbjni` dependency in NOTICE file of flink-dist. 
   
   
   ## Brief change log
   
     - Add `frocksdbjni` dependency in `flink-dist/src/main/resources/META-INF/NOTICE`
     - Add `frocksdbjni` dependency in `NOTICE-binary`
   
   ## Verifying this change
   
   This change is without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: ( no )
     - The runtime per-record code paths (performance sensitive): ( no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no )
     - The S3 file system connector: ( no )
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/19 10:27;githubbot;600","asfgit commented on pull request #8004: [FLINK-11950][dist]Add missing dependencies in NOTICE file of flink-d…
URL: https://github.com/apache/flink/pull/8004
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/19 11:46;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 18 11:48:27 UTC 2019,,,,,,,,,,"0|z00t1s:",9223372036854775807,Add `frocksdbjni` dependency in NOTICE file of flink-dist.,,,,,,,,,,,,,,,,,,,"18/Mar/19 11:48;sunjincheng121;Fixed in master: 66b17aec38d40384a0316f80310c1b3d131a72b8
Fixed in release-1.8: 2d34db8ad1eb76f8651662c068a68d02c5b83d08;;;",,,,,,,,,,,,,,,,,,,,,,,
"Exception in thread ""main"" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""FROM user"" at line 1, column 17.",FLINK-11919,13221625,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,thinktothings,thinktothings,14/Mar/19 09:41,15/Mar/19 01:25,13/Jul/23 08:05,15/Mar/19 01:25,1.7.2,,,,,,,,1.7.2,,,,Table SQL / API,,,0,,,,"Register table name, can not use user, use other names, such as user1 can be normal

 

 

===========================================================

package com.opensourceteams.module.bigdata.flink.example.tableapi.test

import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.table.api.scala._
import org.apache.flink.api.scala._
import org.apache.flink.table.api.TableEnvironment

object Run {



 def main(args: Array[String]): Unit = {


 //得到批环境
 val env = ExecutionEnvironment.getExecutionEnvironment


 val dataSet = env.fromElements((""小明"",15,""男""),(""小李"",25,""女""))

 //得到Table环境
 val tableEnv = TableEnvironment.getTableEnvironment(env)
 //注册table
 tableEnv.registerDataSet(""user"",dataSet,'name,'age,'sex)



 tableEnv.sqlQuery(s""select name,age FROM user"")
 .first(100).print()


 }

}

 

===========================================================

 

Exception in thread ""main"" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""FROM user"" at line 1, column 17.
Was expecting one of:
 <EOF> 
 ""ORDER"" ...
 ""LIMIT"" ...
 ""OFFSET"" ...
 ""FETCH"" ...
 ""FROM"" <IDENTIFIER> ...
 ""FROM"" <QUOTED_IDENTIFIER> ...
 ""FROM"" <BACK_QUOTED_IDENTIFIER> ...
 ""FROM"" <BRACKET_QUOTED_IDENTIFIER> ...
 ""FROM"" <UNICODE_QUOTED_IDENTIFIER> ...
 ""FROM"" ""LATERAL"" ...
 ""FROM"" ""("" ...
 ""FROM"" ""UNNEST"" ...
 ""FROM"" ""TABLE"" ...
 "","" ...
 ""AS"" ...
 <IDENTIFIER> ...
 <QUOTED_IDENTIFIER> ...
 <BACK_QUOTED_IDENTIFIER> ...
 <BRACKET_QUOTED_IDENTIFIER> ...
 <UNICODE_QUOTED_IDENTIFIER> ...
 ""."" ...
 ""NOT"" ...
 ""IN"" ...
 ""<"" ...
 ""<="" ...
 "">"" ...
 "">="" ...
 ""="" ...
 ""<>"" ...
 ""!="" ...
 ""BETWEEN"" ...
 ""LIKE"" ...
 ""SIMILAR"" ...
 ""+"" ...
 ""-"" ...
 ""*"" ...
 ""/"" ...
 ""%"" ...
 ""||"" ...
 ""AND"" ...
 ""OR"" ...
 ""IS"" ...
 ""MEMBER"" ...
 ""SUBMULTISET"" ...
 ""CONTAINS"" ...
 ""OVERLAPS"" ...
 ""EQUALS"" ...
 ""PRECEDES"" ...
 ""SUCCEEDS"" ...
 ""MULTISET"" ...
 ""["" ...
 ""UNION"" ...
 ""INTERSECT"" ...
 ""EXCEPT"" ...
 ""MINUS"" ...
 ""("" ...
 
 at org.apache.flink.table.calcite.FlinkPlannerImpl.parse(FlinkPlannerImpl.scala:94)
 at org.apache.flink.table.api.TableEnvironment.sqlQuery(TableEnvironment.scala:743)
 at com.opensourceteams.module.bigdata.flink.example.tableapi.test.Run$.main(Run.scala:28)
 at com.opensourceteams.module.bigdata.flink.example.tableapi.test.Run.main(Run.scala)

 

 

=======================================================

 

 

!image-2019-03-14-17-41-43-840.png!","os: mac 0.14.3 

java: 1.8.0_191

scala: 2.11.12

code: https://github.com/opensourceteams/flink-maven-scala/blob/master/src/main/scala/com/opensourceteams/module/bigdata/flink/example/sql/user/Run.scala

 ",thinktothings,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/19 09:41;thinktothings;image-2019-03-14-17-41-43-840.png;https://issues.apache.org/jira/secure/attachment/12962451/image-2019-03-14-17-41-43-840.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 15 01:25:06 UTC 2019,,,,,,,,,,"0|z00p6g:",9223372036854775807," some string combinations are already reserved as keywords for future use. If you want to use one of the following strings as a field name, make sure to surround them with backticks (e.g. `value`, `count`).",,,,,,,,,,,,,,,,,,,"15/Mar/19 01:24;thinktothings; some string combinations are already reserved as keywords for future use. If you want to use one of the following strings as a field name, make sure to surround them with backticks (e.g. {{`value`}}, {{`count`}}).

-----------------------------------------------------------------------------------------------

 

package com.opensourceteams.module.bigdata.flink.example.sql.dataset.user

import org.apache.flink.api.scala.\{ExecutionEnvironment, _}
import org.apache.flink.table.api.TableEnvironment
import org.apache.flink.table.api.scala._

object Run {



 def main(args: Array[String]): Unit = {


 //得到批环境
 val env = ExecutionEnvironment.getExecutionEnvironment


 val dataSet = env.fromElements((""小明"",15,""男""),(""小李"",25,""女""))

 //得到Table环境
 val tableEnv = TableEnvironment.getTableEnvironment(env)
 //注册table
 tableEnv.registerDataSet(""user"",dataSet,'name,'age,'sex)



 //系统保留的关键字，是需要加 ` 来使用
 tableEnv.sqlQuery(s""select name,age FROM `user` "")
 .first(100).print()


 }

};;;","15/Mar/19 01:25;thinktothings; some string combinations are already reserved as keywords for future use. If you want to use one of the following strings as a field name, make sure to surround them with backticks (e.g. {{`value`}}, {{`count`}}).;;;",,,,,,,,,,,,,,,,,,,,,,
DataInputViewStream skip returns wrong value,FLINK-11915,13221570,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,SleePy,aprudhomme,aprudhomme,14/Mar/19 01:33,24/Apr/19 11:41,13/Jul/23 08:05,24/Apr/19 11:41,1.6.3,1.6.4,1.7.2,1.8.0,,,,,1.7.3,1.8.1,1.9.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Runtime / Task,,0,pull-request-available,,,"The flink-core:org.apache.flink.api.java.typeutils.runtime.DataInputViewStream overrides the InputSteam skip function. This function should be returning the actual number of bytes skipped, but there is a bug which makes it return a lower value.

The fix should be something simple like:
{code:java}
-  return n - counter - inputView.skipBytes((int) counter);
+  return n - (counter - inputView.skipBytes((int) counter));
{code}
For context, I ran into this when trying to decode an Avro record where the writer schema had fields not present in the reader schema. The decoder would attempt to skip the unneeded data in the stream, but would throw an EOFException because the return value was wrong.",,aprudhomme,SleePy,trohrmann,,,,,,,,,,,,,,,,"ifndef-SleePy commented on pull request #8195: [FLINK-11915][core] Fix miscalculation of DataInputViewStream.skip
URL: https://github.com/apache/flink/pull/8195
 
 
   ## What is the purpose of the change
   
   * Fix miscalculation of DataInputViewStream.skip. Currently it returns a wrong value.
   
   ## Brief change log
   
   * Fix the wrong calculation.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   * Add a new unit test case for DataInputViewStream.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Apr/19 07:29;githubbot;600","asfgit commented on pull request #8195: [FLINK-11915][core] Fix miscalculation of DataInputViewStream.skip
URL: https://github.com/apache/flink/pull/8195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/19 11:41;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 24 11:41:44 UTC 2019,,,,,,,,,,"0|z00ou8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/19 04:06;SleePy;Hi Andrew,

Thank you for reporting and analyzing. It's indeed a bug, will fix as soon as possible.;;;","24/Apr/19 11:41;trohrmann;Fixed via
1.9.0: 8a174833bee081f4f4a24caa5ddc5fe45996de13
1.8.1: f8a214008089d62e3bd12db857cd059367b54dd1
1.7.3: 2e97865e4b95ac0fba1471d29eaf56675d799164;;;",,,,,,,,,,,,,,,,,,,,,,
BlockCompressionTest does not compile with Java 9,FLINK-11905,13221416,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ykt836,chesnay,chesnay,13/Mar/19 14:09,02/Oct/19 17:48,13/Jul/23 08:05,14/Mar/19 13:39,1.9.0,,,,,,,,1.9.0,,,,Runtime / Task,Tests,,0,blink,pull-request-available,,"[https://travis-ci.org/apache/flink/builds/505693580?utm_source=slack&utm_medium=notification]

 
{code:java}
13:58:16.804 [INFO] -------------------------------------------------------------
13:58:16.804 [ERROR] COMPILATION ERROR : 
13:58:16.804 [INFO] -------------------------------------------------------------
13:58:16.804 [ERROR] /home/travis/build/apache/flink/flink/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/compression/BlockCompressionTest.java:[23,16] cannot find symbol
  symbol:   class Cleaner
  location: package sun.misc
13:58:16.804 [ERROR] /home/travis/build/apache/flink/flink/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/compression/BlockCompressionTest.java:[24,15] package sun.nio.ch is not visible
  (package sun.nio.ch is declared in module java.base, which does not export it to the unnamed module)
13:58:16.804 [ERROR] /home/travis/build/apache/flink/flink/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/compression/BlockCompressionTest.java:[187,17] cannot find symbol
  symbol:   class Cleaner
  location: class org.apache.flink.table.runtime.compression.BlockCompressionTest{code}",,twalthr,,,,,,,,,,,,,,,,,,"KurtYoung commented on pull request #7981: [FLINK-11905][table-runtime-blink] Fix BlockCompressionTest does not compile with Java 9
URL: https://github.com/apache/flink/pull/7981
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   Fix BlockCompressionTest does not compile with Java 9
   
   ## Brief change log
   
     - remove unnecessary dependency of `sun.misc.Cleaner`.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
   all no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 02:51;githubbot;600","KurtYoung commented on pull request #7981: [FLINK-11905][table-runtime-blink] Fix BlockCompressionTest does not compile with Java 9
URL: https://github.com/apache/flink/pull/7981
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 13:39;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 14 13:39:49 UTC 2019,,,,,,,,,,"0|z00nvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/19 14:25;twalthr;CC: [~ykt836];;;","14/Mar/19 02:22;ykt836;I will take a look and fix this, thanks for the reporting [~chesnay];;;","14/Mar/19 13:39;ykt836;fixed in c6878aca6c5aeee46581b4d6744b31049db9de95;;;",,,,,,,,,,,,,,,,,,,,,
JarRunHandler wraps all exceptions in a RestHandlerException,FLINK-11902,13221398,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,13/Mar/19 12:48,20/May/19 14:23,13/Jul/23 08:05,14/Mar/19 09:07,1.7.2,1.8.0,,,,,,,1.7.3,1.8.0,,,Runtime / REST,,,0,pull-request-available,,,"The {{JarRunHandle}} wraps every exception during the job-submission in a {{RestHandlerException}}, which should only be used for exceptions where we are aware of the underlying cause.

As a result the stacktraces from exceptions thrown during job-submission are not forwarded to users.",,wzorgdrager,,,,,,,,,,,,,,,,,,"zentol commented on pull request #7974: [FLINK-11902][rest] Do not wrap all exceptions in RestHandlerException 
URL: https://github.com/apache/flink/pull/7974
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Modifies the `JarRunHandler` to not blanket-wrap all exceptions in a `RestHandlerException`, syncing the behavior with the `JobSubmitHandler`. This prevented stacktraces from being forwarded to users.
   
   Additionally contains a small hotfix to improve logging message when dealing with unhandled exceptions.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 13:25;githubbot;600","zentol commented on pull request #7974: [FLINK-11902][rest] Do not wrap all exceptions in RestHandlerException 
URL: https://github.com/apache/flink/pull/7974
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 09:05;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 20 14:23:24 UTC 2019,,,,,,,,,,"0|z00nrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/19 09:07;chesnay;master: f3c5dd960ff81a022ece2391ed3aee86080a352a

1.8: a3e3852973f854b7e73439856f090f22f7953f80

1.7: a35bfef06f359cb369372881016c9777ff5dbda6;;;","20/May/19 14:23;wzorgdrager;Although the exception is now properly exposed in the logs it is not in the webui. iirc there will be a major update on the webui, will that include a fix for that? ;;;",,,,,,,,,,,,,,,,,,,,,,
Update the year in NOTICE files,FLINK-11901,13221377,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,13/Mar/19 11:00,13/Mar/19 14:12,13/Jul/23 08:05,13/Mar/19 14:12,,,,,,,,,1.8.0,,,,Release System,,,0,pull-request-available,,,"The {{NOTICE}} files are still starting with:

{code}
flink-table-planner
Copyright 2014-2018 The Apache Software Foundation
{code}

We need to update the year to 2019.",,twalthr,,,,,,,,,,,,,,,,,,"twalthr commented on pull request #7975:  [FLINK-11901][build] Update NOTICE files with year 2019
URL: https://github.com/apache/flink/pull/7975
 
 
   ## What is the purpose of the change
   
   Provides a script for updating the NOTICE file year and updates it to 2019.
   
   
   ## Brief change log
   
   - Script added
   - NOTICE files updated
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 13:34;githubbot;600","asfgit commented on pull request #7975:  [FLINK-11901][build] Update NOTICE files with year 2019
URL: https://github.com/apache/flink/pull/7975
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 14:03;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 13 14:12:12 UTC 2019,,,,,,,,,,"0|z00nmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/19 14:12;twalthr;Fixed in 1.9.0: 64086ab0e314481cf1ed800ebb5125256bc26e7d & b5ddf2ac80c3621300a832058d14e553184dc88a
Fixed in 1.8.0: cc7aaab11c2b3802f45853092f72866ee208e0cf & f7e7fbd72f78ef4da9b16ce9a6a022bd788ea4fa;;;",,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraphSuspendTest does not wait for all tasks to be submitted,FLINK-11897,13221330,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,moxian,moxian,moxian,13/Mar/19 08:12,04/Apr/19 13:41,13/Jul/23 08:05,04/Apr/19 13:40,,,,,,,,,1.8.1,1.9.0,,,Runtime / Coordination,Tests,,0,pull-request-available,test-stability,,"11:41:09.042 [INFO] Running org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest 11:41:11.009 [ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.964 s <<< FAILURE! - in org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest 11:41:11.010 [ERROR] testSuspendedOutOfRunning(org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest) Time elapsed: 0.052 s <<< FAILURE! java.lang.AssertionError: Expected: is <0> but: was <3> at org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.validateNoInteractions(ExecutionGraphSuspendTest.java:271) at org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.ensureCannotLeaveSuspendedState(ExecutionGraphSuspendTest.java:255) at org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.testSuspendedOutOfRunning(ExecutionGraphSuspendTest.java:110)

 

[https://api.travis-ci.org/v3/job/505154324/log.txt|https://api.travis-ci.org/v3/job/505154324/log.txt]",,moxian,,,,,,,,,,,,,,,,,,"chummyhe89 commented on pull request #7971: [FLINK-11897][tests] should wait all submitTask methods executed,befo…
URL: https://github.com/apache/flink/pull/7971
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   some test cases in `ExecutionGraphSuspendTest` validate no interactions with TaskManager， but they didn't wait until `submitTask` methods had been invoked before resetting counts.
   
   
   ## Brief change log
   
     - *add a CountDownLatch `submitLatch` in InteractionsCountingTaskManagerGateway*
     - *add a new `InteractionsCountingTaskManagerGateway` constructor with an int parameter to initialize `submitLatch`*
     - *add waitAllTasksSubmitted method to wait until all `submitTask` methods had been invoked*
   
   
   ## Verifying this change
   This change is already covered by existing tests, such as
   
    - *ExecutionGraphSuspendTest.testSuspendedOutOfDeploying*
    - *ExecutionGraphSuspendTest.testSuspendedOutOfRunning*
    - *ExecutionGraphSuspendTest.testSuspendedOutOfFailing*
    - *ExecutionGraphSuspendTest.testSuspendedOutOfFailed*
    - *ExecutionGraphSuspendTest.testSuspendedOutOfCanceling*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 09:41;githubbot;600","zentol commented on pull request #7971: [FLINK-11897][tests] should wait all submitTask methods executed,befo…
URL: https://github.com/apache/flink/pull/7971
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Apr/19 13:37;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 04 13:40:18 UTC 2019,,,,,,,,,,"0|z00ncg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/19 13:40;chesnay;master: 8a12e0f63de90375653a030304522ae0600cb3f9
1.8: 9008bea98286dff3718e743f9b78aecf63e85d14 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Port conflict when running nightly end-to-end tests,FLINK-11892,13221257,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyu,sunjincheng121,sunjincheng121,13/Mar/19 00:27,18/Mar/19 14:48,13/Jul/23 08:05,13/Mar/19 11:54,1.8.0,1.9.0,,,,,,,1.8.0,,,,Test Infrastructure,,,0,pull-request-available,,,"When I do the end-to-end check according to `[https://github.com/apache/flink/tree/master/flink-end-to-end-tests]`. I got the follows problem:

1. Executed command， and the message of console as follows:
{code:java}
FLINK_DIR=/Users/jincheng/work/FlinkRelease/1.8/flink-1.8.0/flink-dist/target/flink-1.8.0-bin/flink-1.8.0
export FLINK_DIR

sh flink-end-to-end-tests/run-nightly-tests.sh
...
...
Starting taskexecutor daemon on host jinchengsunjcs-iMac.local.
Dispatcher REST endpoint is up.
Job (180a3cfc35d549417e5807520d7402f9) is running.
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
Waiting for job to process up to 200 records, current progress: 0 records ...
{code}
2. Log info:
{code:java}
2019-03-13 07:56:33,670 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator         - Remoting shut down.

2019-03-13 07:56:33,673 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.

org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:190)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)

at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)

Caused by: java.net.BindException: Could not start actor system on any port in port range 6123

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:172)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)

at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171){code}
3. environment
MacOS: 10.14.3 
Java version ""1.8.0_151""
jinchengsunjcs-iMac:~ jincheng$ echo $0
-bash",,liyu,sunjincheng121,,,,,,,,,,,,,,,,,"carp84 commented on pull request #7972: [FLINK-11892] [docs] Refine README of end-to-end tests to make it more clear
URL: https://github.com/apache/flink/pull/7972
 
 
   ## What is the purpose of the change
   
   * This PR is a document refinement for end-to-end tests.
   
   
   ## Brief change log
   
     - Correct the markdown syntax error so as to show the `flink dir` meaning clearly.
     - Add note to prevent user run the nightly script with explicit bash command.
   
   
   ## Verifying this change
   
   This change is a document refinement without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 10:04;githubbot;600","asfgit commented on pull request #7972: [FLINK-11892] [docs] Refine README of end-to-end tests to make it more clear
URL: https://github.com/apache/flink/pull/7972
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 11:38;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,FLINK-11888,FLINK-11888,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 13 12:02:50 UTC 2019,,,,,,,,,,"0|z00mw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/19 01:33;sunjincheng121;Just now I try to use `flink-end-to-end-tests/run-nightly-tests.sh` the test passed. 
The reason is that the two commands use different shells on my machine:
{code:java}
jinchengsunjcs-iMac:~ jincheng$ sh
Sh-3.2$
Sh-3.2$ exit
Exit
jinchengsunjcs-iMac:~ jincheng$ /usr/bin/env bash
Bash-3.2$
{code}
And the `run-nightly-tests.sh` has defined ` /usr/bin/env bash`. So, I think we should improve the README doc, tell the user not to use sh to execute.
What do you think?;;;","13/Mar/19 03:56;liyu;+1, nice analysis on the root cause!

I encountered the same issue on my macbook (10.14.3 also) and confirmed that directly run the {{run-nightly-tests.sh}} script instead of using {{sh}} to launch it could resolve the problem. Also checked and confirmed in my env {{/bin/sh}} is not linked to {{/bin/bash}} although many linux system will do.

I think the current command documented in current README is correct, but adding a special note about don't explicitly use _*sh*_ to run the script will definitely help.

I've also opened a JIRA (FLINK-11888) to refine the end-to-end test document, so do you mind me to take this JIRA and link it to FLINK-11888 [~sunjincheng121]? Thanks.;;;","13/Mar/19 04:34;sunjincheng121;Hi Thanks for your nice check [~carp84] !

Of course, I am glad to hear that you want help to take this JIRA. and improve the README doc.

I have had assigned the ticket to you! :);;;","13/Mar/19 11:41;sunjincheng121;Fixed in Master: 884d26b2731bbd484006a598d360b28a4033443e
Fixed in release-1.8: 446eab513852d90f4066fefaeee5cb8ade7f0fc0;;;","13/Mar/19 12:02;liyu;More details about the difference between {{/usr/bin/env bash}} and {{/usr/bin/bash}} please refer to [this link|https://stackoverflow.com/questions/16365130/what-is-the-difference-between-usr-bin-env-bash-and-usr-bin-bash];;;",,,,,,,,,,,,,,,,,,,
Latency metrics drift apart,FLINK-11887,13221116,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Suxing Lee,Suxing Lee,Suxing Lee,12/Mar/19 13:05,05/Apr/19 12:00,13/Jul/23 08:05,22/Mar/19 10:40,1.6.3,,,,,,,,1.7.3,1.8.0,,,Runtime / Metrics,,,0,pull-request-available,,,"The operator's latency time is increased by approximately 2.7 minutes per day (see the attached).
We compute the latency by System.currentTimeMillis - marker.getMarkedTime.
There is no guarantee that System.currentTimeMillis and System.nanoTime don't drift apart.
If a GC pause or linux preemptive scheduling happenes, this should affect latency metrics.
Latency metrics drift away from their initial values with time(verify this result via the JVM Heap Dump).",,Suxing Lee,,,,,,,,,,,,,,,,,,"SuXingLee commented on pull request #7966: [FLINK-11887][metrics] Fixed latency metrics drift apart
URL: https://github.com/apache/flink/pull/7966
 
 
   ## What is the purpose of the change
   
   Use ```System.currentTimeMillis``` to replace ```System.nanoTime``` in ```LatencyMarker```,
   in order to fix latency metrics drift apart.
   
   
   ## Verifying this change
   
   LatencyMarker  only affect one latency metric.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/19 13:53;githubbot;600","zentol commented on pull request #7966: [FLINK-11887][metrics] Fixed latency metrics drift apart
URL: https://github.com/apache/flink/pull/7966
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Mar/19 10:36;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/19 13:02;Suxing Lee;flink_taskmanager_job_latency_source_id_operator_id_operator_subtask_index_1_latency.png;https://issues.apache.org/jira/secure/attachment/12962125/flink_taskmanager_job_latency_source_id_operator_id_operator_subtask_index_1_latency.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 22 10:40:06 UTC 2019,,,,,,,,,,"0|z00m0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/19 10:40;chesnay;master: fd7dc33d15e485e62ca7549cf14a10238cabed1f

1.8: 1619f89961c86a25b29e6b4d0b54cb187ed3fd91 

1.7: c655c3b275c927277799dc3ec52dcc7637d6c132 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix empty path check in StreamExEnv#readTextFile,FLINK-11867,13220636,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Tom Goong,Tom Goong,Tom Goong,09/Mar/19 15:53,15/Mar/19 12:36,13/Jul/23 08:05,11/Mar/19 14:29,,,,,,,,,1.7.3,1.8.0,,,API / DataStream,,,0,pull-request-available,,,"In class StreamExecutionEnvironment

 
{code:java}
// code placeholder
public DataStreamSource<String> readTextFile(String filePath, String charsetName) {
Preconditions.checkNotNull(filePath, ""The file path must not be null."");
Preconditions.checkNotNull(filePath.isEmpty(), ""The file path must not be empty."");

TextInputFormat format = new TextInputFormat(new Path(filePath));
format.setFilesFilter(FilePathFilter.createDefaultFilter());
TypeInformation<String> typeInfo = BasicTypeInfo.STRING_TYPE_INFO;
format.setCharsetName(charsetName);

return readFile(format, filePath, FileProcessingMode.PROCESS_ONCE, -1, typeInfo);
}
{code}
 

the *Preconditions.checkNotNull(filePath.isEmpty(), ""The file path must not be empty."");* this will not work",,dwysakowicz,Tom Goong,,,,,,,,,,,,,,,,,"Tom-Goong commented on pull request #7947: [FLINK-11867][Build System]Mistaking in checking filePath's value
URL: https://github.com/apache/flink/pull/7947
 
 
   ## What is the purpose of the change
   `		Preconditions.checkNotNull(filePath, ""The file path must not be null."");
   		Preconditions.checkNotNull(filePath.isEmpty(), ""The file path must not be empty."");`
   The second line of code has no effect
   
   ## Brief change log
   * Changing the to check the filePath's value and the hint
   
   ## Does this pull request potentially affect one of the following parts:
   * Dependencies (does it add or upgrade a dependency): **no**
   * The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
   * The serializers: **no**
   * The runtime per-record code paths (performance sensitive): **no**
   * Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**, restore state when failover using RestartPipelinedRegionStrategy.
   * The S3 file system connector: **no**
   
   ## Documentation
   * Does this pull request introduce a new feature? **no**
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 16:13;githubbot;600","dawidwys commented on pull request #7947: [FLINK-11867]Mistaking in checking filePath's value
URL: https://github.com/apache/flink/pull/7947
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 14:24;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 11 14:29:55 UTC 2019,,,,,,,,,,"0|z00j2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/19 14:29;dwysakowicz;Fixed in:
master via 2995ca16e4feef717343859248ba5992e6813c61
1.8.0: 37879f70529b3574608539b92cedb9de338a2e66
1.7.3: a93381360c06a67cd8dca16e3fb5df228c7a63bf;;;",,,,,,,,,,,,,,,,,,,,,,,
Wrong method name for registering custom types,FLINK-11866,13220635,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,romanovacca,romanovacca,romanovacca,09/Mar/19 15:51,19/Mar/19 13:21,13/Jul/23 08:05,13/Mar/19 09:19,1.7.2,,,,,,,,1.7.3,1.8.0,,,API / Python,Documentation,,0,pull-request-available,,,"The documentation about the data types for the Batch Python API is outdated. 

 
{code:java}
env.register_custom_type(MyObj, MySerializer(), MyDeserializer())
{code}
{{should be :}}
{code:java}
env.register_type(MyObj, MySerializer(), MyDeserializer()){code}",,romanovacca,,,,,,,,,,,,,,,,,,"romanovacca commented on pull request #7946: [FLINK-11866] documentation still shows old function
URL: https://github.com/apache/flink/pull/7946
 
 
   ## What is the purpose of the change
   
   *Improving the documentation*
   
   
   ## Brief change log
     - *changed the outdated example function **register_custom_type** to **register_type***
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 16:07;githubbot;600","zentol commented on pull request #7946: [FLINK-11866][python] documentation still shows old function
URL: https://github.com/apache/flink/pull/7946
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Mar/19 09:12;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 13 09:19:27 UTC 2019,,,,,,,,,,"0|z00j2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/19 09:19;chesnay;master: d158f855faaf5b5379c88ab28dc95bb83a57a47c

1.8: 42f47db34ef4a8bdb9e97e4fc60602f54bc32be7

1.7: c3488dde657816c453a7874c26897f6e7cff05e7;;;",,,,,,,,,,,,,,,,,,,,,,,
Code generation in TraversableSerializer is prohibitively slow,FLINK-11865,13220620,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,igal,aljoscha,aljoscha,09/Mar/19 13:03,13/Mar/19 12:21,13/Jul/23 08:05,12/Mar/19 16:15,,,,,,,,,1.8.0,,,,API / Type Serialization System,,,0,pull-request-available,,,"As discussed in FLINK-11539, the new code generation makes job submissions/translation prohibitively slow.

The solution should be to introduce a Cache for the generated code.",,aljoscha,jkreileder,kisimple,moxian,pnowojski,,,,,,,,,,,,,,"igalshilman commented on pull request #7957: [FLINK-11865]
URL: https://github.com/apache/flink/pull/7957
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The `TraversableSerializer` is generating, at runtime, code that evaluates to an instance of a `CanBuildFrom`. This operation makes job submissions/translation prohibitively slow, and must be cached.
   This PR adds a cache to the `TraversableSerializer`.
   
   ## Brief change log
   
     - da624a47ae Minor refactoring
     - 03f18df5ed Add and use the cache
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `TraversableSerializerSnapshotMigrationTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 22:54;githubbot;600","aljoscha commented on pull request #7957: [FLINK-11865] Improve code generation speed in TraversableSerializer
URL: https://github.com/apache/flink/pull/7957
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/19 16:16;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 13 12:21:30 UTC 2019,,,,,,,,,,"0|z00izc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/19 14:28;aljoscha;Fixed on master in
f0cdd9d2ae09061ca3866b81457ba42482704fb7;;;","12/Mar/19 16:15;aljoscha;Fixed on release-1.8 in
ce7aea06785d8e06fbacffe9c6100fe9fbb02f2a;;;","13/Mar/19 12:21;pnowojski;You might want to consider covering this by our benchmarks for future regressions?;;;",,,,,,,,,,,,,,,,,,,,,
JobMasterTriggerSavepointIT case is not executed,FLINK-11861,13220434,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tiemsn,tiemsn,08/Mar/19 10:01,26/Jun/19 09:32,13/Jul/23 08:05,26/Jun/19 09:32,1.7.2,,,,,,,,1.9.0,,,,Tests,,,0,pull-request-available,,,"The [JobMasterTriggerSavepointIT|https://github.com/apache/flink/blob/master/flink-tests/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTriggerSavepointIT.java] will not be executed as the case name does not follow the style ***ITCase.",,rmetzger,tiemsn,tison,,,,,,,,,,,,,,,,"TisonKun commented on pull request #7943: [FLINK-11861][tests] Fix JobMasterTriggerSavepointIT not executed
URL: https://github.com/apache/flink/pull/7943
 
 
   ## What is the purpose of the change
   
   Reported by @shuai-xu , `JobMasterTriggerSavepointIT` doesn't follow the pattern ""*ITCase.*"" and thus is not executed in `tests` profile(it should be).
   
   Rename `JobMasterTriggerSavepointIT` to `JobMasterTriggerSavepointITCase` should resolve this issue.
   
   ```
                                              <execution>
   						<id>integration-tests</id>
   						<phase>integration-test</phase>
   						<goals>
   							<goal>test</goal>
   						</goals>
   						<configuration>
   							<includes>
   								<include>**/*ITCase.*</include>
   							</includes>
   							<reuseForks>false</reuseForks>
   						</configuration>
   					</execution>
   ```
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector:(no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
   cc @tillrohrmann 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Mar/19 11:51;githubbot;600","StefanRRichter commented on pull request #7943: [FLINK-11861][tests] Fix JobMasterTriggerSavepointIT not executed
URL: https://github.com/apache/flink/pull/7943
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 16:53;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 30 09:24:58 UTC 2019,,,,,,,,,,"0|z00hu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/19 16:31;tison;master via a709ed24ba1e4197551d3b00e55089e0722b1827;;;","28/Mar/19 16:32;tison;[~rmetzger] I think this issue is just test scope. Why you add a {{Runtime / Coordination}} component tag?;;;","30/Apr/19 09:24;rmetzger;I'm trying to make sure that all tickets in the ""Tests"" component have a second component assigned as well. 
Usually, people do not look into the ""Tests"" component. They mostly look into components for certain areas of Flink. In this case, people feeling responsible for the runtime will also see that there's a unstable test in this area.;;;",,,,,,,,,,,,,,,,,,,,,
Race condition in EmbeddedLeaderService,FLINK-11855,13220238,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,07/Mar/19 15:35,08/Apr/19 07:58,13/Jul/23 08:05,29/Mar/19 09:41,1.7.2,1.8.0,,,,,,,1.7.3,1.8.0,,,Runtime / Coordination,,,0,pull-request-available,,,"There is a race condition in the {{EmbeddedLeaderService}} which can occur if the {{EmbeddedLeaderService}} is shut down before the {{GrantLeadershipCall}} has been executed. In this case, the {{contender}} is nulled which leads to a NPE.",,elevy,stevenz3wu,tison,trohrmann,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7935: [FLINK-11855] Fix race condition in EmbeddedLeaderService#GrantLeadershipCall
URL: https://github.com/apache/flink/pull/7935
 
 
   ## What is the purpose of the change
   
   Fix the race condition between executing EmbeddedLeaderService#GrantLeadershipCall
   and a concurrent shutdown of the leader service by making GrantLeadershipCall not
   accessing mutable state outside of a lock.
   
   ## Verifying this change
   
   - Added `EmbeddedLeaderServiceTest#testConcurrentGrantLeadershipAndShutdown`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 16:32;githubbot;600","asfgit commented on pull request #7935: [FLINK-11855] Fix race condition in EmbeddedLeaderService#GrantLeadershipCall
URL: https://github.com/apache/flink/pull/7935
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Mar/19 09:40;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 29 09:41:56 UTC 2019,,,,,,,,,,"0|z00gmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/19 09:41;trohrmann;Fixed via
1.9.0: ccf27ff845836d74bcb94a9b5baf4bc171ea2ff3
1.8.1: 8767a73ce6155923fae184813123a0a327e347fd
1.7.3: 4eb0aebb4832cc0b070afc3bcf0838aebe05578b;;;",,,,,,,,,,,,,,,,,,,,,,,
ClusterEntrypoint provides wrong executor to HaServices,FLINK-11851,13220148,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,07/Mar/19 10:24,19/Apr/20 18:08,13/Jul/23 08:05,07/Mar/19 21:49,1.7.2,1.8.0,,,,,,,1.7.3,1.8.0,,,Runtime / Coordination,,,0,pull-request-available,,,"The {{ClusterEntrypoint}} provides the executor of the common {{RpcService}} to the {{HighAvailabilityServices}} which uses the executor to run io operations. In I/O heavy cases, this can block all {{RpcService}} threads and make the {{RpcEndpoints}} running in the respective {{RpcService}} unresponsive.

I suggest to introduce a dedicated I/O executor which is used for io heavy operations.",,trohrmann,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7924: [FLINK-11851] Introduce dedicated io executor for ClusterEntrypoint and MiniCluster
URL: https://github.com/apache/flink/pull/7924
 
 
   ## What is the purpose of the change
   
   The io executor is responsible for running io operations like discarding checkpoints.
   By using the io executor, we don't risk that the `RpcService` is blocked by blocking
   io operations.
   
   cc @StefanRRichter 
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 11:38;githubbot;600","asfgit commented on pull request #7924: [FLINK-11851] Introduce dedicated io executor for ClusterEntrypoint and MiniCluster
URL: https://github.com/apache/flink/pull/7924
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 21:47;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-17248,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 07 21:49:35 UTC 2019,,,,,,,,,,"0|z00g2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/19 21:49;trohrmann;Fixed via
master: 3c7ed148cf39fa81a18832f7774365d78c3af08c
1.8.0: b7d85c87195200d5cdccb30160d7f3cecd980de8
1.7.3: 215204ca2298eb6c2d71baef5d3ea1dce17d368c;;;",,,,,,,,,,,,,,,,,,,,,,,
Duplicate job submission delete HA files,FLINK-11846,13219982,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,trohrmann,trohrmann,06/Mar/19 17:23,07/Mar/19 21:48,13/Jul/23 08:05,07/Mar/19 21:48,1.8.0,,,,,,,,,,,,Runtime / Coordination,,,0,pull-request-available,,,"Due to changes for FLINK-11383, the {{Dispatcher}} now delete HA files if the client submits twice a job. A duplicate job submission should, however, simply be rejected but not cause that HA files are being deleted.",,stephenc,trohrmann,uce,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7918: [FLINK-11846] Don't delete HA job files in case of duplicate job submission
URL: https://github.com/apache/flink/pull/7918
 
 
   ## What is the purpose of the change
   
   This commit changes the cleanup logic of the Dispatcher to only clean up job HA files
   if the job is not a duplicate (meaning that it is either running or has already been
   executed by the same JobMaster).
   
   ## Verifying this change
   
   - Added `DispatcherResourceCleanupTest#testDuplicateJobSubmissionDoesNotDeleteJobMetaData`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/19 17:48;githubbot;600","asfgit commented on pull request #7918: [FLINK-11846] Don't delete HA job files in case of duplicate job submission
URL: https://github.com/apache/flink/pull/7918
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 21:47;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 07 21:48:29 UTC 2019,,,,,,,,,,"0|z00f1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/19 21:48;trohrmann;Fixed via
master: f2b0e494e6089ccda488d6fa69cdb135a4213cf9
1.8.0: 813286bc3b27068aeb7c8dc2b98b4b6c96fbc8fb;;;",,,,,,,,,,,,,,,,,,,,,,,
Dispatcher fails to recover jobs if leader change happens during JobManagerRunner termination,FLINK-11843,13219946,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,06/Mar/19 15:32,19/May/20 14:29,13/Jul/23 08:05,25/Oct/19 15:38,1.7.2,1.8.0,1.9.0,,,,,,1.10.0,,,,Runtime / Coordination,,,2,pull-request-available,,,"The {{Dispatcher}} fails to recover jobs if a leader change happens during the {{JobManagerRunner}} termination of the previous run. The problem is that we schedule the start future of the recovered {{JobGraph}} using the {{MainThreadExecutor}} and additionally require that this future is completed before any other recovery operation from a subsequent leadership session is executed. If now the leadership changes, the {{MainThreadExecutor}} will be invalidated and the scheduled future will never be completed.

The relevant ML thread: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/1-7-1-job-stuck-in-suspended-state-td26439.html",,andrew_lin,azagrebin,basharaj,elevy,guoyangze,klion26,shixg,stevenz3wu,tison,trohrmann,uce,wind_ljy,xinpu,zhuzh,,,,,"tillrohrmann commented on pull request #9832: [FLINK-11843] Bind lifespan of Dispatcher to leader session
URL: https://github.com/apache/flink/pull/9832
 
 
   ## What is the purpose of the change
   
   This PR is based on #9828 and #9831.
   
   This PR factors leader election and job recovery out of the `Dispatcher`. The motivation for this change is to reduce the `Dispatcher's` mutable state and to limit the lifespan of a `Dispatcher` instance to the lifespan of the leader session. That way, the `Dispatcher` no longer needs to be reused across leader sessions which makes the component much simpler and easier to maintain.
   
   The responsibility of the leader election is now handled by the `DefaultDispatcherRunner` which starts a `DispatcherLeaderProcess` once it is granted leadership.
   
   The `DispatcherLeaderProcess` has two implementations: `JobDispatcherLeaderProcess` and `SessionDispatcherLeaderProcess`. The former simply starts a `DispatcherService` which is a wrapper for the `Dispatcher` with a predetermined `JobGraph`. The latter first tries to recover all persisted jobs and then passes them to the constructor of the `DispatcherService`. That way, the wrapped `Dispatcher` does not have anything to do with job recovery.
   
   The `DispatcherService` simply starts a `Dispatcher` with the provided arguments.
   
   The `DispatcherLeaderProcess` is written in a way that one can close it at any time (revocation of the leadership). All asynchronous operations will be stopped or ignored if the component has been stopped.
   
   ## Verifying this change
   
   - Added `DefaultDispatcherRunnerITCase`, `DefaultDispatcherRunnerTest`, `SessionDispatcherLeaderProcessTest`, `ZooKeeperDefaultDispatcherRunnerTest`
   -For the original problem of FLINK-11843 `DefaultDispatcherRunnerITCase#leaderChange_withBlockingJobManagerTermination_doesNotAffectNewLeader` was added
   - For the original problem of FLINK-11665 `ZooKeeperDefaultDispatcherRunnerTest#testResourceCleanupUnderLeadershipChange` was added
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (yes)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Oct/19 13:33;githubbot;600","tillrohrmann commented on pull request #9832: [FLINK-11843] Bind lifespan of Dispatcher to leader session
URL: https://github.com/apache/flink/pull/9832
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/19 15:34;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-12048,,,,,,,,,,,,,,FLINK-11665,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 25 15:38:33 UTC 2019,,,,,,,,,,"0|z00eu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/19 15:49;azagrebin;As discussed in [PR|https://github.com/apache/flink/pull/7889] for FLINK-11665, FLINK-11665 can be also resolved by the more proper Dispatcher/LeaderStore lifecycle addressed in the scope of this issue FLINK-11843.;;;","09/Apr/19 16:06;trohrmann;The problem actually not only occurs if the termination of the {{JobManagerRunner}} but also if the startup of a {{JobManagerRunner}} takes too long (e.g. if the state restore from ZooKeeper takes longer due to connection problems). The effect is the same: Due to the blocking nature of the {{JobManagerRunner}} instantiation we do execute it in a separate thread. After the creation has completed we splice the execution back into the main thread by using the {{MainThreadExecutor}} in order to update the internal {{Dispatcher}} fields. If the executor has been invalidated in the meantime, then it will never be executed and {{Dispatcher#recoveryOperation}} will never get completed.;;;","26/Apr/19 20:19;basharaj;[~till.rohrmann] do you have an update on when this issue will be fixed? we are currently impacted by https://issues.apache.org/jira/browse/FLINK-11665 which this ticket should cover. ;;;","29/Apr/19 09:14;trohrmann;I'm still working on this. I hope to get it fixed in the next 2-3 weeks. Sorry I was a bit sidetracked by other efforts.;;;","16/Jul/19 02:22;trohrmann;Made this a critial issue for 1.9.0 [~tzulitai], [~ykt836]. Let's see whether we can fix it for the upcoming release.;;;","16/Jul/19 13:43;ykt836;Thanks [~till.rohrmann];;;","18/Jul/19 06:55;tison;[~till.rohrmann] in the attached mail you said ""As a consequence the created future from the 2nd leadership will never be executed.""

I suspect that the previous main thread executor was invalidated but still the created future would be executed but failed for a mismatch fencing token? In this case there would be an error instead of stuck. I'm curious why the future was never executed instead of executed and produced an error.(UPDATE: gotcha that we schedule the start future in the MainThreadExecutor and if leader changes, the action failed but not exception returned because it is a {{tell}} and thus the future never completed)

[~stevenz3wu] could you please share the log here or send to me?;;;","18/Jul/19 17:14;stevenz3wu;[~Tison] where do I email you the log?;;;","18/Jul/19 23:38;tison;[~stevenz3wu] I have sent an email to you :- );;;","25/Oct/19 15:38;trohrmann;Fixed via

1c2097e716
32a3e5a6a0
035c6a1a2e
401f5f3970
7fcb64bf76
e3f202698f
06e79c8eb5
717b77c665
78583463e8
1f2969357c
cdd0c29102
a5438ca696
a805f1b299
c8675858f3
4e9dd7f788
a123c8f50a
2d3426f332
a17b9c0a3e
9bf06697a5
7851cb6966
c1f67ef1f2
c5dee3141c
7a23e02832
b6128ca6c8
8467acc100
cb5fba5bba
f67349db10
358d46c784
07c5b720fc
0b9382bb3b
90db87ddb9
9d447ada31
7956a14c4e
d734ac0bc6
637a6e9eae
431e2a0423
89e9b6f000
;;;",,,,,,,,,,,,,,
ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange failed,FLINK-11835,13219834,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,moxian,gjy,gjy,06/Mar/19 08:22,16/Jan/20 12:41,13/Jul/23 08:05,27/Nov/19 11:04,1.8.0,,,,,,,,1.10.0,1.8.4,1.9.2,,Runtime / Coordination,,,0,pull-request-available,test-stability,,"{noformat}
20:44:07.264 [ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 4.625 s  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (2e957dc4f49feaed042eb8b4a7932610)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:152)
Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (2e957dc4f49feaed042eb8b4a7932610)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:149)

{noformat}

https://api.travis-ci.org/v3/job/502210892/log.txt",,florianschmidt,gjy,klion26,liyu,moxian,SleePy,trohrmann,yunta,,,,,,,,,,,"chummyhe89 commented on pull request #10169: [FLINK-11835][runtime,tests] Waiting the recovered job status become …
URL: https://github.com/apache/flink/pull/10169
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   As JobManagerRunner::closeAsync runs asynchronously, The submitted jobs have a chance to become finished if the unblock method is invoked before the task is cancelled. So 
    the `ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange` test case  is not stable. This pr solves the problem.
   
   ## Brief change log
   
     - *Waiting the recovered job status become `RUNNING` before unblocking operators*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange)*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Nov/19 04:03;githubbot;600","zentol commented on pull request #10169: [FLINK-11835][runtime,tests] Waiting the recovered job status become …
URL: https://github.com/apache/flink/pull/10169
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 11:01;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,FLINK-15150,,,,,,,"14/May/19 12:26;florianschmidt;scratch_22.txt;https://issues.apache.org/jira/secure/attachment/12968677/scratch_22.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 11:04:41 UTC 2019,,,,,,,,,,"0|z00e5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/19 02:15;moxian;i can't reproduce this bug.;;;","14/Mar/19 07:23;yunta;Another instance [https://api.travis-ci.org/v3/job/505826891/log.txt];;;","08/May/19 09:23;chesnay;Another instance: https://travis-ci.org/apache/flink/jobs/529230782;;;","14/May/19 12:27;florianschmidt;After running it ~800 times I was able to reproduce the bug. Log level was set to info and I uploaded the logs;;;","01/Aug/19 12:00;chesnay;[~trohrmann@apache.org] Could this simply be a case of us querying the job status while the currently leading dispatcher is still initializing the JobMaster? This happens asynchronously ({{Dispatcher#waitForTerminatingJobManager}}); after introducing a delay I got a similar exception as we saw on Travis.;;;","02/Aug/19 13:20;trohrmann;This could be the case but without looking into the logs and trying it out it is hard to tell. I think [~florianschmidt] looked into the issue but I can't recall his latest analysis results. I'll try to reach out to him.;;;","14/Aug/19 09:41;trohrmann;The update from Florian is the following: He created a git branch to make the problem reproducible: https://github.com/florianschmidt1994/flink/tree/detect-zookeeper-it-case-bug. In particular if one lets the thread sleep in {{JobManagerRunner::closeAsync}} (line 192 ff), the problem occurred.

The problem occurs in the second iteration when running the test in a loop/repeatedly. The problem seems to be that the {{Dispatcher}} requests the {{JobStatus}} of a job which no longer exists (for whatever reason). The {{JobStatus}} future will be completed exceptionally at {{Dispatcher.java:817}} because the {{JobManagerFuture}} for the given {{JobID}} is no longer in the {{JobManagerFutures}} collection.;;;","14/Aug/19 09:41;trohrmann;[~chesnay] are you still working on this issue?
;;;","14/Aug/19 09:57;chesnay;I'm still somewhat working on it. I checked out Florian's branch and could reproduce one instance of the problem.

What seemed to happen is that the slot allocation fails on one of the first dispatchers, resulting in a failure of the job. Given that we shutdown the ResourceManager in the test it kinda makes sense that this can happen. The job was marked as done in ZK, and subsequent dispatchers never recovered the job, hence the job status can not be queried later.

The exact underlying cause for the RM shutdown usually resulting in a JM failover, but sometime in a job failure, is still unknown.;;;","02/Sep/19 08:26;trohrmann;Another instance: https://api.travis-ci.org/v3/job/579543241/log.txt;;;","29/Oct/19 03:44;yunta;Another instance but with different stack

[https://api.travis-ci.org/v3/job/603576430/log.txt]

 
{code:java}
Test testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase) failed with:
java.util.concurrent.ExecutionException: org.apache.flink.runtime.jobmaster.JobNotFinishedException: The job (34dfd9f8b7e3d6db11c0b00231555349) has been not been finished.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:135)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.flink.runtime.jobmaster.JobNotFinishedException: The job (34dfd9f8b7e3d6db11c0b00231555349) has been not been finished.
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.jobFinishedByOther(JobManagerRunnerImpl.java:247)
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.jobAlreadyDone(JobManagerRunnerImpl.java:348)
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.lambda$verifyJobSchedulingStatusAndStartJobManager$3(JobManagerRunnerImpl.java:309)
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.verifyJobSchedulingStatusAndStartJobManager(JobManagerRunnerImpl.java:306)
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.lambda$grantLeadership$2(JobManagerRunnerImpl.java:295)
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.grantLeadership(JobManagerRunnerImpl.java:292)
	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService.isLeader(ZooKeeperLeaderElectionService.java:236)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:652)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:648)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93)
	at org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:647)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:537)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:64)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:588)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:749)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:522)
	at org.apache.flink.shaded.curator.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:177)
	at org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:598)
	at org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)
{code};;;","04/Nov/19 10:44;liyu;Another instance in nightly run: https://api.travis-ci.org/v3/job/606723581/log.txt

The failure stack looks the same with [~yunta]'s post:
{noformat}
15:47:29.230 [INFO] Running org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase
15:47:43.352 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.117 s <<< FAILURE! - in org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase
15:47:43.352 [ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 14.095 s  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.jobmaster.JobNotFinishedException: The job (d03a24856cb909e579ec95c8ae607021) has been not been finished.
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:135)
Caused by: org.apache.flink.runtime.jobmaster.JobNotFinishedException: The job (d03a24856cb909e579ec95c8ae607021) has been not been finished.
{noformat};;;","13/Nov/19 04:08;moxian;{{As JobManagerRunner::closeAsync}} runs asynchronously, The submitted jobs have a chance to become finished if the unblock method is invoked before the task is cancelled.

I think we can fix this by waiting the job status to become `JobStatus.RUNNING` before we unblock   operators.;;;","27/Nov/19 11:04;chesnay;master: 4c85821cf475c97cae536d5c1262e00b18177668
1.9: e7f540c3ef399ef29576a03d33bd8d2e793116ce
1.8: a77d6fbd63cef72733d293499910c77e81bc520c ;;;",,,,,,,,,,
[checkpoint] FsCheckpointStateOutputStream might store state in files even state size below fileStateThreshold,FLINK-11829,13219617,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yunta,yunta,yunta,05/Mar/19 17:35,06/Mar/19 15:35,13/Jul/23 08:05,06/Mar/19 15:35,1.6.3,1.7.2,,,,,,,1.9.0,,,,,,,0,pull-request-available,,,"This bug might happen when we call below method
{code:java}
FsCheckpointStreamFactory.FsCheckpointStateOutputStream#write(byte[] , int , int)
{code}

If the bytes array to write occupied the size between [writeBuffer.size/2, fileStateThreshold) , it will actually flush to a file and return as {{FileStateHandle}}. However, we hope the returned result as a {{ByteStreamStateHandle}}.",,srichter,yunta,,,,,,,,,,,,,,,,,"Myasuka commented on pull request #7907: [FLINK-11829][checkpoint] Avoid FsCheckpointStateOutputStream to store state in files when size below fileStateThreshold
URL: https://github.com/apache/flink/pull/7907
 
 
   
   ## What is the purpose of the change
   
   Avoid `FsCheckpointStateOutputStream` to store state in files when size below file state threshold.
   
   
   ## Brief change log
     - fix the implementation bug of `FsCheckpointStateOutputStream#write(byte[] , int , int)`
   
   
   ## Verifying this change
   This change added tests and can be verified as follows:
   
     - Modify previous `FsCheckpointStateOutputStreamTest#testStateBelowMemThreshold` to verify this PR fix the bug.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/19 06:17;githubbot;600","StefanRRichter commented on pull request #7907: [FLINK-11829][checkpoint] Avoid FsCheckpointStateOutputStream to store state in files when size below fileStateThreshold
URL: https://github.com/apache/flink/pull/7907
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/19 15:34;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 06 15:35:01 UTC 2019,,,,,,,,,,"0|z00cu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/19 15:35;srichter;Merged in:
master: 753bc10;;;",,,,,,,,,,,,,,,,,,,,,,,
Kafka09ITCase.testRateLimitedConsumer fails on Travis,FLINK-11826,13219534,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,glaksh100,trohrmann,trohrmann,05/Mar/19 11:42,05/Apr/19 11:55,13/Jul/23 08:05,07/Mar/19 13:44,1.8.0,,,,,,,,1.8.0,,,,Connectors / Kafka,Tests,,0,pull-request-available,test-stability,,"The {{Kafka09ITCase.testRateLimitedConsumer}} fails on Travis with:
{code}
20:33:49.887 [ERROR] Errors: 
20:33:49.887 [ERROR]   Kafka09ITCase.testRateLimitedConsumer:204 Â» JobExecution Job execution failed.
{code}

https://api.travis-ci.org/v3/job/501660504/log.txt",,glaksh100,thw,trohrmann,,,,,,,,,,,,,,,,"tweise commented on pull request #7905: [FLINK-11826] Ignore flaky testRateLimitedConsumer
URL: https://github.com/apache/flink/pull/7905
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/19 20:41;githubbot;600","tweise commented on pull request #7905: [FLINK-11826] [hotfix] Ignore flaky testRateLimitedConsumer
URL: https://github.com/apache/flink/pull/7905
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/19 23:16;githubbot;600","asfgit commented on pull request #7900: [FLINK-11826] Fix flaky Kafka ratelimiter test
URL: https://github.com/apache/flink/pull/7900
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 13:45;githubbot;600",,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,FLINK-11501,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 11:55:39 UTC 2019,,,,,,,,,,"0|z00cbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/19 11:44;trohrmann;cc [~glaksh100];;;","05/Mar/19 11:44;trohrmann;The test also runs 30s on my machine. Can we try to speed it up?;;;","05/Mar/19 17:31;thw;Should we mark the test ignored until flakiness is resolved?;;;","07/Mar/19 13:44;trohrmann;Fixed via 741dfd1f792a268128c31d6f663d4b0ff3d0c933;;;","07/Mar/19 13:45;trohrmann;Thanks a lot for fixing this issue so quickly [~glaksh100]!;;;","05/Apr/19 11:55;chesnay;1.8: e42fa0e057eecb8b773e7d57d01fb59459697ffd;;;",,,,,,,,,,,,,,,,,,
TrySerializer#duplicate does not create a proper duplicate,FLINK-11823,13219511,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,05/Mar/19 10:04,05/Mar/19 12:52,13/Jul/23 08:05,05/Mar/19 12:52,1.7.2,,,,,,,,1.7.3,1.8.0,,,API / Type Serialization System,,,0,,,,"In flink 1.7.x TrySerializer#duplicate does not duplicate elemSerializer and throwableSerializer, which additionally is a KryoSerializer and therefore should always be duplicated.
",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 05 12:52:53 UTC 2019,,,,,,,,,,"0|z00c6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/19 10:07;dwysakowicz;Fixed in
master: 186b8df4155a4c171d71f1c806290bd94374416c
1.8: 186b8df4155a4c171d71f1c806290bd94374416c;;;","05/Mar/19 12:52;dwysakowicz;Fixed in 1.7.3 : 70bc26c330bb719d5fec3745605eaf5f530d18f0;;;",,,,,,,,,,,,,,,,,,,,,,
Standby per job mode Dispatchers don't know job's JobSchedulingStatus,FLINK-11813,13219310,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,trohrmann,trohrmann,04/Mar/19 15:44,01/Mar/22 14:56,13/Jul/23 08:05,01/Mar/22 14:56,1.10.3,1.11.3,1.12.4,1.13.1,1.6.4,1.7.2,1.8.0,1.9.3,1.15.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"At the moment, it can happen that standby {{Dispatchers}} in per job mode will restart a terminated job after they gained leadership. The problem is that we currently clear the {{RunningJobsRegistry}} once a job has reached a globally terminal state. After the leading {{Dispatcher}} terminates, a standby {{Dispatcher}} will gain leadership. Without having the information from the {{RunningJobsRegistry}} it cannot tell whether the job has been executed or whether the {{Dispatcher}} needs to re-execute the job. At the moment, the {{Dispatcher}} will assume that there was a fault and hence re-execute the job. This can lead to duplicate results.

I think we need some way to tell standby {{Dispatchers}} that a certain job has been successfully executed. One trivial solution could be to not clean up the {{RunningJobsRegistry}} but then we will clutter ZooKeeper.",,aitozi,Echo Lee,elevy,godfreyhe,kezhuw,klion26,knaufk,maguowei,mapohl,Paul Lin,qinjunjerry,rmetzger,stevenz3wu,tanyuxin,tison,trohrmann,wind_ljy,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21928,FLINK-21979,FLINK-21980,FLINK-19816,FLINK-23874,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 22 10:32:10 UTC 2022,,,,,,,,,,"0|z00ay0:",9223372036854775807,The issue of re-submitting a job in Application Mode when the job finished but failed during cleanup is fixed through the introduction of the new component JobResultStore which enables Flink to persist the cleanup state of a job to the file system. (see FLINK-25431),,,,,,,,,,,,,,,,,,,"05/Mar/19 02:05;tison;We would never keep forever the {{RunningJobsRegistry}} since it obviously causes resource leak.

Here, we meet the problem that if DispatcherA and DispatcherB takes the same job, after DispatcherA finished it, because the execution order, it is possible that DispatcherB re-execute the job. However, if one dispatcher finished a job, and later a new per job cluster was launched with the same job, we could regard them as two different jobs.

Thus we convert the problem to that if a job finished, the current running but not granted leadership(i.e., standby) dispatchers noticed it. Although a dispatcher not the leader should never write ZooKeeper, it was allowed to register a watcher. Then during the standby dispatcher running, if it was notified that children under {{RunningJobsRegistry}} path changes, it can react to check whether its corresponding job could be cancelled to be executed.

Follow this way, as side-effect, we don't need {{DONE}} status since it implied by a transition from {{RUNNING}} to {{PENDING(or say, not running)}}. This would simplify the logic, and currently, it is ambiguous(as this issue figured out) between {{PENDING}} and {{DONE}} (since we clean up on job finished).;;;","10/Apr/19 04:35;zhuzh;I think PENDING and RUNNING together with null(or NONE) status are enough.

If the job is PENDING or RUNNING, the *JobManagerRunner* can start the *JobMaster* once it is granted leadership.

If the job is not found, the *JobManagerRunner* should shutdown, so that a terminated job will not be restarted.

 

Currently job status is PENDING in *RunningJobsRegistry* by default, even if the job does not exist any more. But I think it should be NONE in this case.

The JobSchedulingStatus should change to be PENDING upon job submission in the Dispatcher. A *registerJob()* interface might be needed in *RunningJobsRegistry* to add the job as PENDING in this way.

When the job is globally terminated(including FINISHED/CANCELED/FAILED), we can call an *unregisterJob()* interface in *RunningJobsRegistry* to remove the job status file, rather than change it to be DONE.

 

B.T.W, Seems Flink does not use the PENDING and RUNNING status to make decision currently. They are used in the same way. While in the future we can use them to identify whether it's a JM failover.;;;","10/Apr/19 05:20;tison;[~zhuzh] It looks like an ingenious perspective. With this approach we need not to introduce a notify mechanism or clean up stage.

I'd like to clarify *how* and when {{JobSchedulingStatus}} published(modified).

1. {{NONE}} Initially, there is no znode with path and data {{running_jobs_registry/job_id(data: JobSchedulingStatus)}} and thus we recognized the job in status {{NONE}} (I'd prefer refer it as {{INVALID}}. WDYT?)

2. {{PENDING}} when a job submitted to Dispatcher, *the Dispatcher* published the job scheduling status as {{PENDING}}

3. {{RUNNING}} when a job start to schedule(i.e., a corresponding JobMaster started), *the Dispatcher* published the job scheduling status as {{RUNNING}}

4. Again {{NONE}} when a job globally terminated, *the Dispatcher* remove znode {{running_jobs_registry/job_id(data: JobSchedulingStatus)}} as publishing its status to {{NONE}}

The point is that we could a bit further when consider interfaces like {{(un)registerJob}}. Let only Dispatcher publishes the job scheduling status. Currently a RUNNING status is published by JobManagerRunner, which cause unclear of responsibility. With only Dispatcher publishes status, a JM should only read from jobs registry and decide whether and how to start the job. No extra mechanism need to be involved.;;;","10/Apr/19 09:24;trohrmann;I'm not 100% sure that this approach works. I think treating the {{DONE}} and {{NONE}} case identical leads to problems: 

1. Submitting the two jobs with the same {{JobID}} would not be detected if the first completed before the second gets submitted. 
2. How will a stand-by JM in job mode decide whether it should start executing the job or not if it sees {{NONE}}? It could either be the first JM gaining leadership or the second after another has completed the job and set the state of the job to {{NONE}} again. In the first case, it should start executing the job. In the second case, it should terminate.

I think the main problem is that the entries of the {{RunningJobsRegistry}} are bound to the lifecycle of the {{JobManagerRunner}}/{{Job}} instead of the {{Dispatcher}} and how we clean up the {{DONE}} state. I would propose to do the following:

1. Having the states: {{NONE}}/{{null}}, {{RUNNING}} and {{DONE}}
2. The {{Dispatcher}} is responsible for updating the {{RunningJobsRegistry}} entries.
3. Whenever a {{Dispatcher}} gains leadership, it will restart jobs which are in state {{NONE}} or {{RUNNING}}
4. A {{Dispatcher}} only accepts job submissions for jobs which are in state {{NONE}}
5. When a job submission is accepted/job persisted to {{SubmittedJobGraphStore}} the state is set into {{RUNNING}}
6. If a job reaches a terminal state, the {{Dispatcher}} sets the state to {{DONE}}
7. When the {{Dispatcher}} terminates, it will try to clean up the {{RunningJobsRegistry}} but only if there are no other leader contenders for the cluster id of the cluster

What do you think?;;;","10/Apr/19 11:05;tison;[~till.rohrmann] how the leader {{Dispatcher}} detects whether there are no other leader contenders for the cluster id of the cluster on terminate?;;;","10/Apr/19 11:23;tison;And as discussed in [PR#7889|https://github.com/apache/flink/pull/7889#pullrequestreview-210604709], if we want to use different Dispatcher instance between leader sessions, on the former leader terminated, possibly it found no other contenders but itself could be re-granted later. In this case we'd better not to clean up any data in zookeeper.

Further, it could be a possible race condition that even if a dispatcher can detect no other leader contenders but after the detection a contender launched.

---

For your two questions, the problem is how we identify ""the same job"". If we say, any jobs with the same job id is identical, then we always need a DONE entry. Otherwise we might identify ""the same job"" as jobs with the same job id and have overlap runtime lifecycle, that is, two job instances with the same job id but submitted separated by a long time are considered to be two different jobs. In this case we can drop the use of DONE. Besides, any of strategies mentioned above only identify ""the same job"" in the same cluster because we clean up datas in high-availability backend on cluster terminate.;;;","11/Apr/19 02:46;zhuzh;Hi Till, for your 2 questions above:

1. If one job with jobID xxx terminates, and later another job with the same jobID is submitted, I think Flink can regard it as a valid submission. Currently in our production use, there is a way that the client re-submit previously generated JobGraph to speed up the job launching, when the previous job is FAILED. In this case, job with the same ID are seen as different attempts.

   We did not handle the unexpected duplicated submission if the second submission comes after the first one is completed. Not sure in what case this may happen?

2. The process would be like this
 # submitting job -> setting status in RunningJobsRegistry to be pending in *Dispatcher* (null/NONE -> PENDING)
 # creating and launching JobManagerRunner which will try to acquire the HA leadership
 # once a JobManager is granted leadership, it changes the job status in RunningJobsRegistry to RUNNING  and starts the JobMaster(or creating a new JobMaster as proposed in FLINK-11719) (PENDING -> RUNNING)
 # when this job terminates, the JobManager removes the job from the RunningJobsRegistry (RUNNING -> NONE)

           So if it is the first time to launch the JM, the job status is PENDING so the job will be started. If it is a second time leadership gaining, and the first is completed, the job status would be NONE. Besides, if JM failover happens during the PENDING/RUNNING status, the new leader will also restart the job.

 

I totally agree that ""the main problem is that the entries of the {{RunningJobsRegistry}} are bound to the lifecycle of the {{JobManagerRunner}}/{{Job}} instead of the {{Dispatcher""}}. I think the job submission in the Dispatcher is the beginning of lifecycle.

 

I agree with your proposal too, which can well handle the unexpected submission duplications.

A few questions for the proposal:

1. as in statement 5 the job status is changed to be RUNNING already in job submission, in statement 3 should we restart the job only if it is RUNNING?

2. in statement 7, if a Dispatcher terminates as expected(user stopping it, or job finishes in MiniDispatcher), the Dispatcher can safely clean the RunningJobsRegistry. Otherwise, in unexpected shutdowns, as [~Tison] said, it may be hard to decide whether this Dispatcher is the last to to shutdown. Should we keep the RunningJobsRegistry to avoid affecting running jobs in such corner cases.

3. Seems with this change, the JobManagerLeaderElectionService is not needed any more? 

 

 ;;;","11/Apr/19 10:00;trohrmann;Before going into the details of your questions and the problems I see with the current implementation, let me quickly try to describe the bigger picture:

In HA mode, the cluster should try to execute a job until it reaches a terminal state. In case of component failover the cluster needs to check whether a job needs to be restarted or not. That's where the {{RunningJobsRegistry}} comes into play and tells the cluster what the state of a given job is. 

In case of a session cluster, the {{SubmittedJobGraphStore}} might already be enough by defining that we remove {{JobGraphs}} if they have reached a terminal state. However, in the job mode, the job won't get recovered from the {{SubmittedJobGraphStore}} but is already part of the cluster. Therefore, we need the {{RunningJobsRegistry}}.

Now coming to a bit more controversial point: I think for the sake of simplicity and better defined semantics that each job should have a {{JobID}} which uniquely identifies it in the context of a cluster. Moreover, the cluster should accept a job submission (with a specific {{JobID}}) exactly once. If one wants to submit the same topology again, then this needs to happen as a different job (meaning having a different {{JobID}} assigned). The reason is that we better isolate jobs if they are required to have different {{JobIDs}}. For example, it cannot happen that we submit a different topology under the same {{JobID}} of another job leading to checkpoint recovery failures.

What we do at the moment is that a cluster accepts jobs with the same {{JobID}} as long as they don't overlap. Hence it is possible to submit the same job again after the first execution has terminated. In that sense the current behaviour of the job mode with stand-by JMs is actually correct: After a JM finishes the execution of a job and terminates, another JM gains leadership. This JM re-executes the job, because it ""considers"" it to be a re-submission and it is correct since the two jobs don't overlap.

So in order to solve the problem, I would like to change the semantics that a cluster (session as well as job mode) only executes a job (identified by a {{JobID}}) exactly once during its lifetime. How is the lifetime of a cluster defined? 

Well, one option could be to say that each cluster, identified by a unique cluster id, has an eternal lifetime independent whether it's running or not. In that case, we could never clean up the persisted data of the {{RunningJobsRegistry}} of this cluster. This would effectively mean that we can never clean up the respective ZooKeeper znode. Moreover, in order to execute the same job again one would need to spin up a new cluster.

Another option (and a more practical one) would be to say that the lifetime of a cluster is as long as the cluster is running. When the cluster shuts down, it will clean up all persisted HA data to leave a clean slate. However, this means that until we actually shut down, we must not clean up the {{RunningJobsRegistry}} because we want to execute each job at most once during the lifetime of the cluster. With this semantic, it would be possible to start a new cluster with the same cluster id and execute a job again which has previously been executed on a cluster with the same cluster id (keep this part in mind for the later discussion of corner cases).

Assuming the latter semantics, how would this actually work? In the case on no stand-by components, this becomes trivial: Both the session cluster when it receives the graceful shut down call as well as the job mode when it finishes the job execution will simply clean up the {{RunningJobsRegistry}}. The clean up should be done by the central cluster component (currently the {{Dispatcher}}).

The more interesting part is the case with stand-by components: In the absence of a central communication mean to tell every stand-by component to terminate, only the last instance of the central cluster component should actually initiate the clean up. This requires that there is some mean to detect whether one is the last instance or not. One idea could be to look at the list of leadership contenders in order to figure this out. This is not a bullet proof strategy though: If a stand-by component loses its connection to ZooKeeper it will disappear from the list of leadership contenders. If the cluster is shut down during this time, another component will think that it is the last instance and initiate the clean up. When the other stand-by component reconnects to ZooKeeper, it will see that the {{RunningJobsRegistry}} is empty and starts executing the job. Semantically this is correct because for an outside observer it would not be possible to distinguish this case from starting a new cluster with the same cluster id and submitting the same job after the previous cluster has terminated. If we want to prevent this case from happening, we must not clean up the {{RunningJobsRegistry}} (this comes at a price of cluttering ZooKeeper).

All right, this was some lengthy explanation. Let me try to get to your questions now:

Tison's:
1. With FLINK-11843, we might start for each leader session another {{Dispatcher}}. In this case, the {{Dispatcher}} won't execute the clean up but another component which runs the different {{Dispatcher}} instances.
2. Concerning the race condition: This is fine since the newly started contender will be considered a new cluster instance.

Zhu Zhu's:
1. We could say that a job needs to be in {{RUNNING}} to be restarted. In this case, we need to make sure that the {{RunningJobsRegistry}} of a job mode cluster sets the contained job to {{RUNNING}}.
2. In case of a hard exit of the {{Dispatcher}}, we won't clean up the {{RunningJobsRegistry}}. But this also applies to other clean up operations.
3. I still think that the {{JobManagerLeaderElectionService}} is still needed because you can have potentially multiple JMs running and you need to figure out which one is currently executing the job (e.g. the TaskExecutor needs to know in order to offer its slots to the leader).

I actually think that the {{RunningJobsRegistry}} and the {{SubmittedJobGraphStore}} are quite closely related and could actually be implemented by the same class: 
* The state {{NONE}} would be if not job has with a given {{JobID}} is contained in the {{SubmittedJobGraphStore}}. 
* The state {{RUNNING}} would be if the job is contained
* The state {{DONE}} needs to be modeled explicitly;;;","11/Apr/19 12:34;zhuzh;Thanks Till for the very detailed explanation. With clear definition of job identity, I agree DONE state is truly needed in this way.

 

It will be great if we can leverage SubmittedJobGraphStore to do the RunningJobsRegistry work.

One gap here is the job mode as you mentioned. RunningJobsRegistry is independent of whether it's session mode or job mode. But for SubmittedJobGraphStore, it's hard coded to use SingleJobSubmittedJobGraphStore in MiniDispatcher(job mode), which means the SubmittedJobGraphStore in job mode is not HA.

I think there's need to unify the SubmittedJobGraphStore for session mode and job mode. 

.;;;","12/Apr/19 18:20;tison;Thanks for you clarification and explanation [~till.rohrmann]. I like the idea that bound the entries of {{RunningJobsRegistry}} to the lifecycle of cluster.

For the scope of this JIRA we can say the solution could be simply keep DONE and never cleanup it until the end of cluster's lifecycle.

Further, I'd like to share my thoughts on, ideally, how {{Dispatcher}} scheduled jobs with the help of {{SubmittedJobsGraphStore}} and {{RunningJobsRegistry}}.

----------------------------------------

For the states of {{JobSchedulingStatus}}, since we always need to represent {{DONE}} explicitly, {{RunningJobsRegistry}} is necessary. However, we can involve {{SubmittedJobGraphStore}} when {{Dispatcher}} decided when and how a job get executed.

The underlaying cause of this JIRA is how {{Dispatcher}} schedules jobs. With discussion above, I'd like to restatement how {{Dispatcher}} **should** schedule jobs with the help of {{SubmittedJobsGraphStore}} and {{RunningJobsRegistry}}, as well as that {{JobManager}} can be decoupled with {{RunningJobsRegistry}}.

 1. When a job submitted for the first time, i.e., store has no graph of it and registry status remains {{NONE}}, the {{Dispatcher}} should in order

     # Persist the job graph in job graph store
     # Set registry status of the job to {{RUNNING}}
     # launch a {{JobManagerRunner}} and tell it to start a {{JobMaster}} on granted leadership. (decouple jm with registry)

       (Either 1 or 2 fails would be consider as a failed submission and registry entry remains {{NONE}})

2. When a job submitted with {{Dispatcher}} found that its registry status not {{NONE}}, reject the submission. (statement 4)

3. Whenever a {{Dispatcher}} gains leadership, it would list entries of jobs registry, try to recover from job store with job id that has status {{RUNNING}}. (With master failover, {{Dispatcher}} would tell the jm to reconcile the job instead of direct schedule.)

4. When a job reaches a terminal state, the Dispatcher sets the state to {{DONE}}.

Statements above are similar with 7 statements of [~till.rohrmann], but clarify how {{Dispatcher}} launch a job and emphasize we can decouple jm with registry because how a job get executed can be decided by {{Dispatcher}} instead of jm.

Some comments for master failover(where {{RUNNING}} comes into use)
1. A {{Dispatcher}} recovered a job with registry status {{RUNNING}} should be launch with a jm first try to reconcile instead of directly schedule.
2. Further {{Dispatcher}} might establish heartbeat monitor with job managers to detect master failover.

----------------------------------------

As [~zhuzh] there's need to unify the SubmittedJobGraphStore for session mode and job mode. Follow our discussions, a cluster(job or session) follows the process that 1. start a dispatcher 2. submit job 3. executed job 4. finish job. If job cluster can follow this pattern, i.e., first start a dispatcher and **submit** the job graph to the dispatcher. We need not a specify {{SingleJobSubmittedJobGraphStore}} but all {{highAvailabilityServices#getSubmittedJobGraphStore}}.

This is quite out of the scope here and I think efforts should be taken more on {{CliFrontend}}.;;;","15/Apr/19 14:42;trohrmann;At the moment I would even question whether we need the {{RUNNING}} state. Without having JM reconciliation it simply is not needed. And also if we have such a feature, one could have a reconciliation period during which one does not recovers jobs. In the HA case, one would not even have to wait for this, because the leader election will make sure that there is only one JM executing the job.

Concerning point 4., it is important that the JM does not terminate (gives up leadership) until the {{Dispatcher}} has written the {{DONE}} state. Alternatively, one could allow the JM to write the {{DONE}} state. Otherwise another JM could gain leadership and sees that the state is not yet {{DONE}}.

Concerning the unification of the job and session cluster, I think we should not do this. We deliberately implemented the job cluster to not need an additional job submission step because it makes operations much easier if you don't have additional client cluster communication. Before we always had the problem that the client needs to poll the cluster status to know when to submit the job. This was quite brittle.;;;","16/Apr/19 02:53;zhuzh;I think with SubmittedJobGraphStore been a underlying layer of RunningJobsRegistry, there is no need to update the job status to RUNNING explicitly. We may wrap them in a *JobStore*(or simply extend the SubmittedJobGraphStore interface) to not only provide submitted JobGraphs but also support JobSchedulingStatus queries.

There can be only 2 operations to the store:
 # _*addJob(submittedJobGraph)*_ to add a newly submitted JobGraph
 # _*markDone(jobID)*_ to mark the job status to be DONE, which should also be stored in the SubmittedJobGraphStore (we can even drop the graph file and keep the DONE status only) (b.t.w. the word *DONE* seems to mean that the job is FINISHED, not CANCELLED or FAILED, should we use a more accurate word like *TERMINATED*?)

 

The underly status will be:
 # NONE: job graph does not exist
 # RUNNING: job graph exists and not DONE
 # DONE: job graph exists and DONE

So the JobSchedulingStatus would be transitioned as below:

*NONE* -- _addJob_ --> *RUNNING* – _markDone_ --> *DONE*

 

For job mode, we may need to change current SingleJobSubmittedJobGraphStore to an HA SubmittedJobGraphStore, which would then make the running status sharing possible.The job mode dispatcher(MiniDispatcher) should add the embedded jobGraph to the JobStore once it is granted leadership(duplicated jobGraph will be ignored).

 

 

 ;;;","25/Mar/21 17:02;trohrmann;The more I think about this problem the more I am convinced that the {{RunningJobsRegistry}} must be able to outlive a concrete {{Dispatcher}} in order to solve the standby JobManager problem. Only then it is possible to rely on the registry for filtering out job submissions/restarts for jobs which have actually be completed. This would then imply that the deployer of the Flink cluster is responsible for cleaning this registry once it has received the job result and shut down the cluster.

If the deployer/user/owner of the cluster does not do the clean up, then this would lead to orphaned entries in ZooKeeper or K8s, for example. In order to avoid this, one could make the usage of {{RunningJobsRegistry}} optional which means that one needs to activate it explicitly.;;;","02/Sep/21 13:54;trohrmann;Quick update: [~dmvk] and me are working on a FLIP to fix this problem. The general idea is to introduce a {{JobResultStore}} that can survive a Flink cluster. That way the cluster can store information about a terminated job so that other instances of the cluster know whether to re-execute the job or not.;;;","25/Nov/21 08:28;mapohl;FYI: [FLIP-194|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=195726435] was created to cover this topic;;;","22/Feb/22 10:32;mapohl;Reopening the issue because of the missing documentation.;;;",,,,,,,,
Make sure the CloseableRegistry used in backend builder is registered with task,FLINK-11804,13219245,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,liyu,liyu,04/Mar/19 09:22,29/Mar/19 10:41,13/Jul/23 08:05,04/Mar/19 19:32,1.8.0,,,,,,,,1.8.0,,,,Runtime / State Backends,,,0,,,,"In FLINK-10043 we have moved the restore process into backend builder, but the {{CloseableRegistry}} used in building the backend instance is not registered with the task, which may lead to resource leak when task is canceled before restore complete.",,liyu,srichter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10043,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 04 19:32:45 UTC 2019,,,,,,,,,,"0|z00ak0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/19 19:32;srichter;Merged in:
master: eada52be
release-1.8: e177ba9b;;;",,,,,,,,,,,,,,,,,,,,,,,
"Reject ""DISABLED"" as value for yarn.per-job-cluster.include-user-jar",FLINK-11781,13218607,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gjy,gjy,gjy,28/Feb/19 11:06,21/Jan/20 09:36,13/Jul/23 08:05,06/Mar/19 12:55,1.6.4,1.7.2,1.8.0,,,,,,1.8.0,,,,Deployment / YARN,,,0,pull-request-available,,,"*Description*
Setting {{yarn.per-job-cluster.include-user-jar: DISABLED}} in {{flink-conf.yaml}} is not supported (anymore). Doing so will lead to the job jar not being on the system classpath, which is mandatory if Flink is deployed in job mode. The job will never run.

*Expected behavior*
Documentation should reflect that setting {{yarn.per-job-cluster.include-user-jar: DISABLED}} does not work.
 

 ",,gjy,watters_fish,,,,,,,,,,,,,,,,,"GJL commented on pull request #7883: [FLINK-11781][yarn] Remove ""DISABLED"" as possible value for yarn.per-job-cluster.include-user-jar
URL: https://github.com/apache/flink/pull/7883
 
 
   ## What is the purpose of the change
   
   *This removes `DISABLED` as a possible value for the config option `yarn.per-job-cluster.include-user-jar`. *
   
   
   ## Brief change log
   
     - *See commits*
     
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Mar/19 18:17;githubbot;600","asfgit commented on pull request #7883: [FLINK-11781][yarn] Remove ""DISABLED"" as possible value for yarn.per-job-cluster.include-user-jar
URL: https://github.com/apache/flink/pull/7883
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Mar/19 09:21;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 09:36:32 UTC 2020,,,,,,,,,,"0|z006mg:",9223372036854775807,"Remove ""DISABLED"" from possible values for config option yarn.per-job-cluster.include-user-jar. This feature is broken beginning from Flink 1.5 anyways.",,,,,,,,,,,,,,,,,,,"06/Mar/19 12:55;gjy;Fixed via

1.8: 704f0a60d24ba1358656468861bd06c6ce4ddb88
1.9: 6f840082144c8a340a1ce0a820d268510e2853a1;;;","17/Jan/20 02:08;watters_fish;[~gjy] hi, In Flink 1.7,  does it support to specify yarn.per-job-cluster.include-user-jar with ""-yD yarn.per-job-cluster.include-user-jar FIRST""? 

I found that, ""-yD yarn.tags' is effective, but ""-yD yarn.per-job-cluster.include-user-jar FIRST"" isn't.;;;","21/Jan/20 09:36;gjy;[~watters_fish] There is a codepath using {{UserJarInclusion.FIRST}} [1]. However, I have not tested whether this works correctly. Because 1.7 is not supported anymore, you may want to test the feature with a newer release, such as 1.8 or 1.9. If it still doesn't work, please create a new issue in jira.

[1] https://github.com/apache/flink/blob/release-1.7/flink-yarn/src/main/java/org/apache/flink/yarn/AbstractYarnClusterDescriptor.java#L826;;;",,,,,,,,,,,,,,,,,,,,,
CLI ignores -m parameter if high-availability is ZOOKEEPER ,FLINK-11779,13218600,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,maguowei,gjy,gjy,28/Feb/19 10:40,19/Feb/21 07:24,13/Jul/23 08:05,23/Sep/20 15:12,1.7.2,1.8.0,,,,,,,1.12.0,,,,Command Line Client,,,0,pull-request-available,,,"*Description*
The CLI will ignores the host/port provided by the {{-m}} parameter if {{high-availability: ZOOKEEPER}} is configured in {{flink-conf.yaml}}

*Expected behavior*
* TBD: either document this behavior or give precedence to {{-m}}",,aljoscha,maguowei,tison,trohrmann,wangyang0918,,,,,,,,,,,,,,"leesf commented on pull request #7862: [FLINK-11779] CLI ignores -m parameter if high-availability is ZOOKEEPER
URL: https://github.com/apache/flink/pull/7862
 
 
   
   
   ## What is the purpose of the change
   
   Document CLI ignores -m parameter if high-availability is ZOOKEEPER in flink-conf.yaml.
   
   ## Brief change log
   
   Document CLI ignores -m parameter if high-availability is ZOOKEEPER in flink-conf.yaml.
   
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/19 12:19;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 24 12:59:04 UTC 2020,,,,,,,,,,"0|z006kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/19 08:10;tison;[~gjy]

Isn't ""-m"" ignored because it is run in a yarn-cluster? If so, what we need to do is excluding ""-m"" option from yarn-cluster mode document.

From {{FlinkYarnSessionCli#applyCommandLineOptionsToConfiguration}}.
{code:java}
@Override
	protected Configuration applyCommandLineOptionsToConfiguration(CommandLine commandLine) throws FlinkException {
		// we ignore the addressOption because it can only contain ""yarn-cluster""
...
{code}
;;;","05/Mar/19 09:28;gjy;[~Tison] I am not sure what you are proposing. It does not make sense to specify a host:port when submitting to a per-job cluster but if the user submits to a yarn session cluster with the {{-m host:port}} option, the client could respect the specified host:port. ;;;","05/Mar/19 16:51;tison;Oops, it's my misunderstanding.

For respecting the specified host:port, should we even use {{StandaoloneLeaderRetrivalService}} in ZOOKEEPER mode if host:port configured? Otherwise we let zookeeper find the leader.;;;","05/Mar/19 18:04;gjy;The current behavior already gives precedence to ZooKeeper. Would you leave it like that?;;;","07/Mar/19 14:03;trohrmann;I agree that the behavior is not intuitive and as a user I guess one would expect that {{-m}} has precedence.;;;","27/Jul/20 11:43;aljoscha;Turns out this is a smidge more complicated. Should only the {{-m}} parameter take precedence or also a {{rest.address}} in {{flink-conf.yaml}}? Currently, in the part of the code where we create the client HA services, we cannot differentiate between the two cases. Plus, currently {{jobmanager.rpc.address}} is the fallback option for {{rest.address}} so most setups would get {{localhost}} as the {{rest.address}}.

Also, should there be a single {{rest.address} for both the server side and client side? This is a bigger and somewhat orthogonal question, though.;;;","27/Jul/20 12:08;chesnay;[~aljoscha] {{rest.address}} is only for the client-side; the corresponding server-side setting is {{rest.bind-address}}.;;;","27/Jul/20 12:46;aljoscha;And {{jobmanager.rpc.address}}? That can't really be used from the client anymore but is still the fallback for {{rest.address}}. Maybe we should remove that as a fallback because it is the server-side config?;;;","10/Aug/20 00:45;maguowei;I am not very sure about the detailed scenarios so please correct me if I miss something. 

Maybe we should document that -m(rest.address) is only respected by the StandaloneHaService. This is also consistent with current implementation. For example the yarn/k8s does not respect the “reset.address” set by the user at all.

In the Generic CLI mode there is no -m option. (It does not use the `AbstractCustomCommandLine`) So this is also consistent with the above statement.

I agree with [~aljoscha] that maybe we should not expose the `jobmanager.rpc.address` to the client any more. (But this might introduce some incompatible problem)

 

What do you think?;;;","10/Aug/20 05:51;wangyang0918;I second [~maguowei]'s comments. AFAIK, the {{-m host:port}}, aka {{-Drest.address=host -Drest.port=port}}, should only take effect on standalone non-HA mode. For other deployments, it will always be overridden in {{YarnClusterDescriptor}} or {{KubernetesClusterDescriptor}}, which retrieves from Yarn ApplicationReport, zookeeper or Kubernetes service.

So maybe we just need to document this behavior.;;;","24/Aug/20 12:59;aljoscha;Agreed, I think we need to make this more prominent in the documentation, then, as the original creator of the issue also suggested.;;;",,,,,,,,,,,,,
KryoSerializerSnapshot would fail to deserialize if a type is missing,FLINK-11773,13218437,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,igal,igal,igal,27/Feb/19 20:08,28/Feb/19 11:22,13/Jul/23 08:05,28/Feb/19 11:22,1.8.0,,,,,,,,1.8.0,,,,,,,0,pull-request-available,,,"KryoSerializerSnapshot would fail to read successfully if a previously registered Kryo type is not accessible by the restored job (either removed from the classpath, or failed to load for some other reason)

The source of the bug is an early return at [1] ,which would skip the consumption at [2] 
 
 [1] [https://github.com/apache/flink/blob/cf7b86de436c8714414f563e8637ceb36ea7aabe/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/kryo/KryoSerializerSnapshotData.java#L310]

[2] https://github.com/apache/flink/blob/cf7b86de436c8714414f563e8637ceb36ea7aabe/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/kryo/KryoSerializerSnapshotData.java#L314
",,igal,tzulitai,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #7852: [FLINK-11773][core] Harden KryoSerializerSnapshot
URL: https://github.com/apache/flink/pull/7852
 
 
   This PR builds on top of #7845 .
   
   ## What is the purpose of the change
   
   `KryoSerializerSnapshot` would fail in `readSnapshot` if a previously registered `Kryo` type is not accessible by the restored job anymore (either removed from the classpath, or failed to load for some other reason)
   
   The source of this behaviour is an early return at [1], which would skip the consumption at [2] 
    
    [1] https://github.com/apache/flink/blob/cf7b86de436c8714414f563e8637ceb36ea7aabe/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/kryo/KryoSerializerSnapshotData.java#L310
   
   [2] https://github.com/apache/flink/blob/cf7b86de436c8714414f563e8637ceb36ea7aabe/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/kryo/KryoSerializerSnapshotData.java#L314
   
   ## Brief change log
   * The first two commits are from  #7845 and they are used by 3224e60 (KryoSnapshotTest)
   * 2b95d56 adds an `LinkedOptionalMapSerializer` to robustly read/write `LinkedOptionalMaps`
   * b4b47ef uses `LinkedOptionalMapSerializer` by `KryoSnapshotData` and `PojoSnapshotData`
   
   ## Verifying this change
   
   * A new unit test for `KryoSerializerSnapshot` was introduced.
   
   This change added tests and can be verified as follows:
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/19 20:34;githubbot;600","asfgit commented on pull request #7852: [FLINK-11773][core] Harden KryoSerializerSnapshot
URL: https://github.com/apache/flink/pull/7852
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/19 11:06;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 28 11:22:58 UTC 2019,,,,,,,,,,"0|z005ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/19 11:22;tzulitai;Merged.

1.8.0: 151cd8cf864e8fce73724c2bd3e7d188ff91c71a
1.9.0: 2a60cd7f865c5ef22452d34c673deba543200676;;;",,,,,,,,,,,,,,,,,,,,,,,
InternalTimerServiceSerializationProxy should not be serializing timers' key / namespace serializers anymore,FLINK-11772,13218327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,27/Feb/19 12:05,28/Feb/19 11:12,13/Jul/23 08:05,28/Feb/19 11:12,,,,,,,,,1.8.0,,,,API / DataStream,,,0,pull-request-available,,,"All of the changes done to managed state surrounding how we no longer Java-serialize serializers anymore and only write the serializer snapshot, as well as how serialization compatibility checks should go through the new {{TypeSerializerSnapshot}} / {{TypeSerializerSchemaCompatibility}} interfaces, was not reflected to timers.

This was mainly due to the fact that timers were not handled by state backends (and were therefore not managed state) in the past, and were handled in an isolated manner by the {{InternalTimerServiceSerializationProxy}}.

The {{InternalTimerServiceSerializationProxy}} therefore needs to be updated accordingly.",,tzulitai,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #7849: [FLINK-11772] [DataStream] InternalTimerServiceSerializationProxy should not be using the new serialization compatibility abstractions
URL: https://github.com/apache/flink/pull/7849
 
 
   ## What is the purpose of the change
   
   This PR is based on #7818. The first commit is not relevant.
   
   All of the changes done to managed state surrounding:
   - how we no longer Java-serialize serializers anymore, and only write the serializer
   snapshot
   - All schema compatibility checks of serializers go through `TypeSerializerSnapshot#resolveSchemaCompatibility(TypeSerializer)`
   were not reflected to the timers, due to the fact that timers were not handled by state backends (and were therefore not managed state) in the past, and were handled in an
   isolated manner by the {{InternalTimerServiceSerializationProxy}}.
   
   This PR updates the {{InternalTimerServiceSerializationProxy}} accordingly.
   
   ## Brief change log
   
   - ad7222e: Minor code cleanup related to method / field names
   - 7bc827b: This commit fixes compatibility checks of key / namespace serializers to go through `TypeSerializerSnapshot#resolveSchemaCompatibility`, instead of the broken `CompatibilityUtil`. It also handles serializer reconfiguration properly.
   - d0b5080: This commit upticks the version of {{InternalTimerServiceSerializationProxy}} to 2, and also introduces new `InternalTimersSnapshotWriter` and `InternalTimersSnapshotReader` for version 2, that correctly only writes serializer snapshots as state.
   
   ## Verifying this change
   
   Existing migration IT cases that use timers, such as `WindowOperatorMigrationTest` and `CEPMigrationTest` should cover this change.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? N/A
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/19 13:11;githubbot;600","asfgit commented on pull request #7849: [FLINK-11772] [DataStream] InternalTimerServiceSerializationProxy should be using the new serialization compatibility abstractions
URL: https://github.com/apache/flink/pull/7849
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/19 11:06;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 28 11:12:08 UTC 2019,,,,,,,,,,"0|z004ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/19 11:12;tzulitai;Merged.

1.8.0: a22069ff8e80678c4d7a33c3ad349f0a4499567c
1.9.0: 8b8dd8031819336200005e690ca66deaed5b1268;;;",,,,,,,,,,,,,,,,,,,,,,,
Serializer snapshot cannot be read if directly upgraded in-place to a TypeSerializerSnapshot from a TypeSerializerConfigSnapshot written in 1.7+,FLINK-11771,13218290,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,27/Feb/19 08:38,28/Feb/19 11:23,13/Jul/23 08:05,28/Feb/19 11:23,,,,,,,,,1.8.0,,,,API / Type Serialization System,,,0,,,,"This is a upgrade path that was overlooked in {{TypeSerializerSnapshot#readVersionedSnapshot}}.

If the serializer snapshot was a {{TypeSerializerConfigSnapshot}} when it was written in 1.7+ Flink versions, the actual snapshot content will be prefixed by a magic {{TypeSerializerConfigSnapshot#ADAPTER_VERSION}} int as the snapshot version, as well as Java-serialized prior serializer.

If when restoring, the serializer snapshot was upgraded in-place to a {{TypeSerializerSnapshot}} (in-place meaning, same classname, not introducing a new snapshot class), {{TypeSerializerSnapshot#readVersionedSnapshot}} doesn't work as expected.
Firstly, the provided {{readVersion}} to the user-implemented {{TypeSerializerSnapshot#readSnapshot}} method would be the magic {{ADAPTER_VERSION}}.
Secondly, the remaining stream would contain the Java-serialized prior serializer, which is no longer relevant for the user (because they already upgraded to the new abstraction and would have a fully functional {{restoreSerializer}} implementation). To workaround that, they would have to read and drop that Java-serialized serializer in user code.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 28 11:23:40 UTC 2019,,,,,,,,,,"0|z004oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/19 11:23;tzulitai;Merged.

1.8.0: 93c1515fab1a293b204f3666f1f1d86f449bf366
1.9.0: e9ec5d6729f5eb2770cc48a23cfafce9b3eb7018;;;",,,,,,,,,,,,,,,,,,,,,,,
TTL end-to-end test restores from the savepoint after the job cancelation,FLINK-11745,13217823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,azagrebin,azagrebin,25/Feb/19 13:23,21/Mar/19 12:23,13/Jul/23 08:05,26/Feb/19 12:54,1.6.4,1.7.2,1.8.0,1.9.0,,,,,1.6.5,1.7.3,1.8.0,,Runtime / State Backends,Tests,,0,pull-request-available,,,"The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.",,azagrebin,kkl0u,,,,,,,,,,,,,,,,,"azagrebin commented on pull request #7824: [FLINK-11745][State TTL][E2E] Restore from the savepoint after the job cancelation
URL: https://github.com/apache/flink/pull/7824
 
 
   ## What is the purpose of the change
   
   The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.
   
   ## Brief change log
   
   adjust test_stream_state_ttl.sh
   
   ## Verifying this change
   
   ./run-single-test.sh test-scripts/test_stream_state_ttl.sh file
   ./run-single-test.sh test-scripts/test_stream_state_ttl.sh rocks
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/19 13:44;githubbot;600","azagrebin commented on pull request #7827: [FLINK-11745][State TTL][E2E] Restore from the savepoint after the job cancelation
URL: https://github.com/apache/flink/pull/7827
 
 
   backport to 1.8
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/19 13:55;githubbot;600","azagrebin commented on pull request #7828: [FLINK-11745][State TTL][E2E] Restore from the savepoint after the job cancelation
URL: https://github.com/apache/flink/pull/7828
 
 
   backport to 1.7
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/19 14:29;githubbot;600","azagrebin commented on pull request #7829: [FLINK-11745][State TTL][E2E] Restore from the savepoint after the job cancelation
URL: https://github.com/apache/flink/pull/7829
 
 
   backport to 1.6
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Feb/19 14:30;githubbot;600","kl0u commented on pull request #7824: [FLINK-11745][State TTL][E2E] Restore from the savepoint after the job cancelation
URL: https://github.com/apache/flink/pull/7824
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/19 12:54;githubbot;600","kl0u commented on pull request #7824: [FLINK-11745][State TTL][E2E] Restore from the savepoint after the job cancelation
URL: https://github.com/apache/flink/pull/7824
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/19 12:54;githubbot;600",,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 26 12:54:42 UTC 2019,,,,,,,,,,"0|z001tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/19 12:54;kkl0u;Merged on master with a3d1f815c54f0dce89e625e329a0d0778b3c57d5

on release 1.8 with f1a1b551c782b657c38337cc43b2324b6a4f8689

on release 1.7 with 9185eec91674477e832601863500e105dcf47f5a

on release 1.6 with 950657b2e78ef98be6003c38516e1c2b9edd0397;;;",,,,,,,,,,,,,,,,,,,,,,,
Sticky E2E tests failed on travis,FLINK-11743,13217786,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,srichter,chesnay,chesnay,25/Feb/19 10:02,28/Feb/19 05:50,13/Jul/23 08:05,27/Feb/19 10:20,1.8.0,,,,,,,,1.8.0,,,,Runtime / Coordination,Runtime / State Backends,,0,pull-request-available,test-stability,,"[https://travis-ci.org/apache/flink/builds/497743828]
{code:java}
Running 'Local recovery and sticky scheduling end-to-end test'

==============================================================================

TEST_DATA_DIR: /home/travis/build/apache/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-02822890481

Flink dist directory: /home/travis/build/apache/flink/flink/flink-dist/target/flink-1.8-SNAPSHOT-bin/flink-1.8-SNAPSHOT

Running local recovery test with configuration:

       parallelism: 4

       max attempts: 10

       backend: rocks

       incremental checkpoints: true

       kill JVM: true

Starting zookeeper daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.

Starting HA cluster with 1 masters.

used deprecated key `jobmanager.heap.mb`, please replace with key `jobmanager.heap.size`

Starting standalonesession daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.

used deprecated key `taskmanager.heap.mb`, please replace with key `taskmanager.heap.size`

Starting taskexecutor daemon on host travis-job-82bee23f-b097-4080-9be9-556bd13fbfe1.

Waiting for dispatcher REST endpoint to come up...

Waiting for dispatcher REST endpoint to come up...

Waiting for dispatcher REST endpoint to come up...

Waiting for dispatcher REST endpoint to come up...

Waiting for dispatcher REST endpoint to come up...

Waiting for dispatcher REST endpoint to come up...

Waiting for dispatcher REST endpoint to come up...

Dispatcher REST endpoint is up.

Started TM watchdog with PID 25095.

Starting execution of program

Program execution finished

Job with JobID d5eec8ff1a3f52d89d9c0a14d26afb5f has finished.

Job Runtime: 1698 ms

FAILURE: Found 15 failed attempt(s) for local recovery of correctly scheduled task(s).{code}",,srichter,trohrmann,,,,,,,,,,,,,,,,,"StefanRRichter commented on pull request #7841: [FLINK-11743] Fix RocksDB incremental restore from local state after refactoring
URL: https://github.com/apache/flink/pull/7841
 
 
   ## What is the purpose of the change
   
   This PR addresses a problem with the restore of incremental RocksDB checkpoints from task-local state that was recently introduced with the refactoring of FLINK-10043. Problem was merging code branches between restoring local and remote state, so that local state directories where deleted as well, just like the temporary directories that are usually created for downloading the remote state.
   
   
   ## Brief change log
   
   - Taking care that only temp directories are deleted during restore.
   - As hotfixes some cleanups and introducing a common interface for all incremental keyed state handles that was useful for the restore method.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as the end-to-end tests for local recovery and incremental checkpoints with and without rescaling.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/19 15:21;githubbot;600","StefanRRichter commented on pull request #7841: [FLINK-11743] Fix RocksDB incremental restore from local state after refactoring
URL: https://github.com/apache/flink/pull/7841
 
 
   ## What is the purpose of the change
   
   This PR addresses a problem with the restore of incremental RocksDB checkpoints from task-local state that was recently introduced with the refactoring of FLINK-10043. Problem was merging code branches between restoring local and remote state, so that local state directories where deleted as well, just like the temporary directories that are usually created for downloading the remote state.
   
   
   ## Brief change log
   
   - Taking care that only temp directories are deleted during restore.
   - As hotfixes some cleanups and introducing a common interface for all incremental keyed state handles that was useful for the restore method.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as the end-to-end tests for local recovery and incremental checkpoints with and without rescaling.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Feb/19 15:21;githubbot;600","asfgit commented on pull request #7841: [FLINK-11743] Fix RocksDB incremental restore from local state after refactoring
URL: https://github.com/apache/flink/pull/7841
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/19 10:19;githubbot;600",,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,FLINK-10043,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 10:20:52 UTC 2019,,,,,,,,,,"0|z001lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/19 10:20;srichter;Merged in:
master: 7a078a632c
release-1.8: 060d0e6837;;;",,,,,,,,,,,,,,,,,,,,,,,
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher15e85f5d-a55d-4773-8197-f0db5658f55b#1335897563]] after [10000 ms]. Sender[null] sent,FLINK-11738,13217717,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,thinktothings,thinktothings,25/Feb/19 02:57,28/Feb/19 06:06,13/Jul/23 08:05,28/Feb/19 06:06,1.7.2,,,,,,,,,,,,Command Line Client,,,0,,,,"Akka.ask.timeout 10 seconds, this miniCluster environment is written dead, can not be changed?

---------------------------------------------------------------------------------------------------------------------------------

org.apache.flink.runtime.minicluster.MiniCluster

/**
 * Creates a new Flink mini cluster based on the given configuration.
 *
 * @param miniClusterConfiguration The configuration for the mini cluster
 */
 public MiniCluster(MiniClusterConfiguration miniClusterConfiguration) \{ this.miniClusterConfiguration = checkNotNull(miniClusterConfiguration, ""config may not be null""); this.rpcTimeout = Time.seconds(10L); this.terminationFuture = CompletableFuture.completedFuture(null); running = false; }

---------------------------------------------------------------------------------------------------------------------------------

  !image-2019-02-25-13-11-13-723.png!

 

 

---------------------------------------------------------------------------------------------------------------------------------

 

package com.opensourceteams.module.bigdata.flink.example.stream.worldcount.nc

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
 import org.apache.flink.streaming.api.windowing.time.Time

/**
 * nc -lk 1234 输入数据
 */
 object SocketWindowWordCount {

def main(args: Array[String]): Unit = {

val port = 1234
 // get the execution environment
 val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

// get input data by connecting to the socket
 val dataStream = env.socketTextStream(""localhost"", port, '\n')

import org.apache.flink.streaming.api.scala._
 val textResult = dataStream.flatMap( w => w.split(""
 s"") ).map( w => WordWithCount(w,1))
 .keyBy(""word"")
 /**
 * 每5秒刷新一次，相当于重新开始计数，
 * 好处，不需要一直拿所有的数据统计
 * 只需要在指定时间间隔内的增量数据，减少了数据规模
 */
 .timeWindow(Time.seconds(5))
 .sum(""count"" )

textResult.print().setParallelism(1)

if(args == null || args.size ==0)

{ env.execute(""默认作业"") }

else

{ env.execute(args(0)) }

println(""结束"")

}

// Data type for words with count
 case class WordWithCount(word: String, count: Long)

}

 

---------------------------------------------------------------------------------------------------------------------------------","flink 1.7.2 client

!image-2019-02-25-10-57-20-106.png!

 

!image-2019-02-25-10-57-32-876.png!

 

 

!image-2019-02-25-10-57-39-753.png!

 

 

 

 

 ",klion26,thinktothings,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/19 05:11;thinktothings;image-2019-02-25-13-11-13-723.png;https://issues.apache.org/jira/secure/attachment/12959974/image-2019-02-25-13-11-13-723.png","28/Feb/19 06:05;thinktothings;image-2019-02-28-14-05-51-875.png;https://issues.apache.org/jira/secure/attachment/12960531/image-2019-02-28-14-05-51-875.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 28 06:06:39 UTC 2019,,,,,,,,,,"0|z00168:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Feb/19 07:52;klion26;Hi, [~thinktothings] , I think 11690 has solved the problem. you can get the latest code from master branch. If the problem has been resolved, could you please close this issue.;;;","28/Feb/19 06:05;thinktothings;ok,master is ok,

!image-2019-02-28-14-05-51-875.png!;;;","28/Feb/19 06:06;thinktothings;master is ok;;;",,,,,,,,,,,,,,,,,,,,,
JSON row format is not serializable,FLINK-11727,13217478,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,22/Feb/19 16:01,15/Jul/19 13:14,13/Jul/23 08:05,15/Jul/19 13:14,1.6.3,1.7.2,,,,,,,1.9.0,,,,Table SQL / Ecosystem,,,0,pull-request-available,,,"{\{JsonRowSerializationSchema}} and \{{JsonRowDeserializationSchema}} use reference comparisons for more efficient execution, however, these do not work after serialization of the class when shipping it to a cluster. This error was not immediately visible as the logic in the else clause was also able to partially do the correct logic.",,aitozi,twalthr,,,,,,,,,,,,,,,,,"dawidwys commented on pull request #7932:  [FLINK-11727][formats] Fixed JSON format issues after serialization 
URL: https://github.com/apache/flink/pull/7932
 
 
   ## What is the purpose of the change
   
   Fixes JSON format, which did not work if it was serialized.
   
   ## Brief change log
   
    - 264922b - This commit reworks JSON format to use a runtime converter created based
   on given TypeInformation. Pre this commit conversion logic was based on
   reference comparison of TypeInformation which was not working after
   serialization of the format.
    - e513977 - Introduces builder pattern for JSON formats, prepares for making JSON formats immutable by deprecating its constructors
   
   
   ## Verifying this change
   
   * Tests for `JsonRowSerializationSchema` and `JsonRowDeserializationSchema`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**yes** / no)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 13:56;githubbot;600","asfgit commented on pull request #7932:  [FLINK-11727][formats] Fixed JSON format issues after serialization 
URL: https://github.com/apache/flink/pull/7932
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 12:58;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-12207,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 13:14:23 UTC 2019,,,,,,,,,,"0|yi1a1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 13:14;twalthr;Fixed in 1.10.0: 38e4e2b8f9bc63a793a2bddef5a578e3f80b7376
Fixed in 1.9.0: 88835341b81835fe0069c84d7172b223b85a73be;;;",,,,,,,,,,,,,,,,,,,,,,,
"use flink cli  when  {{-yn}} * {{-ys}} < {{-p}}, the applied resources are erratic",FLINK-11725,13217407,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,shengjk1,shengjk1,22/Feb/19 10:39,28/Feb/19 13:59,13/Jul/23 08:05,25/Feb/19 03:26,1.7.2,,,,,,,,1.7.2,,,,Runtime / Coordination,,,0,,,,"I provide a simple example and used resource's picture for this issue 

this application is about flink consumer kafka,  when submit job such as : 

flink-1.7.1/bin/flink  run -m yarn-cluster -yn 1   -ynm test -p 5  -cMainConnect3  ./flinkDemo-1.0-SNAPSHOT.jar &

or 

flink-1.7.1/bin/flink  run -m yarn-cluster   -ynm test  -cMainConnect4  ./flinkDemo-1.0-SNAPSHOT.jar &

from yarn web ,i see, the total resources are erratic, and in most cases, all the remaining resources of the cluster will be used by the application

 ","flink1.7.2

linux

java8

kafka1.1

scala 2.1

CDH ",shengjk1,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/19 10:20;shengjk1;1111.png;https://issues.apache.org/jira/secure/attachment/12959753/1111.png","22/Feb/19 10:20;shengjk1;2.png;https://issues.apache.org/jira/secure/attachment/12959752/2.png","22/Feb/19 10:20;shengjk1;3.png;https://issues.apache.org/jira/secure/attachment/12959751/3.png","22/Feb/19 10:30;shengjk1;MainConnect3.java;https://issues.apache.org/jira/secure/attachment/12959750/MainConnect3.java","25/Feb/19 02:53;shengjk1;MainConnect4.java;https://issues.apache.org/jira/secure/attachment/12959965/MainConnect4.java",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 25 03:26:08 UTC 2019,,,,,,,,,,"0|yi19ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/19 10:43;shengjk1;i think two ways solve this issue:

1).Add judgment about  if {{-yn}} * {{-ys}} < {{-p}} system.exit(1)  and Print message: {{-yn}} * {{-ys}}  shoule more than the {{-p}}

2).Create taskManager with one solt until {{-yn}} * {{-ys}} >= {{-p}}  as default

maybe 1) is more friendly to users;;;","25/Feb/19 03:26;shengjk1;1.7.2已修复;;;",,,,,,,,,,,,,,,,,,,,,,
ES5: ElasticsearchSinkITCase failing,FLINK-11720,13217245,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,21/Feb/19 16:12,11/Mar/20 22:47,13/Jul/23 08:05,11/Mar/20 22:47,1.10.0,1.6.4,1.8.0,1.9.0,,,,,1.11.0,,,,Connectors / ElasticSearch,,,0,pull-request-available,test-stability,,"On Google Cloud VMs (Ubuntu 18.04), the surefire tests are failing with the following error:
{code:java}
Running org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.972 sec <<< FAILURE! - in org.apache.flin
k.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase
org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase Time elapsed: 14.972 sec <<<
ERROR!
java.lang.IllegalStateException: No match found
at java.util.regex.Matcher.group(Matcher.java:536)
at org.elasticsearch.monitor.os.OsProbe.getControlGroups(OsProbe.java:213)
at org.elasticsearch.monitor.os.OsProbe.getCgroup(OsProbe.java:402)
at org.elasticsearch.monitor.os.OsProbe.osStats(OsProbe.java:454)
at org.elasticsearch.monitor.os.OsService.<init>(OsService.java:45)
at org.elasticsearch.monitor.MonitorService.<init>(MonitorService.java:45)
at org.elasticsearch.node.Node.<init>(Node.java:345)
at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl$Plug
inNode.<init>(EmbeddedElasticsearchNodeEnvironmentImpl.java:78)
at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.star
t(EmbeddedElasticsearchNodeEnvironmentImpl.java:54)
at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.prepare(Elasticsear
chSinkTestBase.java:72)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}
or

{{org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase Time elapsed: 3.196 s <<< FAILURE!}}
 {{java.lang.AssertionError: 0::/user.slice/user-1001.slice/session-33.scope}}",,dwysakowicz,jackylau,liyu,rmetzger,,,,,,,,,,,,,,,"rmetzger commented on pull request #11162: [FLINK-11720][connectors] Bump ElasticSearch5 to 5.3.3
URL: https://github.com/apache/flink/pull/11162
 
 
   ## What is the purpose of the change
   
   The ES5 connector has caused numerous issues in end to end and integration tests (on CI and during release test).
   
   
   ## Brief change log
   
     - Bump version to 5.3.3, which has the cgroup parsing issue fixed
     - adapt code to ES client API changes
     - adapt test scripts 
   
   ## Verifying this change
   
   I verified this change as follows:
   - end to end tests: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5353&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=48867695-c47f-5af3-2f21-7845611247b9
   - integration tests
   - small test setup against a ES 5.1.2 instance locally
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / *no*)
     - The serializers: (yes / *no* / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / *no* / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / *no* / don't know)
     - The S3 file system connector: (yes /*no*/ don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / *no*)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 15:26;githubbot;600","rmetzger commented on pull request #11162: [FLINK-11720][connectors] Bump ElasticSearch5 to 5.3.3
URL: https://github.com/apache/flink/pull/11162
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/20 22:46;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-13687,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 22:47:03 UTC 2020,,,,,,,,,,"0|yi18m0:",9223372036854775807,The ElasticSearch5 connector has been upgraded to version 5.3.3.,,,,,,,,,,,,,,,,,,,"21/Feb/19 16:21;dwysakowicz;I've seen it somewhere before, can't say where though :(

This is unfortunately due to bug in ES on some OSs: https://github.com/elastic/elasticsearch/issues/23486
It was fixed in ES 5.3+, unfortunately we cannot upgrade our ES5 connector to any 5.2+ version (as far as I remember) as they broke some API that we depend on.
Not sure what we can do about this one.;;;","21/Feb/19 16:43;rmetzger;Maybe you saw it in the Flink 1.6.2 release thread before? (I think Fabian mentioned it there)

Compiling against 5.3.0 fails with:
{code:java}
/home/robert/flink/flink-connectors/flink-connector-elasticsearch5/src/test/java/org/apache/flink/streaming/connectors/elasticsearch/EmbeddedElasticsearchNodeEnvironmentImpl.java:[27,39] package org.elasticsearch.node.internal does not exist{code}
If that's the only thing we depend on, I'm sure there's a workaround.

 ;;;","21/Feb/19 16:47;dwysakowicz;There is a bigger issue as far as I remember ([FLINK-7386]), we depend on API in production code that is no longer there. That's why we went for ES6 connector which can be used for 5.2+;;;","22/Feb/19 11:34;rmetzger;I was able to change the ES5 connector version to ES 5.3.0: [https://github.com/apache/flink/compare/master...rmetzger:FLINK-11720?expand=1]

The travis tests have passed for the commit: [https://travis-ci.org/rmetzger/flink/builds/496698557] 

Maybe we were directly calling the BulkProcessor ourselves in older versions?;;;","22/Feb/19 12:24;dwysakowicz;Oh, actually you're right. It seems it got reworked in a way we can upgrade ES version. Then I think this is how we should fix it.

Previously it was calling {{org.elasticsearch.action.bulk.BulkProcessor#add(org.elasticsearch.action.ActionRequest)}} for all kinds of request, and this method was removed in 5.2+;;;","22/Feb/19 12:42;rmetzger;I'm testing my change also on Google Cloud again. if it works there as well, I'll open a PR.;;;","06/Apr/19 07:02;liyu;Observed the same failure on Linux 3.10.0 when verifying release 1.8.0-rc5:
{noformat}
[ERROR] org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase  Time elapsed: 3.186 s  <<< FAILURE!
java.lang.AssertionError: 2:cpuacct,cpu,cpuset:/system.slice/sshd.service
{noformat};;;","04/Feb/20 10:44;rmetzger;Now, I saw this issue in the end to end tests as well:



{noformat}
2020-02-03T16:57:09.7903136Z ==============================================================================
2020-02-03T16:57:09.7903794Z Running 'Elasticsearch (v5.1.2) sink end-to-end test'
2020-02-03T16:57:09.7903895Z ==============================================================================
2020-02-03T16:57:09.7918327Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-09791214290
2020-02-03T16:57:09.9917909Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-02-03T16:57:10.0166226Z Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.2.tar.gz ...
2020-02-03T16:57:10.0268132Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:10.0293586Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:10.0295194Z 
2020-02-03T16:57:10.0441194Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-02-03T16:57:11.0423011Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-02-03T16:57:11.8143710Z   1 31.7M    1  543k    0     0   533k      0  0:01:00  0:00:01  0:00:59  533k
2020-02-03T16:57:11.8144973Z 100 31.7M  100 31.7M    0     0  17.7M      0  0:00:01  0:00:01 --:--:-- 17.7M
2020-02-03T16:57:12.1782169Z Waiting for Elasticsearch node to work...
2020-02-03T16:57:12.1899563Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:12.1900056Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:12.1900290Z 
2020-02-03T16:57:12.1955732Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:13.2100655Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:13.2130016Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:13.2131645Z 
2020-02-03T16:57:13.2199423Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:14.2298688Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:14.2299403Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:14.2299536Z 
2020-02-03T16:57:14.2342166Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:15.2760713Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:15.2763632Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:15.2763911Z 
2020-02-03T16:57:15.2831743Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:16.3399071Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:16.3517859Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:16.3518305Z 
2020-02-03T16:57:16.3520404Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:16.4845623Z [2020-02-03T16:57:16,483][INFO ][o.e.n.Node               ] [] initializing ...
2020-02-03T16:57:16.6859295Z [2020-02-03T16:57:16,684][INFO ][o.e.e.NodeEnvironment    ] [u39Fazq] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [11.8gb], net total_space [83.1gb], spins? [possibly], types [ext4]
2020-02-03T16:57:16.6865435Z [2020-02-03T16:57:16,686][INFO ][o.e.e.NodeEnvironment    ] [u39Fazq] heap size [1.9gb], compressed ordinary object pointers [true]
2020-02-03T16:57:16.6880762Z [2020-02-03T16:57:16,687][INFO ][o.e.n.Node               ] node name [u39Fazq] derived from node ID [u39FazqBSx6gF5LyWwoeIw]; set [node.name] to override
2020-02-03T16:57:16.6901095Z [2020-02-03T16:57:16,689][INFO ][o.e.n.Node               ] version[5.1.2], pid[51259], build[c8c4c16/2017-01-11T20:18:39.146Z], OS[Linux/5.0.0-1028-azure/amd64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_242/25.242-b20]
2020-02-03T16:57:17.4048496Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:17.4076118Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:17.4077841Z 
2020-02-03T16:57:17.4103607Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:17.5055618Z [2020-02-03T16:57:17,505][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [aggs-matrix-stats]
2020-02-03T16:57:17.5059823Z [2020-02-03T16:57:17,505][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [ingest-common]
2020-02-03T16:57:17.5064203Z [2020-02-03T16:57:17,506][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [lang-expression]
2020-02-03T16:57:17.5067827Z [2020-02-03T16:57:17,506][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [lang-groovy]
2020-02-03T16:57:17.5071296Z [2020-02-03T16:57:17,506][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [lang-mustache]
2020-02-03T16:57:17.5073878Z [2020-02-03T16:57:17,507][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [lang-painless]
2020-02-03T16:57:17.5076504Z [2020-02-03T16:57:17,507][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [percolator]
2020-02-03T16:57:17.5081331Z [2020-02-03T16:57:17,507][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [reindex]
2020-02-03T16:57:17.5085923Z [2020-02-03T16:57:17,508][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [transport-netty3]
2020-02-03T16:57:17.5088488Z [2020-02-03T16:57:17,508][INFO ][o.e.p.PluginsService     ] [u39Fazq] loaded module [transport-netty4]
2020-02-03T16:57:17.5095509Z [2020-02-03T16:57:17,509][INFO ][o.e.p.PluginsService     ] [u39Fazq] no plugins loaded
2020-02-03T16:57:18.1227426Z [2020-02-03T16:57:18,121][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]
2020-02-03T16:57:18.1228258Z org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: No match found
2020-02-03T16:57:18.1229050Z 	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:125) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1229853Z 	at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:112) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1230630Z 	at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1231260Z 	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1231815Z 	at org.elasticsearch.cli.Command.main(Command.java:88) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1232579Z 	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:89) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1233284Z 	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:82) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1233586Z Caused by: java.lang.IllegalStateException: No match found
2020-02-03T16:57:18.1233803Z 	at java.util.regex.Matcher.group(Matcher.java:536) ~[?:1.8.0_242]
2020-02-03T16:57:18.1234303Z 	at org.elasticsearch.monitor.os.OsProbe.getControlGroups(OsProbe.java:213) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1234855Z 	at org.elasticsearch.monitor.os.OsProbe.getCgroup(OsProbe.java:402) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1235407Z 	at org.elasticsearch.monitor.os.OsProbe.osStats(OsProbe.java:454) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1236003Z 	at org.elasticsearch.monitor.os.OsService.<init>(OsService.java:45) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1238605Z 	at org.elasticsearch.monitor.MonitorService.<init>(MonitorService.java:45) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1239006Z 	at org.elasticsearch.node.Node.<init>(Node.java:345) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1239289Z 	at org.elasticsearch.node.Node.<init>(Node.java:229) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1239597Z 	at org.elasticsearch.bootstrap.Bootstrap$6.<init>(Bootstrap.java:214) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1239886Z 	at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:214) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1240369Z 	at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:306) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1240705Z 	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:121) ~[elasticsearch-5.1.2.jar:5.1.2]
2020-02-03T16:57:18.1240780Z 	... 6 more
2020-02-03T16:57:18.4304460Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:18.4304739Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:18.4304783Z 
2020-02-03T16:57:18.4308990Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:19.4415688Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:19.4416628Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:19.4417066Z 
2020-02-03T16:57:19.4420744Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:20.4523273Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:20.4525962Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:20.4526191Z 
2020-02-03T16:57:20.4532402Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:21.4640906Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:21.4641509Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:21.4641664Z 
2020-02-03T16:57:21.4684761Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:22.4787130Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:22.4787738Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:22.4787862Z 
2020-02-03T16:57:22.4831270Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:23.4933056Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:23.4933733Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:23.4934285Z 
2020-02-03T16:57:23.4976456Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:24.5086803Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:24.5087552Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:24.5087721Z 
2020-02-03T16:57:24.5130945Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:25.5240246Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:25.5240971Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:25.5241637Z 
2020-02-03T16:57:25.5245009Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:26.5352903Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:26.5353591Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:26.5357401Z 
2020-02-03T16:57:26.5358200Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:27.5469937Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:27.5470793Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:27.5471065Z 
2020-02-03T16:57:27.5474384Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:28.5578695Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:28.5579469Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:28.5579687Z 
2020-02-03T16:57:28.5582885Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:29.5692834Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:29.5693592Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:29.5693833Z 
2020-02-03T16:57:29.5697072Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:30.5803565Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:30.5804371Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:30.5804596Z 
2020-02-03T16:57:30.5852156Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:31.5978806Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:31.5980411Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:31.5980701Z 
2020-02-03T16:57:31.6022605Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:32.6140206Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:32.6140963Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:32.6141248Z 
2020-02-03T16:57:32.6143956Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:33.6278064Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:33.6278886Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:33.6279293Z 
2020-02-03T16:57:33.6320400Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:34.6423791Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:34.6425149Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:34.6425430Z 
2020-02-03T16:57:34.6468169Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:35.6585988Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:35.6587057Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:35.6587449Z 
2020-02-03T16:57:35.6626740Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:36.6727962Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:36.6729031Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:36.6729285Z 
2020-02-03T16:57:36.6732037Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:37.6845335Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:37.6846572Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:37.6846810Z 
2020-02-03T16:57:37.6851139Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:38.6962492Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:38.6963633Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:38.6963927Z 
2020-02-03T16:57:38.6967649Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:39.7072258Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:39.7073366Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:39.7073622Z 
2020-02-03T16:57:39.7115305Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:40.7223410Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:40.7223671Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:40.7223931Z 
2020-02-03T16:57:40.7227432Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:41.7329644Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:41.7330279Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:41.7330447Z 
2020-02-03T16:57:41.7333485Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:42.7441678Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:42.7441989Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:42.7474417Z 
2020-02-03T16:57:42.7475272Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:43.7562899Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:43.7564216Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:43.7564741Z 
2020-02-03T16:57:43.7568818Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:44.7680818Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:44.7701240Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:44.7714577Z 
2020-02-03T16:57:44.7718331Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:45.7846552Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:45.7847123Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:45.7847209Z 
2020-02-03T16:57:45.7850506Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:46.7957005Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:46.7957614Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:46.7957929Z 
2020-02-03T16:57:46.7983469Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:47.8086218Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:47.8087012Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:47.8087298Z 
2020-02-03T16:57:47.8130108Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:48.8243516Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:48.8244074Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:48.8244214Z 
2020-02-03T16:57:48.8247583Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:49.8347051Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:49.8347840Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:49.8347998Z 
2020-02-03T16:57:49.8392300Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:50.8491219Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:50.8492085Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:50.8492312Z 
2020-02-03T16:57:50.8495784Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:51.8602948Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:51.8603555Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:51.8603809Z 
2020-02-03T16:57:51.8607138Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:52.8714750Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:52.8715461Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:52.8715546Z 
2020-02-03T16:57:52.8720517Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:53.8828376Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:53.8828730Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:53.8828795Z 
2020-02-03T16:57:53.8833592Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:54.8946193Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:54.8946706Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:54.8947247Z 
2020-02-03T16:57:54.8996055Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:55.9096394Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:55.9097094Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:55.9097326Z 
2020-02-03T16:57:55.9100977Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:56.9204679Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:56.9204979Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:56.9205061Z 
2020-02-03T16:57:56.9208899Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:57.9308127Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:57.9309047Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:57.9309282Z 
2020-02-03T16:57:57.9355335Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:58.9455068Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:58.9455888Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:58.9456141Z 
2020-02-03T16:57:58.9460244Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:57:59.9590633Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:57:59.9595823Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:57:59.9595946Z 
2020-02-03T16:57:59.9646587Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:00.9741715Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:00.9742465Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:00.9742713Z 
2020-02-03T16:58:00.9791237Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:01.9925175Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:01.9938613Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:01.9938970Z 
2020-02-03T16:58:01.9944081Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:03.0048492Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:03.0049527Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:03.0049735Z 
2020-02-03T16:58:03.0053458Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:04.0158070Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:04.0159016Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:04.0159290Z 
2020-02-03T16:58:04.0161194Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:05.0272436Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:05.0273743Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:05.0273978Z 
2020-02-03T16:58:05.0276193Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:06.0376731Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:06.0377485Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:06.0377683Z 
2020-02-03T16:58:06.0420719Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:07.0526908Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:07.0527640Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:07.0527903Z 
2020-02-03T16:58:07.0531042Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:08.0632905Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:08.0633380Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:08.0633734Z 
2020-02-03T16:58:08.0676514Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:09.0780230Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:09.0781576Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:09.0781788Z 
2020-02-03T16:58:09.0785095Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:10.0916955Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:10.0947653Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:10.0948033Z 
2020-02-03T16:58:10.0997798Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:11.1108080Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:11.1109360Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:11.1109655Z 
2020-02-03T16:58:11.1113517Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:12.1213892Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-02-03T16:58:12.1214228Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-02-03T16:58:12.1214282Z 
2020-02-03T16:58:12.1218065Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
2020-02-03T16:58:13.1241275Z Elasticsearch node is not working
2020-02-03T16:58:13.1244586Z [FAIL] Test script contains errors.
2020-02-03T16:58:13.1254193Z Checking for errors...
2020-02-03T16:58:13.1447726Z No errors in log files.
2020-02-03T16:58:13.1447860Z Checking for exceptions...
2020-02-03T16:58:13.1700454Z No exceptions in log files.
2020-02-03T16:58:13.1701187Z Checking for non-empty .out files...
2020-02-03T16:58:13.1717968Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-02-03T16:58:13.1722347Z No non-empty .out files.
2020-02-03T16:58:13.1722614Z 
2020-02-03T16:58:13.1723071Z [FAIL] 'Elasticsearch (v5.1.2) sink end-to-end test' failed after 1 minutes and 4 seconds! Test exited with exit code 1
2020-02-03T16:58:13.1723624Z 
2020-02-03T16:58:13.5425633Z No taskexecutor daemon to stop on host fv-az725.
2020-02-03T16:58:13.7691029Z No standalonesession daemon to stop on host fv-az725.
{noformat}
;;;","04/Feb/20 10:44;rmetzger;I will try to open a PR for this.;;;","10/Feb/20 13:13;rmetzger;I've unassigned myself again. What Dawid said regarding the incompatible APIs still seems to hold true. I don't think there's an easy fix. 
At some point, it might be worth considering dropping the ElasticSearch 2 connector, and bump the ES5 connector to 5.2+ ?;;;","10/Feb/20 13:28;dwysakowicz;I think it already makes sense to drop support for this connector. ES 2.x is eol for two years now: https://www.elastic.co/support/eol
I think it would be worth dropping it in 1.11.;;;","10/Feb/20 13:33;chesnay;+1 to drop ES2, it is by far the least used ES connector we have.;;;","10/Feb/20 13:43;rmetzger;I guess we need a discussion on dev@ for doing that. I can start a thread, ok?;;;","10/Feb/20 13:45;dwysakowicz;I've just done it.;;;","10/Feb/20 13:48;rmetzger;Great, thanks a lot.
I've just expressed my opinion :) ;;;","18/Feb/20 09:56;rmetzger;ES5 tests are also failing with
{code}
[INFO] Running org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase
ERROR StatusLogger Unable to create class org.apache.flink.streaming.connectors.elasticsearch5.shaded.org.apache.logging.slf4j.SLF4JLoggerContextFactory specified in file:/__w/1/s/flink-connectors/flink-connector-elasticsearch5/target/classes/META-INF/log4j-provider.properties
 java.lang.ClassNotFoundException: org.apache.flink.streaming.connectors.elasticsearch5.shaded.org.apache.logging.slf4j.SLF4JLoggerContextFactory
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at org.apache.logging.log4j.spi.Provider.loadLoggerContextFactory(Provider.java:96)
	at org.apache.logging.log4j.LogManager.<clinit>(LogManager.java:91)
	at org.elasticsearch.common.logging.ESLoggerFactory.getLogger(ESLoggerFactory.java:49)
	at org.elasticsearch.common.logging.Loggers.getLogger(Loggers.java:105)
	at org.elasticsearch.node.Node.<init>(Node.java:237)
	at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl$PluginNode.<init>(EmbeddedElasticsearchNodeEnvironmentImpl.java:78)
	at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.start(EmbeddedElasticsearchNodeEnvironmentImpl.java:54)
	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.prepare(ElasticsearchSinkTestBase.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.437 s <<< FAILURE! - in org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase
[ERROR] org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase  Time elapsed: 3.437 s  <<< FAILURE!
java.lang.AssertionError: 0::/system.slice/docker.service
{code}

I have the suspicion that it is the assertion in OsProbe.getControlGroups() that is stumbling across the cgroup definition on the build system.;;;","11/Mar/20 22:47;rmetzger;Resolved in 9c6aa3287a55636294be78798fdc61c18c84e0d6;;;",,,,,,,
Should use `String.equals()` instead of `==` for String compares in AbstractDispatcherResourceManagerComponentFactory.getHostname(),FLINK-11699,13217066,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,libenchao,libenchao,libenchao,21/Feb/19 02:34,13/Apr/21 20:39,13/Jul/23 08:05,21/Feb/19 09:20,1.7.2,,,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"In `AbstractDispatcherResourceManagerComponentFactory.getHostname()`, the compare between `rpcServiceAddress` and `""""` should use `String.equals` instead of `==`.",,libenchao,trohrmann,,,,,,,,,,,,,,,,,"libenchao commented on pull request #7783: [FLINK-11699][JobManager] Use equals for String compare in AbstractDispatcherResourceManagerComponentFactory
URL: https://github.com/apache/flink/pull/7783
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *Fix String compare bug*
   
   ## Brief change log
   
     - *Use equals for String compare in AbstractDispatcherResourceManagerComponentFactory.getHostName()*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/19 02:44;githubbot;600","asfgit commented on pull request #7783: [FLINK-11699][JobManager] Use equals for String compare in AbstractDispatcherResourceManagerComponentFactory
URL: https://github.com/apache/flink/pull/7783
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/19 09:20;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 21 09:20:10 UTC 2019,,,,,,,,,,"0|yi17i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/19 09:20;trohrmann;Fixed via 60f204bf67e348dacc79136e754f7b22e835737c;;;",,,,,,,,,,,,,,,,,,,,,,,
Use configured RPC timeout in MiniCluster,FLINK-11690,13216936,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,20/Feb/19 14:04,22/Feb/19 08:35,13/Jul/23 08:05,22/Feb/19 08:35,1.7.2,1.8.0,,,,,,,1.7.3,1.8.0,,,Runtime / Task,,,0,pull-request-available,,,The {{MiniCluster}} uses a fixed timeout of 10 seconds instead of the timeout defined in the given {{MiniClusterConfiguration}}.,,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #7788: [FLINK-11690][runtime] Use configured RPC timeout in MiniCluster
URL: https://github.com/apache/flink/pull/7788
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The MiniCluster uses a fixed timeout of 10 seconds instead of the timeout defined in the given MiniClusterConfiguration. This PR changes the constructor to respect the configured RPC timeout.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/19 09:41;githubbot;600","zentol commented on pull request #7788: [FLINK-11690][runtime] Use configured RPC timeout in MiniCluster
URL: https://github.com/apache/flink/pull/7788
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/19 08:25;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 22 08:35:29 UTC 2019,,,,,,,,,,"0|yi16pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/19 08:35;chesnay;master: 6b60e67b2ce664a6ad07667d18bda1d4fd19e40b

1.7: b0c7fe85a3168ecd156d69e486f9f4edc08aee16;;;",,,,,,,,,,,,,,,,,,,,,,,
Flink fails to remove JobGraph from ZK even though it reports it did,FLINK-11665,13216772,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,basharaj,basharaj,19/Feb/19 23:37,31/Jul/20 10:21,13/Jul/23 08:05,25/Oct/19 15:39,1.5.5,1.6.4,1.7.2,1.8.0,,,,,1.10.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"We recently have seen the following issue with Flink 1.5.5:

Given Flink Job ID 1d24cad26843dcebdfca236d5e3ad82a: 

1- A job is activated successfully and the job graph added to ZK:
{code:java}
Added SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null) to ZooKeeper.
{code}
2- Job is deactivated, Flink reports that the job graph has been successfully removed from ZK and the blob is deleted from the blob server (in this case S3):
{code:java}
Removed job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.
{code}
3- JM is later restarted, Flink for some reason attempts to recover the job that it reported earlier it has removed from ZK but since the blob has already been deleted the JM goes into a crash loop. The only way to recover it manually is to remove the job graph entry from ZK:
{code:java}
Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null).	
{code}
and
{code:java}
org.apache.flink.fs.s3presto.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 1BCDFD83FC4546A2), S3 Extended Request ID: OzZtMbihzCm1LKy99s2+rgUMxyll/xYmL6ouMvU2eo30wuDbUmj/DAWoTCs9pNNCLft0FWqbhTo= (Path: s3://blam-state-staging/flink/default/blob/job_1d24cad26843dcebdfca236d5e3ad82a/blob_p-c51b25cc0b20351f6e32a628bb6e674ee48a273e-ccfa96b0fd795502897c73714185dde3)
{code}

My question is under what circumstances would this happen? this seems to happen very infrequently but since the consequence is severe (JM crash loop) we'd like to understand how it would happen.

This  all seems a little similar to https://issues.apache.org/jira/browse/FLINK-9575 but that issue is reported fixed in Flink 1.5.2 and we are already on Flink 1.5.5",,adamonduty,azagrebin,basharaj,elevy,fwiffo,Ming Li,stevenz3wu,tison,trohrmann,,,,,,,,,,"azagrebin commented on pull request #7889: [FLINK-11665] Wait for job termination before recovery in Dispatcher
URL: https://github.com/apache/flink/pull/7889
 
 
   ## What is the purpose of the change
   
   This PR addresses a concurrency problem when dispatcher loses leadership and immediately gains it again. When dispatcher recovers the job it has to wait for the termination of previous job run. Otherwise, it can happen job recovery for the next run adds execution graph in HA store and the concurrent termination of the previous run can subsequently remove it which leads to a further inconsistent state of the storage.
   
   ## Brief change log
   
     - Change future ordering in Dispatcher methods which involve job recovery.
     - Add unit tests to check the problem
   
   ## Verifying this change
   
   run unit tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/19 11:09;githubbot;600","azagrebin commented on pull request #7889: [FLINK-11665] Wait for job termination before recovery in Dispatcher
URL: https://github.com/apache/flink/pull/7889
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 15:53;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-11225,,,,,,,,,FLINK-9575,,,,,,FLINK-11843,"20/Feb/19 22:04;basharaj;FLINK-11665.csv;https://issues.apache.org/jira/secure/attachment/12959506/FLINK-11665.csv",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 25 15:39:04 UTC 2019,,,,,,,,,,"0|yi15p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/19 04:01;tison;Do you see the log below?

{code:java}
Released locks of job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.
{code}
;;;","20/Feb/19 04:16;basharaj;[~Tison] I do, this happens right after the first recovery attempt:


{code}
1/24/19 2:59:58.457 PM: Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null).	
{code}

Followed by: 

{code}
1/24/19 2:59:58.498 PM: Released locks of job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.	
{code}

Subsequent recovery attempts are not following by the ""Released locks"" statement. ;;;","20/Feb/19 04:45;tison;If there a ""Released locks"" before ""Removed job graph""? If a ""release"" was called before ""remove"", then {{addedJobGraphs}} removed JobID 1d24cad26843dcebdfca236d5e3ad82a, which cause that when you call ""remove"", flink couldn't find your Job in {{addedJobGraphs}} so passed the ""delete"" action in ZK. However, it still printed the log ""Removed job graph"". It is a wart but can explain what you met.

It is helpful if you can share the full log (or the related fragment).;;;","20/Feb/19 16:22;basharaj;[~Tison] you're exactly right, the sequence of events is:

{code:java}
1/24/19 2:59:58.457 Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null).
{code}
{code:java}
1/24/19 2:59:58.498 PM: Released locks of job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.	
{code}
{code:java}
1/25/19 6:14:49.915 AM Removed job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper. --> misleading statement
{code}

My question now is, is this expected? why was {{releaseJobGraph(JobID)}} called after {{recoverJobGraph(JobID)}}? is it expected that it will remove the {{JobID}} from {{addedJobGraphs}}? Does that mean we shouldn't attempt to stop a job that has {{releaseJobGraph(JobID)}} already called on it?

Also I think the log message at https://github.com/apache/flink/blob/release-1.5.5/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/ZooKeeperSubmittedJobGraphStore.java#L290 is misleading as it will be logged even if the {{JobID}} wasn't in {{addedJobGraphs}};;;","20/Feb/19 18:56;fwiffo;[~basharaj], I see one code path where jobs are released after they were recovered: https://github.com/apache/flink/blob/release-1.5.5/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L657-L674

What I think is happening is that while the Dispatcher is trying to recover all jobs, it gets an exception. This triggers a release of all recovered job graphs which deletes the lock from zookeeper and removes the job from the {{addedJobGraphs}} cache in {{ZooKeeperSubmittedJobGraphStore}}. When the job is then stopped, you get the error that you noticed where the ZK state is left because the job wasn't in the cache. I don't see anywhere that the exception is logged so I'm not sure that I can confirm that one of the jobs failed to recover causing the release, but it seems most likely based on my reading of the source code.;;;","20/Feb/19 19:17;basharaj;[~fwiffo] that makes sense, [the docs for releaseJobGraph|https://ci.apache.org/projects/flink/flink-docs-release-1.5/api/java/org/apache/flink/runtime/jobmanager/SubmittedJobGraphStore.html#releaseJobGraph-org.apache.flink.api.common.JobID-] says that:

bq. Releases the locks on the specified JobGraph. Releasing the locks allows that another instance can delete the job from the SubmittedJobGraphStore.
bq. 

I am not sure if the second part didn't happen? (_another instance can delete the job from the SubmittedJobGraphStore_). It seems incorrect to me that releaseJobGraph would remove the JobID from {{addedJobGraphs}} cache without actually deleting the path in ZK. This may cause a dangling entry in ZK which is the problem reported here. ;;;","20/Feb/19 22:05;basharaj;[~Tison] I think we found what might be causing this:

1- Current Dispatcher loses leadership, this will terminate all running job managers:
{code}
2019-01-24 06:59:57.434 Dispatcher akka.tcp://flink@flink-jobmanager:6123/user/dispatcher was revoked leadership.
{code}

Notice however that this runs asynchronously, and that's a problem.

2- New Dispatcher is granted leadership with a new fencing token:
{code}
2019-01-24 06:59:58.294 Dispatcher akka.tcp://flink@flink-jobmanager:6123/user/dispatcher was granted leadership with fencing token d4ed78fe-ddda-4f5e-8255-c10e99a1c48e
{code}

3- New Dispatcher recovers job graph 1d24cad26843dcebdfca236d5e3ad82a
{code}
2019-01-24 06:59:58.457 Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null).
{code}

4- Since old Dispatcher is asynchronously terminating, it release locks of job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper after #3 completes:
{code}
2019-01-24 06:59:58.498 Released locks of job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.
{code}

This release lock removes the jobID from {{addedJobGraphs}} cache. Later when yo cancel the job it won't find it in the {{addedJobGraphs}} cache so won't remove it from ZK however it will still remove the Blob from S3.

5- JM restarts, Job Graph is in ZK but the blob is gone, JM goes into crash loop (problem reported in this Jira).

I attached the relevant logs here. [^FLINK-11665.csv] 

Any reason why revokeLeadership() is called asynchronously? this may cause a race condition when a new Dispatcher is started up.
;;;","20/Feb/19 22:47;fwiffo;That makes sense [~basharaj]. I think that's more likely than my previous theory.;;;","21/Feb/19 06:45;tison;[~basharaj] I've checked out your logs and agree with your analysis.

I'd like to involve one of our committers [~till.rohrmann] here.

It looks like that when a {{Dispatcher}} {{#revokeLeadership}} it {{#terminateJobManagerRunners}} and then {{#removeJob}}, where it calls {{#cleanUpJobData}} async by {{#CompletableFuture#thenRunAsync}}. We don't wait for its completion and if there is a concurrent {{#grantLeadership}}, we can meet the error as described above, i.e., recovered the {{SubmittedJobGraph}} and followed by an async released.;;;","21/Feb/19 09:40;trohrmann;Hi [~basharaj], thanks for the analysis. I think your analysis is correct. Instead of waiting for the previous {{JobManagerRunner}} to terminate (waiting on {{jobManagerTerminationFutures}}) before starting the new one, we actually need to do this before we are recovering jobs (this could happen in {{recoverJob}}). This should solve the problem.;;;","09/Apr/19 15:47;azagrebin;As discussed in [PR|https://github.com/apache/flink/pull/7889], this issue can be resolved by the more proper Dispatcher/LeaderStore lifecycle in the scope of another problem from FLINK-11843.;;;","25/Oct/19 15:39;trohrmann;Fixed as part of FLINK-11843;;;",,,,,,,,,,,,
Discarded checkpoint can cause Tasks to fail,FLINK-11662,13216599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,framst,framst,19/Feb/19 11:03,23/Aug/21 09:39,13/Jul/23 08:05,27/Jun/19 11:41,1.7.0,1.8.0,,,,,,,1.9.0,,,,Runtime / Checkpointing,,,0,pull-request-available,,,"Flink's {{CheckpointCoordinator}} discards an ongoing checkpoint as soon as it receives the first decline message. Part of the discard operation is the deletion of the checkpointing directory. Depending on the underlying {{FileSystem}} implementation, concurrent write and read operation to files in the checkpoint directory can then fail (e.g. this is the case with HDFS). If there is still a local checkpointing operation running for some {{Task}} and belonging to the discarded checkpoint, then it can happen that the checkpointing operation fails (e.g. an {{AsyncCheckpointRunnable}}). Depending on the configuration of the {{CheckpointExceptionHandler}}, this can lead to a task failure and a job recovery which is caused by an already discarded checkpoint.

{code:java}
2019-02-16 11:26:29.378 [Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Triggering checkpoint 1389046 @ 1550287589373 for job 599a6ac3c371874d12ebf024978cadbc.
2019-02-16 11:26:29.630 [flink-akka.actor.default-dispatcher-68] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Decline checkpoint 1389046 by task 7239e5d29203c4c720ed2db6f5db33fc of job 599a6ac3c371874d12ebf024978cadbc.
2019-02-16 11:26:29.630 [flink-akka.actor.default-dispatcher-68] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Discarding checkpoint 1389046 of job 599a6ac3c371874d12ebf024978cadbc.
org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineTaskNotReadyException: Task Source: KafkaSource -> mapOperate -> Timestamps/Watermarks (3/3) was not running
at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1166)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
2019-02-16 11:26:29.697 [flink-akka.actor.default-dispatcher-68] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: KafkaSource -> mapOperate -> Timestamps/Watermarks (1/3) (a5657b784d235731cd468164e85d0b50) switched from RUNNING to FAILED.
org.apache.flink.streaming.runtime.tasks.AsynchronousException: java.lang.Exception: Could not materialize checkpoint 1389046 for operator Source: KafkaSource -> mapOperate -> Timestamps/Watermarks (1/3).
at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointExceptionHandler.tryHandleCheckpointException(StreamTask.java:1153)
at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointRunnable.handleExecutionException(StreamTask.java:947)
at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointRunnable.run(StreamTask.java:884)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Could not materialize checkpoint 1389046 for operator Source: KafkaSource -> mapOperate -> Timestamps/Watermarks (1/3).
at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointRunnable.handleExecutionException(StreamTask.java:942)
... 6 common frames omitted
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Could not flush and close the file system output stream to hdfs://.../flink/checkpoints/599a6ac3c371874d12ebf024978cadbc/chk-1389046/84631771-01e2-41bc-950d-c9e39eac26f9 in order to obtain the stream state handle
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.flink.util.FutureUtil.runIfNotDoneAndGet(FutureUtil.java:53)
at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:53)
at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointRunnable.run(StreamTask.java:853)
... 5 common frames omitted
Caused by: java.io.IOException: Could not flush and close the file system output stream to hdfs://.../flink/checkpoints/599a6ac3c371874d12ebf024978cadbc/chk-1389046/84631771-01e2-41bc-950d-c9e39eac26f9 in order to obtain the stream state handle
at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.closeAndGetHandle(FsCheckpointStreamFactory.java:326)
at org.apache.flink.runtime.state.DefaultOperatorStateBackend$DefaultOperatorStateBackendSnapshotStrategy$1.callInternal(DefaultOperatorStateBackend.java:767)
at org.apache.flink.runtime.state.DefaultOperatorStateBackend$DefaultOperatorStateBackendSnapshotStrategy$1.callInternal(DefaultOperatorStateBackend.java:696)
at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:76)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at org.apache.flink.util.FutureUtil.runIfNotDoneAndGet(FutureUtil.java:50)
... 7 common frames omitted
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: Path doesn't exist: /.../flink/checkpoints/599a6ac3c371874d12ebf024978cadbc/chk-1389046/84631771-01e2-41bc-950d-c9e39eac26f9
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkFilePath(FSNamesystem.java:3063)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3089)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3043)
at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:938)
at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:743)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1175)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1171)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1169)

at org.apache.hadoop.ipc.Client.call(Client.java:863)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:227)
at com.sun.proxy.$Proxy5.complete(Unknown Source)
at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
at com.sun.proxy.$Proxy5.complete(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient.closeFile(DFSClient.java:1208)
at org.apache.hadoop.hdfs.DFSClient.access$5300(DFSClient.java:151)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:6693)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:6567)
at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
at org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.close(HadoopDataOutputStream.java:52)
at org.apache.flink.core.fs.ClosingFSDataOutputStream.close(ClosingFSDataOutputStream.java:64)
at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.closeAndGetHandle(FsCheckpointStreamFactory.java:312)
... 12 common frames omitted
{code}

Especially for larger jobs with sources which recover state and, hence, take some time before they accept a checkpoint request, this can lead to an unstable job which can be stuck in some restart loop. 

A workaround for this problem is to disable {{failTaskOnCheckpointError}} in the {{ExecutionConfig}} via {{ExecutionConfig#setFailTaskOnCheckpointError(false)}}. With this setting checkpoint failures won't fail the owning {{Task}}.

In order to properly solve this problem, failing local checkpoint operations belonging to a discarded checkpoint should simply be ignored. A good solution could be to centralize the checkpoint operation failure handling in the {{CheckpointCoordinator}}. The {{CheckpointCoordinator}} knows which checkpoints are still valid and, hence, can distinguish between valid and invalid checkpoint operation failures. It would, furthermore, allow to implement more sophisticated failure handling strategies such as accepting {{n}} checkpoint failures before failing a task.",,aitozi,aljoscha,framst,klion26,liyu,Ming Li,Obradovic,Paul Lin,srichter,stevenz3wu,trohrmann,walteryi,xymaqingxiang,yunta,,,,,"StefanRRichter commented on pull request #8745: [FLINK-11662] Disable task to fail on checkpoint errors
URL: https://github.com/apache/flink/pull/8745
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jun/19 11:41;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,FLINK-12209,,,,FLINK-5820,,,,,FLINK-12194,FLINK-10724,FLINK-10855,FLINK-10615,FLINK-23916,,,"28/Mar/19 14:26;trohrmann;jobmanager.log;https://issues.apache.org/jira/secure/attachment/12964042/jobmanager.log","28/Mar/19 14:28;trohrmann;taskmanager.log;https://issues.apache.org/jira/secure/attachment/12964044/taskmanager.log",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 27 11:41:55 UTC 2019,,,,,,,,,,"0|yi14mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/19 02:18;klion26;Hi [~framst] , I think there are some other issues about the same problem.

And there is an issue https://issues.apache.org/jira/browse/FLINK-10724 to refactor the error handling after the issue has been resolved, we can handle this problem elegantly.;;;","20/Feb/19 06:14;framst;[~klion26] thanks for the infomation. I have read the design documents but this exception is caused by checkpointCoordinator. should the checkpointCoordinator drop checkpointDirectory as a whole on a failure but know nothing about whether tasks are perferming checkpoint. Is this situation should be changed?  [~sewen];;;","20/Feb/19 06:59;yunta;[~framst] we already have some discussion about this problem that checkpoint-coordinator deletes task writing directory in [FLINK-10930|https://issues.apache.org/jira/browse/FLINK-10930].

I think we should now separate this problem into [FLINK-10724|https://issues.apache.org/jira/browse/FLINK-10724] and [FLINK-10855|https://issues.apache.org/jira/browse/FLINK-10855]. Any suggestion is welcome :) .;;;","28/Mar/19 14:17;aljoscha;This also affects 1.5.x and 1.6.x, but those versions are archived so I can't add add to the ""affects versions"" field.;;;","28/Mar/19 15:04;chesnay;if you really want to you can unarchive them, set the affects version and archive them again.;;;","27/Jun/19 11:41;srichter;Merged in:
master: b760d55;;;",,,,,,,,,,,,,,,,,,
TaskExecutor does not wait for Task termination when terminating itself,FLINK-11630,13216025,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kisimple,trohrmann,trohrmann,15/Feb/19 15:49,20/Nov/20 12:14,13/Jul/23 08:05,18/Aug/19 14:21,1.8.0,,,,,,,,1.10.0,1.9.1,,,Runtime / Coordination,,,0,pull-request-available,,,"The {{TaskExecutor}} does not properly wait for the termination of {{Tasks}} when terminating. In fact, it does not even trigger the cancellation of the running {{Tasks}}. I think for better lifecycle management it is important that the {{TaskExecutor}} triggers the termination of all running {{Tasks}} and then wait until all {{Tasks}} have terminated before it terminates itself.",,hwanju,kisimple,moxian,tison,trohrmann,,,,,,,,,,,,,,"kisimple commented on pull request #7757: [FLINK-11630] Triggers the termination of all running Tasks when shutting down TaskExecutor
URL: https://github.com/apache/flink/pull/7757
 
 
   ## What is the purpose of the change
   
   Currently, the `TaskExecutor` does not properly wait for the termination of `Task`s when terminating. This patch triggers the termination of all running `Task`s when shutting down `TaskExecutor`.
   
   ## Brief change log
   
     - Make `Task#stopExecution` returns a `CompletableFuture`
     - Call `Task#stopExecution` when invoking `TaskExecutor#onStop`
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/19 18:20;githubbot;600","azagrebin commented on pull request #9072: [FLINK-11630] Wait for the termination of all running Tasks when shutting down TaskExecutor
URL: https://github.com/apache/flink/pull/9072
 
 
   ## What is the purpose of the change
   
   Add a unit test to #8605.
   
   Currently, the `TaskExecutor` does not properly wait for the termination of Tasks when it shuts down in `TaskExecutor#onStop`. This patch changes `TaskExecutor#onStop` to fail all running tasks and wait  for their termination before stopping all services.
   
   ## Brief change log
   
     - add `TaskCompletionTracker` to track task termination futures in `TaskExecutor`
     - try to fail all running tasks and wait  for their termination before stopping all services
     - add `TaskExecutorTest.testTaskInterruptionAndTerminationOnShutdown`
   
   ## Verifying this change
   
   run `TaskExecutorTest.testTaskInterruptionAndTerminationOnShutdown`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 14:53;githubbot;600","tillrohrmann commented on pull request #9072: [FLINK-11630] Wait for the termination of all running Tasks when shutting down TaskExecutor
URL: https://github.com/apache/flink/pull/9072
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 14:20;githubbot;600","tillrohrmann commented on pull request #7757: [FLINK-11630] Triggers the termination of all running Tasks when shutting down TaskExecutor
URL: https://github.com/apache/flink/pull/7757
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 14:20;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,FLINK-12332,,FLINK-12707,,,FLINK-12285,FLINK-11631,,,FLINK-12707,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 14:21:41 UTC 2019,,,,,,,,,,"0|yi114o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/19 14:21;trohrmann;Fixed via

1.10.0: cee8a38c7cb72a41c6d9ff5a128a279721225fe9
1.9.1: 65e6dbb5dc6d3fb021536363bc9da684cf1c306c;;;",,,,,,,,,,,,,,,,,,,,,,,
Restoring LockableTypeSerializer's snapshot from 1.6 and below requires pre-processing before snapshot is valid to use,FLINK-11591,13215448,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igalshilman,tzulitai,tzulitai,13/Feb/19 08:29,14/Feb/19 09:17,13/Jul/23 08:05,14/Feb/19 09:17,1.7.0,1.7.1,,,,,,,1.8.0,,,,API / Type Serialization System,Library / CEP,,0,pull-request-available,,,"In 1.6 and below, the {{LockableTypeSerializer}} incorrectly returns directly the element serializer's snapshot instead of wrapping it within an independent snapshot class:
https://github.com/apache/flink/blob/release-1.6/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/sharedbuffer/Lockable.java#L188

This results in the fact that the the written state information for this would be {{(LockableTypeSerializer, SomeArbitrarySnapshot)}}.

The problem occurs when restoring this in Flink 1.7+, since compatibility checks are now performed by providing the new serializer to the snapshot, what would happen is:
{{SomeArbitrarySnapshot.resolveSchemaCompatibility(newLockableTypeSerializer)}}, which would not work since the arbitrary snapshot does not recognize the {{LockableTypeSerializer}}.

To fix this, we essentially need to preprocess that arbitrary snapshot when restoring from <= 1.6 version snapshots.

A proposed fix would be to have the following interface:
{code}
public interface RequiresLegacySerializerSnapshotPreprocessing<T> {
    TypeSerializerSnapshot<T> preprocessLegacySerializerSnapshot(TypeSerializerSnapshot<T> legacySnapshot)
}
{code}

The {{LockableTypeSerializer}} would then implement this interface, and in the {{preprocessLegacySerializerSnapshot}} method, properly wrap that arbitrary element serializer snapshot into a {{LockableTypeSerializerSnapshot}}.

In general, this interface is useful to preprocess any problematic snapshot that was returned pre 1.7.

The point-in-time to check if a written serializer in <= 1.6 savepoints implements this interface and preprocesses the read snapshot would be:
https://github.com/apache/flink/blob/a567a1ef628eadad21e11864ec328481cd6d7898/flink-core/src/main/java/org/apache/flink/api/common/typeutils/TypeSerializerSerializationUtil.java#L218",,aljoscha,tzulitai,,,,,,,,,,,,,,,,,"igalshilman commented on pull request #7695: [FLINK-11591][core] Support legacy TypeSerializer snapshot transformations
URL: https://github.com/apache/flink/pull/7695
 
 
   ## What is the purpose of the change
   (copied from the issue description)
   
   In 1.6 and below, the LockableTypeSerializer incorrectly returns directly the element serializer's snapshot instead of wrapping it within an independent snapshot class:
   
   https://github.com/apache/flink/blob/release-1.6/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/sharedbuffer/Lockable.java#L188
   
   This results in the fact that the the written state information for this would be `(LockableTypeSerializer, SomeArbitrarySnapshot)`.
   
   The problem occurs when restoring this in Flink 1.7+, since compatibility checks are now performed by providing the new serializer to the snapshot, what would happen is:
   `SomeArbitrarySnapshot.resolveSchemaCompatibility(newLockableTypeSerializer)`, which would not work since the arbitrary snapshot does not recognize the LockableTypeSerializer.
   
   To fix this, we essentially need to preprocess that arbitrary snapshot when restoring from <= 1.6 version snapshots.
   
   ## Brief change log
   
   - This PR adds an interface `LegacySerializerSnapshotTransformer` that is being respect by the `TypeSerializerSerializationUtil` that allows serializers provide custom transformation logic for their associated snapshot classes.
   - Uses that interface by the `LockableTypeSerializer`.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `CEPMigrationTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/19 17:51;githubbot;600","asfgit commented on pull request #7695: [FLINK-11591][core] Support legacy TypeSerializerSnapshot transformations
URL: https://github.com/apache/flink/pull/7695
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Feb/19 09:05;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 09:17:14 UTC 2019,,,,,,,,,,"0|yi0xlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/19 09:17;tzulitai;Merged for 1.8.0 - f9532cba36c04fbde7e6956a0813ba260ad96953
;;;",,,,,,,,,,,,,,,,,,,,,,,
Prefix matching in ConfigDocsGenerator can result in wrong assignments,FLINK-11585,13215264,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Feb/19 14:52,13/Feb/19 09:47,13/Jul/23 08:05,13/Feb/19 09:47,1.6.3,1.7.1,1.8.0,,,,,,1.6.4,1.7.3,1.8.0,,Documentation,,,0,pull-request-available,,,"There are some cases where the key-prefix matching does not work as intended:

* given the prefixes ""a.b"" and ""a.b.c.d"", then an option with a key ""a.b.c.X"" will be assigned to the default groups instead of ""a.b""
* given a prefix ""a.b"", an option ""a.c.b"" will be matched to that group instead of the default
",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #7686: [FLINK-11585][docs] Fix prefix matching
URL: https://github.com/apache/flink/pull/7686
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixes 2 issues surrounding the key-prefix matching in the config docs generator.
   
   ## Brief change log
   
   * properly remember last encountered group root (i.e. a tree node representing a config group)
   * properly exit tree traversal if no child node could be find fr a given key component
   
   ## Verifying this change
   
   added `org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest#testLongestPrefixMatching`
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/19 14:55;githubbot;600","zentol commented on pull request #7686: [FLINK-11585][docs] Fix prefix matching
URL: https://github.com/apache/flink/pull/7686
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/19 09:44;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 13 09:47:28 UTC 2019,,,,,,,,,,"0|yi0wgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/19 09:47;chesnay;master: 0719db8895452ccb98548da1706cc65c34c99d76
1.7: 26abcfd4934d5ddae517f66f4485ad5d0c2f3593
1.6: 1cfa90c4b458b50e4216dfa7684591b0a025f98a;;;",,,,,,,,,,,,,,,,,,,,,,,
ConfigDocsCompletenessITCase fails DescriptionBuilder#linebreak() is used,FLINK-11584,13215246,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,nkruber,nkruber,12/Feb/19 13:16,12/Feb/19 15:07,13/Jul/23 08:05,12/Feb/19 15:07,1.6.3,1.7.1,1.8.0,,,,,,1.6.4,1.7.3,1.8.0,,Documentation,,,0,pull-request-available,,,"I just tried adding this configuration
{code}
    public static final ConfigOption<String> SSL_PROVIDER =
        key(""security.ssl.provider"")
            .defaultValue(""JDK"")
            .withDescription(Description.builder()
                    .text(""The SSL engine provider to use for the ssl transport:"")
                    .list(
                        TextElement.text(""%s: default Java-based SSL engine"", TextElement.code(""JDK"")),
                        TextElement.text(""%s: openSSL-based SSL engine using system libraries""
                            + "" (falls back to JDK if not available)"", TextElement.code(""OPENSSL""))
                    ).linebreak()
                    .text(""Please note: OPENSSL requires a custom build of %s in our "" +
                        ""shaded package namespace which is not (yet) available but can be built manually (see %s)."",
                        link(""http://netty.io/wiki/forked-tomcat-native.html#wiki-h2-4"", ""netty-tcnative""),
                        link(""https://issues.apache.org/jira/browse/FLINK-11579"", ""FLINK-11579""))
                    .build()
                );
{code}
and was presented with {{java.lang.AssertionError: Documentation is outdated, please regenerate it according to the instructions in flink-docs/README.md.}} in {{ConfigOptionsDocsCompletenessITCase}} even though I did regenerate the docs. As it turns out, the line break is rendered differently: {{<br/>}} (supposed) vs. {{<br>}} (documented). Strangely, {{DescriptionHtmlTest#testDescriptionWithLineBreak}} does verify the supposed variant, so I don't know what's wrong here.",,nkruber,,,,,,,,,,,,,,,,,,"zentol commented on pull request #7685: [FLINK-11584][docs] Add space in self-closing linebreak tag
URL: https://github.com/apache/flink/pull/7685
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   2 issues are being addressed here:
   * The configuration docs completeness test was parsing the document using html syntax, which has no support for self-closing tags. As a result the comparison of descriptions that contained linebreaks always failed, since we expected a self-closing tag (`<br/>`) but were returned something else (`<br>`).
   * Jsoup (which we use for parsing html) always adds a space in self-closing tags, (i.e., `<br />` instead of `<br/>`). Functionally these behave the same, and since the rest of our documentation also includes a space I have modified the `HtmlFormatter` accordingly.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/19 13:44;githubbot;600","zentol commented on pull request #7685: [FLINK-11584][docs] Add space in self-closing linebreak tag
URL: https://github.com/apache/flink/pull/7685
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/19 15:04;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 12 15:07:20 UTC 2019,,,,,,,,,,"0|yi0wcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/19 15:07;chesnay;master: 9839126b7d05eb4b6966e77e0893fb09b445803f
1.7: e64d3a8111b1faf2c06863fe144155e69ab27471
1.6: 284010633ab3c0be0fad1eed395c617f4dd4c749;;;",,,,,,,,,,,,,,,,,,,,,,,
ConfigOptions cannot have fallback and deprecated keys at the same time,FLINK-11583,13215201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Feb/19 10:09,13/Feb/19 14:54,13/Jul/23 08:05,13/Feb/19 14:54,1.8.0,,,,,,,,1.8.0,,,,Runtime / Configuration,,,0,pull-request-available,,,"{{ConfigOption#withFallbackKeys}} and  {{ConfigOption#withDeprecatedKeys}} both simply override the fallbackKeys array instead of checking whether it already contains some keys.

As a result, using both methods results in one of the given key sets to be overwritten.",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #7682: [FLINK-11583][configuration] Support deprecated and fallback keys at once
URL: https://github.com/apache/flink/pull/7682
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Modify `ConfigOption#withFallbackKeys` and `ConfigOption#withDeprecatedKeys` to take deprecated/fallback keys that were configured previously also into account.
   
   ## Brief change log
   
   * merge newly passed keys with existing keys in both methods
   * setup ordering such that fallback keys are always at the front, and deprecated keys always at the end
   
   ## Verifying this change
   
   see `ConfigurationTest#testFallbackAndDeprecatedKeys`
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/19 10:29;githubbot;600","zentol commented on pull request #7682: [FLINK-11583][configuration] Support deprecated and fallback keys at once
URL: https://github.com/apache/flink/pull/7682
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/19 14:52;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 13 14:54:45 UTC 2019,,,,,,,,,,"0|yi0w2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/19 14:54;chesnay;master: d20f57cbaa1f7b5913a1d04505fde82b5a30cec5;;;",,,,,,,,,,,,,,,,,,,,,,,
DispatcherHATest.testFailingRecoveryIsAFatalError fails locally with @Nonnull check,FLINK-11553,13214582,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,azagrebin,azagrebin,08/Feb/19 10:59,08/Feb/19 21:07,13/Jul/23 08:05,08/Feb/19 21:05,1.8.0,,,,,,,,1.8.0,,,,Runtime / Coordination,Tests,,0,pull-request-available,,,"DispatcherHATest.testFailingRecoveryIsAFatalError fails because it tries to instantiate 

HATestingDispatcher with fencingTokens = null which is annotated as @Nonnull.",,azagrebin,trohrmann,,,,,,,,,,,,,,,,,"azagrebin commented on pull request #7669: [FLINK-11553] Create an empty fencing tokens queue instead of using null in DispatcherHATest.testFailingRecoveryIsAFatalError
URL: https://github.com/apache/flink/pull/7669
 
 
   ## What is the purpose of the change
   
   DispatcherHATest.testFailingRecoveryIsAFatalError fails because it tries to instantiate HATestingDispatcher with fencingTokens = null which is annotated as @Nonnull.
   The PR suggests to create an empty fencing tokens queue instead of using null.
   
   ## Brief change log
   
     - fix DispatcherHATest.createDispatcher
     - small style fixes in DispatcherHATest
   
   ## Verifying this change
   
   run DispatcherHATest locally, e.g. in IDE Idea
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Feb/19 11:16;githubbot;600","asfgit commented on pull request #7669: [FLINK-11553] Create an empty fencing tokens queue instead of using null in DispatcherHATest.testFailingRecoveryIsAFatalError
URL: https://github.com/apache/flink/pull/7669
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Feb/19 21:07;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 08 21:05:47 UTC 2019,,,,,,,,,,"0|yi0s9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/19 21:05;trohrmann;Fixed via e8784b066a7a50892ef3975b279b2a66079996da;;;",,,,,,,,,,,,,,,,,,,,,,,
JsonMappingException in DynamoDBStreamsSchema,FLINK-11547,13214328,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tinder-dthomson,tinder-dthomson,tinder-dthomson,07/Feb/19 09:24,24/Jul/20 16:28,13/Jul/23 08:05,24/Jul/20 16:28,1.8.0,,,,,,,,1.12.0,,,,Connectors / Kinesis,,,0,pull-request-available,,,"If DynamoDBStreamsSchema is used as the deserializer for FlinkDynamoDBStreamsConsumer, an exception occurs during deserialization of a record. The stack trace is attached.

 

This is a blocker for using DynamoDBStreamsSchema, but can be worked around by implementing a custom deserializer. The issue appears to be due to the usage of vanilla ObjectMapper:

 
{code:java}
private static final ObjectMapper MAPPER = new ObjectMapper();
{code}
When it should be using the appropriate mix-ins offered by the dynamodb stream adapter library:
{code:java}
private static final ObjectMapper MAPPER = new RecordObjectMapper();
{code}
This appears to resolve the issue, I tested by using my own deserializer implementation.

 ",,aljoscha,jiaweiwu,thw,tinder-dthomson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-4582,,,,,,,,,"07/Feb/19 09:24;tinder-dthomson;full_flink_trace.txt;https://issues.apache.org/jira/secure/attachment/12957869/full_flink_trace.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 03:48:52 UTC 2020,,,,,,,,,,"0|yi0qpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/19 09:25;tinder-dthomson;[~thw] [~yxu-apache] FYI;;;","09/Feb/19 06:50;thw;[~tinder-dthomson] thanks for reporting the issue. Would you like to open a pull request to fix it?;;;","12/Feb/19 23:24;tinder-dthomson;[~thw] sure! I'll send a PR sometime this week;;;","20/Apr/20 08:00;jiaweiwu;+1 I have seen a similiar issue. The proposed fix solved the issue. Is anyone working on the PR?;;;","13/May/20 21:15;tinder-dthomson;[~jiaweiwu] I almost forgot about this ticket! Glad to see I'm not the only one with the issue. I will submit a PR by the end of Q2 (when we're more actively working on Flink in my team).;;;","17/Jul/20 00:41;tinder-dthomson;I have (finally!) submitted a PR here: [https://github.com/apache/flink/pull/12916]

[~thw] how shall you like me to proceed?;;;","17/Jul/20 02:41;thw;[~tinder-dthomson] thanks! I will take a look at the PR.;;;","21/Jul/20 18:43;tinder-dthomson;[~thw] the PR is complaining that this Jira ticket is unassigned. Can you assign to me (or whoever is most appropriate)?;;;","22/Jul/20 03:48;thw;Done!;;;",,,,,,,,,,,,,,,
Missing configuration sections since 1.5 docs,FLINK-11542,13214194,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,nkruber,nkruber,06/Feb/19 16:06,07/Mar/19 08:42,13/Jul/23 08:05,07/Mar/19 08:42,1.5.6,1.6.3,1.7.1,1.8.0,,,,,1.7.3,1.8.0,,,Documentation,,,0,pull-request-available,,,"It seems that during the merge of FLINK-8475, some of the content from the TM options didn't make it into the new doc generators, e.g. the notes in {{taskmanager.memory.preallocate}}. Before [commit eaff4da15b|https://github.com/apache/flink/commit/eaff4da15bdd7528dcc0d8a37fd59642cee53850], there were several sections with introduction, meaningful summary of important settings (not just an alphabetical list) and the option texts actually also contained some more information than the new descriptions.

I'd like to see those back again, in particular the (advanced) managed memory configuration notes for batch users.",,aljoscha,nkruber,,,,,,,,,,,,,,,,,"zentol commented on pull request #7687: [FLINK-11542][config][docs] Extend memory configuration descriptions 
URL: https://github.com/apache/flink/pull/7687
 
 
   Based on #7686.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Re-introduces lost advanced taskmanager memory documentation.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Feb/19 14:58;githubbot;600","zentol commented on pull request #7687: [FLINK-11542][config][docs] Extend memory configuration descriptions 
URL: https://github.com/apache/flink/pull/7687
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/19 08:27;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 07 08:42:24 UTC 2019,,,,,,,,,,"0|yi0pvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/19 08:42;chesnay;master: 76bf54e10f8bb1176b64ecae7e1a6214063f836a

1.8: 1d72ad4539e920df006705af39af0be5c8745226

1.7: fa53d747c6716afe451cf015778794148b049a87;;;",,,,,,,,,,,,,,,,,,,,,,,
Heavy deployment E2E test failed on Travis,FLINK-11541,13214182,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,srichter,chesnay,chesnay,06/Feb/19 15:30,22/Feb/19 17:19,13/Jul/23 08:05,22/Feb/19 17:19,1.8.0,,,,,,,,1.8.0,,,,Runtime / Coordination,Test Infrastructure,,0,pull-request-available,,,"https://travis-ci.org/apache/flink/builds/489492244

{code}
------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: Could not retrieve the execution result. (JobID: 83f517adcbeae8a0e04e9824adb6ebc4)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:261)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:487)
	at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:66)
	at org.apache.flink.deployment.HeavyDeploymentStressTestProgram.main(HeavyDeploymentStressTestProgram.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:813)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:287)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-1564303328]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:748)

End of exception on server side>]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:389)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:373)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[FAIL] Test script contains errors.
Checking of logs skipped.

[FAIL] 'Heavy deployment end-to-end test' failed after 1 minutes and 52 seconds! Test exited with exit code 1
{code}",,rmetzger,srichter,trohrmann,,,,,,,,,,,,,,,,"StefanRRichter commented on pull request #7768: [FLINK-11541][tests] Reduce scale of heavy deployment e2e test for hi…
URL: https://github.com/apache/flink/pull/7768
 
 
   …gher stability
   
   The test failures look similar to what we saw before when the scale of the test was to big. I guess there is some variance in the performance of machines that Travis provides, so I scaled down the test a bit further.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/19 11:07;githubbot;600","StefanRRichter commented on pull request #7768: [FLINK-11541][tests] Reduce scale of heavy deployment e2e test for hi…
URL: https://github.com/apache/flink/pull/7768
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/19 17:17;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 22 17:19:12 UTC 2019,,,,,,,,,,"0|yi0pt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Feb/19 08:42;rmetzger;Is this a test instability or is this likely a bug in the system?;;;","19/Feb/19 09:44;srichter;I would assume it is a test instability.;;;","21/Feb/19 15:10;srichter;I analyzed it a bit more and it seems that the changes to run the execution graph single-threaded can have a negative impact on performance with slow CPUs and very large deployment descriptors, because the serialization of the deployment descriptors is now all done in one thread.

I think we can solve this problem by using the future executor for the  calls to `submitTask` and then sync back the result into the main thread. Will prepare a fix.;;;","21/Feb/19 15:27;trohrmann;Great, thanks for the investigation and fixing it [~srichter].;;;","22/Feb/19 17:19;srichter;Merged in:
master: 42d9df8fa2;;;",,,,,,,,,,,,,,,,,,,
ExecutionGraph does not reach terminal state when JobMaster lost leadership,FLINK-11537,13214119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,trohrmann,trohrmann,06/Feb/19 09:45,09/Apr/20 16:32,13/Jul/23 08:05,20/Feb/19 17:20,1.8.0,,,,,,,,1.8.0,,,,Runtime / Coordination,,,1,,,,"The {{ExecutionGraph}} sometimes does not reach a terminal state if the {{JobMaster}} lost the leadership. The reason is that we use the fenced main thread executor to execute {{ExecutionGraph}} changes and we don't wait for the {{ExecutionGraph}} to reach the terminal state before we set the fencing token {{null}}.

One possible solution would be to wait for the {{ExecutionGraph}} to reach the terminal state before clearing the fencing token. This has, however, the downside that the {{JobMaster}} is still reachable until the {{ExecutionGraph}} has been properly terminated. Alternatively, we could use the unfenced main thread executor to send the cancel calls out.

A Travis run where the problem occurred is here: https://travis-ci.org/tillrohrmann/flink/jobs/489119926

Update: The underlying problem is that {{ExecutionGraph#suspend}} does not transition the {{ExecutionGraph}} atomically into a terminal state. Changing this should solve the underlying problem.",,elevy,hwanju,sewen,shixg,stevenz3wu,tison,trohrmann,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10439,,,,,,,FLINK-10439,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 17:20:55 UTC 2019,,,,,,,,,,"0|yi0pf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/19 08:37;uce;Is this related to/a duplicate of https://issues.apache.org/jira/browse/FLINK-10439?;;;","11/Feb/19 08:41;sewen;This seems quite a fundamental thing which may warrant some more thoughts.

Just a thought: we could, have fencing tokens for the execution graph and that never changes. When leadership is lost, the old executiongraph and its fencing token stay around. Gaining leadership builds a new execution graph with a new fencing token.;;;","14/Feb/19 09:45;trohrmann;[~uce] yes this seems to be related to FLINK-10439.

I agree that this needs to work properly.

Having multiple fencing tokens, one per {{ExecutionGraph}}, is currently not supported by the {{FencedRpcEndpoint}}. Since the {{updateTaskExecutionState}} are sent from the {{TaskExecutor}} to the {{JobMaster}}, it is also hard to distinguish between messages dedicated for different {{ExecutionGraphs}} on the {{AkkaRpcActor}} level.

I think waiting for the termination of the {{ExecutionGraph}} before giving up the leadership could work. Of course, the {{JobMaster}} won't be able to regain leadership until he has completed the leadership revocation. That way it would also better reflect the lifecycle dependencies between the leadership of the {{JobMaster}} and the lifetime of the {{ExecutionGraph}} --> the {{ExecutionGraph}} is bound to the leader session of the {{JobMaster}} and we need to terminate the {{ExecutionGraph}} when ending the leader session.;;;","14/Feb/19 09:54;sewen;I feel a bit uneasy about holding off releasing leadership.
Is that even something that can be delayed, won't ZK / etcd / consul simply assign another leader?

What would speak against going directly to state ""SUSPENDED"", skipping ""SUSPENDING"". The ExecutionGraph fires out cancellation messages (or not, when we do JM reconciliation in the future).

That's it, the TMs would cancel and the confirmation of the cancellation would be ignored (which should be fine).;;;","14/Feb/19 12:22;trohrmann;Holding off releasing the leadership would only happen locally for the {{JobMaster}} and not globally. In the meantime, ZK/etcd/consul will elect a new leader and if it should be the same {{JobMaster}}, then it would not get the leadership until it has completed the leadership revocation.

Similarly to {{JobStatus#CANCELLING}}, {{JobStatus#SUSPENDING}} has been introduced to reflect that suspending the {{ExecutionGraph}} in the current implementation entails a clean up phase where all {{Executions}} will get canceled. Only after all {{Executions}} have reached a terminal state, the {{ExecutionGraph}} will transition into the state {{SUSPENDED}}. The idea behind this implementation was to properly bind the lifetime of a remote {{Task}} to the lifetime of the {{ExecutionGraph}}.

We can also change the implementation of {{ExecutionGraph#suspend}} to transition the {{Executions}} directly into a terminal state {{CANCELLED}} instead of {{CANCELLING}} and waiting for the state transition. I would consider this a slightly orthogonal issue though (e.g. make {{ExecutionGraph#suspend}} atomically transition to terminal state) and still think that it would be worthwhile to wait until the {{ExecutionGraph}} reaches a terminal state before setting the {{JobMaster's}} fencing token to {{null}}. With a changed {{#suspend}} behaviour the fencing token would then be nulled immediately.

;;;","14/Feb/19 12:30;sewen;Suspending is not a clean form of shutdown (as cancelling attempts to be).
It also does not do most cleanup (it is not globally terminal).

I think the problem comes from the following mismatch;
  1. The JM tries to transition the EG gracefully to cancellation as if it were still in change of graceful coordination
  2. The JM has lost authority over the execution in the first place

I would break with behavior (1). The JM should not attempt graceful coordination. It can send out best effort cancellation (which should anyways be ignored by the TM if the TM has discovered a new leader already). The TM will cancel tasks anyways once it learns about the new leader.

Trying to tweak (2) (the JM retains some form of leader authority) so that we can keep (1) (graceful cancellation) seems counter intuitive to me.;;;","19/Feb/19 10:07;trohrmann;I agree that we should make the suspend operation atomically finish instead of requiring a round trip to collect the cancellation completions.;;;","20/Feb/19 17:20;trohrmann;Fixed via c9e392b53b48c8ae0b189905a9b4cf878bf741e4;;;",,,,,,,,,,,,,,,,
SQL Client jar does not contain table-api-java,FLINK-11535,13213922,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,05/Feb/19 14:34,06/Feb/19 09:48,13/Jul/23 08:05,06/Feb/19 09:48,,,,,,,,,1.8.0,,,,Table SQL / Client,,,0,pull-request-available,,,The SQL Client jar does not package {{flink-table-api-java}}.,,twalthr,,,,,,,,,,,,,,,,,,"twalthr commented on pull request #7652:  [FLINK-11535][table] Include table-api-java in SQL Client jar
URL: https://github.com/apache/flink/pull/7652
 
 
   ## What is the purpose of the change
   
   Explicitly adds `flink-table-common` and `flink-table-api-java` to the SQL Client `pom.xml`.
   
   
   ## Brief change log
   
   - Dependencies updated
   
   
   ## Verifying this change
   
   Run SQL Client end-to-end test
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): yes
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/19 14:48;githubbot;600","asfgit commented on pull request #7652:  [FLINK-11535][table] Include table-api-java in SQL Client jar
URL: https://github.com/apache/flink/pull/7652
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Feb/19 09:45;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 06 09:48:34 UTC 2019,,,,,,,,,,"0|yi0o7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/19 09:48;twalthr;Fixed in 1.8.0: ca4ff379c5187836643dac011b7a986d608b0dee;;;",,,,,,,,,,,,,,,,,,,,,,,
"""Powered by Flink"" page does not display correctly on Firefox",FLINK-11462,13212800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,plucas,plucas,plucas,30/Jan/19 12:04,01/Feb/19 11:39,13/Jul/23 08:05,01/Feb/19 11:39,,,,,,,,,,,,,Project Website,,,0,pull-request-available,,,"The JavaScript that sets the height of each ""tile"" does not work as expected in Firefox, causing them to overlap (see attached screenshot). [This Stack Overflow post|https://stackoverflow.com/questions/12184133/jquery-wrong-values-when-trying-to-get-div-height] suggests using jQuery's {{outerHeight}} instead of {{height}} method, and making this change seems to make the page display correctly in both Firefox and Chrome.",,fhueske,plucas,,,,,,,,,,,,,,,,,"patricklucas commented on pull request #152: [FLINK-11462] Fix 'Powered By' page on Firefox
URL: https://github.com/apache/flink-web/pull/152
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 12:14;githubbot;600","asfgit commented on pull request #152: [FLINK-11462] Fix 'Powered By' page on Firefox
URL: https://github.com/apache/flink-web/pull/152
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Feb/19 11:38;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/19 12:03;plucas;Screen Shot 2019-01-30 at 12.13.03.png;https://issues.apache.org/jira/secure/attachment/12956890/Screen+Shot+2019-01-30+at+12.13.03.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 01 11:39:52 UTC 2019,,,,,,,,,,"0|yi0hao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/19 11:39;fhueske;Fixed with 1b8aadf5988b04061338caee9232676a3cc28fb6

Thanks [~plucas]!;;;",,,,,,,,,,,,,,,,,,,,,,,
Java deserialization failure of the AvroSerializer when used in an old CompositeSerializers,FLINK-11436,13212065,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,igal,igal,igal,27/Jan/19 07:34,29/Jan/19 07:18,13/Jul/23 08:05,29/Jan/19 07:18,1.7.0,1.7.1,,,,,,,1.7.2,,,,API / Type Serialization System,,,0,pull-request-available,,,"During the release of Flink 1.7, the value of AvroSerializer, serialVersionUID was uptick to 2L (was 1L before).

Although the AvroSerializer (along with it's snapshot class) was migrated to the new serialization abstraction (hence free of Java serialization), there were composite serializers that were not yet migrated during that release (i.e. StreamElementSerializer), and were serialized with Java serialization into the checkpoint/savepoint.

In case that one of the nested serializers were the AvroSerializer we would
bump into deserialization exception due to a wrong serialVersionUID.

This issue was initially reported on the user mailing list:

http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Trouble-migrating-state-from-1-6-3-to-1-7-1-td25694.html
h3.  ",,hwanju,igal,tzulitai,,,,,,,,,,,,,,,,"igalshilman commented on pull request #7580: [FLINK-11436] Fix Java deserialization failure of the AvroSerializer
URL: https://github.com/apache/flink/pull/7580
 
 
   ## What is the purpose of the change
   
   During the release of Flink-1.7 the value of `AvroSerializer.serialVersionUID` was uptick to 2 (was 1 before)
   And although the AvroSerializer (along with it's snapshot class) were migrated to the new serialization
   abstraction (hence free of Java serialization), there were composite serializers that were not migrated and were serialized with Java serialization.
   
   In case that one of the nested serializers were Avro we would bump into deserialization exception due to a wrong `serialVersionUID`. 
   
   Unfortunately it is not possible to revert the `serialVersionUID` back to 1, because users might have snapshots with `serialVersionUID = 2` present already.
   To overcome this we first need to make sure that the `AvroSerializer` is being Java deserialized with `FailureTolerantObjectInputStream`, and then we determine the serialized layout by looking at the fields in `readObject`: 
   
   ```		
   pre 1.7 	field order (left to right):   	[schemaString, 	type]
   post 1.7 	field order (left to right):	[previousSchema,	schema,		type]
   ```
   
   We would use the first field to distinguish between the two different layouts.
   To complicate things even further in pre 1.7, the field @schemaString could be
   null or a string, but, in post 1.7, the field @previousSchema was never set to null, therefore
   we can use the first field to determine the version.
   
   ## Brief change log
   
   * 0225979548 Add the `AvroSerializer` to the list of serializers that we ignore `serialVersionUID` mismatches for.
   * 5045a5a46a Minor refactoring to enable creating a `SerializedAvroSchema` from a `schemaString`
   * 9aef45c569 Explicitly reading the fields in `AvroSerializer#readObject`.
   
   ## Verifying this change
   
   The changes are covered with `AvroSerializerMigrationTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jan/19 20:01;githubbot;600","asfgit commented on pull request #7580: [FLINK-11436] Fix Java deserialization failure of the AvroSerializer
URL: https://github.com/apache/flink/pull/7580
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jan/19 07:08;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 29 07:17:47 UTC 2019,,,,,,,,,,"0|yi0csw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/19 07:17;tzulitai;Merged.
Thanks for fixing [~igal].

1.8.0: 479ebd59872160cb2060605b08dcd0d86c3cb78e
1.7.2: 482116806bf9c4159a505d2c013dcc298e7c8174;;;",,,,,,,,,,,,,,,,,,,,,,,
DatadogHttpReporter throws ConcurrentModificationException if gauge throws exception or returns a non-numeric value,FLINK-11424,13211490,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lining,lining,lining,24/Jan/19 09:06,02/Oct/19 17:49,13/Jul/23 08:05,15/Feb/19 12:55,1.6.3,1.7.1,1.8.0,,,,,,1.7.3,1.8.0,,,Runtime / Metrics,,,0,pull-request-available,,,"{code:java}
for (Map.Entry<Gauge, DGauge> entry : gauges.entrySet()) {
   DGauge g = entry.getValue();
   try {
      // Will throw exception if the Gauge is not of Number type
      // Flink uses Gauge to store many types other than Number
      g.getMetricValue();
      request.addGauge(g);
   } catch (Exception e) {
      // Remove that Gauge if it's not of Number type
      gauges.remove(entry.getKey());
   }
}
{code}
in this method remove will java.util.ConcurrentModificationException",,lining,,,,,,,,,,,,,,,,,,"jinglining commented on pull request #7582: [FLINK-11424][metrics]fix remove error type Gauge in DatadogHttpReporter
URL: https://github.com/apache/flink/pull/7582
 
 
   ## What is the purpose of the change
   - fix DatadogHttpReporter remove error type Gauge bug
   
   ## Brief change log
   
   - add notNumberGauges for error type Gauge
   
   
   ## Verifying this change
   
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jan/19 02:38;githubbot;600","zentol commented on pull request #7582: [FLINK-11424][metrics]fix remove error type Gauge in DatadogHttpReporter
URL: https://github.com/apache/flink/pull/7582
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/19 12:51;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 15 12:54:27 UTC 2019,,,,,,,,,,"0|yi0994:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/19 09:08;lining;why delete the key？ ;;;","24/Jan/19 09:28;chesnay;Because a gauge that throws exceptions is probably fundamentally broken and will just fail again and again, and hence should be ignored in further reporters. The code fails when doing so, but the intended behavior is correct.;;;","28/Jan/19 02:40;lining;Hi, [~chesnay]. Add notNumberGauges  for fix it. Can you review it. Thanks for your reply.;;;","15/Feb/19 12:54;chesnay;master: f1ab95ad4a7f0c0ffd4b74878e7cb5e2db051a35
1.7: 90043718f5ac5e859d77cf984d0be7cf1a36de46;;;",,,,,,,,,,,,,,,,,,,,
"Serialization of case classes containing a Map[String, Any] sometimes throws ArrayIndexOutOfBoundsException",FLINK-11420,13211415,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,jkreileder,jkreileder,24/Jan/19 00:42,01/Oct/19 15:39,13/Jul/23 08:05,12/Mar/19 08:10,1.7.1,,,,,,,,1.7.3,1.8.0,,,API / Type Serialization System,,,0,pull-request-available,,,"We frequently run into random ArrayIndexOutOfBounds exceptions when flink tries to serialize Scala case classes containing a Map[String, Any] (Any being String, Long, Int, or Boolean) with the FsStateBackend. (This probably happens with any case class containing a type requiring Kryo, see this thread for instance: [http://mail-archives.apache.org/mod_mbox/flink-user/201710.mbox/%3cCANNGFpjX4gjV=Df6TLfeOJsB_rHWxs_rUoyLqcqv2gVWQTtfhA@mail.gmail.com%3e])

Disabling asynchronous snapshots seems to work around the problem, so maybe something is not thread-safe in CaseClassSerializer.

Our objects look like this:
{code}
case class Event(timestamp: Long, [...], content: Map[String, Any]
case class EnrichedEvent(event: Event, additionalInfo: Map[String, Any])
{code}
I've looked at a few of the exceptions in a debugger. It always happens when serializing the right-hand side a tuple from EnrichedEvent -> Event -> content, e.g: 13 from (""foo"", 13) or false from (""bar"", false).

Stacktrace:
{code:java}
java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 0
 at com.esotericsoftware.kryo.util.IntArray.pop(IntArray.java:157)
 at com.esotericsoftware.kryo.Kryo.reference(Kryo.java:822)
 at com.esotericsoftware.kryo.Kryo.copy(Kryo.java:863)
 at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:217)
 at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:101)
 at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
 at org.apache.flink.api.scala.typeutils.TraversableSerializer.$anonfun$copy$1(TraversableSerializer.scala:69)
 at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:234)
 at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:465)
 at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:465)
 at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:69)
 at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:33)
 at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:101)
 at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
 at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:101)
 at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
 at org.apache.flink.api.common.typeutils.base.ListSerializer.copy(ListSerializer.java:99)
 at org.apache.flink.api.common.typeutils.base.ListSerializer.copy(ListSerializer.java:42)
 at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.get(CopyOnWriteStateTable.java:287)
 at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.get(CopyOnWriteStateTable.java:311)
 at org.apache.flink.runtime.state.heap.HeapListState.add(HeapListState.java:95)
 at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:391)
 at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)
 at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)
 at java.base/java.lang.Thread.run(Thread.java:834){code}
 

 

 ",,aljoscha,dwysakowicz,elevy,jkreileder,lamber-ken,srichter,,,,,,,,,,,,,"dawidwys commented on pull request #7872: [FLINK-11420][core] Fixed duplicate method of TraversableSerializer
URL: https://github.com/apache/flink/pull/7872
 
 
   ## What is the purpose of the change
   
   The duplicate method of TypeSerializer used an equality check rather
   than reference check of the element serializer to decide if we need a
   deep copy. This commit uses proper reference comparison.
   
   ## Brief change log
   
   *(for example:)*
     - enabled additional tests in SerializerTestInstance
     - fixed duplicate method of TraversableSerializer
   
   
   ## Verifying this change
   
   * enabled additional test (including `duplicate` method test in `SerializerTestInstance`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicabl**e / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/19 07:59;githubbot;600","dawidwys commented on pull request #7873: [FLINK-11420][core][bp1.8] Fixed duplicate method of TraversableSerializer
URL: https://github.com/apache/flink/pull/7873
 
 
   ## What is the purpose of the change
   
   The duplicate method of TypeSerializer used an equality check rather
   than reference check of the element serializer to decide if we need a
   deep copy. This commit uses proper reference comparison.
   
   ## Brief change log
   
   *(for example:)*
     - enabled additional tests in SerializerTestInstance
     - fixed duplicate method of TraversableSerializer
   
   
   ## Verifying this change
   
   * enabled additional test (including `duplicate` method test in `SerializerTestInstance`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicabl**e / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/19 08:00;githubbot;600","dawidwys commented on pull request #7873: [FLINK-11420][core][bp1.8] Fixed duplicate method of TraversableSerializer
URL: https://github.com/apache/flink/pull/7873
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/19 09:42;githubbot;600","dawidwys commented on pull request #7894: [FLINK-11420][core] Fixed duplicate method of TraversableSerializer 
URL: https://github.com/apache/flink/pull/7894
 
 
   backport of #7872 See the description there.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/19 14:58;githubbot;600","dawidwys commented on pull request #7872: [FLINK-11420][core] Fixed duplicate method of TraversableSerializer
URL: https://github.com/apache/flink/pull/7872
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/19 17:54;githubbot;600","dawidwys commented on pull request #7894: [FLINK-11420][core][bp-1.8] Fixed duplicate method of TraversableSerializer 
URL: https://github.com/apache/flink/pull/7894
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/19 17:55;githubbot;600","dawidwys commented on pull request #7951:  [FLINK-11420][datastream] Fix duplicate and createInstance methods of CoGroupedStreams.UnionSerializer
URL: https://github.com/apache/flink/pull/7951
 
 
   ## What is the purpose of the change
   
   It fixes duplicate method of `org.apache.flink.streaming.api.datastream.CoGroupedStreams.UnionSerializer`
   
   
   ## Brief change log
   
   *(for example:)*
    - a284a92 - minor improvement to `ScalaCaseClassSerializerTest.scala`
    - 997b3c7 - fixed `duplicate` and `createInstance` methods of `org.apache.flink.streaming.api.datastream.CoGroupedStreams.UnionSerializer`
      
   ## Verifying this change
   
   * added test class: `org.apache.flink.streaming.api.datastream.UnionSerializerTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 12:13;githubbot;600","dawidwys commented on pull request #7953:  [FLINK-11420][datastream][bp-1.8] Fix duplicate and createInstance methods of CoGroupedStreams.UnionSerializer
URL: https://github.com/apache/flink/pull/7953
 
 
   Backport for 1.8 See #7951 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 12:17;githubbot;600","dawidwys commented on pull request #7954:  [FLINK-11420][datastream][bp-1.7] Fix duplicate and createInstance methods of CoGroupedStreams.UnionSerializer
URL: https://github.com/apache/flink/pull/7954
 
 
   Backport for 1.7 See #7951 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 12:18;githubbot;600","dawidwys commented on pull request #7951:  [FLINK-11420][datastream] Fix duplicate and createInstance methods of CoGroupedStreams.UnionSerializer
URL: https://github.com/apache/flink/pull/7951
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 16:44;githubbot;600","dawidwys commented on pull request #7953:  [FLINK-11420][datastream][bp-1.8] Fix duplicate and createInstance methods of CoGroupedStreams.UnionSerializer
URL: https://github.com/apache/flink/pull/7953
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 16:45;githubbot;600","dawidwys commented on pull request #7954:  [FLINK-11420][datastream][bp-1.7] Fix duplicate and createInstance methods of CoGroupedStreams.UnionSerializer
URL: https://github.com/apache/flink/pull/7954
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Mar/19 16:46;githubbot;600",,0,7200,,,0,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 12 07:12:42 UTC 2019,,,,,,,,,,"0|yi08sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/19 10:06;dwysakowicz;Hi [~jkreileder]
Could you try reproducing this issue with debug logs enabled? This would enable checks if there are concurrent accesses to KryoSerializer which could lead to what you describe.;;;","01/Mar/19 08:04;dwysakowicz;I found out that there is a bug in {{TraversableSerializer#duplicate}} method.;;;","04/Mar/19 18:09;dwysakowicz;Fixed in: 
master 00e59e7afe2f6a60cfd91c4c75fedd4b96d463a3
1.8 7fad886d2736fa5937830b89fd019adb598ed8e7;;;","05/Mar/19 12:53;dwysakowicz;Fixed in 1.7.3 via: 54cf610494c0601ddbfde91ea62a07dc080feeb1;;;","08/Mar/19 14:50;jkreileder;Just made a test run with 1.8-SNAPSHOT (commit 9d87fe1) and I've seen a few of those exceptions again:
{code:java}
java.lang.ArrayIndexOutOfBoundsException: -1
        at com.esotericsoftware.kryo.util.IntArray.pop(IntArray.java:157)
        at com.esotericsoftware.kryo.Kryo.reference(Kryo.java:822)
        at com.esotericsoftware.kryo.Kryo.copy(Kryo.java:863)
        at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:248)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.$anonfun$copy$1(TraversableSerializer.scala:90)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:230)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:461)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:90)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:33)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.$anonfun$copy$1(TraversableSerializer.scala:90)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:230)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:461)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:90)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:33)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.copy(CoGroupedStreams.java:537)
        at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.copy(CoGroupedStreams.java:506)
        at org.apache.flink.api.common.typeutils.base.ListSerializer.copy(ListSerializer.java:99)
        at org.apache.flink.api.common.typeutils.base.ListSerializer.copy(ListSerializer.java:42)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.get(CopyOnWriteStateTable.java:297)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.get(CopyOnWriteStateTable.java:321)
        at org.apache.flink.runtime.state.heap.HeapListState.add(HeapListState.java:95)
        at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:391)
        at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)
        at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
        at java.lang.Thread.run(Thread.java:748)
{code};;;","08/Mar/19 20:06;dwysakowicz;Hi [~jkreileder]
Have you tried enabling DEBUG logs? Have you seen exceptions claiming concurrent access to KryoSerializer?

Another idea, are you trying to restore from checkpoint taken with a previous version? I just wonder if the checkpoint might be corrupted if it was taken when this bug was still present. [~srichter] What do you think?;;;","08/Mar/19 20:27;jkreileder;Hi [~dwysakowicz],

I'm just playing around a bit with 1.8 without restoring any state. I'll give DEBUG a try.

Testing is a bit of a pain, though: The test jobs take around 10 minutes to actually start, it took only seconds with 1.7. The culprit seems to be the new compilation code TraversableSerializer. Is that a an expected phenomenon? ;;;","08/Mar/19 21:05;dwysakowicz;I wouldn't expect any of the changes introduced to fix this issue to have impact on start up time.;;;","08/Mar/19 21:22;jkreileder;Yep, concurrent access:
{code:java}
2019-03-08 21:11:55,674 DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask           - Cleanup AsyncCheckpointRunnable for checkpoint 9 of Generate Finding Commands 377823 (1/1).
2019-03-08 21:11:55,680 INFO  org.apache.flink.runtime.taskmanager.Task                     - Generate Finding Commands 377823 (1/1) (8bed70dd3de95f9c0c82daa19679a4be) switched from RUNNING to FAILED.
java.lang.IllegalStateException: Concurrent access to KryoSerializer. Thread 1: Generate Finding Commands 377823 (1/1) , Thread 2: pool-138-thread-1
        at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.enterExclusiveThread(KryoSerializer.java:630)
        at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:242)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.$anonfun$copy$1(TraversableSerializer.scala:90)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:230)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:461)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:90)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:33)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.$anonfun$copy$1(TraversableSerializer.scala:90)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:230)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:461)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:461)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:461)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:90)
        at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:33)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
        at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
        at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.copy(CoGroupedStreams.java:537)
        at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.copy(CoGroupedStreams.java:506)
        at org.apache.flink.api.common.typeutils.base.ListSerializer.copy(ListSerializer.java:99)
        at org.apache.flink.api.common.typeutils.base.ListSerializer.copy(ListSerializer.java:42)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.get(CopyOnWriteStateTable.java:297)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.get(CopyOnWriteStateTable.java:321)
        at org.apache.flink.runtime.state.heap.HeapListState.add(HeapListState.java:95)
        at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:391)
        at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)
        at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
        at java.lang.Thread.run(Thread.java:748)
{code}
The slow-down in job submission is unrelated. I think it's caused by FLINK-11539.;;;","11/Mar/19 12:29;dwysakowicz;Hi [~jkreileder] I found there was a similar issue with {{CoGroupedStreams$UnionSerializer}}. Would you mind giving this PR a go: https://github.com/apache/flink/pull/7951 to see if there are any more issues?;;;","11/Mar/19 22:58;jkreileder;[~dwysakowicz], looks good so far on 1.7.3. Thanks!

 ;;;","12/Mar/19 08:10;dwysakowicz;Additional issue with {{CoGroupedStreams.UnionSerializer}} fixed in:
master: 3133a6dc684c485e3942d9536a26fe5d6b31f17d
1.8.0: db0b874c41c3ae5f61942c5ca0e6950a2694f4ac
1.7.3: f04d2cb87775b42aa54161ffb3bdaeb1f9d4af3c;;;","12/Sep/19 07:12;lamber-ken;(y);;;",,,,,,,,,,,
StreamingFileSink fails to recover after taskmanager failure,FLINK-11419,13211344,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,edRojas,edRojas,edRojas,23/Jan/19 18:15,02/Oct/19 17:44,13/Jul/23 08:05,04/Feb/19 10:02,1.7.1,,,,,,,,1.7.2,1.8.0,,,Connectors / FileSystem,,,1,pull-request-available,,,"If a job with a StreamingFileSink sending data to HDFS is running in a cluster with multiple taskmanagers and the taskmanager executing the job goes down (for some reason), when the other task manager start executing the job, it fails saying that there is some ""missing data in tmp file"" because it's not able to perform a truncate in the file.

 Here the full stack trace:
{code:java}
java.io.IOException: Missing data in tmp file: hdfs://path/to/hdfs/2019-01-20/.part-0-0.inprogress.823f9c20-3594-4fe3-ae8c-f57b6c35e191
	at org.apache.flink.runtime.fs.hdfs.HadoopRecoverableFsDataOutputStream.<init>(HadoopRecoverableFsDataOutputStream.java:93)
	at org.apache.flink.runtime.fs.hdfs.HadoopRecoverableWriter.recover(HadoopRecoverableWriter.java:72)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.restoreInProgressFile(Bucket.java:140)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.<init>(Bucket.java:127)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.restore(Bucket.java:396)
	at org.apache.flink.streaming.api.functions.sink.filesystem.DefaultBucketFactoryImpl.restoreBucket(DefaultBucketFactoryImpl.java:64)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.handleRestoredBucketState(Buckets.java:177)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.initializeActiveBuckets(Buckets.java:165)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.initializeState(Buckets.java:149)
	at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.initializeState(StreamingFileSink.java:334)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:278)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:738)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:289)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to TRUNCATE_FILE /path/to/hdfs/2019-01-20/.part-0-0.inprogress.823f9c20-3594-4fe3-ae8c-f57b6c35e191 for DFSClient_NONMAPREDUCE_-2103482360_62 on x.xxx.xx.xx because this file lease is currently owned by DFSClient_NONMAPREDUCE_1834204750_59 on x.xx.xx.xx
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3190)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.truncateInternal(FSNamesystem.java:2282)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.truncateInt(FSNamesystem.java:2228)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.truncate(FSNamesystem.java:2198)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.truncate(NameNodeRpcServer.java:1056)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.truncate(ClientNamenodeProtocolServerSideTranslatorPB.java:622)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy49.truncate(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.truncate(ClientNamenodeProtocolTranslatorPB.java:314)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy50.truncate(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.truncate(DFSClient.java:1632)
	at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:777)
	at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:774)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.truncate(DistributedFileSystem.java:774)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.fs.hdfs.HadoopRecoverableFsDataOutputStream.truncate(HadoopRecoverableFsDataOutputStream.java:179)
	at org.apache.flink.runtime.fs.hdfs.HadoopRecoverableFsDataOutputStream.<init>(HadoopRecoverableFsDataOutputStream.java:91)
	... 17 more
{code}
 

I noticed there is already some code to handle this kind of situations, I also compared to BucketingSink (as there is no issue with BucketingSink) and I noticed that in the StreamingFileSink the ""truncate"" is done before waiting until the lease of the file is free... whereas in the BucketingSink the ""truncate"" is done after waiting for the lease is free.

 

For reference: 
 BucketingSink: [https://github.com/apache/flink/blob/release-1.7.1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java#L830-L853]

StreaminFileSink: [https://github.com/apache/flink/blob/release-1.7.1/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopRecoverableFsDataOutputStream.java#L89-L96]",,aljoscha,cjolif@apache.org,edRojas,kisimple,kkl0u,xEviL,,,,,,,,,,,,,"EAlexRojas commented on pull request #7588: [FLINK-11419][filesystem] For recovery, wait until lease is revoked before truncate file
URL: https://github.com/apache/flink/pull/7588
 
 
   ## What is the purpose of the change
   
   This pull request performs the truncate of the hadoop file only after waiting for the lease to be revoked.
   
   ## Brief change log
   - When resuming execution after a failure, wait until lease is revoked before performing truncate on the file.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `testRecoverWithState`, `testRecoverAfterMultiplePersistsState`, etc in AbstractRecoverableWriterTest class.
   
   An additional integration test could be added to test the case when this is used with a StreamingFileSink and simulate a taskmanager failure (?)
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jan/19 15:45;githubbot;600","asfgit commented on pull request #7588: [FLINK-11419][filesystem] For recovery, wait until lease is revoked before truncate file
URL: https://github.com/apache/flink/pull/7588
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/19 09:05;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 04 10:02:57 UTC 2019,,,,,,,,,,"0|yi08co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/19 10:10;kkl0u;Hi [~edRojas]. Thanks for reporting this. 

I would assume that if the only problem is the lease not being released yet, then if you restart the job after a while it will start normally, and without any problems.

If it does not, then are you sure that there is no other task manager writing to the same file? This can happen in the following cases:
1) if 2 jobs write to the same bucket or 
2) if there are Task Managers from a previous execution of the job that are still running.

Can you see if restarting the job fixes the problem? ;;;","24/Jan/19 10:15;kkl0u;In any case, from your report it seems that the initial message ""Missing data in tmp file:..."" is misleading and the truncate may have to move after the waiting for the lease to be released.;;;","28/Jan/19 12:39;edRojas;Hi [~kkl0u], I performed several tests and even waiting for several minutes, when trying to restart the job from latest successful checkpoint (in order to don't lose state) the error continues to appear. 

When waiting for several minutes the error is a little different, instead of a ""AlreadyBeingCreatedException"" I get a ""RecoveryInProgressException"".

 
{code:java}
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.RecoveryInProgressException): Failed to TRUNCATE_FILE /path/to/hdfs/2019-01-27/.part-0-0.inprogress.c1cab2ae-fd85-48b9-8441-a36af02225ca for DFSClient_NONMAPREDUCE_187299180_62 on xx.x.x.x because lease recovery is in progress. Try again later.
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2437)
at org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp.truncate(FSDirTruncateOp.java:111)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.truncate(FSNamesystem.java:2056)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.truncate(NameNodeRpcServer.java:1043)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.truncate(ClientNamenodeProtocolServerSideTranslatorPB.java:629)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:868)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:814)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2603)
{code}
 

The cluster is configured with a ""Fixed Delay"" restart strategy and the next time the job is restarted, I get the error with the ""AlreadyBeingCreatedException"".

The error continues appearing even waiting for an hour to restart... and even if this could have solved the issue, we cannot afford to wait that longer to restart the job.

 

I verified that there is no other job writing to the same bucket and that there is no other taskmanager from previous executions running.

 

I'm also able to reproduce the issue running everything in local, a Flink cluster with one jobmanager and 2 taskmanagers, local kafka and local HDFS. I send events to kafka and the job starts writing into HDFS, then I kill one taskmanager and when the other takes over, the Error occurs.

 

When using the BucketingSink I don't have any issue. The other taskmanager takes over and continue without any problem.;;;","28/Jan/19 15:12;edRojas;I performed more tests now by doing the modification I proposed in the description of the issue, this is by waiting for the lease to be released before trying to perform the truncate and with this the tests are successful.  This is solving the issue in my use case and test scenario.

I will issue a pull request for this.;;;","28/Jan/19 15:28;kkl0u;Thanks a lot [~edRojas] for looking into it. Looking forward to seeing the PR.;;;","29/Jan/19 16:33;edRojas;[~kkl0u] I created the PR, you can review it now;;;","30/Jan/19 18:33;edRojas;[~kkl0u] Travis build is failing but it's not related to the changes made in this PR.

[~sewen] I saw that you were the last person to modify this file, maybe you could take a look at the changes proposed.;;;","04/Feb/19 10:02;kkl0u;Merged on master with 892ff1dff01f7ec10df7e8e41ed990b92eca0b34
and release-1.7 with 11bbf952f0fa0a4b897c37027031dccebf11485e
and release-1.6 with 4e78d586207d752669620a1ffd606dfd30e53fbe;;;",,,,,,,,,,,,,,,,
Unable to build docs in Docker image,FLINK-11418,13211305,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,23/Jan/19 15:40,13/Feb/19 11:02,13/Jul/23 08:05,13/Feb/19 11:02,1.8.0,,,,,,,,1.8.0,,,,Documentation,,,0,pull-request-available,,,"Running 
{code:java}
cd flink/docs/docker
./run.sh{code}
 

And then in the container
{code:java}
Welcome to Apache Flink docs
To build, execute
./build_docs.sh
To watch and regenerate automatically
./build_docs.sh -p
and access http://localhost:4000

bash-4.4$ ./build_docs.sh -p
Traceback (most recent call last):
2: from /usr/local/bin/bundle:23:in `<main>'
1: from /usr/share/rubygems/rubygems.rb:308:in `activate_bin_path'
/usr/share/rubygems/rubygems.rb:289:in `find_spec_for_exe': can't find gem bundler (>= 0.a) with executable bundle (Gem::GemNotFoundException){code}
I believe there's something wrong.

 ",,rmetzger,,,,,,,,,,,,,,,,,,"rmetzger commented on pull request #7575: [FLINK-11418][docs] Fix version of bundler to 1.16.1
URL: https://github.com/apache/flink/pull/7575
 
 
   ## What is the purpose of the change
   
   It is currently not possible to build the docs on OS X Mojave, following the instructions from the readme.
   Also, the provided Dockerfile does not result in a working build environment.
   
   
   ## Brief change log
   
   - Update `docs/README.md`
   - Update the `Dockerfile`
   
   
   ## Verifying this change
   - go into the `docs/docker` directory and run `./run.sh` to build the image.
   - in the image, run `build_docs.sh`
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jan/19 13:21;githubbot;600","asfgit commented on pull request #7575: [FLINK-11418][docs] Fix version of bundler to 1.16.1
URL: https://github.com/apache/flink/pull/7575
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Feb/19 11:02;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 13 11:02:21 UTC 2019,,,,,,,,,,"0|yi0848:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/19 16:31;chesnay;Did you try following the docker instructions that were added recently? (see [here|https://flink.apache.org/improve-website.html#update-or-extend-the-documentation]);;;","23/Jan/19 16:34;rmetzger;Ah, I didn't know about these. The readme file in the ""docs"" directory did not mention them.

I'll check them out.;;;","23/Jan/19 16:37;chesnay;oh, this is about the _flink_ docs, not flink-web :/;;;","23/Jan/19 16:59;rmetzger;:) I have found a fix for the issue. Will open a PR soon.;;;","13/Feb/19 11:02;rmetzger;Pushed fix to master in 5e8e00b463 for release 1.8.0;;;",,,,,,,,,,,,,,,,,,,
JobManagerRunner does not wait for suspension of JobMaster,FLINK-11400,13210805,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,21/Jan/19 15:53,30/Jan/19 11:13,13/Jul/23 08:05,30/Jan/19 11:12,1.6.3,1.7.1,1.8.0,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"The {{JobManagerRunner}} does not wait for the suspension of the {{JobMaster}} to finish before granting leadership again. This can lead to a state where the {{JobMaster}} tries to start the {{ExecutionGraph}} but the {{SlotPool}} is still stopped.

I suggest to linearize the leadership operations (granting and revoking leadership) similarly to the {{Dispatcher}}.",,tison,trohrmann,xEviL,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7565: [FLINK-11400] Linearize leadership operations in JobManagerRunner
URL: https://github.com/apache/flink/pull/7565
 
 
   ## What is the purpose of the change
   
   Introduce a leadershipOperation future in the JobManagerRunner. This future is completed whenever
   a leadership operation (granting or revoking leadership) has been fully completed. All subsequent
   leadership operations wait for their predecessors to complete before they are processed. This
   guarantees that the JobMaster is properly shut down and there cannot be a race condition between
   revoking and granting leadership.
   
   This PR is based on #7564.
   
   ## Verifying this change
   
   - Added `JobManagerRunnerTest#testConcurrentLeadershipOperations`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/19 09:54;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 11:12:47 UTC 2019,,,,,,,,,,"0|yi051c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/19 16:37;tison;Do you mean introduce a {{recoveryOperation}} in {{JobManagerRunner}}?;;;","30/Jan/19 11:12;trohrmann;No, I mean to wait for a previous leadership operation to finish before starting a new one.;;;","30/Jan/19 11:12;trohrmann;Fixed via d4ba83e3baf31e041514e7340d4564ca0bee882a;;;",,,,,,,,,,,,,,,,,,,,,
Incorrectly use job information when call getSerializedTaskInformation in class TaskDeploymentDescriptor,FLINK-11389,13210323,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yuqi,yuqi,yuqi,18/Jan/19 08:23,31/Jan/19 14:51,13/Jul/23 08:05,31/Jan/19 14:51,1.6.3,1.7.1,1.8.0,,,,,,1.6.4,1.7.2,1.8.0,,Runtime / Coordination,,,0,pull-request-available,,,"See TaskDeploymentDescriptor


{code:java}
@Nullable
	public SerializedValue<TaskInformation> getSerializedTaskInformation() {
		if (serializedJobInformation instanceof NonOffloaded) {
			NonOffloaded<TaskInformation> jobInformation =
				(NonOffloaded<TaskInformation>) serializedTaskInformation;
			return jobInformation.serializedValue;
		} else {
			throw new IllegalStateException(
				""Trying to work with offloaded serialized job information."");
		}
	}
{code}

the condition serializedJobInformation instanceof NonOffloaded is not correctly, 
as serializedJobInformation and serializedTaskInformation are passed from ExecutionVertex#createDeploymentDescriptor


{code:java}

		if (jobInformationOrBlobKey.isLeft()) {
			serializedJobInformation = new TaskDeploymentDescriptor.NonOffloaded<>(jobInformationOrBlobKey.left());
		} else {
			serializedJobInformation = new TaskDeploymentDescriptor.Offloaded<>(jobInformationOrBlobKey.right());
		}

		final Either<SerializedValue<TaskInformation>, PermanentBlobKey> taskInformationOrBlobKey;

		try {
			taskInformationOrBlobKey = jobVertex.getTaskInformationOrBlobKey();
		} catch (IOException e) {
			throw new ExecutionGraphException(
				""Could not create a serialized JobVertexInformation for "" +
					jobVertex.getJobVertexId(), e);
		}

		final TaskDeploymentDescriptor.MaybeOffloaded<TaskInformation> serializedTaskInformation;

		if (taskInformationOrBlobKey.isLeft()) {
			serializedTaskInformation = new TaskDeploymentDescriptor.NonOffloaded<>(taskInformationOrBlobKey.left());
		} else {
			serializedTaskInformation = new TaskDeploymentDescriptor.Offloaded<>(taskInformationOrBlobKey.right());
		}
{code}
serializedJobInformation and serializedTaskInformation are not necessarily shared the class NonOffloaded or Offloaded

",,tison,trohrmann,yuqi,,,,,,,,,,,,,,,,"yuqi1129 commented on pull request #7532: [FLINK-11389] Fix Incorrectly use job information when call getSerial…
URL: https://github.com/apache/flink/pull/7532
 
 
   
   ## What is the purpose of the change
   
   - *Fix Incorrectly use job information when call getSerializedTaskInformation in class TaskDeploymentDescriptor*
   
   
   ## Brief change log
   
     - *Change the condition that should use serializedTaskInformation instead of  serializedJobInformation in the method of getSerializedTaskInformation*
   
   
   ## Verifying this change
   
     - *Added simple unit tests in TaskDeploymentDescriptorTest#testSerialization*
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - no
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jan/19 08:57;githubbot;600","asfgit commented on pull request #7532: [FLINK-11389] Fix Incorrectly use job information when call getSerial…
URL: https://github.com/apache/flink/pull/7532
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 14:50;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 31 14:51:02 UTC 2019,,,,,,,,,,"0|yi022g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/19 14:51;trohrmann;Fixed via
1.8.0:
446145f5ca0cd92c64a6944c19ca1ab53104030f
acf228cc5c88872428ccb10296aa1646837c16c2

1.7.2:
a5c6102bf642a0bfeff4438921aad3ab8d16cf1c
c39192a21098882338f0dcd9636f9241814099ce

1.6.4:
941ed4d816489990cd1a420d90b88f167a89db1a
ea90666b4415d1a7f510053b35101880438eab40;;;",,,,,,,,,,,,,,,,,,,,,,,
Flink task can not restart from failed after running two days.,FLINK-11387,13210313,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,jiweiautohome,jiweiautohome,18/Jan/19 07:19,18/Jan/19 08:52,13/Jul/23 08:05,18/Jan/19 08:52,1.7.0,,,,,,,,,,,,Deployment / YARN,Table SQL / API,,0,,,,"The sql statement is as below.

SELECT k AS key, SUM(p) AS pv, SUM(u) AS uv
FROM (
 SELECT concat_ws('|', udf_pcmplat(site_id), action, position_mark, udf_hour(_time)) AS k
 , COUNT(*) AS p
 , udf_countdistinct(cuid) AS u
 FROM xxxx
 WHERE action NOT IN (
 'unknown', 
 'nil', 
 'null', 
 'UNKNOWN', 
 'NIL', 
 'NULL'
 )
 AND udf_pcmplat(site_id) IS NOT NULL
 GROUP BY md5_2(cuid),udf_hour(_time), udf_pcmplat(site_id), action, position_mark
) t
GROUP BY k

 

This is the error log.

!image-2019-01-18-15-17-57-250.png!

Do you have any idea?"," 

 ",jiweiautohome,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/19 07:15;jiweiautohome;image-2019-01-18-15-15-36-281.png;https://issues.apache.org/jira/secure/attachment/12955347/image-2019-01-18-15-15-36-281.png","18/Jan/19 07:18;jiweiautohome;image-2019-01-18-15-17-57-250.png;https://issues.apache.org/jira/secure/attachment/12955346/image-2019-01-18-15-17-57-250.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Jan 18 08:11:19 UTC 2019,,,,,,,,,,"0|yi0208:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/19 07:35;tison;[~jiweiautohome] I would suggest you to ask/post this question on the user mail list [user@flink.apache.org|mailto:user@flink.apache.org]

There is where Flink community answers question like this.;;;","18/Jan/19 08:11;jiweiautohome;[~Tison] Thanks for your suggestion!;;;",,,,,,,,,,,,,,,,,,,,,,
Dispatcher does not clean up blobs of failed submissions,FLINK-11383,13210190,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,17/Jan/19 15:42,28/Feb/19 15:02,13/Jul/23 08:05,18/Jan/19 10:12,1.6.3,1.7.1,1.8.0,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"The {{Dispatcher}} does not clean up blobs originating from a failed submissions. Compared to the legacy code, this is a regression and we should remove relevant blobs.",,trohrmann,,,,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7523: [FLINK-11383][blob] Clean up blobs of failed submissions
URL: https://github.com/apache/flink/pull/7523
 
 
   ## What is the purpose of the change
   
   Let the dispatcher clean up blobs of failed submissions.
   
   ## Brief change log
   
   - Check whether `Dispatcher#submitJob` completes with an exception and if so clean up the job files
   
   ## Verifying this change
   
   - Added `DispatcherResourceCleanupTest#testBlobServerCleanupWhenJobSubmissionFails`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/19 16:22;githubbot;600","tillrohrmann commented on pull request #7523: [FLINK-11383][blob] Clean up blobs of failed submissions
URL: https://github.com/apache/flink/pull/7523
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jan/19 10:11;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,FLINK-9647,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 18 10:12:01 UTC 2019,,,,,,,,,,"0|yi018w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/19 10:12;trohrmann;Fixed via https://github.com/apache/flink/commit/b96add35cf656894c86fbf9e17e7adeef4772a71;;;",,,,,,,,,,,,,,,,,,,,,,,
YarnFlinkResourceManagerTest test case crashed ,FLINK-11380,13210092,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,yanghua,yanghua,17/Jan/19 08:46,31/Jan/19 13:29,13/Jul/23 08:05,31/Jan/19 13:29,,,,,,,,,1.8.0,,,,Tests,,,0,pull-request-available,test-stability,,"context:
{code:java}
17:18:44.415 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-yarn_2.11: There are test failures.
17:18:44.415 [ERROR] 
17:18:44.415 [ERROR] Please refer to /home/travis/build/apache/flink/flink-yarn/target/surefire-reports for the individual test results.
17:18:44.415 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
17:18:44.415 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
17:18:44.415 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-yarn && /usr/lib/jvm/java-8-oracle/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-yarn/target/surefire/surefirebooter3487840902331471745.jar /home/travis/build/apache/flink/flink-yarn/target/surefire 2019-01-16T17-02-23_939-jvmRun2 surefire3706271590182708448tmp surefire_332496616764820906947tmp
17:18:44.416 [ERROR] Error occurred in starting fork, check output in log
17:18:44.416 [ERROR] Process Exit Code: 243
17:18:44.416 [ERROR] Crashed tests:
17:18:44.416 [ERROR] org.apache.flink.yarn.YarnFlinkResourceManagerTest
17:18:44.416 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
17:18:44.416 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-yarn && /usr/lib/jvm/java-8-oracle/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-yarn/target/surefire/surefirebooter3487840902331471745.jar /home/travis/build/apache/flink/flink-yarn/target/surefire 2019-01-16T17-02-23_939-jvmRun2 surefire3706271590182708448tmp surefire_332496616764820906947tmp
17:18:44.416 [ERROR] Error occurred in starting fork, check output in log
17:18:44.416 [ERROR] Process Exit Code: 243
17:18:44.416 [ERROR] Crashed tests:
17:18:44.416 [ERROR] org.apache.flink.yarn.YarnFlinkResourceManagerTest
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
17:18:44.416 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
17:18:44.416 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
17:18:44.416 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
17:18:44.416 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
17:18:44.416 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
17:18:44.416 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
17:18:44.416 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
17:18:44.416 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
17:18:44.416 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
17:18:44.416 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
17:18:44.416 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
17:18:44.416 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
17:18:44.416 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
17:18:44.416 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
17:18:44.416 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
17:18:44.416 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
17:18:44.416 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-yarn && /usr/lib/jvm/java-8-oracle/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-yarn/target/surefire/surefirebooter3487840902331471745.jar /home/travis/build/apache/flink/flink-yarn/target/surefire 2019-01-16T17-02-23_939-jvmRun2 surefire3706271590182708448tmp surefire_332496616764820906947tmp
17:18:44.416 [ERROR] Error occurred in starting fork, check output in log
17:18:44.416 [ERROR] Process Exit Code: 243
17:18:44.416 [ERROR] Crashed tests:
17:18:44.416 [ERROR] org.apache.flink.yarn.YarnFlinkResourceManagerTest
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
17:18:44.416 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
17:18:44.416 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
17:18:44.416 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
17:18:44.416 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
17:18:44.416 [ERROR] at java.lang.Thread.run(Thread.java:748)
17:18:44.416 [ERROR] -> [Help 1]
17:18:44.416 [ERROR] 
17:18:44.416 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
17:18:44.416 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
17:18:44.416 [ERROR] 
17:18:44.416 [ERROR] For more information about the errors and possible solutions, please read the following articles:
17:18:44.416 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
17:18:44.416 [ERROR] 
17:18:44.416 [ERROR] After correcting the problems, you can resume the build with the command
17:18:44.416 [ERROR]   mvn <goals> -rf :flink-yarn_2.11
{code}
detail log : https://api.travis-ci.org/v3/job/480382807/log.txt

 ",,lining,trohrmann,yanghua,yunta,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7617: [FLINK-11380][tests] Finish mocking of resourceManagerClient before using it
URL: https://github.com/apache/flink/pull/7617
 
 
   ## What is the purpose of the change
   
   The problem was that we started the ResourceManager before we finished the mocking of the
   resourceManagerClient. This lead then to concurrent modifications of the mock which made
   the test fail.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 10:20;githubbot;600","tillrohrmann commented on pull request #7617: [FLINK-11380][tests] Finish mocking of resourceManagerClient before using it
URL: https://github.com/apache/flink/pull/7617
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 13:28;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,FLINK-11106,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 31 13:29:10 UTC 2019,,,,,,,,,,"0|yi00o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/19 11:47;chesnay;I've seen this multiple times in recent days.;;;","22/Jan/19 03:05;yunta;Another instance https://api.travis-ci.org/v3/job/481937341/log.txt;;;","23/Jan/19 13:15;trohrmann;Another instance: https://api.travis-ci.org/v3/job/483320202/log.txt;;;","24/Jan/19 07:56;trohrmann;Another instance: https://api.travis-ci.org/v3/job/483415561/log.txt;;;","30/Jan/19 02:12;lining;I met this too. [https://api.travis-ci.org/v3/job/485210100/log.txt];;;","30/Jan/19 12:36;trohrmann;Another instance: https://api.travis-ci.org/v3/job/486361850/log.txt;;;","31/Jan/19 10:07;trohrmann;Another instance: https://api.travis-ci.org/v3/job/486805136/log.txt;;;","31/Jan/19 13:29;trohrmann;Fixed via https://github.com/apache/flink/commit/c68bb9efb04bf518b776c70faf882c1173958166;;;",,,,,,,,,,,,,,,,
OutOfMemoryError when loading large TaskDeploymentDescriptor,FLINK-11379,13210088,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunhaibotb,sunhaibotb,sunhaibotb,17/Jan/19 08:35,02/Oct/19 17:44,13/Jul/23 08:05,01/Mar/19 09:37,1.7.0,1.7.1,,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"When TM loads a offloaded TDD with large size, it may throw a ""java.lang.OutOfMemoryError: Direct Buffer Memory"" error. The loading uses nio's _Files.readAllBytes()_ to read serialized TDD. In the call stack of _Files.readAllBytes()_ , it will allocate a direct memory buffer which's size is equal the length of the file. This will cause OutOfMemoryErro error when direct memory is not enough.

If the length of a file is large than a maximum buffer size,  the maximum size direct-buffer should be used to read bytes of the file to avoid direct memory OutOfMemoryError.  The maximum buffer size can be 8K or others.

The exception stack is as follows (this exception stack is from an old Flink version, but the master branch has the same problem).

Caused by: java.lang.OutOfMemoryError: Direct buffer memory
   at java.nio.Bits.reserveMemory(Bits.java:706)
   at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
   at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
   at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:241)
   at sun.nio.ch.IOUtil.read(IOUtil.java:195)
   at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:182)
   at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
   at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
   at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
   at java.nio.file.Files.read(Files.java:3105)
   at java.nio.file.Files.readAllBytes(Files.java:3158)
   at org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.loadBigData(TaskDeploymentDescriptor.java:338)
   at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:397)
   at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:211)
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:155)
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$onReceive$1(AkkaRpcActor.java:133)
   at akka.actor.ActorCell$$anonfun$become$1.applyOrElse(ActorCell.scala:544)
   at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
   at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
   ... 9 more",,sewen,sunhaibotb,sunjincheng121,,,,,,,,,,,,,,,,"sunhaibotb commented on pull request #7797: [FLINK-11379] Fix OutOfMemoryError caused by Files.readAllBytes() when TM loads a large size TDD
URL: https://github.com/apache/flink/pull/7797
 
 
   ## What is the purpose of the change
   
   This pull request makes TM do not throw a ""java.lang.OutOfMemoryError: Direct Buffer Memory"" error In the case of loading a offloaded TDD with large size.  The loading uses NIO's Files.readAllBytes() to read serialized TDD, and in the call stack of Files.readAllBytes() , it will allocate a direct memory buffer which's size is equal to the length of the file. This will cause OutOfMemoryError when the file is very large.
   
   
   ## Brief change log
   
     - A new method readAllBytes() which limits the used size of the direct buffer is added to the FileUtils class.
       - *The reason for adding this new method is that no an available method of Java API or third-party interface has been found. The available method should be not only as efficient and scalable (auto-scaling allocated byte array) as NIO's Files.available() , but also limits the used size of the direct buffer.*
       - *The new method is an implementation that follow nio's Files.readAllBytes(),  and the only difference is that it limits the used size of the direct buffer.*
     - When TM loads a offloaded TDD, FileUtils.readAllBytes() is called to replace nio's Files.readAllBytes().
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     -  Added test that validates that the results of FileUtils.readAllBytes() are correct in the following cases:
       - The file size (1K) is smaller than the direct-buffer size (4K).
       -  The file size (4K) is equal to the direct-buffer size (4K).
       -  The file size (5K) is greater than the direct-buffer size (4K).
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper:  no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/19 03:54;githubbot;600","asfgit commented on pull request #7797: [FLINK-11379] Fix OutOfMemoryError caused by Files.readAllBytes() when TM loads a large size TDD
URL: https://github.com/apache/flink/pull/7797
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/19 01:14;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 01 01:20:33 UTC 2019,,,,,,,,,,"0|yi00nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/19 12:35;sewen;+1 for your proposal.

In many places of Flink, we use 4K as the default buffer size, but 8K should work as well.;;;","25/Jan/19 06:42;sunhaibotb;Thanks for the response [~sewen].  I will use 4K as the default buffer size to keep up with other places of Flink.;;;","28/Jan/19 12:18;sewen;Sounds good.;;;","01/Mar/19 01:20;sunjincheng121;Fixed in master: 1f5359a5259987b3c9d506d559b6421af961ab0a

Fixed in release-1.8: c2bc493cb38258281a98a80848370b3a5b5c01e8;;;",,,,,,,,,,,,,,,,,,,,
Concurrent modification to slot pool due to SlotSharingManager releaseSlot directly ,FLINK-11375,13210052,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,eaglewatcher,tiemsn,tiemsn,17/Jan/19 04:01,28/Feb/19 14:00,13/Jul/23 08:05,21/Feb/19 09:06,1.7.1,,,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"In SlotPool, the AvailableSlots is lock free, so all access to it should in the main thread of SlotPool, and so all the public methods are called through SlotPoolGateway except the releaseSlot directly called by SlotSharingManager. This may cause a ConcurrentModificationException.

 2019-01-16 19:50:16,184 INFO [flink-akka.actor.default-dispatcher-161] org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: BlinkStoreScanTableSource feature_memory_entity_store-entity_lsc_page_detail_feats_group_178-Batch -> SourceConversion(table:[_DataStreamTable_12, source: [BlinkStoreScanTableSource feature_memory_entity_store-entity_lsc_page_detail_feats_group_178]], fields:(f0)) -> correlate: table(ScanBlinkStore_entity_lsc_page_detail_feats_group_1786($cor6.f0)), select: item_id,mainse_searcher_rank__cart_uv,mainse_searcher_rank__cart_uv_14,mainse_searcher_rank__cart_uv_30,mainse_searcher_rank__cart_uv_7,mainse_s (433/500) (bd34af8dd7ee02d04a4a25e698495f0a) switched from RUNNING to FINISHED.
 2019-01-16 19:50:16,187 INFO [jobmanager-future-thread-90] org.apache.flink.runtime.executiongraph.ExecutionGraph - scheduleVertices meet exception, need to fail global execution graph
 java.lang.reflect.UndeclaredThrowableException
 at org.apache.flink.runtime.rpc.akka.$Proxy26.allocateSlots(Unknown Source)
 at org.apache.flink.runtime.jobmaster.slotpool.SlotPool$ProviderAndOwner.allocateSlots(SlotPool.java:1955)
 at org.apache.flink.runtime.executiongraph.ExecutionGraph.schedule(ExecutionGraph.java:965)
 at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleVertices(ExecutionGraph.java:1503)
 at org.apache.flink.runtime.jobmaster.GraphManager$ExecutionGraphVertexScheduler.scheduleExecutionVertices(GraphManager.java:349)
 at org.apache.flink.runtime.schedule.StepwiseSchedulingPlugin.scheduleOneByOne(StepwiseSchedulingPlugin.java:132)
 at org.apache.flink.runtime.schedule.StepwiseSchedulingPlugin.onExecutionVertexFailover(StepwiseSchedulingPlugin.java:107)
 at org.apache.flink.runtime.jobmaster.GraphManager.notifyExecutionVertexFailover(GraphManager.java:163)
 at org.apache.flink.runtime.executiongraph.ExecutionGraph.resetExecutionVerticesAndNotify(ExecutionGraph.java:1372)
 at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.restart(FailoverRegion.java:213)
 at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.reset(FailoverRegion.java:198)
 at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.allVerticesInTerminalState(FailoverRegion.java:97)
 at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.lambda$cancel$0(FailoverRegion.java:169)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:186)
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:299)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1147)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
 at java.lang.Thread.run(Thread.java:834)
 Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invokeRpc(AkkaInvocationHandler.java:213)
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.invoke(AkkaInvocationHandler.java:125)
 ... 23 more
 Caused by: java.util.ConcurrentModificationException
 at java.util.HashMap$ValueSpliterator.tryAdvance(HashMap.java:1643)
 at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
 at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
 at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
 at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464)
 at org.apache.flink.runtime.jobmaster.slotpool.PreviousAllocationSchedulingStrategy.findPreviousAllocation(PreviousAllocationSchedulingStrategy.java:77)
 at org.apache.flink.runtime.jobmaster.slotpool.PreviousAllocationSchedulingStrategy.findMatchWithLocality(PreviousAllocationSchedulingStrategy.java:61)
 at org.apache.flink.runtime.jobmaster.slotpool.SlotPool$AvailableSlots.poll(SlotPool.java:1755)
 at org.apache.flink.runtime.jobmaster.slotpool.SlotPool$AvailableSlots.poll(SlotPool.java:1790)
 at org.apache.flink.runtime.jobmaster.slotpool.SlotPool.pollAndAllocateSlots(SlotPool.java:1094)
 at org.apache.flink.runtime.jobmaster.slotpool.SlotPool.requestAllocatedSlots(SlotPool.java:886)
 at org.apache.flink.runtime.jobmaster.slotpool.SlotPool.allocateSlots(SlotPool.java:590)
 at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:247)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:162)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
 at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
 at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
 at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
 at akka.actor.ActorCell.invoke(ActorCell.scala:495)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
 at akka.dispatch.Mailbox.run(Mailbox.scala:224)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
 at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",,eaglewatcher,kisimple,srichter,tiemsn,tison,trohrmann,,,,,,,,,,,,,"eaglewatcherwb commented on pull request #7644: [FLINK-11375][schedule] releaseSlot in SlotSharingManager using
URL: https://github.com/apache/flink/pull/7644
 
 
   SlotPoolGateway to avoid concurrent modification to slot pool
   
   Change-Id: I63bd752a090da8da8b9f6a38d9d3fe4ba443b90b
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   releaseSlot in SlotSharingManager using SlotPoolGateway to avoid concurrent modification to slot pool
   
   ## Brief change log
   
     - use SlotProviderAndOwner#cancelSlotRequest instead of AllocatedSlotActions#releaseSlot in SlotSharingManager#release
     - accommodate unit test for the fix
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive):no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Feb/19 02:27;githubbot;600","eaglewatcherwb commented on pull request #7644: [FLINK-11375][schedule] releaseSlot in SlotSharingManager using
URL: https://github.com/apache/flink/pull/7644
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Feb/19 01:45;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,FLINK-10431,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 21 09:06:23 UTC 2019,,,,,,,,,,"0|yi00fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/19 09:12;trohrmann;Nice catch!;;;","01/Feb/19 08:45;eaglewatcher;Hi, has anyone fixed this? If not, I would like to take it.;;;","21/Feb/19 09:06;srichter;This issue is already fixed as a byproduct of the changes in FLINK-10431.;;;",,,,,,,,,,,,,,,,,,,,,
CliFrontend cuts off reason for error messages,FLINK-11373,13209979,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xleesf,mxm,mxm,16/Jan/19 17:50,05/Feb/20 14:19,13/Jul/23 08:05,05/Feb/20 14:19,1.5.6,1.6.3,1.7.1,,,,,,1.11.0,,,,Command Line Client,,,0,pull-request-available,starter,,"The CliFrontend seems to only print the first message in the strace trace and not any of its causes.

{noformat}
bin/flink run /non-existing/path
Could not build the program from JAR file.

Use the help option (-h or --help) to get help on the command.
{noformat}

Notice, the underlying cause of this message is FileNotFoundException.

Consider changing 
a) the error message for this particular case 
b) the way the stack trace messages are trimmed",,aljoscha,AT-Fieldless,gjy,mxm,xleesf,,,,,,,,,,,,,,"leesf commented on pull request #7555: [FLINK-11373] CliFrontend cuts off reason for error messages
URL: https://github.com/apache/flink/pull/7555
 
 
   
   ## What is the purpose of the change
   
   Show error message of CliArgsException in CliFrontend.
   
   
   ## Brief change log
   
   Change System.out.println(e.getMessage()) to e.printStackTrace().
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/19 12:13;githubbot;600","aljoscha commented on pull request #7555: [FLINK-11373] CliFrontend cuts off reason for error messages
URL: https://github.com/apache/flink/pull/7555
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 14:18;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 14:19:21 UTC 2020,,,,,,,,,,"0|y00308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/19 14:28;AT-Fieldless;Hello,is anyone still working on this issue?If not,do you mind assigning to me?;;;","15/Feb/19 01:01;xleesf;[~AT-Fieldless], hi, thanks for your advise, but i am still working on this issue.;;;","05/Feb/20 14:19;aljoscha;master: 34db538ba176c63b42d33fc16e6b7d80f87a1bfb;;;",,,,,,,,,,,,,,,,,,,,,
AvroParquetReader not closed after use,FLINK-11371,13209949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fokko,fokko,fokko,16/Jan/19 16:04,15/Mar/19 11:45,13/Jul/23 08:05,17/Jan/19 12:23,1.7.1,,,,,,,,1.8.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,,The AvroParquetReader is not being closed,,aljoscha,fokko,,,,,,,,,,,,,,,,,"Fokko commented on pull request #7511: [FLINK-11371] The AvroParquetReader is not being closed
URL: https://github.com/apache/flink/pull/7511
 
 
   https://issues.apache.org/jira/browse/FLINK-11371
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/19 16:05;githubbot;600","aljoscha commented on pull request #7511: [FLINK-11371] The AvroParquetReader is not being closed
URL: https://github.com/apache/flink/pull/7511
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/19 12:22;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 17 12:23:49 UTC 2019,,,,,,,,,,"0|y002tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/19 12:23;aljoscha;Merged to master in
0a54983ca0207444477cfe991dbee1b611496204;;;",,,,,,,,,,,,,,,,,,,,,,,
Flink HA didn't remove ZK metadata,FLINK-11336,13209631,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,shengjk1,shengjk1,15/Jan/19 11:58,11/Jun/20 17:00,13/Jul/23 08:05,05/Mar/19 15:52,,,,,,,,,1.8.0,,,,Runtime / Coordination,,,0,pull-request-available,,,"Flink HA didn't remove ZK metadata

such as 

go to zk cli  : ls /flinkone

!image-2019-01-15-19-42-21-902.png!

 

i suggest we should delete this metadata when the application  cancel or throw exception",,felixzheng,sewen,shengjk1,trohrmann,,,,,,,,,,,,,,,"tillrohrmann commented on pull request #7880: [FLINK-11336][zk] Delete ZNodes when ZooKeeperHaServices#closeAndCleanupAllData
URL: https://github.com/apache/flink/pull/7880
 
 
   ## What is the purpose of the change
   
   When calling ZooKeeperHaServices#closeAndCleanupAllData we should delete the
   HA_CLUSTER_ID znode which is owned by the respective ZooKeeperHaServices.
   Moreover, the method tries to go up the chain of parent znodes and tries to
   delete all empty parent nodes. This should clean up otherwisely orphaned
   parent znodes.
   
   ## Verifying this change
   
   - Added `ZooKeeperHaServicesTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/19 16:00;githubbot;600","asfgit commented on pull request #7880: [FLINK-11336][zk] Delete ZNodes when ZooKeeperHaServices#closeAndCleanupAllData
URL: https://github.com/apache/flink/pull/7880
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Mar/19 15:51;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,FLINK-6522,FLINK-10694,,,,,,,,,,,,,,,"15/Jan/19 11:42;shengjk1;image-2019-01-15-19-42-21-902.png;https://issues.apache.org/jira/secure/attachment/12954971/image-2019-01-15-19-42-21-902.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 05 15:52:20 UTC 2019,,,,,,,,,,"0|y000v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/19 12:58;sewen;Flink should remove the metadata when the job terminates, which means
  - finished (for batch and bounded streams)
  - failed with no further retries
  - cancelled

It does not remove the metadata if you just kill the YARN application or stop all containers. In that case Flink does not know that this was not a failure, but an intended shutdown.

Can you confirm that this was a proper termination (as described above).
If yes, which way did you start the Flink job?
;;;","18/Jan/19 04:46;shengjk1;Unfamiliar with batch and bounded streams,so Inconvenient conclusion but  such as unbounded streams

 when

    failed with no further retries

    cancelled

we can remove the metadata ,As for how to start, you can start normally.I have already tried it, no problems in 1.8.0_151  flink 1.7.1 CDH5.13.1

 ;;;","18/Jan/19 13:22;sewen;Sorry, I cannot follow. What is the behavior now you observed?;;;","20/Jan/19 03:11;shengjk1;1.No matter what form stop flink, such  as cancel,failed with no further retries,kill, metadata not be deleted.

2.when cancel,failed with no further retries,kill,manually deleting metadata has no effect on newly launched programs even if there has a savepoint

this is my observed behavior;;;","22/Jan/19 15:48;sewen;What way did you start Flink?

  - standalone
  - Yarn (per job or as a session)
  - Mesos
  - Container;;;","23/Jan/19 05:53;shengjk1;Yarn (per job or as a session);;;","01/Mar/19 12:27;shengjk1;hi, [~till.rohrmann] 

I have other questions and suggestions:

 1. I want to know if  will also delete invalid directories on HDFS, similar to zk metadata?  because most of the metadata of HA is stored on HDFS. such as when  job is failed.

 2. when the job is canceled, the job's metadata is  deleted as default , but i think it also should  delete the corresponding directory, such as \{{jobId}}/shared and \{{jobId}}/taskowned. 

 

 ;;;","01/Mar/19 12:58;trohrmann;Hi [~shengjk1], I think you are right that we should also delete the checkpoint directories {{jobid/shared}} and {{jobId/taskowned}} if the job reaches a globally terminal state.

In order to not blow up the scope of this issue I would, however, suggest to create a separate issue for the cleanup of these directories. This issue tries to address the problems of the ZooKeeper meta data cleanup.;;;","01/Mar/19 13:09;trohrmann;I've opened the issue FLINK-11789 to track the checkpoint directory clean up [~shengjk1].;;;","01/Mar/19 13:45;shengjk1;[~till.rohrmann]  Yay, i think too, thank you ;;;","05/Mar/19 15:52;trohrmann;Fixed via
1.9.0: 11a8234b50586d476371c64f9d751e8e65fdad0a
1.8.0: f53594d8aad42d1de44fa8a33acc043cbd4c1d57;;;",,,,,,,,,,,,,
Allow negative offsets in window assigners,FLINK-11326,13209579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kezhuw,winipanda,winipanda,15/Jan/19 08:44,23/May/19 09:22,13/Jul/23 08:05,30/Jan/19 08:12,1.6.3,1.7.1,,,,,,,1.8.0,,,,API / DataStream,,,0,pull-request-available,,,"According to comments, we can use offset to adjust windows to timezones other than UTC-0. For example, in China you would have to specify an offset of {{Time.hours(-8)}}. 

 
{code:java}
/**
 * Creates a new {@code TumblingEventTimeWindows} {@link WindowAssigner} that assigns
 * elements to time windows based on the element timestamp and offset.
 *
 * <p>For example, if you want window a stream by hour,but window begins at the 15th minutes
 * of each hour, you can use {@code of(Time.hours(1),Time.minutes(15))},then you will get
 * time windows start at 0:15:00,1:15:00,2:15:00,etc.
 *
 * <p>Rather than that,if you are living in somewhere which is not using UTC±00:00 time,
 * such as China which is using UTC+08:00,and you want a time window with size of one day,
 * and window begins at every 00:00:00 of local time,you may use {@code of(Time.days(1),Time.hours(-8))}.
 * The parameter of offset is {@code Time.hours(-8))} since UTC+08:00 is 8 hours earlier than UTC time.
 *
 * @param size The size of the generated windows.
 * @param offset The offset which window start would be shifted by.
 * @return The time policy.
 */
public static TumblingEventTimeWindows of(Time size, Time offset) {
 return new TumblingEventTimeWindows(size.toMilliseconds(), offset.toMilliseconds());
}{code}
 

but when offset is smaller than zero, a IllegalArgumentException will be throwed.

 
{code:java}
protected TumblingEventTimeWindows(long size, long offset) {
 if (offset < 0 || offset >= size) {
 throw new IllegalArgumentException(""TumblingEventTimeWindows parameters must satisfy 0 <= offset < size"");
 }

 this.size = size;
 this.offset = offset;
}{code}
 ",,aljoscha,kezhuw,winipanda,wxchunjhyy@163.com,,,,,,,,,,,,,,,"kezhuw commented on pull request #7548: [FLINK-11326] Fix forbidden negative offset in window assigners
URL: https://github.com/apache/flink/pull/7548
 
 
   ## What is the purpose of the change
   Allow negative window offset in window assignment as the javadoc says.
   
   ## Brief change log
   - Allow negative window offset in window assignment.
   - Throw `IllegalArgumentException` if offset is out of range for `SlidingEventTimeWindows.of` and `SlidingProcessingTimeWindows.of`. 
   
   ## Verifying this change
   
   This change is already covered by existing tests and new test cases has been added to allow negative window offset.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
   ## Breaking changes
   * `@PublicEvolving` API `SlidingEventTimeWindows.of` and `SlidingProcessingTimeWindows.of` allows out of window offset previously, this merge request forbid this behavior. This way they behaves same as `TumblingEventTimeWindows.of` and `TumblingProcessingTimeWindows.of`. I think it is easier for caller to understand [sliding windows](https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/windows.html#sliding-windows).
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/19 00:42;githubbot;600",,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 23 09:20:41 UTC 2019,,,,,,,,,,"0|y000jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/19 08:12;aljoscha;Implemented in
master: 53044a08236a6b3a34199f5bd65187fad9014e20;;;","23/May/19 09:20;wxchunjhyy@163.com;Add affects version 1.7.2;;;",,,,,,,,,,,,,,,,,,,,,,
"The link to Zookeeper is broken in ""jobmanager_high_availability"" document",FLINK-11312,13209311,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,SleePy,SleePy,SleePy,14/Jan/19 04:01,16/Jan/19 09:43,13/Jul/23 08:05,14/Jan/19 08:58,1.7.1,,,,,,,,1.8.0,,,,Documentation,,,0,pull-request-available,,,"There is a broken link to Zookeeper getting started in ""jobmanager_high_availability"" document.",,SleePy,trohrmann,,,,,,,,,,,,,,,,,"ifndef-SleePy commented on pull request #7478: [FLINK-11312][docs] Fix broken link to Zookeeper getting started
URL: https://github.com/apache/flink/pull/7478
 
 
   ## What is the purpose of the change
   
   * Fix broken link to Zookeeper getting started in ""jobmanager_high_availability"" document
   
   ## Brief change log
   
   * Fix broken link to Zookeeper getting started in ""jobmanager_high_availability"" document
   
   ## Verifying this change
   
   * This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/19 06:22;githubbot;600","tillrohrmann commented on pull request #7478: [FLINK-11312][docs] Fix broken link to Zookeeper getting started
URL: https://github.com/apache/flink/pull/7478
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/19 08:58;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 09:43:18 UTC 2019,,,,,,,,,,"0|u00sqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/19 08:58;trohrmann;Fixed via https://github.com/apache/flink/commit/6c8abc13a80655470732dc948b2d127aaf2b9802;;;","16/Jan/19 09:43;chesnay;It would've been good to check why the broken-link detection didn't catch this one.;;;",,,,,,,,,,,,,,,,,,,,,,
FlinkS3FileSystem uses an incorrect path for temporary files,FLINK-11302,13208803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,artsem.semianenka,artsem.semianenka,artsem.semianenka,10/Jan/19 14:41,15/Mar/19 12:32,13/Jul/23 08:05,08/Feb/19 09:48,1.7.0,1.7.1,1.8.0,,,,,,1.7.2,1.8.0,,,FileSystems,,,0,pull-request-available,,,"I'm running the Flink job which stores the parquet files on S3.
For that purpose, I use StreamingFileSink (in Bulk format) which under the hood uses FlinkS3FileSystem from flink-s3-fs-hadoop-1.7.1.jar

When I try to submit it in Yarn cluster I got the following exception:
{code:java}
java.nio.file.NoSuchFileException: /mnt/e1/yarn/nm/usercache/ec2-user/appcache/application_1544473029846_0487,/mnt/e2/yarn/nm/usercache/ec2-user/appcache/application_1544473029846_0487/.tmp_d7a04184-6fed-4c69-9d2f-f950e06bf5f0
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:434)
	at java.nio.file.Files.newOutputStream(Files.java:216)
	at org.apache.flink.fs.s3.common.utils.RefCountedTmpFileCreator.apply(RefCountedTmpFileCreator.java:95)
	at org.apache.flink.fs.s3.common.utils.RefCountedTmpFileCreator.apply(RefCountedTmpFileCreator.java:43)
	at org.apache.flink.fs.s3.common.utils.RefCountedBufferingFileStream.openNew(RefCountedBufferingFileStream.java:174)
	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.boundedBufferingFileStream(S3RecoverableFsDataOutputStream.java:271)
	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.newStream(S3RecoverableFsDataOutputStream.java:236)
	at org.apache.flink.fs.s3.common.writer.S3RecoverableWriter.open(S3RecoverableWriter.java:85)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.rollPartFile(Bucket.java:221)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.write(Bucket.java:212)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.onElement(Buckets.java:268)
	at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.invoke(StreamingFileSink.java:370)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)
	....
{code}
As we can see the target path form stacktrace consist of two paths separated by a comma

It happens because in [AbstractS3FileSystemFactory|https://github.com/apache/flink/blob/bf5dc6c8bac73e7f8d54e983be2080cf6ce48a94/flink-filesystems/flink-s3-fs-base/src/main/java/org/apache/flink/fs/s3/common/AbstractS3FileSystemFactory.java#L141] we use the value of [CoreOptions.TMP_DIRS|https://github.com/apache/flink/blob/43a74977de27869e41e97878e821dcf363d81911/flink-core/src/main/java/org/apache/flink/configuration/CoreOptions.java#L216] ""as is"" and we expect that the value will contain only one path. But in reality, it may contain more than one path.

Proposed solution: Split path by comma and ""|"" and use only the first path.

Bugfix already exist and I will create a pull request soon",,aljoscha,artsem.semianenka,kkl0u,,,,,,,,,,,,,,,,"art4ul commented on pull request #7458: [FLINK-11302] FlinkS3FileSystem uses an incorrect path for temporary files.
URL: https://github.com/apache/flink/pull/7458
 
 
   ## What is the purpose of the change
   
   This pull request fixes the bug [FLINK-11302](https://issues.apache.org/jira/browse/FLINK-11302)
   [AbstractS3FileSystemFactory](https://github.com/apache/flink/blob/bf5dc6c8bac73e7f8d54e983be2080cf6ce48a94/flink-filesystems/flink-s3-fs-base/src/main/java/org/apache/flink/fs/s3/common/AbstractS3FileSystemFactory.java#L141) uses config value  [CoreOptions.TMP_DIRS](https://github.com/apache/flink/blob/43a74977de27869e41e97878e821dcf363d81911/flink-core/src/main/java/org/apache/flink/configuration/CoreOptions.java#L216).
   This value represented as a list of paths separated by the separator. 
   Instead of using the config value ""as is"" I split it by separator and use the first path as a temporary path for FlinkS3FileSystem
   
   
   ## Brief change log
   
     - * split TMP_DIRS config value by separator and use the first path as a temporary path for FlinkS3FileSystem*
   
   
   ## Verifying this change
   
     *Added test that validates that produced by AbstractS3FileSystemFactory object will contain an only first path from multiple paths in config  [link](https://github.com/art4ul/flink/blob/639cc3a97d40bd20a90bfa82df76589db596e2e8/flink-filesystems/flink-s3-fs-base/src/test/java/org/apache/flink/fs/s3/common/S3EntropyFsFactoryTest.java#L56)*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**no**)
     - The serializers: ( **no** )
     - The runtime per-record code paths (performance sensitive): (**no**)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**no**)
     - The S3 file system connector: (**yes**)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( **no**)
     - If yes, how is the feature documented? (**not applicable**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/19 16:38;githubbot;600","asfgit commented on pull request #7458: [FLINK-11302] FlinkS3FileSystem uses an incorrect path for temporary files.
URL: https://github.com/apache/flink/pull/7458
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/19 14:12;githubbot;600",,,,,,,,,,,1800,600,1200,66%,1800,600,1200,,,,,,,,FLINK-11496,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 14:13:17 UTC 2019,,,,,,,,,,"0|u00pmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/19 14:13;kkl0u;Merged on master with 291373a0b61f4651ddbbaf0dc3f9dd9fd68db611
and on release-1.7 with f2635afb94e0164987df55fc4f4838cb6f4580f3;;;",,,,,,,,,,,,,,,,,,,,,,,
Travis failed: No output has been received in the last 10m0s,FLINK-11301,13208774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hequn8128,hequn8128,10/Jan/19 12:48,28/Nov/19 09:29,13/Jul/23 08:05,28/Nov/19 09:29,,,,,,,,,,,,,Travis,,,0,,,,"-[https://api.travis-ci.org/v3/job/477777082/log.txt]-

new log link: [https://api.travis-ci.org/v3/job/483359744/log.txt|https://api.travis-ci.org/v3/job/483359744/log.txt]",,hequn8128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 02:59;hequn8128;error.log;https://issues.apache.org/jira/secure/attachment/12956079/error.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 24 12:19:12 UTC 2019,,,,,,,,,,"0|u00pg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/19 14:51;chesnay;The linked build finished successfully.;;;","11/Jan/19 11:56;hequn8128;Sorry, it seems travis will overwrite the log when restart the test. I will close this issue. ;;;","24/Jan/19 02:56;hequn8128;I encountered the same error in my last PR.  Log link has been updated in the Description.;;;","24/Jan/19 08:44;chesnay;We've been experiencing issues with{{transfer.sh}} for a while now, but for now I'd observe it a bit longer before taking any action.;;;","24/Jan/19 12:19;hequn8128;[~chesnay] Sounds good. We can observe it a bit longer. If this error happens frequently, we may add some retry logic. ;;;",,,,,,,,,,,,,,,,,,,
RocksDBListState should use actual registered state serializer instead of serializer provided by descriptor,FLINK-11287,13208327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tzulitai,tzulitai,tzulitai,08/Jan/19 14:25,09/Jan/19 12:04,13/Jul/23 08:05,09/Jan/19 12:04,,,,,,,,,1.8.0,,,,Runtime / State Backends,,,0,pull-request-available,,,"Currently, when creating a {{RocksDBListState}}, the element serializer wrapped by the {{RocksDBListState}} is retrieved from the state descriptor:

{code:java}
static <E, K, N, SV, S extends State, IS extends S> IS create(
    StateDescriptor<S, SV> stateDesc,
    Tuple2<ColumnFamilyHandle, RegisteredKeyValueStateBackendMetaInfo<N, SV>> registerResult,
    RocksDBKeyedStateBackend<K> backend) {

    return (IS) new RocksDBListState<>(
        registerResult.f0,
        registerResult.f1.getNamespaceSerializer(),
        (TypeSerializer<List<E>>) registerResult.f1.getStateSerializer(),
        (List<E>) stateDesc.getDefaultValue(),
        ((ListStateDescriptor<E>) stateDesc).getElementSerializer(), // incorrect
        backend);
}
{code}

This is incorrect, since new serializers retrieved from state descriptors have not been checked for compatibility with restored state.

Instead, the element serializer should be retrieved from the register result, which contains the actual state serializer registered in the state backend for state access (and has already been checked for compatibility / reconfigured appropriately).",,tzulitai,,,,,,,,,,,,,,,,,,"tzulitai commented on pull request #7434: [FLINK-11287] [rocksdb] RocksDBListState should be using registered serializer in state meta infos
URL: https://github.com/apache/flink/pull/7434
 
 
   ## What is the purpose of the change
   
   `RocksDBListState` was using the serializer provided by the state descriptor, which is incorrect because that serializer have not been checked for compatibility or reconfigured if required.
   
   Instead, it should be using the actual resulting registered serializer in the backend's state meta infos, which is guaranteed to have been checked for compatibility.
   
   ## Brief change log
   
   - 9317611: the main change as described in description
   - 6b7401c: Validates the fix by activating a previously ignored test in `StateBackendMigrationTestBase`. That test wasn't passing due to FLINK-11073 and FLINK-11287 (which this PR fixes).
   
   ## Verifying this change
   
   The previously ignored test `StateBackendMigrationTestBase.testKeyedListStateSerializerReconfiguration` should now pass, as FLINK-11287 is partially the reason of why it wasn't passing before.
   
   That test verifies that `RocksDBListState` is indeed using the correct, compatibility-checked / reconfigured serializer.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/19 17:40;githubbot;600","asfgit commented on pull request #7434: [FLINK-11287] [rocksdb] RocksDBListState should be using registered serializer in state meta infos
URL: https://github.com/apache/flink/pull/7434
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/19 11:57;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 12:04:15 UTC 2019,,,,,,,,,,"0|u00mp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/19 12:04;tzulitai;Merged for 1.8.0: e5ed8c85139d1dfdb4b6a47a63ac36143a7b5c64;;;",,,,,,,,,,,,,,,,,,,,,,,
RocksDBSerializedCompositeKeyBuilder's key serializer is not being reconfigured properly,FLINK-11280,13208222,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tzulitai,tzulitai,tzulitai,08/Jan/19 06:41,08/Jan/19 14:12,13/Jul/23 08:05,08/Jan/19 14:08,,,,,,,,,1.8.0,,,,Runtime / State Backends,,,0,pull-request-available,,,"This can be confirmed by failing tests:
[https://travis-ci.org/apache/flink/jobs/476533588]

The failing test is {{StateBackendMigrationTestBase.testStateBackendRestoreSucceedsIfNewKeySerializerRequiresReconfiguration}}, which was a new test added in FLINK-11073.

This test started to fail when the changes were rebased on top of FLINK-9702.

The problem is that starting from FLINK-11073, all state serializers (including key serializer) should be wrapped within / retrieved from a {{StateSerializerProvider}}, which handles the logic of compatibility checks of state serializers and reassigning serializer references to reconfigured instances if required.

The new {{RocksDBSerializedCompositeKeyBuilder}} introduced in FLINK-9702, however, holds its own final reference directly to the key serializer, instead of using a {{StateSerializerProvider}}.
This change essentially makes the key serializer non-reconfigurable.",,aljoscha,eaglewatcher,tzulitai,,,,,,,,,,,,,,,,"tzulitai commented on pull request #7428: [FLINK-11280] [state backends] Let RocksDBSerializedCompositeKeyBuilder accept key serializer lazily
URL: https://github.com/apache/flink/pull/7428
 
 
   ## What is the purpose of the change
   
   This PR fixes the failing test in master:
   `StateBackendMigrationTestBase.testStateBackendRestoreSucceedsIfNewKeySerializerRequiresReconfiguration`.
   
   The failing test indicates that a key serializer that was supposed to
   have been reconfigured was still being used.
   Serializers are assumed to be immutable, and therefore when a
   reconfiguration occurs, the key serializer instance to use would have changed
   from the originally provided key serializer.
   
   The problem is in the `RocksDBSerializedCompositeKeyBuilder`, which
   keeps a reference to the original key serializer and always uses that.
   This essentially ignores any key serializer reconfiguration that may
   have happened.
   
   Prior to this PR, the `RocksDBSerializedCompositeKeyBuilder` expects a
   key serializer when creating the builder. This PR fixes the failing tests by
   changing the usage of `RocksDBSerializedCompositeKeyBuilder` so that the
   key serializer is only provided lazily when the builder is used to build a composite
   serialized key.
   
   ## Verifying this change
   
   The previously failing test `StateBackendMigrationTestBase.testStateBackendRestoreSucceedsIfNewKeySerializerRequiresReconfiguration` should now pass.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): **yes**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/19 08:00;githubbot;600","tzulitai commented on pull request #7429:  [FLINK-11280] [rocksdb] Lazily create RocksDBSerializedCompositeKeyBuilder only after restore
URL: https://github.com/apache/flink/pull/7429
 
 
   ## What is the purpose of the change
   
   This PR fixes the failing test in master:
   `StateBackendMigrationTestBase.testStateBackendRestoreSucceedsIfNewKeySerializerRequiresReconfiguration`.
   
   The test reconfigures the key serializer and verifies that the reconfigured version is used instead of the original one. The test fails because when writing keys in the `RocksDBKeyedStateBackend`, the `RocksDBSerializedCompositeKeyBuilder` was using the invalid, non-reconfigured key serializer.
   
   The culprit is that the composite key builder is created in the constructor of the `RocksDBKeyedStateBackend`. The creation of the builder requires providing a key serializer.
   
   This is problematic, because the key serializer may be reconfigured during the restore phase, therefore invalidating the key serializer used by the composite key builder.
   
   This commit resolves this by lazily creating the composite key builder only after the restore phase, which would be the point-in-time when we are certain the key serializer will no longer be changed and is final.
   
   ## Verifying this change
   
   The previously failing test `StateBackendMigrationTestBase.testStateBackendRestoreSucceedsIfNewKeySerializerRequiresReconfiguration` should now pass.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/19 10:35;githubbot;600","tzulitai commented on pull request #7428: [FLINK-11280] [state backends] Let RocksDBSerializedCompositeKeyBuilder accept key serializer lazily
URL: https://github.com/apache/flink/pull/7428
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/19 10:36;githubbot;600",,,,,,,,,,,0,1800,,,0,1800,,,,,,,,FLINK-11281,FLINK-11284,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 08 14:07:35 UTC 2019,,,,,,,,,,"0|u00m1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/19 06:46;tzulitai;I think the same goes for some of our other state handle classes, e.g. {{RocksDBListState}} which holds its own copy of the list's element serializer. Will need to double check them.;;;","08/Jan/19 14:07;tzulitai;Fixed in master (for 1.8.0): 31685a36d1344e34c37b678c0335d560ccf6f4f4.;;;",,,,,,,,,,,,,,,,,,,,,,
Invalid week interval parsing in ExpressionParser,FLINK-11279,13208201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,x1q1j1,x1q1j1,x1q1j1,08/Jan/19 04:36,08/Jan/19 15:11,13/Jul/23 08:05,08/Jan/19 10:43,1.7.0,1.7.1,,,,,,,1.7.2,1.8.0,,,Table SQL / API,,,0,pull-request-available,,,"Execute the following code:
    testAllApis(""2016-03-31"".toDate - 1.week,
      ""'2016-03-31'.toDate - 1.week"",
      ""timestampadd(WEEK, -1, date '2016-03-31')"",
      ""2016-03-24"")
Please see the screenshot for the error report.",,twalthr,x1q1j1,,,,,,,,,,,,,,,,,"XuQianJin-Stars commented on pull request #7426: [FLINK-11279][Table API & SQL]The bug of Error parsing ExpressionParser
URL: https://github.com/apache/flink/pull/7426
 
 
   ## What is the purpose of the change
   
   *(This PR is designed to solve the problem of The bug of Error parsing ExpressionParser.)*
   
   ## Brief change log
   
   *(To solve this bug, I added a file BOM header encoding check to determine the current file encoding, so that when user-defined encoding format and file with a BOM header encoding format conflict processing, specific changes in the following::)*
   
   - I will change WEEKS to WEEK, as follows:
   
     before
   
     ```scala
       case expr ~ _ ~ (WEEKS.key | WEEKS.key) => toMilliInterval(expr, 7 * MILLIS_PER_DAY)
     ```
   
     after
   
     ```scala
       case expr ~ _ ~ (WEEKS.key | WEEK.key) => toMilliInterval(expr, 7 * MILLIS_PER_DAY)
     ```
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
   
   - *I added a test  to `ScalarFunctionsTest`.*
   
   ## Does this pull request potentially affect one of the following parts:
   
   - Dependencies (does it add or upgrade a dependency): (yes)
   - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
   - The serializers: (no)
   - The runtime per-record code paths (performance sensitive): (no )
   - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
   - The S3 file system connector: (no)
   
   ## Documentation
   
   - Does this pull request introduce a new feature? (yes / no)
   
   - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/19 04:58;githubbot;600","asfgit commented on pull request #7426: [FLINK-11279][Table API & SQL]The bug of Error parsing ExpressionParser
URL: https://github.com/apache/flink/pull/7426
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/19 10:40;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/19 04:37;x1q1j1;20190108123404.png;https://issues.apache.org/jira/secure/attachment/12954098/20190108123404.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 08 15:11:26 UTC 2019,,,,,,,,,,"0|u00lx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/19 10:42;twalthr;Fixed in 1.8: 90f3b424c74b3673f90093acc8384acf1ef69f74
Fixed in 1.7: 6528a3a5e651d9e98a028983f749e78645a078ca;;;","08/Jan/19 15:11;x1q1j1;hi [~twalthr] Thank you very much

Best,

qianjin;;;",,,,,,,,,,,,,,,,,,,,,,
Queryable state (rocksdb) end-to-end test fails,FLINK-11273,13208048,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,twalthr,twalthr,07/Jan/19 15:26,08/Jan/19 10:15,13/Jul/23 08:05,07/Jan/19 20:13,,,,,,,,,,,,,API / Type Serialization System,,,0,,,,"It seems the queryable state e2e fails consistently on Travis:

{code}
Exception in thread ""main"" java.util.concurrent.ExecutionException: java.lang.RuntimeException: Failed request 15.
 Caused by: java.lang.RuntimeException: Failed request 15.
 Caused by: java.lang.RuntimeException: Error while processing request with ID 15. Caused by: org.apache.flink.util.FlinkRuntimeException: Error while deserializing the user value.
	at org.apache.flink.contrib.streaming.state.RocksDBMapState$RocksDBMapEntry.getValue(RocksDBMapState.java:428)
	at org.apache.flink.queryablestate.client.state.serialization.KvStateSerializer.serializeMap(KvStateSerializer.java:222)
	at org.apache.flink.contrib.streaming.state.RocksDBMapState.getSerializedValue(RocksDBMapState.java:297)
	at org.apache.flink.queryablestate.server.KvStateServerHandler.getSerializedValue(KvStateServerHandler.java:107)
	at org.apache.flink.queryablestate.server.KvStateServerHandler.handleRequest(KvStateServerHandler.java:84)
	at org.apache.flink.queryablestate.server.KvStateServerHandler.handleRequest(KvStateServerHandler.java:48)
	at org.apache.flink.queryablestate.network.AbstractServerHandler$AsyncRequestTask.run(AbstractServerHandler.java:236)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException: No more bytes left.
	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(NoFetchingInput.java:79)
	at com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:355)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:315)
	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:415)
	at org.apache.flink.contrib.streaming.state.RocksDBMapState.deserializeUserValue(RocksDBMapState.java:322)
	at org.apache.flink.contrib.streaming.state.RocksDBMapState.access$100(RocksDBMapState.java:66)
	at org.apache.flink.contrib.streaming.state.RocksDBMapState$RocksDBMapEntry.getValue(RocksDBMapState.java:426)
	... 11 more

	at org.apache.flink.queryablestate.server.KvStateServerHandler.handleRequest(KvStateServerHandler.java:95)
	at org.apache.flink.queryablestate.server.KvStateServerHandler.handleRequest(KvStateServerHandler.java:48)
	at org.apache.flink.queryablestate.network.AbstractServerHandler$AsyncRequestTask.run(AbstractServerHandler.java:236)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at org.apache.flink.queryablestate.network.AbstractServerHandler$AsyncRequestTask.lambda$run$0(AbstractServerHandler.java:273)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778)
	at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140)
	at org.apache.flink.queryablestate.network.AbstractServerHandler$AsyncRequestTask.run(AbstractServerHandler.java:236)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.streaming.tests.queryablestate.QsStateClient.getMapState(QsStateClient.java:121)
	at org.apache.flink.streaming.tests.queryablestate.QsStateClient.main(QsStateClient.java:73)
{code}

Logs can be found at: https://travis-ci.org/apache/flink/builds/476312849?utm_source=slack&utm_medium=notification",,srichter,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11277,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 20:13:46 UTC 2019,,,,,,,,,,"0|u00kz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/19 20:13;srichter;Merged in:
master: 0b28bfd0d9;;;",,,,,,,,,,,,,,,,,,,,,,,
Invalid reference to AvroSinkWriter in example AvroKeyValueSinkWriter in javadoc,FLINK-11265,13207643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fokko,fokko,fokko,04/Jan/19 10:04,07/Jan/19 12:49,13/Jul/23 08:05,07/Jan/19 12:49,1.7.1,,,,,,,,1.8.0,,,,Documentation,,,0,pull-request-available,,,,,dwysakowicz,fokko,,,,,,,,,,,,,,,,,"Fokko commented on pull request #7413: [FLINK-11265][docs] AvroSinkWriter → AvroKeyValueSinkWriter
URL: https://github.com/apache/flink/pull/7413
 
 
   AvroSinkWriter should be AvroKeyValueSinkWriter
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jan/19 10:06;githubbot;600","dawidwys commented on pull request #7413: [FLINK-11265][docs] AvroSinkWriter → AvroKeyValueSinkWriter
URL: https://github.com/apache/flink/pull/7413
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/19 12:47;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 12:49:24 UTC 2019,,,,,,,,,,"0|u00ih4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/19 12:49;dwysakowicz;Fixed in ec48beaa2c86b6e1d6c09d4be8bc3564dcdd0ead;;;",,,,,,,,,,,,,,,,,,,,,,,
Incorrect way to stop yarn session described in yarn_setup document,FLINK-11253,13207400,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xleesf,Tao Yang,Tao Yang,03/Jan/19 04:27,11/Jan/19 06:30,13/Jul/23 08:05,10/Jan/19 17:29,,,,,,,,,1.8.0,,,,Documentation,,,0,pull-request-available,,,"There are two ways to stop yarn session described in yarn_setup document:
{noformat}
Stop the YARN session by stopping the unix process (using CTRL+C) or by entering ‘stop’ into the client.
{noformat}
But in fact, yarn session application still can run after stopping the unix process (using CTRL+C).
We can either update the yarn_setup document to remove this incorrect way or add ShutdownHook to stop yarn session in FlinkYarnSessionCli to make it correct.
Looking forward to the feedbacks and would like to work on this ticket. Thanks.",,guoyangze,JaryZhen,Tao Yang,trohrmann,zjffdu,,,,,,,,,,,,,,"yt526 commented on pull request #7414: [FLINK-11253] Add shutdown hook for yarn session client in attached mode
URL: https://github.com/apache/flink/pull/7414
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This pull request makes yarn session cluster shutdown through adding shutdown hook if it's submitted in attached mode. 
   
   ## Brief change log
   
     - *Add shutdown hook for the attached mode in FlinkYarnSessionCli, and move the shutdown logic to a independent method which can be called by shutdown hook and the handling process of stop command*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? docs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jan/19 10:43;githubbot;600","asfgit commented on pull request #7414: [FLINK-11253] Add shutdown hook for yarn session client in attached mode
URL: https://github.com/apache/flink/pull/7414
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/19 17:31;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 11 06:30:04 UTC 2019,,,,,,,,,,"0|u00gz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/19 05:51;zjffdu;I think this is a bug that needs to be fixed if ctrl+c doesn't kill the yarn app. ;;;","03/Jan/19 09:49;trohrmann;I think you're right. We should register the shutdown hooks when running the {{FlinkYarnSessionCli}} from the main method.;;;","04/Jan/19 10:51;Tao Yang;Thanks [~zjffdu] and [~till.rohrmann] for your suggestions.

I have submitted a PR to add shutdown hook for yarn seesion client in attached mode, could you please help to review in your free time? Thanks.;;;","10/Jan/19 17:29;trohrmann;Fixed via
a07ce7f6c88dc7d0c0d2ba55a0ab3f2283bf247c
1abf8d14d159a2330029082ed715cbf7dbcc08b3;;;","11/Jan/19 02:13;Tao Yang;Thanks [~till.rohrmann] for the review and commit.;;;","11/Jan/19 06:30;Tao Yang;Hi, [~till.rohrmann]. 
Could you kindly give me the contributor permission or help to reassign this issue to me? Thanks.;;;",,,,,,,,,,,,,,,,,,
Add Scala 2.12 download column,FLINK-11252,13207279,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,02/Jan/19 12:47,04/Jan/19 10:44,13/Jul/23 08:05,04/Jan/19 10:44,,,,,,,,,,,,,Project Website,,,0,pull-request-available,,,"The download page has a ""Scala 2.11"" column, that was used in the past to provide distinct download links for different scala versions.
We currently however list releases separately for each scala version.
We should either remove the column title or refactor the download page to also have a ""Scala 2.12"" column.",,,,,,,,,,,,,,,,,,,,"zentol commented on pull request #144: [FLINK-11252] Add scala 2.12 download column 
URL: https://github.com/apache/flink-web/pull/144
 
 
   This PR modifies the download page to feature Scala 2.11 / Scala 2.12 columns with their respective download links, instead of listing each scala-variant of a release as a separate item.
   
   ![new_downloads](https://user-images.githubusercontent.com/5725237/50593646-6479cd80-0e99-11e9-82ac-d8dfc68bda9c.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 13:47;githubbot;600","zentol commented on pull request #144: [FLINK-11252] Add scala 2.12 download column 
URL: https://github.com/apache/flink-web/pull/144
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jan/19 10:43;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/19 12:47;chesnay;downloads.png;https://issues.apache.org/jira/secure/attachment/12953509/downloads.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 04 10:44:28 UTC 2019,,,,,,,,,,"0|u00g8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/19 10:44;chesnay;asf-site: e7ece77db1621c1b993932b53803fa69b3f8629d;;;",,,,,,,,,,,,,,,,,,,,,,,
Incompatible metric name on prometheus reporter,FLINK-11251,13207253,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tonywei,tonywei,tonywei,02/Jan/19 10:06,09/Jan/19 08:25,13/Jul/23 08:05,09/Jan/19 08:25,1.5.6,1.6.3,1.7.0,,,,,,1.6.4,1.7.2,1.8.0,,Runtime / Metrics,,,0,pull-request-available,,,"{code}
# HELP flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_4_currentOffsets currentOffsets (scope: taskmanager_job_task_operator_KafkaConsumer_topic_partition_4)
# TYPE flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_4_currentOffsets gauge
flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_4_currentOffsets{task_attempt_id=""5137e35cf7319787f6cd627621fd2ea7"",host=""localhost"",task_attempt_num=""0"",tm_id=""e72a527652f5af1358bdbc0f5bf6f49d"",partition=""4"",topic=""rt_lookback_state"",job_id=""546cf6f0d1f0b818afd9697c612f715c"",task_id=""d7b1ad914351f9ee5272ffff67f51160"",operator_id=""d7b1ad914351f9ee5272ffff67f51160"",operator_name=""Source:_kafka_lookback_state_source"",task_name=""Source:_kafka_lookback_state_source"",job_name=""FlinkRuleMatchPipeline"",subtask_index=""7"",} 1.456090927E9
# HELP flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_24_committedOffsets committedOffsets (scope: taskmanager_job_task_operator_KafkaConsumer_topic_partition_24)
# TYPE flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_24_committedOffsets gauge
flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_24_committedOffsets{task_attempt_id=""9b666af68ec4734b25937b8b94cc5c84"",host=""localhost"",task_attempt_num=""0"",tm_id=""e72a527652f5af1358bdbc0f5bf6f49d"",partition=""24"",topic=""rt_event"",job_id=""546cf6f0d1f0b818afd9697c612f715c"",task_id=""61252f73469d3ffba207c548d29a0267"",operator_id=""61252f73469d3ffba207c548d29a0267"",operator_name=""Source:_kafka_source"",task_name=""Source:_kafka_source____sampling____parse_and_filter"",job_name=""FlinkRuleMatchPipeline"",subtask_index=""27"",} 3.001186523E9
{code}

This is a snippet from my flink prometheus reporter. It showed that kafka current offsets and committed offsets metric names changed after I migrated my flink job from 1.6.0 to 1.6.3.

The origin metrics name should not contain {{partition index}} in metric name, i.e. the metric name should be {{flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_currentOffsets}} and {{flink_taskmanager_job_task_operator_KafkaConsumer_topic_partition_committedOffsets}}.

After digging into the source code, I found that the incompatibility started from this [PR|https://github.com/apache/flink/pull/7095], because it overloaded a new {{getLogicalScope(CharacterFilter, char, int)}} and didn't override in {{GenericValueMetricGroup}} class.
When the tail metric group from a metric is {{GenericValueMetricGroup}} and this new {{getLogicalScope}} is called, i.e. calling {{FrontMetricGroup#getLogicalScope}}, the value group name will not be ignored, but it should be in previous released version.
",,tonywei,,,,,,,,,,,,,,,,,,"tony810430 commented on pull request #7398: [FLINK-11251] [metrics] GenericValueMetricGroup should always ignore its group name in logical scope
URL: https://github.com/apache/flink/pull/7398
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fixes an issue in the metric system where value from user-defined variable wasn't ignored in logic scope in some situations. As a result, some reporters using logic scope as metric name, like prometheus reporter, will get an incompatible metric name.
   
   ## Brief change log
   
     - override createLogicalScope in GenericMetricGroup
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - run `MetricGroupTest#testLogicalScopeShouldIgnoreValueGroupName`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 10:28;githubbot;600","zentol commented on pull request #7398: [FLINK-11251] [metrics] GenericValueMetricGroup should always ignore its group name in logical scope
URL: https://github.com/apache/flink/pull/7398
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/19 08:12;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 08:25:32 UTC 2019,,,,,,,,,,"0|u00g2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/19 08:25;chesnay;master: 9742ef7a10d89d27dce1a2fa3d5e91e4adb296ed
1.7: fdf7cb794604e72d0c7fe7cbf7f6fb18aff0e5ed 
1.6: fb9b15cce151d4f7783a1c2cce33df58004f865f ;;;",,,,,,,,,,,,,,,,,,,,,,,
fix thread leaked when StreamTask switched from DEPLOYING to CANCELING,FLINK-11250,13207250,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,akalashnikov,lamber-ken,lamber-ken,02/Jan/19 09:55,17/Sep/21 02:17,13/Jul/23 08:05,17/Sep/21 02:17,1.5.6,1.6.3,1.7.1,,,,,,1.7.3,,,,Runtime / Task,,02/Jan/19 00:00,1,auto-deprioritized-major,auto-unassigned,pull-request-available,"begin flink-1.5.x version, streamRecordWriters was created in StreamTask's constructor, which start OutputFlusher daemon thread. so when task switched from DEPLOYING to CANCELING state, the daemon thread will be leaked.

 

*reproducible example*
{code:java}
public static void main(String[] args) throws Exception {

    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.enableCheckpointing(5000);
    env
            .addSource(new SourceFunction<String>() {
                @Override
                public void run(SourceContext<String> ctx) throws Exception {

                    for (int i = 0; i < 100000000; i++) {
                        Thread.sleep(100);
                        ctx.collect(""data "" + i);
                    }
                }

                @Override
                public void cancel() {

                }
            })
            .addSink(new RichSinkFunction<String>() {

                @Override
                public void open(Configuration parameters) throws Exception {
                    System.out.println(1 / 0);
                }

                @Override
                public void invoke(String value, Context context) throws Exception {

                }

            }).setParallelism(2);


    env.execute();

}{code}
*some useful log*
{code:java}
2019-01-02 03:03:47.525 [thread==> jobmanager-future-thread-2] executiongraph.Execution#transitionState:1316 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from CREATED to SCHEDULED.
2019-01-02 03:03:47.526 [thread==> flink-akka.actor.default-dispatcher-5] slotpool.SlotPool#allocateSlot:326 Received slot request [SlotRequestId{12bfcf1674f5b96567a076086dbbfd1b}] for task: Attempt #1 (Source: Custom Source (1/1)) @ (unassigned) - [SCHEDULED]
2019-01-02 03:03:47.527 [thread==> flink-akka.actor.default-dispatcher-5] slotpool.SlotSharingManager#createRootSlot:151 Create multi task slot [SlotRequestId{494e47eb8318e2c09999a1db91dda6b8}] in slot [SlotRequestId{6d7f0173c1d48e5559f6a14b080ee817}].
2019-01-02 03:03:47.527 [thread==> flink-akka.actor.default-dispatcher-5] slotpool.SlotSharingManager$MultiTaskSlot#allocateSingleTaskSlot:426 Create single task slot [SlotRequestId{12bfcf1674f5b96567a076086dbbfd1b}] in multi task slot [SlotRequestId{494e47eb8318e2c09999a1db91dda6b8}] for group bc764cd8ddf7a0cff126f51c16239658.
2019-01-02 03:03:47.528 [thread==> flink-akka.actor.default-dispatcher-2] slotpool.SlotSharingManager$MultiTaskSlot#allocateSingleTaskSlot:426 Create single task slot [SlotRequestId{8a877431375df8aeadb2fd845cae15fc}] in multi task slot [SlotRequestId{494e47eb8318e2c09999a1db91dda6b8}] for group 0a448493b4782967b150582570326227.
2019-01-02 03:03:47.528 [thread==> flink-akka.actor.default-dispatcher-2] slotpool.SlotSharingManager#createRootSlot:151 Create multi task slot [SlotRequestId{56a36d3902ee1a7d0e2e84f50039c1ca}] in slot [SlotRequestId{dbf5c9fa39f1e5a0b34a4a8c10699ee5}].
2019-01-02 03:03:47.528 [thread==> flink-akka.actor.default-dispatcher-2] slotpool.SlotSharingManager$MultiTaskSlot#allocateSingleTaskSlot:426 Create single task slot [SlotRequestId{5929c12b52dccee682f86afbe1cff5cf}] in multi task slot [SlotRequestId{56a36d3902ee1a7d0e2e84f50039c1ca}] for group 0a448493b4782967b150582570326227.
2019-01-02 03:03:47.529 [thread==> flink-akka.actor.default-dispatcher-5] executiongraph.Execution#transitionState:1316 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from SCHEDULED to DEPLOYING.
2019-01-02 03:03:47.529 [thread==> flink-akka.actor.default-dispatcher-5] executiongraph.Execution#deploy:576 Deploying Source: Custom Source (1/1) (attempt #1) to localhost
2019-01-02 03:03:47.530 [thread==> flink-akka.actor.default-dispatcher-2] state.TaskExecutorLocalStateStoresManager#localStateStoreForSubtask:162 Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/tmp/localState/aid_AllocationID{7b5faad9073d7fac6759e40981197b8d}], jobID=06e76f6e31728025b22fdda9fadd6f01, jobVertexID=bc764cd8ddf7a0cff126f51c16239658, subtaskIndex=0}} for 06e76f6e31728025b22fdda9fadd6f01 - bc764cd8ddf7a0cff126f51c16239658 - 0 under allocation id AllocationID{7b5faad9073d7fac6759e40981197b8d}.
2019-01-02 03:03:47.530 [thread==> flink-akka.actor.default-dispatcher-2] partition.ResultPartition#<init>:172 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef): Initialized ResultPartition 85c7415bae559d6198b8bb69d4c6e49f@74a4ed4bb2f80aa2b98e11bd09ea64ef [PIPELINED_BOUNDED, 2 subpartitions, 2 pending references]
2019-01-02 03:03:47.530 [thread==> flink-akka.actor.default-dispatcher-2] taskexecutor.TaskExecutor#submitTask:541 Received task Source: Custom Source (1/1).
2019-01-02 03:03:47.532 [thread==> Source: Custom Source (1/1)] taskmanager.Task#transitionState:992 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from CREATED to DEPLOYING.
2019-01-02 03:03:47.532 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:655 Creating FileSystem stream leak safety net for task Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING]
2019-01-02 03:03:47.532 [thread==> flink-akka.actor.default-dispatcher-2] state.TaskExecutorLocalStateStoresManager#localStateStoreForSubtask:166 Found existing local state store for 06e76f6e31728025b22fdda9fadd6f01 - 0a448493b4782967b150582570326227 - 0 under allocation id AllocationID{7b5faad9073d7fac6759e40981197b8d}: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@176e8a65
2019-01-02 03:03:47.535 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:662 Loading JAR files for task Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING].
2019-01-02 03:03:47.536 [thread==> Source: Custom Source (1/1)] taskmanager.Task#createUserCodeClassloader:947 Getting user code class loader for task 74a4ed4bb2f80aa2b98e11bd09ea64ef at library cache manager took 0 milliseconds
2019-01-02 03:03:47.537 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:688 Registering task at network: Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING].
2019-01-02 03:03:47.537 [thread==> Source: Custom Source (1/1)] buffer.LocalBufferPool#<init>:125 Using a local buffer pool with 2-12 buffers
2019-01-02 03:03:47.537 [thread==> Source: Custom Source (1/1)] partition.ResultPartitionManager#registerResultPartition:62 Registered ResultPartition 85c7415bae559d6198b8bb69d4c6e49f@74a4ed4bb2f80aa2b98e11bd09ea64ef [PIPELINED_BOUNDED, 2 subpartitions, 2 pending references].
2019-01-02 03:03:47.537 [thread==> Source: Custom Source (1/1)] network.TaskEventDispatcher#registerPartition:59 registering 85c7415bae559d6198b8bb69d4c6e49f@74a4ed4bb2f80aa2b98e11bd09ea64ef
2019-01-02 03:03:47.537 [thread==> flink-akka.actor.default-dispatcher-2] state.TaskExecutorLocalStateStoresManager#localStateStoreForSubtask:166 Found existing local state store for 06e76f6e31728025b22fdda9fadd6f01 - 0a448493b4782967b150582570326227 - 1 under allocation id AllocationID{d8a1fd06e3e52101b003a04355f64be1}: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@57bf2d8e
2019-01-02 03:03:47.537 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:714 =======> next, kick off the background copying of files for the distributed cache: Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING].
2019-01-02 03:03:47.538 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:729 =======> isCanceledOrFailed: Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING].
2019-01-02 03:03:47.540 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:737 =======> call the user code initialization methods: Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING].
2019-01-02 03:03:47.543 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:765 =======> now load and instantiate the task's invokable code: Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [DEPLOYING].
2019-01-02 03:03:47.543 [thread==> flink-akka.actor.default-dispatcher-5] executiongraph.ExecutionGraph#transitionState:1356 Job Flink Streaming Job (06e76f6e31728025b22fdda9fadd6f01) switched from state RUNNING to FAILING.
2019-01-02 03:03:47.544 [thread==> Source: Custom Source (1/1)] tasks.StreamTask#createStreamRecordWriter:1214 Using partitioner REBALANCE for output 0 of task Source: Custom Source
2019-01-02 03:03:47.544 [thread==> Source: Custom Source (1/1)] io.StreamRecordWriter$OutputFlusher#<init>:174 StreamRecordWriter start : outputflusher1546369427544, timeout: 100
2019-01-02 03:03:47.544 [thread==> flink-akka.actor.default-dispatcher-5] executiongraph.Execution#transitionState:1316 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from DEPLOYING to CANCELING.
2019-01-02 03:03:47.544 [thread==> flink-akka.actor.default-dispatcher-2] taskmanager.Task#cancelExecution:1055 Attempting to cancel task Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef).
2019-01-02 03:03:47.544 [thread==> flink-akka.actor.default-dispatcher-2] taskmanager.Task#cancelOrFailAndCancelInvokable:1078 cancelOrFailAndCancelInvokable_current_execution_state==> task: Source: Custom Source (1/1), state: DEPLOYING, targetState CANCELING
2019-01-02 03:03:47.545 [thread==> flink-akka.actor.default-dispatcher-2] taskmanager.Task#transitionState:992 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from DEPLOYING to CANCELING.
2019-01-02 03:03:47.545 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#releaseSlot:745 Releasing slot [SlotRequestId{5929c12b52dccee682f86afbe1cff5cf}] because: Slot is being returned to the SlotPool.
2019-01-02 03:03:47.545 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#releaseSlot:745 Releasing slot [SlotRequestId{dbf5c9fa39f1e5a0b34a4a8c10699ee5}] because: Release multi task slot because all children have been released.
2019-01-02 03:03:47.545 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#tryFulfillSlotRequestOrMakeAvailable:842 Adding returned slot [AllocationID{d8a1fd06e3e52101b003a04355f64be1}] to available slots
2019-01-02 03:03:47.546 [thread==> flink-akka.actor.default-dispatcher-2] taskexecutor.TaskExecutor#failPartition:656 Discarding the results produced by task execution 8aeff98e78ed3d348af9d4d5679f2b26.
2019-01-02 03:03:47.546 [thread==> flink-akka.actor.default-dispatcher-2] partition.ResultPartitionManager#releasePartitionsProducedBy:105 Released all partitions produced by 8aeff98e78ed3d348af9d4d5679f2b26.
2019-01-02 03:03:47.547 [thread==> Source: Custom Source (1/1)] taskmanager.Task#transitionState:992 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from CANCELING to CANCELED.
2019-01-02 03:03:47.547 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:892 Freeing task resources for Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef).
2019-01-02 03:03:47.547 [thread==> Source: Custom Source (1/1)] network.NetworkEnvironment#unregisterTask:263 Unregister task Source: Custom Source (1/1) from network environment (state: CANCELED).
2019-01-02 03:03:47.547 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:47.547 [thread==> Source: Custom Source (1/1)] partition.ResultPartition#release:313 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef): Releasing ResultPartition 85c7415bae559d6198b8bb69d4c6e49f@74a4ed4bb2f80aa2b98e11bd09ea64ef [PIPELINED_BOUNDED, 2 subpartitions, 2 pending references].
2019-01-02 03:03:47.548 [thread==> Source: Custom Source (1/1)] partition.PipelinedSubpartition#release:137 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef): Released PipelinedSubpartition#0 [number of buffers: 0 (0 bytes), number of buffers in backlog: 0, finished? false, read view? false].
2019-01-02 03:03:47.548 [thread==> Source: Custom Source (1/1)] partition.PipelinedSubpartition#release:137 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef): Released PipelinedSubpartition#1 [number of buffers: 0 (0 bytes), number of buffers in backlog: 0, finished? false, read view? false].
2019-01-02 03:03:47.548 [thread==> Source: Custom Source (1/1)] partition.ResultPartitionManager#releasePartitionsProducedBy:105 Released all partitions produced by 74a4ed4bb2f80aa2b98e11bd09ea64ef.
2019-01-02 03:03:47.548 [thread==> Source: Custom Source (1/1)] network.TaskEventDispatcher#unregisterPartition:78 unregistering 85c7415bae559d6198b8bb69d4c6e49f@74a4ed4bb2f80aa2b98e11bd09ea64ef
2019-01-02 03:03:47.548 [thread==> Source: Custom Source (1/1)] taskmanager.Task#run:919 Ensuring all FileSystem streams are closed for task Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) [CANCELED]
2019-01-02 03:03:47.549 [thread==> flink-akka.actor.default-dispatcher-2] taskexecutor.TaskExecutor#unregisterTaskAndNotifyFinalState:1337 Un-registering task and sending final execution state CANCELED to JobManager for task Source: Custom Source 74a4ed4bb2f80aa2b98e11bd09ea64ef.
2019-01-02 03:03:47.562 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#releaseSlot:745 Releasing slot [SlotRequestId{8a877431375df8aeadb2fd845cae15fc}] because: Slot is being returned to the SlotPool.
2019-01-02 03:03:47.566 [thread==> flink-akka.actor.default-dispatcher-3] executiongraph.Execution#transitionState:1316 Source: Custom Source (1/1) (74a4ed4bb2f80aa2b98e11bd09ea64ef) switched from CANCELING to CANCELED.
2019-01-02 03:03:47.566 [thread==> flink-akka.actor.default-dispatcher-3] executiongraph.Execution#processFail:1079 Ignoring transition of vertex Source: Custom Source (1/1) - execution #1 to FAILED while being CANCELED.
2019-01-02 03:03:47.567 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#releaseSlot:745 Releasing slot [SlotRequestId{12bfcf1674f5b96567a076086dbbfd1b}] because: Slot is being returned to the SlotPool.
2019-01-02 03:03:47.567 [thread==> flink-akka.actor.default-dispatcher-3] executiongraph.ExecutionGraph#tryRestartOrFail:1477 Try to restart or fail the job Flink Streaming Job (06e76f6e31728025b22fdda9fadd6f01) if no longer possible.
2019-01-02 03:03:47.567 [thread==> flink-akka.actor.default-dispatcher-3] executiongraph.ExecutionGraph#transitionState:1356 Job Flink Streaming Job (06e76f6e31728025b22fdda9fadd6f01) switched from state FAILING to RESTARTING.
2019-01-02 03:03:47.567 [thread==> flink-akka.actor.default-dispatcher-3] executiongraph.ExecutionGraph#tryRestartOrFail:1487 Restarting the job Flink Streaming Job (06e76f6e31728025b22fdda9fadd6f01).
2019-01-02 03:03:47.567 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#releaseSlot:745 Releasing slot [SlotRequestId{6d7f0173c1d48e5559f6a14b080ee817}] because: Release multi task slot because all children have been released.
2019-01-02 03:03:47.567 [thread==> flink-akka.actor.default-dispatcher-3] slotpool.SlotPool#tryFulfillSlotRequestOrMakeAvailable:842 Adding returned slot [AllocationID{7b5faad9073d7fac6759e40981197b8d}] to available slots
2019-01-02 03:03:47.647 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:47.748 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:47.848 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:47.948 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:48.049 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:48.149 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:48.249 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:48.349 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:48.450 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
2019-01-02 03:03:48.550 [thread==> OutputFlusher for Source: Custom Source] io.StreamRecordWriter$OutputFlusher#run:192 Running OutputFlusher outputflusher1546369427544
{code}
 

 ",,akalashnikov,hwanju,kezhuw,lamber-ken,pnowojski,RustedBones,stevenz3wu,wind_ljy,ym,zjwang,,,,,,,,,"lamber-ken commented on pull request #7396: [FLINK-11250] fix thread lack when StreamTask switched from DEPLOYING to CANCELING
URL: https://github.com/apache/flink/pull/7396
 
 
   ## What is the purpose of the change
   
   begin flink-1.5.x version, streamRecordWriters was created in StreamTask's constructor, which start OutputFlusher daemon thread. so when task switched from DEPLOYING to CANCELING state, the daemon thread will be lacked.
   
   ## Brief change log
   
   lazy init streamRecordWriters, streamRecordWriters are created in invoke method
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 10:26;githubbot;600","lamber-ken commented on pull request #7396: [FLINK-11250][streaming] fix thread lack when StreamTask switched from DEPLOYING to CANCELING
URL: https://github.com/apache/flink/pull/7396
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 21:37;githubbot;600","lamber-ken commented on pull request #7396: [FLINK-11250][streaming] fix thread lack when StreamTask switched from DEPLOYING to CANCELING
URL: https://github.com/apache/flink/pull/7396
 
 
   ## What is the purpose of the change
   
   begin flink-1.5.x version, streamRecordWriters was created in StreamTask's constructor, which start OutputFlusher daemon thread. so when task switched from DEPLOYING to CANCELING state, the daemon thread will be lacked.
   
   ## Brief change log
   
   lazy init streamRecordWriters, streamRecordWriters are created in invoke method
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 23:21;githubbot;600","lamber-ken commented on pull request #7396: [FLINK-11250][streaming] fix thread lack when StreamTask switched from DEPLOYING to CANCELING
URL: https://github.com/apache/flink/pull/7396
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jul/19 14:47;githubbot;600",,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 17 02:17:01 UTC 2021,,,,,,,,,,"0|u00g20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/19 08:47;RustedBones;This also seems to affect Flink 1.8.1

 

We suspected a class-loader leak from our application so we made a dump from the JobManager process.

Some of the GC roots from our leaked classes point to the flink OutputFlusher thread.

 

When the dump was made, only 2 jobs were running with a source of parallelism 1. I expect to have only 2 OutputFlusher threads. However, the dumps shows we have 86 daemon threads for this source.

 

Those thread having a reference to the user class loader, creates the memory leak leads to a OOM of our application after some time.;;;","16/Apr/21 11:16;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 23:10;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","10/Jun/21 11:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Jun/21 10:40;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Sep/21 07:26;akalashnikov;I actually don't think that this ticket is still relevant. As I can see OutputFlusher is properly terminated in RecordWriter#close. More precysly, the closing chain looks like: StreamTask#cleanUp -> StreamTask -> releaseOutputResources -> RegularOperationChain#close -> RecordWriterOutput#close -> RecordWriter#close -> OutupFlusher#terminate.
I am not really ready to say which ticket exactly fixed this bug because there were a lot of them since this bug was created. But perhaps, merging `StreamRecordWriter` into `RecordWriter`(FLINK-11282) helped or one of the improvements of StreamTask#cleanUp which were a lot.

So if nobody minds I suggest closing this ticket as `not reproduced` since the last news about this bug was two years ago and since the attached reproducer doesn't reproduce anything(it works perfectly fine).;;;","06/Sep/21 10:14;pnowojski;I think [~akalashnikov] you are missing:
{quote}
 so when task switched from DEPLOYING to CANCELING state
{quote}
Is the key point here. In this case inside {{Task#doRun}} we can instantiate [{{SteamTask}}|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L739:L741] (and start {{OutputFlusher}} threads) but in case of this [piece of code|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L756:L758] {{StreamTask#cleanUp()}} will not be called.;;;","17/Sep/21 02:17;ym;merged commit [{{3b6b522}}|https://github.com/apache/flink/commit/3b6b5229df09c05fbeeaf88bf91df6d18c71097a] into apache:master;;;",,,,,,,,,,,,,,,,
Fix distinct AGG visibility issues,FLINK-11246,13207200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,sunjincheng121,sunjincheng121,02/Jan/19 02:39,02/Jan/19 05:48,13/Jul/23 08:05,02/Jan/19 05:48,,,,,,,,,1.7.2,1.8.0,,,Table SQL / API,,,0,pull-request-available,,,"1. DistinctAggregateFunction# distinct -> remove private[flink] 

2. case class DistinctAgg#distinct  -> remove private[flink]",,sunjincheng121,,,,,,,,,,,,,,,,,,"dianfu commented on pull request #7394: [FLINK-11246][table] Fix distinct AGG visibility issue
URL: https://github.com/apache/flink/pull/7394
 
 
   ## What is the purpose of the change
   
   *This pull request Fixes the visibility issue of distinct in Table API*
   
   ## Brief change log
   
   *(for example:)*
     - *Remove private[flink] of DistinctAggregateFunction#distinct*
     - *Remove private[flink] of DistinctAgg#distinct*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 03:09;githubbot;600","asfgit commented on pull request #7394: [FLINK-11246][table] Fix distinct AGG visibility issue
URL: https://github.com/apache/flink/pull/7394
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/19 05:43;githubbot;600",,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 02 05:48:54 UTC 2019,,,,,,,,,,"0|u00fqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/19 05:48;sunjincheng121;Fixed in master: 3d8813709b87e93c081d17b484875d4dfc0c3b3d
Fixed in release-1.7: d3233738c9dccad12c5526d648d30d7f66743e41;;;",,,,,,,,,,,,,,,,,,,,,,,
