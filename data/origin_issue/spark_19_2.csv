Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Parent Feature),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
mark all running map stages of finished job as finished,SPARK-30388,13276869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liangxs,liangxs,liangxs,30/Dec/19 12:14,05/Mar/20 13:19,13/Jul/23 08:49,03/Mar/20 15:33,2.4.0,,,,,,,,,,3.0.0,3.1.0,,Spark Core,Web UI,,,,0,,,,"When a job finished, its running (re-submitted) map stages should be marked as finished if  not used by other jobs.

And the ListenerBus should be notified too, otherwise, these map stage items will stay on the ""Active Stages"" page of web UI  and never gone.",,liangxs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31052,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-12-30 12:14:30.0,,,,,,,,,,"0|z0a2uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebUI occasionally throw IOException on stop(),SPARK-30385,13276848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,Ngone51,Ngone51,30/Dec/19 09:57,08/May/20 08:42,13/Jul/23 08:49,08/May/20 08:42,3.0.0,,,,,,,,,,3.1.0,,,Web UI,,,,,0,,,,"While using ./bin/spark-shell, recently, I occasionally see IOException when I try to quit:
{code:java}
19/12/30 17:33:21 WARN AbstractConnector:
java.io.IOException: No such file or directory
 at sun.nio.ch.NativeThread.signal(Native Method)
 at sun.nio.ch.ServerSocketChannelImpl.implCloseSelectableChannel(ServerSocketChannelImpl.java:292)
 at java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:234)
 at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:115)
 at org.eclipse.jetty.server.ServerConnector.close(ServerConnector.java:368)
 at org.eclipse.jetty.server.AbstractNetworkConnector.shutdown(AbstractNetworkConnector.java:105)
 at org.eclipse.jetty.server.Server.doStop(Server.java:439)
 at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 
 at org.apache.spark.ui.ServerInfo.stop(JettyUtils.scala:499)
 at org.apache.spark.ui.WebUI.$anonfun$stop$2(WebUI.scala:173)
 at org.apache.spark.ui.WebUI.$anonfun$stop$2$adapted(WebUI.scala:173)
 at scala.Option.foreach(Option.scala:407)
 at org.apache.spark.ui.WebUI.stop(WebUI.scala:173)
 at org.apache.spark.ui.SparkUI.stop(SparkUI.scala:101)
 at org.apache.spark.SparkContext.$anonfun$stop$6(SparkContext.scala:1972)
 at org.apache.spark.SparkContext.$anonfun$stop$6$adapted(SparkContext.scala:1972)
 at scala.Option.foreach(Option.scala:407)
 at org.apache.spark.SparkContext.$anonfun$stop$5(SparkContext.scala:1972)
 at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
 at org.apache.spark.SparkContext.stop(SparkContext.scala:1972)
 at org.apache.spark.repl.Main$.$anonfun$doMain$3(Main.scala:79)
 at org.apache.spark.repl.Main$.$anonfun$doMain$3$adapted(Main.scala:79)
 at scala.Option.foreach(Option.scala:407)
 at org.apache.spark.repl.Main$.doMain(Main.scala:79)
 at org.apache.spark.repl.Main$.main(Main.scala:58)
 at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) 
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
I don't find a way to reproduce it stably, but it will increase possibility if you stay in spark-shell for not a short time.  

A possible way to reproduce this is: start ./bin/spark-shell , wait for 5 min, then use :q or :quit to quit.

 ","MacOS 10.14.6

Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_231

Scala version 2.12.10",apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14233,,,,,,,,CASSANDRA-12513,CASSANDRA-8220,NIFI-437,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 02 19:34:35 UTC 2020,,,,,,,,,,"0|z0a2q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/May/20 19:34;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28437;;;",,,,,,,,,,,,,,,,,,,,,,,
start-thriftserver throws ClassNotFoundException,SPARK-30382,13276766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajithshetty,ajithshetty,ajithshetty,29/Dec/19 13:06,07/Jan/20 22:27,13/Jul/23 08:49,07/Jan/20 22:27,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"start-thriftserver.sh --help throws

{code}
.....
     

Thrift server options:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/spi/LoggerContextFactory
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:167)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:82)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.spi.LoggerContextFactory
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 3 more


{code}",,ajithshetty,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 22:27:15 UTC 2020,,,,,,,,,,"0|z0a27s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Jan/20 22:27;dongjoon;Issue resolved by pull request 27042
[https://github.com/apache/spark/pull/27042];;;",,,,,,,,,,,,,,,,,,,,,,,
InputMetrics are not updated for DataSourceRDD V2 ,SPARK-30362,13276538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,sandeep.katta2007,sandeep.katta2007,27/Dec/19 03:52,31/Jan/20 06:06,13/Jul/23 08:49,31/Jan/20 06:06,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,InputMetrics is not updated for DataSourceRDD,,cloud_fan,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/19 03:56;sandeep.katta2007;inputMetrics.png;https://issues.apache.org/jira/secure/attachment/12989513/inputMetrics.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 31 06:06:47 UTC 2020,,,,,,,,,,"0|z0a0t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Jan/20 06:06;cloud_fan;Issue resolved by pull request 27021
[https://github.com/apache/spark/pull/27021];;;",,,,,,,,,,,,,,,,,,,,,,,
Monitoring URL do not redact information about environment,SPARK-30361,13276474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajithshetty,ajithshetty,ajithshetty,26/Dec/19 13:04,30/Dec/19 15:11,13/Jul/23 08:49,30/Dec/19 15:11,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"UI and event logs redact sensitive information. But the monitoring URL,  https://spark.apache.org/docs/latest/monitoring.html#rest-api , specifically  /applications/[app-id]/environment does not, which is a security issue. ",,ajithshetty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 30 15:11:04 UTC 2019,,,,,,,,,,"0|z0a0ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Dec/19 15:11;srowen;Issue resolved by pull request 27018
[https://github.com/apache/spark/pull/27018];;;",,,,,,,,,,,,,,,,,,,,,,,
Avoid Redact classpath entries in History Server UI,SPARK-30360,13276471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,ajithshetty,ajithshetty,26/Dec/19 12:53,23/Jan/20 04:55,13/Jul/23 08:49,28/Dec/19 18:59,3.0.0,,,,,,,,,,3.0.0,,,Web UI,,,,,0,,,,"Currently SPARK history server display the classpath entries in the Environment tab with classpaths redacted, this is because EventLog file has the entry values redacted while writing. But when same is seen from a running application UI, its seen that it is not redacted. Classpath entries redact is not needed and can be avoided",,ajithshetty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 28 18:59:57 UTC 2019,,,,,,,,,,"0|z0a0e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Dec/19 18:59;srowen;Issue resolved by pull request 27016
[https://github.com/apache/spark/pull/27016];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test:  org.apache.spark.deploy.master.MasterSuite.SPARK-27510: Master should avoid dead loop while launching executor failed in Worker,SPARK-30348,13276232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,24/Dec/19 13:36,30/Dec/19 06:57,13/Jul/23 08:49,30/Dec/19 06:38,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115664/testReport/]

 
{code:java}
org.apache.spark.deploy.master.MasterSuite.SPARK-27510: Master should avoid dead loop while launching executor failed in Worker 

org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 656 times over 10.002408616 seconds. Last failure message: Map() did not contain key ""app-20191223154506-0000"". 

sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 656 times over 10.002408616 seconds. Last failure message: Map() did not contain key ""app-20191223154506-0000"".
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391)
	at org.apache.spark.deploy.master.MasterSuite.eventually(MasterSuite.scala:111)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:337)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:336)
	at org.apache.spark.deploy.master.MasterSuite.eventually(MasterSuite.scala:111)
	at org.apache.spark.deploy.master.MasterSuite.$anonfun$new$40(MasterSuite.scala:681)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.deploy.master.MasterSuite.org$scalatest$BeforeAndAfter$$super$runTest(MasterSuite.scala:111)
	at org.scalatest.BeforeAndAfter.runTest(BeforeAndAfter.scala:203)
	at org.scalatest.BeforeAndAfter.runTest$(BeforeAndAfter.scala:192)
	at org.apache.spark.deploy.master.MasterSuite.runTest(MasterSuite.scala:111)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1124)
	at org.scalatest.Suite.run$(Suite.scala:1106)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.deploy.master.MasterSuite.org$scalatest$BeforeAndAfter$$super$run(MasterSuite.scala:111)
	at org.scalatest.BeforeAndAfter.run(BeforeAndAfter.scala:258)
	at org.scalatest.BeforeAndAfter.run$(BeforeAndAfter.scala:256)
	at org.apache.spark.deploy.master.MasterSuite.run(MasterSuite.scala:111)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Map() did not contain key ""app-20191223154506-0000""
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:503)
	at org.apache.spark.deploy.master.MasterSuite.$anonfun$new$41(MasterSuite.scala:685)
	at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395)
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409)
	... 58 more {code}",,cloud_fan,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 30 06:38:17 UTC 2019,,,,,,,,,,"0|z09yx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Dec/19 06:38;cloud_fan;Issue resolved by pull request 27004
[https://github.com/apache/spark/pull/27004];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test failure: ThriftServerWithSparkContextSuite.SPARK-29911: Uncache cached tables when session closed,SPARK-30345,13276210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,24/Dec/19 09:37,19/Feb/20 05:40,13/Jul/23 08:49,27/Dec/19 07:50,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115646/testReport/]
{noformat}
 org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextSuite.SPARK-29911: Uncache cached tables when session closed Error Detailsjava.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:15441: java.net.ConnectException: Connection refused (Connection refused) Stack Tracesbt.ForkMain$ForkError: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:15441: java.net.ConnectException: Connection refused (Connection refused)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:224)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:247)
	at org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextSuite.$anonfun$withJdbcStatement$1(ThriftServerWithSparkContextSuite.scala:94)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextSuite.withJdbcStatement(ThriftServerWithSparkContextSuite.scala:94)
	at org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextSuite.$anonfun$new$1(ThriftServerWithSparkContextSuite.scala:62)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1124)
	at org.scalatest.Suite.run$(Suite.scala:1106)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:266)
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:38)
	at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:311)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:196)
	... 58 more
Caused by: sbt.ForkMain$ForkError: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 62 more {noformat}",,cloud_fan,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28883,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 27 07:50:45 UTC 2019,,,,,,,,,,"0|z09ys8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Dec/19 07:50;cloud_fan;Issue resolved by pull request 27001
[https://github.com/apache/spark/pull/27001];;;",,,,,,,,,,,,,,,,,,,,,,,
check overflow for interval arithmetic operations,SPARK-30341,13276165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Dec/19 03:32,02/Feb/20 04:06,13/Jul/23 08:49,02/Jan/20 18:06,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"the interval arithmetic functions, e.g. add/subtract/negative/multiply/divide, should enable overflow check when ansi is on, and add/subtract/negative should result NULL when overflow happens and ansi is off as multiply/divide.",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 18:06:43 UTC 2020,,,,,,,,,,"0|z09yi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jan/20 18:06;cloud_fan;Issue resolved by pull request 26995
[https://github.com/apache/spark/pull/26995];;;",,,,,,,,,,,,,,,,,,,,,,,
Python tests failed on arm64/x86,SPARK-30340,13276149,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,huangtianhua,huangtianhua,24/Dec/19 02:34,23/Mar/20 03:22,13/Jul/23 08:49,23/Mar/20 03:22,3.0.0,,,,,,,,,,,,,ML,,,,,0,,,,"Jenkins job spark-master-test-python-arm failed after the commit c6ab7165dd11a0a7b8aea4c805409088e9a41a74:

{code}
File ""/home/jenkins/workspace/spark-master-test-python-arm/python/pyspark/ml/classification.py"", line 2790, in __main__.FMClassifier
 Failed example:
 model.transform(test0).select(""features"", ""probability"").show(10, False)
 Expected:
 +----------+----------------------------------------+
|features|probability|

+----------+----------------------------------------+
|[-1.0]|[0.9999999997574736,2.425264676902229E-10]|
|[0.5]|[0.47627851732981163,0.5237214826701884]|
|[1.0]|[5.491554426243495E-4,0.9994508445573757]|
|[2.0]|[2.005766663870645E-10,0.9999999997994233]|

+----------+----------------------------------------+
 Got:
 +----------+----------------------------------------+
|features|probability|

+----------+----------------------------------------+
|[-1.0]|[0.9999999997574736,2.425264676902229E-10]|
|[0.5]|[0.47627851732981163,0.5237214826701884]|
|[1.0]|[5.491554426243495E-4,0.9994508445573757]|
|[2.0]|[2.005766663870645E-10,0.9999999997994233]|

+----------+----------------------------------------+
 <BLANKLINE>
 **********************************************************************
 File ""/home/jenkins/workspace/spark-master-test-python-arm/python/pyspark/ml/classification.py"", line 2803, in __main__.FMClassifier
 Failed example:
 model.factors
 Expected:
 DenseMatrix(1, 2, [0.0028, 0.0048], 1)
 Got:
 DenseMatrix(1, 2, [-0.0122, 0.0106], 1)
 **********************************************************************
 2 of 10 in __main__.FMClassifier
 ***Test Failed*** 2 failures.
{code}
 

The details see [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-python-arm/91/console]

And seems the tests failed on x86：

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115668/console]

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115665/console]",,ganeshraju,huangtianhua,v_ganeshraju,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 03:22:35 UTC 2020,,,,,,,,,,"0|z09yeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Mar/20 03:22;huangtianhua;Seems the tests are success now.;;;",,,,,,,,,,,,,,,,,,,,,,,
Bump  jackson-databind to 2.6.7.3 ,SPARK-30333,13276030,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,sandeep.katta2007,sandeep.katta2007,23/Dec/19 12:02,17/Jan/20 19:01,13/Jul/23 08:49,24/Dec/19 05:17,2.4.4,,,,,,,,,,2.4.5,,,Spark Core,,,,,0,,,,"To fix below CVE

 

CVE-2018-14718

CVE-2018-14719

CVE-2018-14720

CVE-2018-14721

CVE-2018-19360,

CVE-2018-19361

CVE-2018-19362",,maropu,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 24 05:17:36 UTC 2019,,,,,,,,,,"0|z09xo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Dec/19 05:17;maropu;Resolved by [https://github.com/apache/spark/pull/26986];;;",,,,,,,,,,,,,,,,,,,,,,,
markPartitionCompleted cause task status inconsistent,SPARK-30325,13275858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,haiyangyu,yuhaiyang,yuhaiyang,21/Dec/19 09:08,17/Jan/20 07:15,13/Jul/23 08:49,14/Jan/20 12:01,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"h3. Corner case

The bugs occurs in the coren case as follows:
 # The stage occurs for fetchFailed and some task hasn't finished, scheduler will resubmit a new stage as retry with those unfinished tasks.
 # The unfinished task in origin stage finished and the same task on the new retry stage hasn't finished, it will mark the task partition on the new retry stage as succesuful.  !image-2019-12-21-17-11-38-565.png|width=427,height=154!
 # The executor running those 'successful task' crashed, it cause taskSetManager run executorLost to rescheduler the task on the executor, here will cause copiesRunning decreate 1 twice, beause those 'successful task' are not finished, the variable copiesRunning will decreate to -1 as result. !image-2019-12-21-17-15-51-512.png|width=437,height=340!!image-2019-12-21-17-16-40-998.png|width=398,height=139!
 # 'dequeueTaskFromList' will use copiesRunning equal 0 as reschedule basis when rescheduler tasks, and now it is -1, can't to reschedule, and the app will hung forever. !image-2019-12-21-17-17-42-244.png|width=366,height=282!",,cloud_fan,dongjoon,yuhaiyang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/19 09:11;yuhaiyang;image-2019-12-21-17-11-38-565.png;https://issues.apache.org/jira/secure/attachment/12989314/image-2019-12-21-17-11-38-565.png","21/Dec/19 09:15;yuhaiyang;image-2019-12-21-17-15-51-512.png;https://issues.apache.org/jira/secure/attachment/12989315/image-2019-12-21-17-15-51-512.png","21/Dec/19 09:16;yuhaiyang;image-2019-12-21-17-16-40-998.png;https://issues.apache.org/jira/secure/attachment/12989316/image-2019-12-21-17-16-40-998.png","21/Dec/19 09:17;yuhaiyang;image-2019-12-21-17-17-42-244.png;https://issues.apache.org/jira/secure/attachment/12989317/image-2019-12-21-17-17-42-244.png",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 17 07:14:42 UTC 2020,,,,,,,,,,"0|z09wm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Dec/19 10:05;yuhaiyang;[https://github.com/apache/spark/pull/26975];;;","14/Jan/20 12:01;cloud_fan;Issue resolved by pull request 26975
[https://github.com/apache/spark/pull/26975];;;","17/Jan/20 07:14;dongjoon;Hi, All.
Since there is no UT, it's difficult to validate. According to the commit history, this bug seems to exist with SPARK-24755 and before it. Could you update `Affects Version/s:` more if it does? For example, 2.3.4 and 2.2.3?;;;",,,,,,,,,,,,,,,,,,,,,
Flaky test: MasterSuite.master/worker web ui available with reverseProxy,SPARK-30313,13275610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,vanzin,vanzin,19/Dec/19 23:32,06/Jan/20 23:30,13/Jul/23 08:49,06/Jan/20 16:42,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,"Saw this test fail a few times on PRs. e.g.:

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115583/testReport/org.apache.spark.deploy.master/MasterSuite/master_worker_web_ui_available_with_reverseProxy/]

 
{noformat}

Error Message

org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 43 times over 5.0642265779999995 seconds. Last failure message: Server returned HTTP response code: 500 for URL: http://localhost:45395/proxy/worker-20191219134839-localhost-36054/json/.

Stacktrace

sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 43 times over 5.0642265779999995 seconds. Last failure message: Server returned HTTP response code: 500 for URL: http://localhost:45395/proxy/worker-20191219134839-localhost-36054/json/.
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391)
	at org.apache.spark.deploy.master.MasterSuite.eventually(MasterSuite.scala:111)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:308)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:307)
	at org.apache.spark.deploy.master.MasterSuite.eventually(MasterSuite.scala:111)
	at org.apache.spark.deploy.master.MasterSuite.$anonfun$new$14(MasterSuite.scala:318)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	---
Caused by: sbt.ForkMain$ForkError: java.io.IOException: Server returned HTTP response code: 500 for URL: http://localhost:45395/proxy/worker-20191219134839-localhost-36054/json/
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492)
	at java.net.URL.openStream(URL.java:1045)
	at scala.io.Source$.fromURL(Source.scala:144)
	at scala.io.Source$.fromURL(Source.scala:134)
{noformat}
",,Ankitraj,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 16:42:23 UTC 2020,,,,,,,,,,"0|z09v2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Dec/19 00:02;vanzin;From the logs (in case jenkins cleans them up):

{noformat}
19/12/19 13:48:39.160 dispatcher-event-loop-4 INFO Worker: WorkerWebUI is available at http://localhost:8080/proxy/worker-20191219
134839-localhost-36054
19/12/19 13:48:39.296 WorkerUI-52072 WARN JettyUtils: GET /json/ failed: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.spark.deploy.worker.ui.WorkerPage.renderJson(WorkerPage.scala:39)
        at org.apache.spark.ui.WebUI.$anonfun$attachPage$2(WebUI.scala:91)
        at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:80)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
        at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
        at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
        at org.eclipse.jetty.server.Server.handle(Server.java:505)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
        at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
{noformat}
;;;","20/Dec/19 01:04;Ankitraj;[~vanzin], i will check this issue;;;","20/Dec/19 01:31;Ankitraj;[~vanzin], I run in my local system, it's getting passed.;;;","20/Dec/19 01:44;kabhwan;Flaky test means it doesn't fail consistently. Normally it tends to be harder to debug and have to rely on log or try to reproduce, via various approach including running the test infinitely till hitting it.;;;","20/Dec/19 01:47;Ankitraj;ok, i will try to reproduce it.;;;","26/Dec/19 05:28;kabhwan;Similar test failure: [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115700/testReport/]

 ;;;","06/Jan/20 16:42;vanzin;Issue resolved by pull request 27010
[https://github.com/apache/spark/pull/27010];;;",,,,,,,,,,,,,,,,,
Preserve path permission when truncate table,SPARK-30312,13275601,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,19/Dec/19 22:09,08/Jun/20 19:59,13/Jul/23 08:49,10/Jan/20 19:52,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,SQL,,,,,0,release-notes,,,"When Spark SQL truncates table, it deletes the paths of table/partitions, then re-create new ones. If custom permission/acls are set on the paths, the metadata will be deleted.

We should preserve original permission/acls if possible.",,cloud_fan,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30797,SPARK-31163,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 19:59:11 UTC 2020,,,,,,,,,,"0|z09v0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Jan/20 19:52;dongjoon;This is resolved via https://github.com/apache/spark/pull/26956;;;","08/Jun/20 16:09;cloud_fan;why does this bug fix need release notes? What shall we tell about it to the users?;;;","08/Jun/20 17:39;viirya;I am not sure. [~dongjoon] Do you remember this?;;;","08/Jun/20 19:57;dongjoon;It's because this is a security fix. Without this patch, ACL is removed, isn't it?;;;","08/Jun/20 19:59;dongjoon;To be complete, we need to mention the final status after all of SPARK-30312, SPARK-30797, SPARK-31163 although we don't need to mention one by one.;;;",,,,,,,,,,,,,,,,,,,
SparkUncaughtExceptionHandler halts running process unexpectedly,SPARK-30310,13275593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tinhto-000,tinhto-000,tinhto-000,19/Dec/19 20:55,29/Jan/20 22:42,13/Jul/23 08:49,17/Jan/20 15:47,2.4.0,3.0.0,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"During 2.4.x testing, we have many occasions where the Worker process would just DEAD unexpectedly, with the Worker log ends with:

 

{{ERROR SparkUncaughtExceptionHandler: scala.MatchError:  <...callstack...>}}

 

We get the same callstack during our 2.3.x testing but the Worker process stays up.

Upon looking at the 2.4.x SparkUncaughtExceptionHandler.scala compared to the 2.3.x version,  we found out SPARK-24294 introduced the following change:


{{exception catch {}}
{{  case _: OutOfMemoryError =>}}
{{    System.exit(SparkExitCode.OOM)}}
{{  case e: SparkFatalException if e.throwable.isInstanceOf[OutOfMemoryError] =>}}
{{    // SPARK-24294: This is defensive code, in case that SparkFatalException is}}
{{    // misused and uncaught.}}
{{    System.exit(SparkExitCode.OOM)}}
{{  case _ if exitOnUncaughtException =>}}
{{    System.exit(SparkExitCode.UNCAUGHT_EXCEPTION)}}
{{}}}

 

This code has the _ if exitOnUncaughtException case, but not the other _ cases.  As a result, when exitOnUncaughtException is false (Master and Worker) and exception doesn't match any of the match cases (e.g., IllegalStateException), Scala throws MatchError(exception) (""MatchError"" wrapper of the original exception).  Then the other catch block down below thinks we have another uncaught exception, and halts the entire process with SparkExitCode.UNCAUGHT_EXCEPTION_TWICE.

 

{{catch {}}
{{  case oom: OutOfMemoryError => Runtime.getRuntime.halt(SparkExitCode.OOM)}}
{{  case t: Throwable => Runtime.getRuntime.halt(SparkExitCode.UNCAUGHT_EXCEPTION_TWICE)}}
{{}}}

 

Therefore, even when exitOnUncaughtException is false, the process will halt.",,dongjoon,nmarion,tinhto-000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,Resolved SparkUncaughtExceptionHandler halting process unexpectedly.,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 29 22:41:51 UTC 2020,,,,,,,,,,"0|z09uz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Jan/20 15:47;srowen;Issue resolved by pull request 26955
[https://github.com/apache/spark/pull/26955];;;","29/Jan/20 22:41;dongjoon;This is backported to branch-2.4 via https://github.com/apache/spark/pull/27384;;;",,,,,,,,,,,,,,,,,,,,,,
Percentile will hit match err when percentage is null,SPARK-30303,13275443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Qin Yao,Qin Yao,19/Dec/19 08:36,06/Mar/21 04:58,13/Jul/23 08:49,06/Mar/21 04:58,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"{code:java}
spark-sql> select percentile(1, null);
19/12/19 16:35:04 ERROR SparkSQLDriver: Failed in [select percentile(1, null)]
scala.MatchError: null


spark-sql> select percentile(1, array(0.0, null)); -- null is illegal but treat as zero
{code}
 ",,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30266,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-12-19 08:36:31.0,,,,,,,,,,"0|z09u1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datetimes as fields of complex types to hive string results wrong,SPARK-30301,13275396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,19/Dec/19 03:18,25/Mar/20 09:41,13/Jul/23 08:49,20/Dec/19 11:56,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"{code:java}
+-- !query 47
+select array(cast('1582-10-13' as date), date '1582-10-14', date '1582-10-15', null)
+-- !query 47 schema
+struct<array(CAST(1582-10-13 AS DATE), DATE '1582-10-14', DATE '1582-10-15', CAST(NULL AS DATE)):array<date>>
+-- !query 47 output
+[1582-10-03,1582-10-04,1582-10-15,null]
+
+
+-- !query 48
+select cast('1582-10-13' as date), date '1582-10-14', date '1582-10-15'
+-- !query 48 schema
+struct<CAST(1582-10-13 AS DATE):date,DATE '1582-10-14':date,DATE '1582-10-15':date>
+-- !query 48 output
+1582-10-13     1582-10-14      1582-10-15
{code}
 ",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 11:56:16 UTC 2019,,,,,,,,,,"0|z09trc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Dec/19 11:56;cloud_fan;Issue resolved by pull request 26942
[https://github.com/apache/spark/pull/26942];;;",,,,,,,,,,,,,,,,,,,,,,,
Update correct string in UI for metrics when driver updates same metrics id as tasks.,SPARK-30300,13275378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nartal1,nartal1,nartal1,19/Dec/19 00:26,20/Dec/19 13:30,13/Jul/23 08:49,20/Dec/19 13:30,3.0.0,,,,,,,,,,3.0.0,,,SQL,Web UI,,,,0,,,,"There is a bug in displaying of additional max metrics (stageID (attemptID):task Id).

If driver is updating the same metric which was updated by tasks and if the drivers value exceeds max, then it is not captured. Need to capture this case and update the UI accordingly.  ",,Ankitraj,nartal1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 01:21:13 UTC 2019,,,,,,,,,,"0|z09tnc:",9223372036854775807,,,,,tgraves,,,,,,,,,,,,,,,,,"19/Dec/19 00:57;Ankitraj;I will start working on this issue ;;;","19/Dec/19 00:59;Ankitraj;[~nartal1], can you please attached snapshot of UI. ;;;","19/Dec/19 01:09;nartal1;[~Ankitraj], Sorry I should have updated here right after filing the bug.  If you haven't already started would it be okay if i fix this. I have some basic fix and currently running unit tests on it.;;;","19/Dec/19 02:07;nartal1;Submitted PR: [https://github.com/apache/spark/pull/26941];;;","20/Dec/19 01:21;Ankitraj;[~nartal1], no problem finally we need to fix a bug, thank you for raising PR.;;;",,,,,,,,,,,,,,,,,,,
bucket join cannot work for self-join with views,SPARK-30298,13275264,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imback82,xiaojuwu,xiaojuwu,18/Dec/19 13:33,07/Mar/20 05:28,13/Jul/23 08:49,23/Jan/20 23:25,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"This UT may fail at the last line:
{code:java}
test(""bucket join cannot work for self-join with views"") {
    withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> ""1"") {
      withTable(""t1"") {
        val df = (0 until 20).map(i => (i, i)).toDF(""i"", ""j"").as(""df"")
        df.write
          .format(""parquet"")
          .bucketBy(8, ""i"")
          .saveAsTable(""t1"")

        sql(s""create view v1 as select * from t1"").collect()

        val plan1 = sql(""SELECT * FROM t1 a JOIN t1 b ON a.i = b.i"").queryExecution.executedPlan
        assert(plan1.collect { case exchange : ShuffleExchangeExec => exchange }.isEmpty)

        val plan2 = sql(""SELECT * FROM t1 a JOIN v1 b ON a.i = b.i"").queryExecution.executedPlan
        assert(plan2.collect { case exchange : ShuffleExchangeExec => exchange }.isEmpty)
      }
    }
  }
{code}

It's because View will add Project with Alias, then Join's requiredDistribution is based on Alias, but ProjectExec passes child's outputPartition up without Alias. Then the satisfies check cannot meet in EnsureRequirement.

",,Ankitraj,imback82,maropu,mingmwang,petertoth,xiaojuwu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19468,SPARK-25951,SPARK-19981,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 23:25:04 UTC 2020,,,,,,,,,,"0|z09sy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Dec/19 01:03;Ankitraj;I will raise pr for this;;;","19/Dec/19 03:34;imback82;[~Ankitraj] did you already start on this? I faced a similar issue a month ago, and I have a fix almost ready for PR and found this JIRA. Do you mind if I take this up? Thanks!;;;","19/Dec/19 06:07;imback82;[~Ankitraj] I created a PR: [https://github.com/apache/spark/pull/26943]. Please chime in if you like. Thanks!;;;","19/Dec/19 10:55;Ankitraj;[~imback82] ok , Thank you for raising PR.;;;","23/Jan/20 23:25;maropu;Resolved by https://github.com/apache/spark/pull/26943;;;",,,,,,,,,,,,,,,,,,,
Partitioned by Nested Column for `InMemoryTable`,SPARK-30289,13275108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dbtsai,dbtsai,17/Dec/19 22:56,04/Mar/20 05:35,13/Jul/23 08:49,14/Feb/20 21:49,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,dbtsai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 21:49:11 UTC 2020,,,,,,,,,,"0|z09rzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Feb/20 21:49;dbtsai;Issue resolved by pull request 26929
[https://github.com/apache/spark/pull/26929];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix deadlock between LiveListenerBus#stop and AsyncEventQueue#removeListenerOnError,SPARK-30285,13274949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wang-shuo,wang-shuo,wang-shuo,17/Dec/19 09:32,03/Jan/20 02:42,13/Jul/23 08:49,03/Jan/20 00:46,2.3.0,2.4.0,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"There is a deadlock between LiveListenerBus#stop and AsyncEventQueue#removeListenerOnError.

we can reproduce as follows:
 # Post some events to LiveListenerBus
 # Call LiveListenerBus#stop and hold the synchronized lock of bus, waiting until all the events are processed by listeners, then remove all the queues
 # Event queue would drain out events by posting to its listeners. If a listener is interrupted, it will call AsyncEventQueue#removeListenerOnError,  inside it will call bus.removeListener, trying to acquire synchronized lock of bus, resulting in deadlock",,irashid,vanzin,wang-shuo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 00:46:52 UTC 2020,,,,,,,,,,"0|z09r00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jan/20 00:46;vanzin;Issue resolved by pull request 26924
[https://github.com/apache/spark/pull/26924];;;",,,,,,,,,,,,,,,,,,,,,,,
CREATE VIEW should track the current catalog and namespace,SPARK-30284,13274928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Dec/19 08:20,02/Jan/20 17:49,13/Jul/23 08:49,02/Jan/20 17:45,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 17:45:08 UTC 2020,,,,,,,,,,"0|z09qvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jan/20 17:45;cloud_fan;Issue resolved by pull request 26923
[https://github.com/apache/spark/pull/26923];;;",,,,,,,,,,,,,,,,,,,,,,,
'archive' option in FileStreamSource misses to consider partitioned and recursive option,SPARK-30281,13274901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,17/Dec/19 05:49,08/Jan/20 23:23,13/Jul/23 08:49,08/Jan/20 17:16,3.0.0,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"Cleanup option for FileStreamSource is introduced in SPARK-20568.

To simplify the condition of verifying archive path, it took the fact that FileStreamSource reads the files where these files meet one of conditions: 1) parent directory matches the source pattern 2) the file itself matches the source pattern.

We found there're other cases during post-hoc review which invalidate above fact: partitioned, and recursive option. With these options, FileStreamSource can read the arbitrary files in subdirectories which match the source pattern, so simply checking the depth of archive path doesn't work.

We need to restore the path check logic, though it would be not easy to explain to end users.",,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 17:16:17 UTC 2020,,,,,,,,,,"0|z09qpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Dec/19 05:51;kabhwan;Will submit a PR soon.;;;","08/Jan/20 17:16;vanzin;Issue resolved by pull request 26920
[https://github.com/apache/spark/pull/26920];;;",,,,,,,,,,,,,,,,,,,,,,
Support 32 or more grouping attributes for GROUPING_ID ,SPARK-30279,13274888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,17/Dec/19 03:35,06/Mar/20 07:59,13/Jul/23 08:49,06/Mar/20 07:59,2.4.6,3.0.0,,,,,,,,,3.1.0,,,SQL,,,,,0,,,,"This ticket targets to support 32 or more grouping attributes for GROUPING_ID. In the current master, an integer overflow can occur to compute grouping IDs;
https://github.com/apache/spark/blob/e75d9afb2f282ce79c9fd8bce031287739326a4f/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala#L613

For example, the query below generates wrong grouping IDs in the master;
{code}
scala> val numCols = 32 // or, 31
scala> val cols = (0 until numCols).map { i => s""c$i"" }
scala> sql(s""create table test_$numCols (${cols.map(c => s""$c int"").mkString("","")}, v int) using parquet"")
scala> val insertVals = (0 until numCols).map { _ => 1 }.mkString("","")
scala> sql(s""insert into test_$numCols values ($insertVals,3)"")
scala> sql(s""select grouping_id(), sum(v) from test_$numCols group by grouping sets ((${cols.mkString("","")}), (${cols.init.mkString("","")}))"").show(10, false)
scala> sql(s""drop table test_$numCols"")

// numCols = 32
+-------------+------+
|grouping_id()|sum(v)|
+-------------+------+
|0            |3     |
|0            |3     | // Wrong Grouping ID
+-------------+------+

// numCols = 31
+-------------+------+
|grouping_id()|sum(v)|
+-------------+------+
|0            |3     |
|1            |3     |
+-------------+------+
{code}",,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 07:59:11 UTC 2020,,,,,,,,,,"0|z09qmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Mar/20 07:59;maropu;Resolved by [https://github.com/apache/spark/pull/26918];;;",,,,,,,,,,,,,,,,,,,,,,,
Avoid BytesToBytesMap lookup hang forever when holding keys reaching max capacity,SPARK-30274,13274870,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,17/Dec/19 01:26,08/Jun/20 20:57,13/Jul/23 08:49,17/Dec/19 19:46,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"BytesToBytesMap.append allows to append keys until the number of keys reaches MAX_CAPACITY. But once the the pointer array in the map holds MAX_CAPACITY keys, next time call of lookup will hand forever.",,cloud_fan,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 20:57:45 UTC 2020,,,,,,,,,,"0|z09qig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Dec/19 19:46;dongjoon;Issue resolved by pull request 26914
[https://github.com/apache/spark/pull/26914];;;","08/Jun/20 16:09;cloud_fan;what shall we write for the release notes?;;;","08/Jun/20 17:40;viirya;I think this is internal bug fix and should not change user behavior. [~dongjoon] Do you remember why putting release-notes label?;;;","08/Jun/20 19:55;dongjoon;It was because we had a community report on the hang on that PR.
- https://github.com/apache/spark/pull/26914#issuecomment-566452079;;;","08/Jun/20 20:57;dongjoon;I removed the label, `release-notes`, since we don't mention all bug fixes in the release notes.;;;",,,,,,,,,,,,,,,,,,,
dynamic allocation won't release some executor in some case.,SPARK-30271,13274773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,angerszhuuu,angerszhuuu,16/Dec/19 14:14,09/Oct/20 07:18,13/Jul/23 08:49,09/Oct/20 07:18,2.4.0,,,,,,,,,,,,,Scheduler,Spark Core,,,,0,,,,"Case :
max executor 5
min executor 0
idle time  5s

stage-1 10 tasks run in 5 executors.
If stage-1 finished in 5 all executors, all executor added to `removeTimes` when taskEnd event.
After 5s, start release process, since stage-2 have 20 tasks, then executor won't be removed since existing executor num < executorTargetNum., and executor will be removed from `removeTimes`. 
But if task won't be scheduled to all these executors, if executor-1 won't have task to run in it, it won't be put into `removeTimes` and if there are no more tasks, executor won't be removed forever",,angerszhuuu,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-12-16 14:14:35.0,,,,,,,,,,"0|z09pww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect pyspark package name when releasing preview version,SPARK-30268,13274762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,16/Dec/19 13:31,12/Dec/22 18:10,13/Jul/23 08:49,17/Dec/19 01:25,3.0.0,,,,,,,,,,3.0.0,,,Project Infra,,,,,0,,,,"{noformat}
cp: cannot stat 'spark-3.0.0-preview2-bin-hadoop2.7/python/dist/pyspark-3.0.0.dev02.tar.gz': No such file or directory
gpg: can't open 'pyspark-3.0.0.dev02.tar.gz': No such file or directory
gpg: signing failed: No such file or directory
gpg: pyspark-3.0.0.dev02.tar.gz: No such file or directory
{noformat}

But it is:

{noformat}
yumwang@ubuntu-3513086:~/spark-release/output$ ll spark-3.0.0-preview2-bin-hadoop2.7/python/dist/
total 214140
drwxr-xr-x 2 yumwang stack      4096 Dec 16 06:17 ./
drwxr-xr-x 9 yumwang stack      4096 Dec 16 06:17 ../
-rw-r--r-- 1 yumwang stack 219267173 Dec 16 06:17 pyspark-3.0.0.dev2.tar.gz
{noformat}



{noformat}
/usr/local/lib/python3.6/dist-packages/setuptools/dist.py:476: UserWarning: Normalizing '3.0.0.dev02' to '3.0.0.dev2'
  normalized_version,
{noformat}

",,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 01:25:13 UTC 2019,,,,,,,,,,"0|z09pug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Dec/19 01:25;gurwls223;Issue resolved by pull request 26909
[https://github.com/apache/spark/pull/26909];;;",,,,,,,,,,,,,,,,,,,,,,,
Int overflow and MatchError in ApproximatePercentile ,SPARK-30266,13274726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,16/Dec/19 10:14,06/Mar/21 04:58,13/Jul/23 08:49,25/Dec/19 12:05,2.4.4,3.0.0,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"accuracyExpression can accept Long which may cause overflow error.

accuracyExpression can accept fractions which are implicitly floored.

accuracyExpression can accept null which is implicitly changed to 0.

percentageExpression can accept null but cause MatchError.

percentageExpression can accept ArrayType(_, nullable=true) in which the nulls are implicitly changed to zeros.

 

 ",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30303,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 25 12:05:08 UTC 2019,,,,,,,,,,"0|z09pmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Dec/19 12:05;cloud_fan;Issue resolved by pull request 26905
[https://github.com/apache/spark/pull/26905];;;",,,,,,,,,,,,,,,,,,,,,,,
Do not change R version when releasing preview versions,SPARK-30265,13274695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,16/Dec/19 08:10,22/Jan/20 21:47,13/Jul/23 08:49,16/Dec/19 11:55,3.0.0,,,,,,,,,,3.0.0,,,Project Infra,,,,,0,,,,"
{code:sh}
./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr
{code}

{noformat}

++ . /opt/spark-rm/output/spark-3.0.0-preview2-bin-hadoop2.7/R/find-r.sh
+++ '[' -z /usr/bin ']'
++ /usr/bin/Rscript -e ' if(""devtools"" %in% rownames(installed.packages())) { library(devtools); devtools::document(pkg=""./pkg"", roclets=c(""rd"")) }'
Loading required package: usethis
Updating SparkR documentation
First time using roxygen2. Upgrading automatically...
Loading SparkR
Invalid DESCRIPTION:
Malformed package version.

See section 'The DESCRIPTION file' in the 'Writing R Extensions'
manual.

Error: invalid version specification '3.0.0-preview2'
In addition: Warning message:
roxygen2 requires Encoding: UTF-8
Execution halted
[ERROR] Command execution failed.
org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
    at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:404)
    at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:166)
    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:804)
    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:751)
    at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:313)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.0.0-preview2:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [ 18.619 s]
[INFO] Spark Project Tags ................................. SUCCESS [ 13.652 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  5.673 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  2.081 s]
[INFO] Spark Project Networking ........................... SUCCESS [  3.509 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  0.993 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  7.556 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  5.522 s]
[INFO] Spark Project Core ................................. FAILURE [01:06 min]
[INFO] Spark Project ML Local Library ..................... SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project Graph API ............................ SKIPPED
[INFO] Spark Project Cypher ............................... SKIPPED
[INFO] Spark Project Graph ................................ SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:04 min
[INFO] Finished at: 2019-12-16T08:02:45Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:exec (sparkr-pkg) on project spark-core_2.12: Command execution failed.: Process exited with an error: 1 (Exit value: 1) -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :spark-core_2.12

{noformat}
",,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 16 11:55:43 UTC 2019,,,,,,,,,,"0|z09pfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Dec/19 11:55;yumwang;Issue resolved by pull request 26904
https://github.com/apache/spark/pull/26904;;;",,,,,,,,,,,,,,,,,,,,,,,
Don't log values of ignored non-Spark properties,SPARK-30263,13274567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,14/Dec/19 13:43,14/Dec/19 21:14,13/Jul/23 08:49,14/Dec/19 21:14,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"Comment per Aaron Steers:

Is it expected that this error would print aws security keys to log files? Seems like a serious security concern.

{code}
Warning: Ignoring non-spark config property: fs.s3a.access.key={full-access-key}
Warning: Ignoring non-spark config property: fs.s3a.secret.key={full-secret-key}
{code}

Could we not accomplish the same thing by printing the name of the key without the key's value?


I think we can also redact these, but, also no big reason to log the value of ignored properties here anyway.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 14 21:14:51 UTC 2019,,,,,,,,,,"0|z09on4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Dec/19 21:14;dongjoon;Issue resolved by pull request 26893
[https://github.com/apache/spark/pull/26893];;;",,,,,,,,,,,,,,,,,,,,,,,
Avoid NumberFormatException when totalSize is empty,SPARK-30262,13274545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,southernriver,southernriver,southernriver,14/Dec/19 07:56,18/Dec/19 23:14,13/Jul/23 08:49,18/Dec/19 23:14,2.4.3,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"We could get the Partitions Statistics Info.But in some specail case, The Info  like  totalSize，rawDataSize，rowCount maybe empty. When we do some ddls like   
{code:java}
desc formatted partition{code}
 ,the NumberFormatException is showed as below:
{code:java}
spark-sql> desc formatted table1 partition(year='2019', month='10', day='17', hour='23');
19/10/19 00:02:40 ERROR SparkSQLDriver: Failed in [desc formatted table1 partition(year='2019', month='10', day='17', hour='23')]
java.lang.NumberFormatException: Zero length BigInteger
at java.math.BigInteger.(BigInteger.java:411)
at java.math.BigInteger.(BigInteger.java:597)
at scala.math.BigInt$.apply(BigInt.scala:77)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$31.apply(HiveClientImpl.scala:1056)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$31.apply(HiveClientImpl.scala:1056)
at scala.Option.map(Option.scala:146)
at org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$readHiveStats(HiveClientImpl.scala:1056)
at org.apache.spark.sql.hive.client.HiveClientImpl$.fromHivePartition(HiveClientImpl.scala:1048)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionOption$1$$anonfun$apply$16.apply(HiveClientImpl.scala:659)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionOption$1$$anonfun$apply$16.apply(HiveClientImpl.scala:659)
at scala.Option.map(Option.scala:146)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionOption$1.apply(HiveClientImpl.scala:659)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionOption$1.apply(HiveClientImpl.scala:656)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:281)
at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:219)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:218)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:264)
at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionOption(HiveClientImpl.scala:656)
at org.apache.spark.sql.hive.client.HiveClient$class.getPartitionOption(HiveClient.scala:194)
at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionOption(HiveClientImpl.scala:84)
at org.apache.spark.sql.hive.client.HiveClient$class.getPartition(HiveClient.scala:174)
at org.apache.spark.sql.hive.client.HiveClientImpl.getPartition(HiveClientImpl.scala:84)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getPartition$1.apply(HiveExternalCatalog.scala:1125)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getPartition$1.apply(HiveExternalCatalog.scala:1124)
{code}
Although we can use 'Analyze table partition ' to update the totalSize,rawDataSize or rowCount, it's unresonable for normal SQL to throw NumberFormatException for Empty totalSize.We should fix the empty case when readHiveStats.

Here is the empty case:
 !screenshot-1.png!",,dongjoon,southernriver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/19 08:25;southernriver;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12988849/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 23:14:11 UTC 2019,,,,,,,,,,"0|z09oi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Dec/19 03:15;dongjoon;Hi, [~southernriver] . Thank you for filing Jira issue and making a PR.

BTW, `Fix Version` and `Target Version` are used when your PR is merged. So, please don't fill them at the beginning.;;;","18/Dec/19 23:14;dongjoon;Issue resolved by pull request 26892
[https://github.com/apache/spark/pull/26892];;;",,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE throw error when session catalog specified,SPARK-30259,13274526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wwg28103,wwg28103,wwg28103,14/Dec/19 03:23,16/Dec/19 06:02,13/Jul/23 08:49,14/Dec/19 23:36,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Spark throw error when the session catalog is specified explicitly in ""CREATE TABLE"" and ""CREATE TABLE AS SELECT"" command, eg. 
{code:java}
CREATE TABLE spark_catalog.tbl USING json AS SELECT 1 AS i;
{code}
the error message is like below: 
{noformat}
19/12/14 10:56:08 INFO HiveMetaStore: 0: get_table : db=spark_catalog tbl=tbl
19/12/14 10:56:08 INFO audit: ugi=fuwhu ip=unknown-ip-addr      cmd=get_table : db=spark_catalog tbl=tbl        
19/12/14 10:56:08 INFO HiveMetaStore: 0: get_database: spark_catalog
19/12/14 10:56:08 INFO audit: ugi=fuwhu ip=unknown-ip-addr      cmd=get_database: spark_catalog 
19/12/14 10:56:08 WARN ObjectStore: Failed to get database spark_catalog, returning NoSuchObjectException
Error in query: Database 'spark_catalog' not found;{noformat}",,dongjoon,wwg28103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 14 23:36:37 UTC 2019,,,,,,,,,,"0|z09oe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Dec/19 23:36;dongjoon;Issue resolved by pull request 26887
[https://github.com/apache/spark/pull/26887];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix use custom escape lead to LikeSimplification optimize failed,SPARK-30254,13274353,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,13/Dec/19 09:16,23/Mar/20 04:49,13/Jul/23 08:49,19/Dec/19 00:01,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,We should also sync the escape used by `LikeSimplification`.,,dongjoon,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31210,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 00:01:01 UTC 2019,,,,,,,,,,"0|z09nbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Dec/19 00:01;dongjoon;Issue resolved by pull request 26880
[https://github.com/apache/spark/pull/26880];;;",,,,,,,,,,,,,,,,,,,,,,,
DROP TABLE doesn't work if session catalog name is provided,SPARK-30248,13274310,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,13/Dec/19 06:10,13/Dec/19 13:48,13/Jul/23 08:49,13/Dec/19 13:48,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"If a table name is qualified with session catalog name (""spark_catalog""), the DROP TABLE command fails.

For example, the following
{code:java}
sql(""CREATE TABLE tbl USING json AS SELECT 1 AS i"")
sql(""DROP TABLE spark_catalog.tbl"")
{code}
fails with:
{code:java}
org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'spark_catalog' not found;
   at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists(ExternalCatalog.scala:42)
   at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists$(ExternalCatalog.scala:40)
   at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireDbExists(InMemoryCatalog.scala:45)
   at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.tableExists(InMemoryCatalog.scala:336)
{code}",,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-12-13 06:10:13.0,,,,,,,,,,"0|z09n20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Yarn External Shuffle Service Memory Leak,SPARK-30246,13274300,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,henriquedsg89,UncleHuang,UncleHuang,13/Dec/19 04:47,15/Feb/20 08:29,13/Jul/23 08:49,15/Jan/20 21:36,2.4.3,,,,,,,,,,2.4.5,3.0.0,,Shuffle,Spark Core,,,,1,,,,"In our large busy yarn cluster which deploy Spark external shuffle service as part of YARN NM aux service, we encountered OOM in some NMs.
after i dump the heap memory and found there are some StremState objects still in heap, but the app which the StreamState belongs to is already finished.

Here is some relate Figures:
!https://raw.githubusercontent.com/012huang/public_source/master/SparkPRFigures/nm_oom.png|width=100%!

The heap dump below shows that the memory consumption mainly consists of two parts:
*(1) OneForOneStreamManager (4,429,796,424 (77.11%) bytes)*
*(2) PoolChunk(occupy 1,059,201,712 (18.44%) bytes. )*

!https://raw.githubusercontent.com/012huang/public_source/master/SparkPRFigures/nm_heap_overview.png|width=100%!

dig into the OneForOneStreamManager, there are some StreaStates still remained :
!https://raw.githubusercontent.com/012huang/public_source/master/SparkPRFigures/streamState.png|width=100%!


incomming references to StreamState::associatedChannel: 
!https://raw.githubusercontent.com/012huang/public_source/master/SparkPRFigures/associatedChannel_incomming_reference.png|width=100%!","hadoop 2.7.3
spark 2.4.3
jdk 1.8.0_60",colin,dongjoon,henriquedsg89,hzfeiwang,jfilipiak,Steven Rand,UncleHuang,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 19 14:26:54 UTC 2020,,,,,,,,,,"0|z09mzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Dec/19 04:26;jfilipiak;Hello, we are facing similiiar issues at the moment,

hence I am also looking into this. The cleanup logic seems pretty legitimate. Could you list all the incomming references to StreamState::associatedChannel from your dump. 

I think its's either because there is no read timeout on the Network channel (associatedChannel would have incoming references from netty) or the connectionInactive handler isn't called on read timeouts (that would be a bug in code and not the config and the associatedChannel would have no incoming references from netty).

 

 ;;;","19/Dec/19 08:00;UncleHuang;hi , [~jfilipiak], I add a figure about the incomming references to StreamState::associatedChannel above.;;;","19/Dec/19 08:02;UncleHuang;I'm working on this and have added a patch for verification last week, so far it looks good by monitoring streams size and nm's heap memory usage.  I will add a PR for this after verification.;;;","20/Dec/19 09:39;jfilipiak;Hi, Feel free to send the PR along later i can double check it.

 

You can clearly see the connection is still hold on to by Netty, that probably indicates that this connection doesn't time out properly.

Looks like (in your versin 2.4.3)

org.apache.spark.network.yarn.YarnShuffleService.serviceInit(Configuration)#L185

initializes TransportContext like this

TransportContext transportContext = new TransportContext(transportConf, blockHandler);

leaving closeIdleConnections set to false. and hence your idle connection wouldn't get closed

 ;;;","15/Jan/20 21:36;vanzin;Issue resolved by pull request 27064
[https://github.com/apache/spark/pull/27064];;;","19/Jan/20 14:26;jfilipiak;[~vanzin] have to disagree, this is the wrong link.

 

The issue in this task seems to be fixed upstream regardless. Someone added a true, closeIdleConnections to Netty TransportContext constructor.;;;",,,,,,,,,,,,,,,,,,
hive partition pruning can only support string and integral types,SPARK-30238,13274220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,12/Dec/19 18:41,17/Dec/19 10:28,13/Jul/23 08:49,12/Dec/19 21:07,2.4.0,,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,,,cloud_fan,dongjoon,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30181,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 21:07:39 UTC 2019,,,,,,,,,,"0|z09mi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Dec/19 21:07;dongjoon;Issue resolved by pull request 26871
[https://github.com/apache/spark/pull/26871];;;",,,,,,,,,,,,,,,,,,,,,,,
Keeping compatibility with 2.4 external shuffle service regarding host local shuffle blocks reading,SPARK-30235,13274152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,attilapiros,attilapiros,attilapiros,12/Dec/19 13:35,17/Dec/19 18:33,13/Jul/23 08:49,17/Dec/19 18:33,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,When `spark.shuffle.readHostLocalDisk.enabled` is true then a new message is used which is not supported by Spark 2.4.,,attilapiros,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 18:33:08 UTC 2019,,,,,,,,,,"0|z09m2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Dec/19 13:35;attilapiros;I am working on this.;;;","17/Dec/19 18:33;vanzin;Issue resolved by pull request 26869
[https://github.com/apache/spark/pull/26869];;;",,,,,,,,,,,,,,,,,,,,,,
Update zstd-jni to 1.4.4-3,SPARK-30228,13274007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,12/Dec/19 02:03,12/Dec/22 18:10,13/Jul/23 08:49,12/Dec/19 05:16,3.0.0,,,,,,,,,,2.4.8,3.0.0,,Build,,,,,0,correctness,releasenotes,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34536,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 17:35:19 UTC 2021,,,,,,,,,,"0|z09l6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Dec/19 05:16;gurwls223;Issue resolved by pull request 26856
[https://github.com/apache/spark/pull/26856];;;","25/Feb/21 11:22;apachespark;User 'seayoun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31645;;;","25/Feb/21 11:23;apachespark;User 'seayoun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31645;;;","03/Mar/21 17:35;dongjoon;This patch is backported to branch-2.4 as a correctness issue. I'll switch the issue information.;;;",,,,,,,,,,,,,,,,,,,,
"""Stream is corrupted at"" exception on reading disk-spilled data of a shuffle operation",SPARK-30225,13273941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,mkempanna,mkempanna,11/Dec/19 18:54,12/Dec/22 18:10,13/Jul/23 08:49,03/Jan/20 09:38,2.4.0,,,,,,,,,,2.4.5,3.0.0,,Input/Output,,,,,0,,,,"There is issues with spark.unsafe.sorter.spill.read.ahead.enabled in spark 2.4.0, which is introduced by https://issues.apache.org/jira/browse/SPARK-23366

 

Workaround for this problem is to disable readahead of unsafe spill with following.
 --conf spark.unsafe.sorter.spill.read.ahead.enabled=false

 

This issue can be reproduced on Spark 2.4.0 by following the steps in this comment of Jira SPARK-18105.

https://issues.apache.org/jira/browse/SPARK-18105?focusedCommentId=16981461&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16981461

 

Exception looks like below: 
{code:java}
19/12/10 01:51:31 INFO sort.ShuffleExternalSorter: Thread 142 spilling sort data of 5.1 GB to disk (1  time so far)19/12/10 01:51:31 INFO sort.ShuffleExternalSorter: Thread 142 spilling sort data of 5.1 GB to disk (1  time so far)19/12/10 01:52:48 INFO sort.ShuffleExternalSorter: Thread 142 spilling sort data of 5.1 GB to disk (2  times so far)19/12/10 01:53:53 ERROR executor.Executor: Exception in task 6.0 in stage 0.0 (TID 6)java.io.IOException: Stream is corrupted at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:228) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) at org.apache.spark.io.ReadAheadInputStream$1.run(ReadAheadInputStream.java:168) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)19/12/10 01:53:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3319/12/10 01:53:53 INFO executor.Executor: Running task 8.1 in stage 0.0 (TID 33)19/12/10 01:54:00 INFO sort.UnsafeExternalSorter: Thread 142 spilling sort data of 3.3 GB to disk (0  time so far)19/12/10 01:54:30 INFO executor.Executor: Executor is trying to kill task 8.1 in stage 0.0 (TID 33), reason: Stage cancelled19/12/10 01:54:30 INFO executor.Executor: Executor killed task 8.1 in stage 0.0 (TID 33), reason: Stage cancelled19/12/10 01:54:52 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown{code}
 

 

 

 

 ",,mkempanna,mmarcini,petertoth,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18105,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 09:38:42 UTC 2020,,,,,,,,,,"0|z09ks0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Dec/19 22:38;vanzin;The changes in SPARK-23366 may not necessarily have caused this. But that change actually flipped the configuration's default value from false to true; so 2.3 has the feature disabled by default, and 2.4 has it enabled by default. So the bug may have existed in the 2.3 version of the code, making it a bit harder to track. Still taking a look at the code but nothing popped up yet...;;;","03/Jan/20 09:38;gurwls223;Issue resolved by pull request 27084
[https://github.com/apache/spark/pull/27084];;;",,,,,,,,,,,,,,,,,,,,,,
HiveOutputWriter standardOI should use ObjectInspectorCopyOption.DEFAULT,SPARK-30201,13273590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ulysses,ulysses,ulysses,10/Dec/19 11:11,06/Oct/20 03:13,13/Jul/23 08:49,17/Dec/19 04:39,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.0,,,,,2.4.8,3.0.0,,SQL,,,,,0,correctness,,,"Now spark use `ObjectInspectorCopyOption.JAVA` as oi option which will convert any string to UTF-8 string. When write non UTF-8 code data, then `EFBFBD` will appear.
We should use `ObjectInspectorCopyOption.DEFAULT` to support pass the bytes.

Here is the way to reproduce:
1. make a file contains 16 radix 'AABBCC' which is not the UTF-8 code.
2. create table test1 (c string) location '$file_path';
3. select hex(c) from test1; // AABBCC
4. craete table test2 (c string) as select c from test1;
5. select hex(c) from test2; // EFBFBDEFBFBDEFBFBD
",,anuragmantri,apachespark,cloud_fan,dongjoon,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 03:13:55 UTC 2020,,,,,,,,,,"0|z09im0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Dec/19 04:39;cloud_fan;Issue resolved by pull request 26831
[https://github.com/apache/spark/pull/26831];;;","05/Oct/20 19:16;dongjoon;Hi, [~ulysses] and [~cloud_fan]. Is this only critical on 3.0.0?;;;","05/Oct/20 22:33;apachespark;User 'anuragmantri' has created a pull request for this issue:
https://github.com/apache/spark/pull/29948;;;","05/Oct/20 22:38;anuragmantri;[~cloud_fan], [~ulysses], [~dongjoon] - I verified this issue is present in branch-2.4. Test failure below:



{{[info] == Results ==}}
{{[info] !== Correct Answer - 1 == == Spark Answer - 1 ==}}
{{[info] !struct<> struct<hex(c):string>}}
{{[info] ![AABBCC] [EFBFBDEFBFBDEFBFBD] (QueryTest.scala:163)}}
{{[info] org.scalatest.exceptions.TestFailedException:}}

 

I created a PR to backport it to branch 2.4. It was a clean cherry-pick, could you please take a look? Thanks

[https://github.com/apache/spark/pull/29948];;;","05/Oct/20 23:31;dongjoon;I labeled this issue as `correctness` because the query result is wrong.;;;","06/Oct/20 03:13;dongjoon;This lands at `branch-2.4` via https://github.com/apache/spark/pull/29948 ;;;",,,,,,,,,,,,,,,,,,
Recover spark.ui.port and spark.blockManager.port from checkpoint,SPARK-30199,13273530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aaruna,dongjoon,dongjoon,10/Dec/19 06:12,24/Apr/20 04:24,13/Jul/23 08:49,12/Dec/19 02:22,2.4.4,3.0.0,,,,,,,,,2.4.6,3.0.0,,DStreams,,,,,0,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31544,,,,,,,,,,,SPARK-31544,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 02:22:09 UTC 2019,,,,,,,,,,"0|z09i8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Dec/19 02:22;dongjoon;Issue resolved by pull request 26827
[https://github.com/apache/spark/pull/26827];;;",,,,,,,,,,,,,,,,,,,,,,,
BytesToBytesMap does not grow internal long array as expected,SPARK-30198,13273529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,10/Dec/19 06:12,13/Dec/19 07:25,13/Jul/23 08:49,11/Dec/19 22:59,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"One Spark job on our cluster hangs forever at BytesToBytesMap.safeLookup. After inspecting, the long array size is 536870912.

Currently in BytesToBytesMap.append, we only grow the internal array if the size of the array is less than its MAX_CAPACITY that is 536870912. So in above case, the array can not be grown up, and safeLookup can not find an empty slot forever.

But it is wrong because we use two array entries per key, so the array size is twice the capacity. We should compare the current capacity of the array, instead of its size.



",,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 11 22:59:10 UTC 2019,,,,,,,,,,"0|z09i8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Dec/19 22:59;dongjoon;Issue resolved by pull request 26828
[https://github.com/apache/spark/pull/26828];;;",,,,,,,,,,,,,,,,,,,,,,,
Improve test in SingleSessionSuite,SPARK-30179,13273259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,leoluan,yumwang,yumwang,09/Dec/19 06:55,12/Dec/22 18:10,13/Jul/23 08:49,10/Dec/19 01:58,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,"https://github.com/apache/spark/blob/58be82ad4b98fc17e821e916e69e77a6aa36209d/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala#L782-L824

We should also verify the UDF works.",,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 01:58:23 UTC 2019,,,,,,,,,,"0|z09gk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Dec/19 06:56;yumwang;Example: https://github.com/apache/spark/blob/58be82ad4b98fc17e821e916e69e77a6aa36209d/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala#L605-L614;;;","10/Dec/19 01:58;gurwls223;Issue resolved by pull request 26812
[https://github.com/apache/spark/pull/26812];;;",,,,,,,,,,,,,,,,,,,,,,
Log4j configuration for REPL can't override the root logger properly.,SPARK-30167,13273174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,08/Dec/19 14:45,13/Dec/19 22:30,13/Jul/23 08:49,13/Dec/19 22:30,3.0.0,,,,,,,,,,3.0.0,,,Spark Shell,,,,,0,,,,"SPARK-11929 enabled REPL's log4j configuration to override root logger but SPARK-26753 seems to have broken the feature.

You can see one example when you modifies the default log4j configuration like as follows.
{code:java}
# Change the log level for rootCategory to DEBUG
log4j.rootCategory=DEBUG, console

...

# The log level for repl.Main remains WARN
log4j.logger.org.apache.spark.repl.Main=WARN{code}
If you launch REPL with the configuration, INFO level logs appear even though the log level for REPL is WARN.
{code:java}
・・・

19/12/08 23:31:38 INFO Utils: Successfully started service 'sparkDriver' on port 33083.
19/12/08 23:31:38 INFO SparkEnv: Registering MapOutputTracker
19/12/08 23:31:38 INFO SparkEnv: Registering BlockManagerMaster
19/12/08 23:31:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/12/08 23:31:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/12/08 23:31:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat

・・・{code}
 

Before SPARK-26753 was applied, those INFO level logs are not shown with the same log4j.properties.",,roczei,sarutak,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 22:30:41 UTC 2019,,,,,,,,,,"0|z09g1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Dec/19 22:30;vanzin;Issue resolved by pull request 26798
[https://github.com/apache/spark/pull/26798];;;",,,,,,,,,,,,,,,,,,,,,,,
Exclude Hive domain in Unidoc build explicitly,SPARK-30164,13272969,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,07/Dec/19 22:37,12/Dec/22 18:10,13/Jul/23 08:49,09/Dec/19 04:17,3.0.0,,,,,,,,,,3.0.0,,,Documentation,,,,,0,,,,"We don't publish this as a part of Spark documentation (see also https://github.com/apache/spark/blob/master/docs/_plugins/copy_api_dirs.rb#L30) and most of them are copy of Hive thrift server so that we can officially use Hive 2.3 release.

It doesn't much make sense to check the documentation generation against another domain, and that we don't use in documentation publish.",,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 09 04:17:02 UTC 2019,,,,,,,,,,"0|z09ers:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Dec/19 04:17;gurwls223;Issue resolved by pull request 26800
[https://github.com/apache/spark/pull/26800];;;",,,,,,,,,,,,,,,,,,,,,,,
Add PushedFilters to metadata in Parquet DSv2 implementation,SPARK-30162,13272899,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,nasirali,nasirali,07/Dec/19 07:30,12/Dec/22 17:35,13/Jul/23 08:49,21/Apr/20 20:31,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Filters are not pushed down in Spark 3.0 preview. Also the output of ""explain"" method is different. It is hard to debug in 3.0 whether filters were pushed down or not. Below code could reproduce the bug:

 
{code:java}
// code placeholder
df = spark.createDataFrame([(""usr1"",17.00, ""2018-03-10T15:27:18+00:00""),
                            (""usr1"",13.00, ""2018-03-11T12:27:18+00:00""),
                            (""usr1"",25.00, ""2018-03-12T11:27:18+00:00""),
                            (""usr1"",20.00, ""2018-03-13T15:27:18+00:00""),
                            (""usr1"",17.00, ""2018-03-14T12:27:18+00:00""),
                            (""usr2"",99.00, ""2018-03-15T11:27:18+00:00""),
                            (""usr2"",156.00, ""2018-03-22T11:27:18+00:00""),
                            (""usr2"",17.00, ""2018-03-31T11:27:18+00:00""),
                            (""usr2"",25.00, ""2018-03-15T11:27:18+00:00""),
                            (""usr2"",25.00, ""2018-03-16T11:27:18+00:00"")
                            ],
                           [""user"",""id"", ""ts""])
df = df.withColumn('ts', df.ts.cast('timestamp'))
df.write.partitionBy(""user"").parquet(""/home/cnali/data/"")df2 = spark.read.load(""/home/cnali/data/"")df2.filter(""user=='usr2'"").explain(True)
{code}
{code:java}
// Spark 2.4 output
== Parsed Logical Plan ==
'Filter ('user = usr2)
+- Relation[id#38,ts#39,user#40] parquet== Analyzed Logical Plan ==
id: double, ts: timestamp, user: string
Filter (user#40 = usr2)
+- Relation[id#38,ts#39,user#40] parquet== Optimized Logical Plan ==
Filter (isnotnull(user#40) && (user#40 = usr2))
+- Relation[id#38,ts#39,user#40] parquet== Physical Plan ==
*(1) FileScan parquet [id#38,ts#39,user#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/cnali/data], PartitionCount: 1, PartitionFilters: [isnotnull(user#40), (user#40 = usr2)], PushedFilters: [], ReadSchema: struct<id:double,ts:timestamp>{code}
{code:java}
// Spark 3.0.0-preview output
== Parsed Logical Plan ==
'Filter ('user = usr2)
+- RelationV2[id#0, ts#1, user#2] parquet file:/home/cnali/data== Analyzed Logical Plan ==
id: double, ts: timestamp, user: string
Filter (user#2 = usr2)
+- RelationV2[id#0, ts#1, user#2] parquet file:/home/cnali/data== Optimized Logical Plan ==
Filter (isnotnull(user#2) AND (user#2 = usr2))
+- RelationV2[id#0, ts#1, user#2] parquet file:/home/cnali/data== Physical Plan ==
*(1) Project [id#0, ts#1, user#2]
+- *(1) Filter (isnotnull(user#2) AND (user#2 = usr2))
   +- *(1) ColumnarToRow
      +- BatchScan[id#0, ts#1, user#2] ParquetScan Location: InMemoryFileIndex[file:/home/cnali/data], ReadSchema: struct<id:double,ts:timestamp>
{code}
I have tested it on much larger dataset. Spark 3.0 tries to load whole data and then apply filter. Whereas Spark 2.4 push down the filter. Above output shows that Spark 2.4 applied partition filter but not the Spark 3.0 preview.

 

Minor: in Spark 3.0 ""explain()"" output is truncated (maybe fixed length?) and it's hard to debug.  spark.sql.orc.cache.stripe.details.size=10000 doesn't work.

 
{code:java}
// pyspark 3 shell output
$ pyspark
Python 3.6.8 (default, Aug  7 2019, 17:28:10) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Warning: Ignoring non-spark config property: java.io.dir=/md2k/data1,/md2k/data2,/md2k/data3,/md2k/data4,/md2k/data5,/md2k/data6,/md2k/data7,/md2k/data8
19/12/09 07:05:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
19/12/09 07:05:36 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview
      /_/Using Python version 3.6.8 (default, Aug  7 2019 17:28:10)
SparkSession available as 'spark'.
{code}
{code:java}
// pyspark 2.4.4 shell output
pyspark
Python 3.6.8 (default, Aug  7 2019, 17:28:10) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
2019-12-09 07:09:07 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0
      /_/Using Python version 3.6.8 (default, Aug  7 2019 17:28:10)
SparkSession available as 'spark'.

{code}
 

 ","pyspark 3.0 preview

Ubuntu/Centos

pyarrow 0.14.1 ",aman_omer,dongjoon,jonathak,nasirali,sandeep.katta2007,sunnys,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/20 03:03;nasirali;Screenshot from 2020-01-01 21-01-18.png;https://issues.apache.org/jira/secure/attachment/12989784/Screenshot+from+2020-01-01+21-01-18.png","02/Jan/20 03:03;nasirali;Screenshot from 2020-01-01 21-01-32.png;https://issues.apache.org/jira/secure/attachment/12989785/Screenshot+from+2020-01-01+21-01-32.png","21/Apr/20 20:28;dongjoon;partition_pruning.png;https://issues.apache.org/jira/secure/attachment/13000743/partition_pruning.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 20:31:38 UTC 2020,,,,,,,,,,"0|z09ec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Dec/19 05:10;toopt4;did u try on scala sparkshell?;;;","08/Dec/19 05:58;nasirali;No, I only use pyspark shell.;;;","09/Dec/19 04:58;aman_omer;Kindly share the results of spark-shell.
Thanks;;;","09/Dec/19 13:10;nasirali;[~aman_omer] added in my question;;;","12/Dec/19 16:34;dongjoon;Issue resolved by pull request 26857
[https://github.com/apache/spark/pull/26857];;;","02/Jan/20 03:02;nasirali;This issue has not been fixed. It shows pushed filters but it does not actually push filters. For example, look at 2.4.* output; there it clearly shows 'user' as partition filter but not in 3.* version.  Have a look at spark ui sql graph.

 

On a bigger dataset, spark 2.4.* takes less than a second to show one row. Whereas, spark 3.* takes way too long to produce the same result. Code is exactly same (load parquet, filter based on partition key user)

 ;;;","21/Apr/20 20:09;dongjoon;Hi, [~sowen].
Is there a reason for you to remove the fixed version? It's fixed since `3.0.0-preview2`, isn't it?

**Apache Spark 3.0.0-preview2**
{code}
>>> spark.range(10).write.mode(""overwrite"").parquet(""/tmp/foo"")
>>> spark.read.parquet(""/tmp/foo"").filter(""5 > id"").explain()
== Physical Plan ==
*(1) Project [id#3L]
+- *(1) Filter (isnotnull(id#3L) AND (5 > id#3L))
   +- *(1) ColumnarToRow
      +- BatchScan[id#3L] ParquetScan Location: InMemoryFileIndex[file:/tmp/foo], ReadSchema: struct<id:bigint>, PushedFilters: [IsNotNull(id), LessThan(id,5)]
{code}

**Apache Spark 3.0.0-RC1**
{code}
>>> sql(""set spark.sql.sources.useV1SourceList=''"")
DataFrame[key: string, value: string]
>>> spark.range(10).write.mode(""overwrite"").parquet(""/tmp/foo"")
>>> spark.read.parquet(""/tmp/foo"").filter(""5 > id"").explain()
== Physical Plan ==
*(1) Project [id#53L]
+- *(1) Filter (isnotnull(id#53L) AND (5 > id#53L))
   +- *(1) ColumnarToRow
      +- BatchScan[id#53L] ParquetScan DataFilters: [isnotnull(id#53L), (5 > id#53L)], Location: InMemoryFileIndex[file:/tmp/foo], PartitionFilters: [], ReadSchema: struct<id:bigint>, PushedFilters: [IsNotNull(id), LessThan(id,5)]
{code};;;","21/Apr/20 20:14;nasirali;[~dongjoon] This bug has not been fixed. Please have a look at my previous comment and uploaded screenshots. Yes you added logs to debug but it doesn't perform filtering on partition key yet.;;;","21/Apr/20 20:17;dongjoon;Could you try 3.0.0-RC1, [~nasirali]?
- https://dist.apache.org/repos/dist/dev/spark/v3.0.0-rc1-bin/;;;","21/Apr/20 20:29;dongjoon;In 3.0.0-RC1, I can see `number of partitions read: 1`.
!partition_pruning.png!;;;","21/Apr/20 20:31;dongjoon;Please feel free to reopen this if you are facing a bug with 3.0.0-RC1. For me, it looks correctly.;;;",,,,,,,,,,,,,
MLP param map missing,SPARK-30144,13272612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,cyborgdroid,cyborgdroid,05/Dec/19 23:18,22/Jan/20 21:07,13/Jul/23 08:49,03/Jan/20 18:01,2.4.4,,,,,,,,,,3.0.0,,,ML,,,,,0,release-notes,,,"Param maps for fitted classifiers are available with all classifiers except for the MultilayerPerceptronClassifier.
  
 There is no way to track or know what parameters were best during a crossvalidation or which parameters were used for submodels.
  
{code:java}
{
Param(parent='MultilayerPerceptronClassifier_eeab0cc242d1', name='featuresCol', doc='features column name'): 'features', 
Param(parent='MultilayerPerceptronClassifier_eeab0cc242d1', name='labelCol', doc='label column name'): 'fake_banknote', 
Param(parent='MultilayerPerceptronClassifier_eeab0cc242d1', name='predictionCol', doc='prediction column name'): 'prediction', 
Param(parent='MultilayerPerceptronClassifier_eeab0cc242d1', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', 
Param(parent='MultilayerPerceptronClassifier_eeab0cc242d1', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction'}{code}
 
 GBTClassifier for example shows all parameters:
  
{code:java}
  {
Param(parent='GBTClassifier_a0e77b3430aa', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False, 
Param(parent='GBTClassifier_a0e77b3430aa', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10, 
Param(parent='GBTClassifier_a0e77b3430aa', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'all', 
Param(parent='GBTClassifier_a0e77b3430aa', name='featuresCol', doc='features column name'): 'features', 
Param(parent='GBTClassifier_a0e77b3430aa', name='labelCol', doc='label column name'): 'fake_banknote', Param(parent='GBTClassifier_a0e77b3430aa', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic', 
Param(parent='GBTClassifier_a0e77b3430aa', name='maxBins', doc='Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.'): 8, 
Param(parent='GBTClassifier_a0e77b3430aa', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_a0e77b3430aa', name='maxIter', doc='maximum number of iterations (>= 0)'): 20, 
Param(parent='GBTClassifier_a0e77b3430aa', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256, 
Param(parent='GBTClassifier_a0e77b3430aa', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, 
Param(parent='GBTClassifier_a0e77b3430aa', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, 
Param(parent='GBTClassifier_a0e77b3430aa', name='predictionCol', doc='prediction column name'): 'prediction', 
Param(parent='GBTClassifier_a0e77b3430aa', name='seed', doc='random seed'): 1234, 
Param(parent='GBTClassifier_a0e77b3430aa', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, 
Param(parent='GBTClassifier_a0e77b3430aa', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}{code}
 
See attached ipynb or example notebook here:

[https://colab.research.google.com/drive/1lwSHioZKlLh96FhGkdYFe6FUuRfTcSxH]",,cyborgdroid,huaxingao,podongfeng,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/19 15:14;cyborgdroid;MLP_params_missing.ipynb;https://issues.apache.org/jira/secure/attachment/12987746/MLP_params_missing.ipynb","06/Dec/19 15:14;cyborgdroid;data_banknote_authentication.csv;https://issues.apache.org/jira/secure/attachment/12987747/data_banknote_authentication.csv",,,,2.0,,,,,,,,,,,,,,,,,,,,,"From 3.0, MultilayerPerceptronClassificationModel extends MultilayerPerceptronParams to expose the training params. As a result, 
layers in MultilayerPerceptronClassificationModel has been changed from Array[Int] to IntArrayParam. User should use MultilayerPerceptronClassificationModel.getLayers instead of MultilayerPerceptronClassificationModel.layers to retrieve the size of layers. ",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 18:01:36 UTC 2020,,,,,,,,,,"0|z09ckg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Dec/19 23:12;huaxingao;Currently, MultilayerPerceptronClassificationModel only has params featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol. 

[~viirya]  [~podongfeng]  Are there any reasons why MultilayerPerceptronClassificationModel doesn't extend MultilayerPerceptronParams? If not, I will make it extend MultilayerPerceptronParams.;;;","09/Dec/19 01:39;podongfeng;[~huaxingao]  It seems that MultilayerPerceptronClassificationModel should extend MultilayerPerceptronParams to expose the training params.;;;","09/Dec/19 02:33;viirya;Seems so. MultilayerPerceptronParams is missed.;;;","03/Jan/20 18:01;srowen;Issue resolved by pull request 26838
[https://github.com/apache/spark/pull/26838];;;",,,,,,,,,,,,,,,,,,,,
New auth engine does not keep client ID in TransportClient after auth,SPARK-30129,13272290,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,04/Dec/19 17:36,05/Dec/19 17:03,13/Jul/23 08:49,05/Dec/19 01:13,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"Found a little bug when working on a feature; when auth is on, it's expected that the {{TransportClient}} provides the authenticated ID of the client (generally the app ID), but the new auth engine is not setting that information.",,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 17:03:05 UTC 2019,,,,,,,,,,"0|z09akw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Dec/19 01:13;dongjoon;This is resolved via https://github.com/apache/spark/pull/26760;;;","05/Dec/19 17:03;dongjoon;This is backported to branch-2.4 via https://github.com/apache/spark/pull/26764;;;",,,,,,,,,,,,,,,,,,,,,,
sparkContext.addFile fails when file path contains spaces,SPARK-30126,13272259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ankitraj,Valendin,Valendin,04/Dec/19 14:29,13/Dec/19 15:45,13/Jul/23 08:49,12/Dec/19 12:32,2.4.3,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"When uploading a file to the spark context via the addFile function, an exception is thrown when file path contains a space character. Escaping the space with %20 or \\ or + doesn't change the result.

 

to reproduce: 

file_path = ""/home/user/test dir/config.conf""

sparkContext.addFile(file_path)

 

results in:

py4j.protocol.Py4JJavaError: An error occurred while calling o131.addFile.
: java.io.FileNotFoundException: File file:/home/user/test%20dir/config.conf does not exist",,Ankitraj,cloud_fan,Valendin,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30145,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 12:32:17 UTC 2019,,,,,,,,,,"0|z09ae0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Dec/19 17:10;Ankitraj;i will check this issue;;;","12/Dec/19 12:32;cloud_fan;Issue resolved by pull request 26773
[https://github.com/apache/spark/pull/26773];;;",,,,,,,,,,,,,,,,,,,,,,
Fix memory usage in sbt build script,SPARK-30121,13272188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,Qin Yao,Qin Yao,Qin Yao,04/Dec/19 11:22,05/Dec/19 17:51,13/Jul/23 08:49,05/Dec/19 17:51,2.4.4,3.0.0,,,,,,,,,3.0.0,,,Build,,,,,0,,,,"1. the default memory setting is missing in usage instructions 

{code:java}
```
build/sbt -h
```

```
-mem    <integer>  set memory options (default: , which is -Xms2048m -Xmx2048m -XX:ReservedCodeCacheSize=256m)
```

{code}
2. the Perm space is not needed anymore, since java7 is removed.",,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 17:51:31 UTC 2019,,,,,,,,,,"0|z099y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Dec/19 17:51;srowen;Issue resolved by pull request 26757
[https://github.com/apache/spark/pull/26757];;;",,,,,,,,,,,,,,,,,,,,,,,
spark R dockerfile fails to build,SPARK-30111,13272027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ifilonenko,shaneknapp,shaneknapp,03/Dec/19 18:20,17/May/20 18:25,13/Jul/23 08:49,04/Dec/19 02:00,3.0.0,,,,,,,,,,3.0.0,,,Build,jenkins,Kubernetes,Spark Core,,0,,,,"all recent k8s builds have been failing when trying to build the sparkR dockerfile:

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/19565/console]

[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-k8s/426/console|https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-k8s/]

[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-k8s-jdk11/76/console|https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-k8s-jdk11/]

[~ifilonenko]",,ifilonenko,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 00:05:52 UTC 2019,,,,,,,,,,"0|z098yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Dec/19 00:05;ifilonenko;The error seems to be from:

{code:yaml}
Step 6/12 : RUN apt install -y python python-pip &&     apt install -y python3 python3-pip &&     rm -r /usr/lib/python*/ensurepip &&     pip install --upgrade pip setuptools &&     rm -r /root/.cache && rm -rf /var/cache/apt/*
 ---> Running in f3d520c3435b
{code}

so this is relating to pyspark dockerfile
The error is because:  *404  Not Found [IP: 151.101.188.204 80]* ;;;",,,,,,,,,,,,,,,,,,,,,,,
global temp db name can be used as a table name under v2 catalog,SPARK-30104,13271816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,02/Dec/19 23:00,16/Dec/19 09:43,13/Jul/23 08:49,11/Dec/19 08:57,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Currently, 'global_temp' can be used in certain commands (CREATE) but not in others (DESCRIBE) because catalog look up logic only considers the first element of the multi-part name and always uses the session catalog if it is set to 'global_temp'.

For example:
{code:java}
// Assume ""spark.sql.globalTempDatabase"" is set to ""global_temp"".
sql(s""CREATE TABLE testcat.t (id bigint, data string) USING foo"")
sql(s""CREATE TABLE testcat.global_temp (id bigint, data string) USING foo"")
sql(""USE testcat"")

sql(s""DESCRIBE TABLE t"").show
+---------------+---------+-------+
|       col_name|data_type|comment|
+---------------+---------+-------+
|             id|   bigint|       |
|           data|   string|       |
|               |         |       |
| # Partitioning|         |       |
|Not partitioned|         |       |
+---------------+---------+-------+

sql(s""DESCRIBE TABLE global_temp"").show
org.apache.spark.sql.AnalysisException: Table not found: global_temp;;
  'DescribeTable 'UnresolvedV2Relation [global_temp], org.apache.spark.sql.connector.InMemoryTableSessionCatalog@2f1af64f, `global_temp`, false
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:47)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:46)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)
{code}",,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 11 08:57:31 UTC 2019,,,,,,,,,,"0|z097o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Dec/19 08:57;cloud_fan;Issue resolved by pull request 26741
[https://github.com/apache/spark/pull/26741];;;",,,,,,,,,,,,,,,,,,,,,,,
Improve Analyzed Logical Plan as duplicate AnalysisExceptions are coming,SPARK-30099,13271650,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,aman_omer,jobitmathew,jobitmathew,02/Dec/19 09:06,06/Dec/19 07:41,13/Jul/23 08:49,04/Dec/19 05:53,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Spark SQL 
 explain extended select * from any non existing table shows duplicate AnalysisExceptions.
{code:java}
 spark-sql>explain extended select * from wrong

== Parsed Logical Plan ==
 'Project [*]
 +- 'UnresolvedRelation `wrong`

== Analyzed Logical Plan ==
 org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 p
 os 31
 org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 p
 os 31
 == Optimized Logical Plan ==
 org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 p
 os 31
 == Physical Plan ==
 org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 p
 os 31
 Time taken: 6.0 seconds, Fetched 1 row(s)
 19/12/02 14:33:32 INFO SparkSQLCLIDriver: Time taken: 6.0 seconds, Fetched 1 row
 (s)
 spark-sql>
{code}",,aman_omer,cloud_fan,jobitmathew,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 14:08:36 UTC 2019,,,,,,,,,,"0|z096n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Dec/19 05:53;cloud_fan;Issue resolved by pull request 26734
[https://github.com/apache/spark/pull/26734];;;","05/Dec/19 09:14;aman_omer;[~cloud_fan] can you assign this jira ticket to me?
id: aman_omer;;;","05/Dec/19 14:08;cloud_fan;ah sorry I made a mistake. Fixed now.;;;",,,,,,,,,,,,,,,,,,,,,
Current namespace is not used during table resolution,SPARK-30094,13271607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,02/Dec/19 03:46,17/Dec/19 03:15,13/Jul/23 08:49,17/Dec/19 03:15,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"The following example shows the scenario where the current namespace is not respected:
{code:java}
sql(""CREATE TABLE testcat.t USING foo AS SELECT 1 AS id"")
sql(""USE testcat"")
sql(""SHOW CURRENT NAMESPACE"").show
+-------+---------+
|catalog|namespace|
+-------+---------+
|testcat|         |
+-------+---------+

// `t` is resolved to `testcat.t`.
sql(""DESCRIBE t"").show
+---------------+---------+-------+
|       col_name|data_type|comment|
+---------------+---------+-------+
|             id|      int|       |
|               |         |       |
| # Partitioning|         |       |
|Not partitioned|         |       |
+---------------+---------+-------+

// Now create a table under `ns` namespace.
sql(""CREATE TABLE testcat.ns.t USING foo AS SELECT 1 AS id"")
sql(""USE testcat.ns"")
sql(""SHOW CURRENT NAMESPACE"").show
+-------+---------+
|catalog|namespace|
+-------+---------+
|testcat|       ns|
+-------+---------+

// `t` is not resolved any longer since the current namespace `ns` is not used.
sql(""DESCRIBE t"").show
org.apache.spark.sql.AnalysisException: Invalid command: 't' is a view not a table.; line 1 pos 0;
'DescribeTable 'UnresolvedV2Relation [t], org.apache.spark.sql.connector.InMemoryTableCatalog@2c5ead80, `t`, false

{code}",,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 03:15:19 UTC 2019,,,,,,,,,,"0|z096dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/19 03:54;imback82;[~cloud_fan] this seems to be an unexpected behavior right?;;;","17/Dec/19 03:15;cloud_fan;Issue resolved by pull request 26894
[https://github.com/apache/spark/pull/26894];;;",,,,,,,,,,,,,,,,,,,,,,
Improve error message for creating views,SPARK-30093,13271593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aman_omer,aman_omer,aman_omer,02/Dec/19 03:02,05/Dec/19 07:29,13/Jul/23 08:49,05/Dec/19 07:29,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Improve error message for creating views.

https://github.com/apache/spark/pull/26317#discussion_r352377363",,aman_omer,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 07:29:05 UTC 2019,,,,,,,,,,"0|z096ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Dec/19 07:29;cloud_fan;Issue resolved by pull request 26731
[https://github.com/apache/spark/pull/26731];;;",,,,,,,,,,,,,,,,,,,,,,,
visitArithmeticUnary should wrap PLUS case with UnaryPositive for type checking,SPARK-30083,13271458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,29/Nov/19 18:23,03/Dec/19 19:21,13/Jul/23 08:49,03/Dec/19 15:45,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"For PLUS case, visitArithmeticUnary do not wrap the expr with UnaryPositive, so it escapes from type checking",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 03 15:45:11 UTC 2019,,,,,,,,,,"0|z095gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Dec/19 15:45;cloud_fan;Issue resolved by pull request 26716
[https://github.com/apache/spark/pull/26716];;;",,,,,,,,,,,,,,,,,,,,,,,
Zeros are being treated as NaNs,SPARK-30082,13271409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jayad,jayad,jayad,29/Nov/19 13:19,12/Dec/22 18:11,13/Jul/23 08:49,03/Dec/19 16:07,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,,,,,,2.4.5,3.0.0,,SQL,,,,,0,correctness,,,"If you attempt to run
{code:java}
df = df.replace(float('nan'), somethingToReplaceWith)
{code}
It will replace all {{0}} s in columns of type {{Integer}}

Example code snippet to repro this:
{code:java}
from pyspark.sql import SQLContext
spark = SQLContext(sc).sparkSession
df = spark.createDataFrame([(1, 0), (2, 3), (3, 0)], (""index"", ""value""))
df.show()
df = df.replace(float('nan'), 5)
df.show()
{code}
Here's the output I get when I run this code:
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 3.7.5 (default, Nov  1 2019 02:16:32)
SparkSession available as 'spark'.
>>> from pyspark.sql import SQLContext
>>> spark = SQLContext(sc).sparkSession
>>> df = spark.createDataFrame([(1, 0), (2, 3), (3, 0)], (""index"", ""value""))
>>> df.show()
+-----+-----+
|index|value|
+-----+-----+
|    1|    0|
|    2|    3|
|    3|    0|
+-----+-----+

>>> df = df.replace(float('nan'), 5)
>>> df.show()
+-----+-----+
|index|value|
+-----+-----+
|    1|    5|
|    2|    3|
|    3|    5|
+-----+-----+

>>>
{code}",,cloud_fan,jayad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 03 16:07:40 UTC 2019,,,,,,,,,,"0|z0955k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Nov/19 16:59;jayad;Just thought i'd update on this, the {{replace}} function seems to be, correctly, replacing {{NaNs. Here's a better example that also demonstrates that the problem is limited to columns of type Integer}}:
{code:java}
>>> df = spark.createDataFrame([(1.0, 0), (0.0, 3), (float('nan'), 0)], (""index"", ""value""))
>>> df.show()
+-----+-----+
|index|value|
+-----+-----+
|  1.0|    0|
|  0.0|    3|
|  NaN|    0|
+-----+-----+
>>> df.replace(float('nan'), 2).show()
+-----+-----+
|index|value|
+-----+-----+
|  1.0|    2|
|  0.0|    3|
|  2.0|    2|
+-----+-----+ {code};;;","02/Dec/19 02:15;gurwls223;[~jayad] are you interested in submitting a PR to fix this?;;;","02/Dec/19 13:07;jayad;[~hyukjin.kwon] already working on a PR;;;","03/Dec/19 16:07;cloud_fan;Issue resolved by pull request 26738
[https://github.com/apache/spark/pull/26738];;;",,,,,,,,,,,,,,,,,,,,
FlatMapGroupsWithStateSuite failure (big-endian),SPARK-30078,13271342,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,salamani,salamani,29/Nov/19 08:49,27/Mar/20 11:03,13/Jul/23 08:49,27/Mar/20 11:00,2.4.4,,,,,,,,,,,,,SQL,Tests,,,,0,big-endian,,,"I have built Apache Spark v2.4.4 on Big Endian Platform with AdoptJDK OpenJ9 1.8.0_202.

My build is successful. However while running the scala tests of ""Spark Project SQL"" module, I am facing failures at with FlatMapGroupsWithStateSuite, Error Log as attached.",,salamani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/19 08:50;salamani;FlatMapGroupsWithStateSuite.txt;https://issues.apache.org/jira/secure/attachment/12987131/FlatMapGroupsWithStateSuite.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 11:00:16 UTC 2020,,,,,,,,,,"0|z094qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Mar/20 11:00;salamani;Passed on rerun in new system. ;;;",,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexType doesn't implement hashCode correctly,SPARK-30075,13271320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,29/Nov/19 06:46,02/Jan/20 07:03,13/Jul/23 08:49,02/Dec/19 15:06,2.4.4,3.0.0,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"[https://github.com/apache/spark/blob/master/common/kvstore/src/test/java/org/apache/spark/util/kvstore/ArrayKeyIndexType.java]
{code:java}
public class ArrayKeyIndexType {
  @KVIndex
  public int[] key;

  @KVIndex(""id"")
  public String[] id;  

  @Override
  public boolean equals(Object o) {
    if (o instanceof ArrayKeyIndexType) {
      ArrayKeyIndexType other = (ArrayKeyIndexType) o;
      return Arrays.equals(key, other.key) && Arrays.equals(id, other.id);
    }
    return false;
  }

  @Override
  public int hashCode() {
    return key.hashCode();
  }
} {code}
Here hashCode() simply calls key.hashCode() which won't work as our intention - as the implementation would be Object.hashCode(). We need to call Array.hashCode(key) instead.",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30405,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 02 15:06:56 UTC 2019,,,,,,,,,,"0|z094ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Nov/19 06:48;kabhwan;Working on the fix. Actually the fix is also available in [https://github.com/apache/spark/pull/25811|https://github.com/apache/spark/pull/25811/files] , but the review progress doesn't go fast enough, so I'm splitting out this fix from the PR.;;;","02/Dec/19 15:06;srowen;Issue resolved by pull request 26709
[https://github.com/apache/spark/pull/26709];;;",,,,,,,,,,,,,,,,,,,,,,
The maxNumPostShufflePartitions config should obey reducePostShufflePartitions enabled,SPARK-30074,13271318,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,29/Nov/19 05:44,02/Dec/19 09:43,13/Jul/23 08:49,02/Dec/19 04:39,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"The config ""spark.sql.adaptive.shuffle.maxNumPostShufflePartitions"" should be controlled by not only ""spark.sql.adaptive.enabled"", but also ""spark.sql.adaptive.shuffle.reducePostShufflePartitions.enabled"", that keeps the same behavior of minNumPostShufflePartitions and targetPostShuffleInputSize.",,cloud_fan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 02 04:39:58 UTC 2019,,,,,,,,,,"0|z094lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/19 04:39;cloud_fan;Issue resolved by pull request 26664
[https://github.com/apache/spark/pull/26664];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix fragment offset comparison in getBlockHosts,SPARK-30067,13271167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,madianjun,madianjun,madianjun,28/Nov/19 09:52,06/Dec/19 07:46,13/Jul/23 08:49,06/Dec/19 07:41,2.4.4,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"There is a bug in the getBlockHosts() function. In the case ""The fragment ends at a position within this block"", the end of fragment should be before the end of block，where the ""end of block"" means {{b.getOffset + b.getLength}}，not {{b.getLength}}.",,dongjoon,madianjun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 07:41:03 UTC 2019,,,,,,,,,,"0|z093ns:",9223372036854775807,,,,,srowen,,,,,,,,,,,,,,,,,"28/Nov/19 10:00;madianjun;[~srowen]  This is the PR: 

[https://github.com/apache/spark/pull/26650];;;","28/Nov/19 12:13;srowen;Yep, thanks. I think it's at least a Minor bug; do we have any info on the effect it might have? I don't think anything might actually break, but might choose a suboptimal host.;;;","06/Dec/19 07:41;dongjoon;Issue resolved by pull request 26650
[https://github.com/apache/spark/pull/26650];;;",,,,,,,,,,,,,,,,,,,,,
Unable to drop na with duplicate columns,SPARK-30065,13271120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,28/Nov/19 00:29,08/Jun/20 16:14,13/Jul/23 08:49,02/Dec/19 04:28,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"Trying to drop rows with null values fails even when no columns are specified. This should be allowed:


{code:java}
scala> val left = Seq((""1"", null), (""3"", ""4"")).toDF(""col1"", ""col2"")
left: org.apache.spark.sql.DataFrame = [col1: string, col2: string]

scala> val right = Seq((""1"", ""2""), (""3"", null)).toDF(""col1"", ""col2"")
right: org.apache.spark.sql.DataFrame = [col1: string, col2: string]

scala> val df = left.join(right, Seq(""col1""))
df: org.apache.spark.sql.DataFrame = [col1: string, col2: string ... 1 more field]

scala> df.show
+----+----+----+
|col1|col2|col2|
+----+----+----+
|   1|null|   2|
|   3|   4|null|
+----+----+----+


scala> df.na.drop(""any"")
org.apache.spark.sql.AnalysisException: Reference 'col2' is ambiguous, could be: col2, col2.;
  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:240)
{code}
",,cloud_fan,dongjoon,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29890,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 31 07:03:21 UTC 2020,,,,,,,,,,"0|z093dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Nov/19 00:29;imback82;I am working on this.;;;","02/Dec/19 04:28;cloud_fan;Issue resolved by pull request 26700
[https://github.com/apache/spark/pull/26700];;;","31/Jan/20 07:03;dongjoon;This is backported to `branch-2.4` via [https://github.com/apache/spark/pull/27411] .;;;",,,,,,,,,,,,,,,,,,,,,
Add IMMEDIATE statement to the DB2 dialect truncate implementation,SPARK-30062,13270973,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ikarol,huineng,huineng,27/Nov/19 12:19,12/Dec/22 18:11,13/Jul/23 08:49,26/Jan/22 03:22,2.4.4,,,,,,,,,,3.2.2,3.3.0,,PySpark,,,,,1,db2,pyspark,,"using DB2Driver using mode(""overwrite"") option(""truncate"",True) gives sql error

 
{code:java}
dfClient.write\
 .format(""jdbc"")\
 .mode(""overwrite"")\
 .option('driver', 'com.ibm.db2.jcc.DB2Driver')\
 .option(""url"",""jdbc:db2://xxxx"")\
 .option(""user"",""xxx"")\
 .option(""password"",""xxxx"")\
 .option(""dbtable"",""xxxx"")\
 .option(""truncate"",True)\{code}
 

 gives the error below

in summary i belief the semicolon is misplaced or malformated

 
{code:java}
EXPO.EXPO#CMR_STG;IMMEDIATE{code}
 

 

full error
{code:java}
An error occurred while calling o47.save. : com.ibm.db2.jcc.am.SqlSyntaxErrorException: DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=END-OF-STATEMENT;LE EXPO.EXPO#CMR_STG;IMMEDIATE, DRIVER=4.19.77 at com.ibm.db2.jcc.am.b4.a(b4.java:747) at com.ibm.db2.jcc.am.b4.a(b4.java:66) at com.ibm.db2.jcc.am.b4.a(b4.java:135) at com.ibm.db2.jcc.am.kh.c(kh.java:2788) at com.ibm.db2.jcc.am.kh.d(kh.java:2776) at com.ibm.db2.jcc.am.kh.b(kh.java:2143) at com.ibm.db2.jcc.t4.ab.i(ab.java:226) at com.ibm.db2.jcc.t4.ab.c(ab.java:48) at com.ibm.db2.jcc.t4.p.b(p.java:38) at com.ibm.db2.jcc.t4.av.h(av.java:124) at com.ibm.db2.jcc.am.kh.ak(kh.java:2138) at com.ibm.db2.jcc.am.kh.a(kh.java:3325) at com.ibm.db2.jcc.am.kh.c(kh.java:765) at com.ibm.db2.jcc.am.kh.executeUpdate(kh.java:744) at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.truncateTable(JdbcUtils.scala:113) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:56) at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80) at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676) at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676) at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73) at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676) at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748){code}
 ",,apachespark,huineng,jason.depp.dev@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 22 20:03:24 UTC 2022,,,,,,,,,,"0|z092go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/19 02:23;gurwls223;The error message indicate it's from DB2;;;","02/Dec/19 12:12;huineng;why would it be a db2 issue. DB2 requires since version 9.7 the key word IMMEDIATE, so spark jdbc doesn't support this new keyword

the standard jdbc truncate statement is 

 
{code:java}
@Since(""2.4.0"")
 def getTruncateQuery(
 table: String,
 cascade: Option[Boolean] = isCascadingTruncateTable): String = {
 s""TRUNCATE TABLE $table""
 }{code}
 

 

[https://github.com/apache/spark/blob/eb037a8180be4ab7570eda1fa9cbf3c84b92c3f7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala#L137]

 

so it's a matter of JdbcDialects that needs to be adapted for db2 either completely add the immediate or add it via an option for example like it's done for progress

 

 
{code:java}
override def getTruncateQuery(
 table: String,
 cascade: Option[Boolean] = isCascadingTruncateTable): String = {
 cascade match {
 case Some(true) => s""TRUNCATE TABLE ONLY $table CASCADE""
 case _ => s""TRUNCATE TABLE ONLY $table""
 }
 }
{code}
 

Anyway i think it can only be handled in spark sql

thanks;;;","04/Dec/19 01:56;gurwls223;Can you open a PR to fix DB2 dialect then?;;;","04/Dec/19 02:08;huineng;ok;;;","10/Jun/20 06:23;jason.depp.dev@gmail.com;I've ran into the same issue under Scala + Spark2.4.5;;;","22/Jan/22 20:02;apachespark;User 'ikarol' has created a pull request for this issue:
https://github.com/apache/spark/pull/35283;;;","22/Jan/22 20:03;apachespark;User 'ikarol' has created a pull request for this issue:
https://github.com/apache/spark/pull/35283;;;",,,,,,,,,,,,,,,,,
analyze table and rename table should not erase the bucketing metadata at hive side,SPARK-30050,13270813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,26/Nov/19 21:43,02/Dec/19 06:01,13/Jul/23 08:49,02/Dec/19 05:50,3.0.0,,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,Analyze table and rename table command will erase existing bucketing info of a Hive table.,,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 02 05:50:57 UTC 2019,,,,,,,,,,"0|z091h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/19 05:50;cloud_fan;Issue resolved by pull request 26685
[https://github.com/apache/spark/pull/26685];;;",,,,,,,,,,,,,,,,,,,,,,,
SQL fails to parse when comment contains an unmatched quote character,SPARK-30049,13270806,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,javier_ivanov,jlowe,jlowe,26/Nov/19 20:49,12/Dec/22 18:11,13/Jul/23 08:49,03/Mar/20 15:57,3.0.0,,,,,,,,,,3.0.0,3.1.0,,SQL,,,,,0,,,,"A SQL statement that contains a comment with an unmatched quote character can lead to a parse error.  These queries parsed correctly in older versions of Spark.  For example, here's an excerpt from an interactive spark-sql session on a recent Spark-3.0.0-SNAPSHOT build (commit e23c135e568d4401a5659bc1b5ae8fc8bf147693):
{noformat}
spark-sql> SELECT 1 -- someone's comment here
         > ;
Error in query: 
extraneous input ';' expecting <EOF>(line 2, pos 0)

== SQL ==
SELECT 1 -- someone's comment here
;
^^^
{noformat}
",,javier_ivanov,jlowe,oleg_bonar,tgraves,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31102,,SPARK-26321,,,,,,,,,,,,"18/Dec/19 12:41;javier_ivanov;Screen Shot 2019-12-18 at 9.26.29 AM.png;https://issues.apache.org/jira/secure/attachment/12989102/Screen+Shot+2019-12-18+at+9.26.29+AM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 07:33:09 UTC 2020,,,,,,,,,,"0|z091fk:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"26/Nov/19 20:52;jlowe;Interestingly, if the comment appears on its own line then the query parses correctly.  For example, this query works:
{noformat}
SELECT 1
-- someone's comment here
;
{noformat};;;","02/Dec/19 02:27;gurwls223;Please avoid to set Target version which is reserved for committers.;;;","02/Dec/19 02:28;gurwls223;Do other shells in other DBMSes support such cases as well?;;;","02/Dec/19 14:59;jlowe;I just tested SQLite, and it supports it:
{noformat}
sqlite> SELECT 1 -- someone's comment here
   ...> ;
1
{noformat}

It's telling that the apostrophe character within the comment decides whether Spark parses it properly or not.  I don't recall apostrophe or quote characters within SQL comments being significant, so this seems to be a bug in the Spark SQL parser.
;;;","16/Dec/19 14:48;oleg_bonar;I would like to investigate this issue.;;;","17/Dec/19 15:37;jlowe;Found some time to track this down to the following commit which first regressed the parsing behavior:
{noformat}
commit 148cd26799c69ab9cfdc2b3b8000a194c12518b8 (HEAD, refs/bisect/bad)
Author: Yuming Wang <yumwang@ebay.com>
Date:   Sat Oct 12 22:21:14 2019 -0700

    [SPARK-26321][SQL] Port HIVE-15297: Hive should not split semicolon within quoted string literals
    
    ## What changes were proposed in this pull request?
    
    This pr port [HIVE-15297](https://issues.apache.org/jira/browse/HIVE-15297) to fix **spark-sql** should not split semicolon within quoted string literals.
    
    ## How was this patch tested?
    unit tests and manual tests:
    ![image](https://user-images.githubusercontent.com/5399861/60395592-5666ea00-9b68-11e9-99dc-0e8ea98de32b.png)
    
    Closes #25018 from wangyum/SPARK-26321.
    
    Authored-by: Yuming Wang <yumwang@ebay.com>
    Signed-off-by: Yuming Wang <wgyumg@gmail.com>
{noformat}

[~yumwang] would you mind taking a look?;;;","18/Dec/19 03:24;yumwang;Thank you [~jlowe]. Hive also has this issue:
{noformat}
LM-SHC-16502798:apache-hive-2.3.6-bin yumwang$ bin/hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/yumwang/Downloads/apache-hive-2.3.6-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/yumwang/software/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/Users/yumwang/Downloads/apache-hive-2.3.6-bin/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> SELECT 1 -- someone's comment here
    > ;
NoViableAltException(350@[319:1: constant : ( ( intervalLiteral )=> intervalLiteral | Number | dateLiteral | timestampLiteral | StringLiteral | stringLiteralSequence | IntegralLiteral | NumberLiteral | charSetStringLiteral | booleanValue | KW_NULL -> TOK_NULL );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:116)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.constant(HiveParser_IdentifiersParser.java:5267)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6755)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6988)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:7324)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:7380)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7542)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7685)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7828)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceConcatenateExpression(HiveParser_IdentifiersParser.java:7967)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:8177)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:8314)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceSimilarExpressionMain(HiveParser_IdentifiersParser.java:8801)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceSimilarExpression(HiveParser_IdentifiersParser.java:8697)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:9537)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9703)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9812)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9953)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6686)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:41850)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectItem(HiveParser_SelectClauseParser.java:1662)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectList(HiveParser_SelectClauseParser.java:1155)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:950)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:41988)
	at org.apache.hadoop.hive.ql.parse.HiveParser.atomSelectStatement(HiveParser.java:36720)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:36987)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:36633)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:35822)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:35710)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2284)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 2:0 cannot recognize input near '1' ';' '<EOF>' in constant
{noformat}
;;;","18/Dec/19 08:00;yumwang;[~oleg_bonar] Any progress?;;;","18/Dec/19 09:21;oleg_bonar;Hi [~yumwang]! Investigating SparkSQLCLIDriver and SparkSQLDriver classes. Also have made a test method to test the failure. Any help is appreciated.;;;","18/Dec/19 12:39;javier_ivanov;That issue seems to be fixed in SPARK-30295 [~yumwang] 

!Screen Shot 2019-12-18 at 9.26.29 AM.png!;;;","08/Jan/20 14:17;tgraves;that Pr doesn't seem to be in yet. I'm not sure the context on that.  if that doesn't look like it is going to go in we should fix this another way.

Do you know what part of that pr may have fixed it?

[~oleg_bonar] are you still working on this?;;;","21/Jan/20 16:59;javier_ivanov;I have raised a PR with a simple fix and unit tests. Let me know if it looks good or not. ;;;","23/Jan/20 07:33;oleg_bonar;[~tgraves], no, i'm not.

 ;;;",,,,,,,,,,,
Use RegexChecker instead of TokenChecker to check `org.apache.commons.lang.`,SPARK-30030,13270531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Nov/19 19:48,26/Nov/19 16:13,13/Jul/23 08:49,25/Nov/19 20:03,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Project Infra,,,,,0,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 25 20:03:39 UTC 2019,,,,,,,,,,"0|z08zqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Nov/19 20:03;dongjoon;Issue resolved by pull request 26666
[https://github.com/apache/spark/pull/26666];;;",,,,,,,,,,,,,,,,,,,,,,,
Continuous shuffle block fetching should be disabled by default when the old fetch protocol is used,SPARK-30025,13270426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,25/Nov/19 11:59,02/Dec/19 14:34,13/Jul/23 08:49,02/Dec/19 08:00,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"The new feature of continuous shuffle block fetching in SPARK-9853 also depends on the latest version of shuffle fetch protocol, we should keep this constraint in `BlockStoreShuffleReader.fetchContinuousBlocksInBatch`.",,cloud_fan,sandeep.katta2007,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 02 08:00:01 UTC 2019,,,,,,,,,,"0|z08z34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Dec/19 08:00;cloud_fan;Issue resolved by pull request 26663
[https://github.com/apache/spark/pull/26663];;;",,,,,,,,,,,,,,,,,,,,,,,
"the dataType of collect_list and collect_set aggregate functions should be ArrayType(_, false)",SPARK-30008,13270316,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Nov/19 15:46,27/Nov/19 04:49,13/Jul/23 08:49,27/Nov/19 04:40,2.3.4,2.4.4,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"
{code:java}
// Do not allow null values. We follow the semantics of Hive's collect_list/collect_set here.
// See: org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator
{code}

These two functions do not allow null values as they are defined, so their elements should not contain null.

Casting collect_list(a) to ArrayType(_, false) will fail.
",,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 04:40:49 UTC 2019,,,,,,,,,,"0|z08yeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Nov/19 04:40;dongjoon;Issue resolved by pull request 26651
[https://github.com/apache/spark/pull/26651];;;",,,,,,,,,,,,,,,,,,,,,,,
can't lookup v1 tables whose names specify the session catalog,SPARK-30001,13270081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,cloud_fan,cloud_fan,22/Nov/19 12:14,17/Dec/19 04:44,13/Jul/23 08:49,06/Dec/19 07:46,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"A simple way to reproduce it
{code}
scala> sql(""create table t using hive as select 1 as i"")
res2: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
|  i|
+---+
|  1|
+---+

scala> sql(""select * from spark_catalog.t"").show
org.apache.spark.sql.AnalysisException: Table or view not found: spark_catalog.t; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [spark_catalog, t]
{code}

The reason is that, we first go into `ResolveTables`, which lookups the table successfully, but then give up because it's a v1 table. Next we go into `ResolveRelations`, which do not recognize catalog name at all.

Similar to https://issues.apache.org/jira/browse/SPARK-29966 , we should make `ResolveRelations` responsible for lookup both v1 and v2 tables from the session catalog, and correctly recognize catalog name.",,cloud_fan,imback82,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 07:46:04 UTC 2019,,,,,,,,,,"0|z08wyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Nov/19 12:16;cloud_fan;cc [~rdblue]  [~imback82];;;","06/Dec/19 07:46;cloud_fan;Issue resolved by pull request 26684
[https://github.com/apache/spark/pull/26684];;;",,,,,,,,,,,,,,,,,,,,,,
Writing empty partition via DSv2 implementation of FileStreamSink throws FileNotFoundException,SPARK-29999,13270070,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kabhwan,kabhwan,kabhwan,22/Nov/19 11:16,26/Jun/20 07:23,13/Jul/23 08:49,24/Nov/19 23:31,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"I've discovered this issue while playing with Spark 3.0.0-preview.

Unlike DSv1 version of file sink, DSv2 version of file sink lazily creates output file which leads empty partition to not actually create output file. The behavior itself is not a problem, but while committing task in ManifestFileCommitProtocol, `fs.getFileStatus` throws FileNotFoundException since the file is not created.

 ",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 07:23:48 UTC 2020,,,,,,,,,,"0|z08ww0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Nov/19 11:16;kabhwan;Will submit a patch soon.;;;","24/Nov/19 23:31;dongjoon;Issue resolved by pull request 26639
[https://github.com/apache/spark/pull/26639];;;","26/Jun/20 07:23;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28930;;;","26/Jun/20 07:23;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28930;;;",,,,,,,,,,,,,,,,,,,,
Use nano time to calculate 'processedRowsPerSecond' to avoid 'NaN'/'Infinity',SPARK-29973,13269600,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,20/Nov/19 14:11,25/Nov/19 01:52,13/Jul/23 08:49,24/Nov/19 14:08,3.0.0,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"The {{""processingTimeSec""}} of batch may be less than 1 millis.  As {{""processingTimeSec""}} is calculated in millis, so {{""processingTimeSec""}} equals 0L. If there is no data in this batch, the {{""processedRowsPerSecond""}} equals {{""0/0.0d""}}, i.e. {{""Double.NaN""}}. If there are some data in this batch, the {{""processedRowsPerSecond""}} equals {{""N/0.0d""}}, i.e. {{""Double.Infinity""}}.",,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 24 14:08:37 UTC 2019,,,,,,,,,,"0|z08tzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Nov/19 14:08;srowen;Issue resolved by pull request 26610
[https://github.com/apache/spark/pull/26610];;;",,,,,,,,,,,,,,,,,,,,,,,
Multiple possible buffer leaks in TransportFrameDecoder and TransportCipher,SPARK-29971,13269535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,norman,norman,norman,20/Nov/19 08:42,25/Nov/19 18:15,13/Jul/23 08:49,22/Nov/19 23:27,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,TransportFrameDecoder and TransportCipher currently not carefully manage the life-cycle of ByteBuf instances and so leak memory in some cases.,,beettlle,norman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/spark/pull/26609,,Patch,,,,,,,,9223372036854775807,,,Wed Nov 20 08:46:05 UTC 2019,,,,,,,,,,"0|z08tl4:",9223372036854775807,,,,,dtsai,,,,,,,,,,,,,,,,,"20/Nov/19 08:45;norman;PR for a fix is here:

https://github.com/apache/spark/pull/26609

The issue was first reported in netty itself:
https://github.com/netty/netty/issues/9784;;;","20/Nov/19 08:46;norman;I am pretty sure the same bug exists in all versions that use netty so you may need to adjust the ""affected versions"";;;",,,,,,,,,,,,,,,,,,,,,,
open/close state is not preserved for Timelineview,SPARK-29970,13269527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,20/Nov/19 08:04,24/Nov/19 18:32,13/Jul/23 08:49,24/Nov/19 00:16,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,,,,,,2.4.5,3.0.0,,Web UI,,,,,0,,,,"open/close state for Timelineview is intended to be preserved as well as any other UI features like DAG Vis but it's not actually preserved. So if we go back to All Jobs page/Job page/Stage page from another page, Timelineview is always closed.",,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 24 00:16:45 UTC 2019,,,,,,,,,,"0|z08tjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Nov/19 00:16;dongjoon;Issue resolved by pull request 26607
[https://github.com/apache/spark/pull/26607];;;",,,,,,,,,,,,,,,,,,,,,,,
File stream source cleanup options may break a file sink output,SPARK-29953,13269195,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,zsxwing,zsxwing,18/Nov/19 23:14,06/Dec/19 05:55,13/Jul/23 08:49,06/Dec/19 05:48,3.0.0,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"SPARK-20568 added options to file streaming source to clean up processed files. However, when applying these options to a directory that was written by a file streaming sink, it will make the directory not queryable any more because we delete files from the directory but they are still tracked by file sink logs.

I think we should block the options if the input source is a file streaming sink path (has ""_spark_metadata"" folder).",,kabhwan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20568,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 04:31:40 UTC 2019,,,,,,,,,,"0|z08rhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/19 23:19;zsxwing;cc [~kabhwan];;;","19/Nov/19 00:19;kabhwan;Thanks for pinging me, [~zsxwing]. Totally makes sense.

Btw, I thought about the opposite shortly (removing files in `_spark_metadata`), but looks like that's really tricky as other queries have been reading the files, as well as it's only possible to remove entities in compacted file for metadata. Even this can be done technically, it would change the query result in any way, so doesn't feel safe to do it.

I'll craft the patch and ask for reviewing. Thanks again!;;;","19/Nov/19 04:31;kabhwan;[~zsxwing] Just raised the PR https://github.com/apache/spark/pull/26590. Please take a look when you have time. Thanks in advance!;;;",,,,,,,,,,,,,,,,,,,,,
Make the behavior of Postgre dialect independent of ansi mode config,SPARK-29951,13269154,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,18/Nov/19 20:29,21/Nov/19 05:09,13/Jul/23 08:49,20/Nov/19 16:57,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"As the comment in [SPARK-29873|[https://github.com/apache/spark/pull/26497#discussion_r345708065],] Postgre dialect should not be affected by the ANSI mode config.

During reran the existing tests, only the LEFT/RIGHT build-in SQL function break the assumption, we fix this by following [https://www.postgresql.org/docs/12/sql-keywords-appendix.html]: LEFT/RIGHT reserved (can be function or type)",,cloud_fan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 16:57:28 UTC 2019,,,,,,,,,,"0|z08r8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Nov/19 16:57;cloud_fan;Issue resolved by pull request 26584
[https://github.com/apache/spark/pull/26584];;;",,,,,,,,,,,,,,,,,,,,,,,
JSON/CSV formats timestamps incorrectly,SPARK-29949,13269124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,18/Nov/19 17:49,19/Nov/19 09:11,13/Jul/23 08:49,19/Nov/19 09:11,2.4.4,,,,,,,,,,2.4.5,,,SQL,,,,,0,,,,"For example:
{code}
scala> val t = java.sql.Timestamp.valueOf(""2019-11-18 11:56:00.123456"")
t: java.sql.Timestamp = 2019-11-18 11:56:00.123456
scala> Seq(t).toDF(""t"").select(to_json(struct($""t""), Map(""timestampFormat"" -> ""yyyy-MM-dd HH:mm:ss.SSSSSS""))).show(false)
+-------------------------------------------------+
|structstojson(named_struct(NamePlaceholder(), t))|
+-------------------------------------------------+
|{""t"":""2019-11-18 11:56:00.000123""}               |
+-------------------------------------------------+
{code}",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 09:11:04 UTC 2019,,,,,,,,,,"0|z08r28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Nov/19 09:11;cloud_fan;Issue resolved by pull request 26582
[https://github.com/apache/spark/pull/26582];;;",,,,,,,,,,,,,,,,,,,,,,,
Add batching in alter table add partition flow,SPARK-29938,13268995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prakharjain09,prakharjain09,prakharjain09,18/Nov/19 06:27,17/Apr/20 00:34,13/Jul/23 08:49,20/Dec/19 14:54,2.3.4,2.4.4,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"When lot of new partitions are added by an Insert query on a partitioned datasource table, sometimes the query fails with -
{noformat}
An error was encountered: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out; at
org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106) at
org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:928) at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:798) at
org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:448) at
org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.refreshUpdatedPartitions$1(InsertIntoHadoopFsRelationCommand.scala:137)
{noformat}
This happens because adding thousands of partition in a single call takes lot of time and the client eventually timesout.

Also adding lot of partitions can lead to OOM in Hive Metastore (similar issue in [recover partition flow|https://github.com/apache/spark/pull/14607] fixed).

Steps to reproduce -
{noformat}
case class Partition(data: Int, partition_key: Int)
val df = sc.parallelize(1 to 15000, 15000).map(x => Partition(x,x)).toDF
df.registerTempTable(""temp_table"")

spark.sql(""""""CREATE TABLE `test_table` (`data` INT, `partition_key` INT) USING parquet PARTITIONED BY (partition_key) """""")
spark.sql(""INSERT OVERWRITE TABLE test_table select * from temp_table"").collect()
{noformat}",,abmodi,prakharjain09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 14:54:40 UTC 2019,,,,,,,,,,"0|z08q9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Dec/19 14:54;srowen;Issue resolved by pull request 26569
[https://github.com/apache/spark/pull/26569];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix SparkR lint errors and add lint-r GitHub Action,SPARK-29936,13268956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,17/Nov/19 22:50,19/Nov/19 04:13,13/Jul/23 08:49,18/Nov/19 05:01,2.4.5,3.0.0,,,,,,,,,3.0.0,,,SparkR,,,,,0,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29935,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 18 05:01:26 UTC 2019,,,,,,,,,,"0|z08q0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Nov/19 05:01;dongjoon;Issue resolved by pull request 26564
[https://github.com/apache/spark/pull/26564];;;",,,,,,,,,,,,,,,,,,,,,,,
lint-r should do non-zero exit in case of errors,SPARK-29932,13268892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,17/Nov/19 04:30,19/Nov/19 19:32,13/Jul/23 08:49,17/Nov/19 18:10,2.3.4,2.4.4,3.0.0,,,,,,,,2.4.5,3.0.0,,SparkR,Tests,,,,0,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 17 18:10:36 UTC 2019,,,,,,,,,,"0|z08pmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/19 18:10;dongjoon;Issue resolved by pull request 26561
[https://github.com/apache/spark/pull/26561];;;",,,,,,,,,,,,,,,,,,,,,,,
Parsing failure on interval '20 15' day to hour,SPARK-29920,13268634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,15/Nov/19 17:32,05/Jun/20 19:44,13/Jul/23 08:49,11/Dec/19 17:11,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"
{code:sql}
spark-sql> select interval '20 15' day to hour;
Error in query:
requirement failed: Interval string must match day-time format of 'd h:m:s.n': 20 15(line 1, pos 16)

== SQL ==
select interval '20 15' day to hour
----------------^^^
{code}
",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 19:44:02 UTC 2020,,,,,,,,,,"0|z08o1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Dec/19 17:11;cloud_fan;Issue resolved by pull request 26473
[https://github.com/apache/spark/pull/26473];;;","05/Jun/20 19:43;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/26473;;;","05/Jun/20 19:44;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/26473;;;",,,,,,,,,,,,,,,,,,,,,
RecordBinaryComparator should check endianness when compared by long,SPARK-29918,13268622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,EdisonWang,EdisonWang,EdisonWang,15/Nov/19 16:58,27/Jul/20 10:51,13/Jul/23 08:49,19/Nov/19 08:14,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,SQL,,,,,0,correctness,,,"If the architecture supports unaligned or the offset is 8 bytes aligned, RecordBinaryComparator compare 8 bytes at a time by reading 8 bytes as a long. Otherwise, it will compare bytes by bytes. 

However, on little-endian machine,  the result of compared by a long value and compared bytes by bytes maybe different. If the architectures in a yarn cluster is different(Some is unaligned-access capable while others not), then the sequence of two records after sorted is undetermined, which will result in the same problem as in https://issues.apache.org/jira/browse/SPARK-23207

 ",,apachespark,cloud_fan,dongjoon,EdisonWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31172,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 10:51:49 UTC 2020,,,,,,,,,,"0|z08nyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Nov/19 18:30;dongjoon;Hi, [~EdisonWang]. What about the older Spark versions?;;;","19/Nov/19 08:14;cloud_fan;Issue resolved by pull request 26548
[https://github.com/apache/spark/pull/26548];;;","27/Jul/20 10:50;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29259;;;","27/Jul/20 10:51;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29259;;;",,,,,,,,,,,,,,,,,,,,
Cache table may memory leak when session closed,SPARK-29911,13268489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,15/Nov/19 07:01,04/Jan/22 07:48,13/Jul/23 08:49,21/Nov/19 00:19,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"How to reproduce:
1. create a local temporary view v1
2. cache it in memory
3. close session without drop v1.
The application will hold the memory forever. In a long running thrift server scenario. It's worse.
{code}
0: jdbc:hive2://localhost:10000> CACHE TABLE testCacheTable AS SELECT 1;
CACHE TABLE testCacheTable AS SELECT 1;
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (1.498 seconds)
0: jdbc:hive2://localhost:10000> !close
!close
Closing: 0: jdbc:hive2://localhost:10000
0: jdbc:hive2://localhost:10000 (closed)> !connect 'jdbc:hive2://localhost:10000'
!connect 'jdbc:hive2://localhost:10000'
Connecting to jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000:
lajin
Enter password for jdbc:hive2://localhost:10000:
***
Connected to: Spark SQL (version 3.0.0-SNAPSHOT)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
1: jdbc:hive2://localhost:10000> select * from testCacheTable;
select * from testCacheTable;
Error: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: testCacheTable; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [testCacheTable] (state=,code=0)
{code}
 !Screen Shot 2019-11-15 at 2.03.49 PM.png! ",,cltlfcjin,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30470,,,,,,,,,,,,,,,,,"15/Nov/19 07:01;cltlfcjin;Screen Shot 2019-11-15 at 2.03.49 PM.png;https://issues.apache.org/jira/secure/attachment/12985915/Screen+Shot+2019-11-15+at+2.03.49+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 21 00:19:56 UTC 2019,,,,,,,,,,"0|z08n54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Nov/19 18:33;dongjoon;Hi, [~cltlfcjin]. Since is reported as a memory leakage issue, could you check the older Spark version and update the `Affected Versions` of this JIRA issue please?;;;","21/Nov/19 00:19;srowen;Issue resolved by pull request 26543
[https://github.com/apache/spark/pull/26543];;;",,,,,,,,,,,,,,,,,,,,,,
Parse timestamps in microsecond precision by JSON/CSV datasources,SPARK-29904,13268422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,14/Nov/19 20:48,03/Feb/20 17:43,13/Jul/23 08:49,15/Nov/19 23:42,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,,,,2.4.5,,,SQL,,,,,0,,,,"Currently, Spark can parse strings with timestamps from JSON/CSV in millisecond precision. Internally, timestamps have microsecond precision. The ticket aims to modify parsing logic in Spark 2.4 to support the microsecond precision. Porting of DateFormatter/TimestampFormatter from Spark 3.0-preview is risky, so, need to find another lighter solution.",,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27224,SPARK-10681,SPARK-6385,,,,,SPARK-29927,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 23:42:49 UTC 2019,,,,,,,,,,"0|z08mq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Nov/19 23:42;dongjoon;This is resolved via https://github.com/apache/spark/pull/26507;;;",,,,,,,,,,,,,,,,,,,,,,,
Unable to fill na with 0 with duplicate columns,SPARK-29890,13268249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,sandeshyapuram,sandeshyapuram,14/Nov/19 09:30,18/Jan/21 12:56,13/Jul/23 08:49,25/Nov/19 16:20,2.0.2,2.1.3,2.2.3,2.3.3,2.4.3,,,,,,2.4.5,3.0.0,,Spark Shell,,,,,0,,,,"Trying to fill out na values with 0.
{noformat}
scala> :paste
// Entering paste mode (ctrl-D to finish)
val parent = spark.sparkContext.parallelize(Seq((1,2),(3,4),(5,6))).toDF(""nums"", ""abc"")
val c1 = parent.filter(lit(true))
val c2 = parent.filter(lit(true))
c1.join(c2, Seq(""nums""), ""left"")
.na.fill(0).show{noformat}
{noformat}
9/11/14 04:24:24 ERROR org.apache.hadoop.security.JniBasedUnixGroupsMapping: error looking up the name of group 820818257: No such file or directory
org.apache.spark.sql.AnalysisException: Reference 'abc' is ambiguous, could be: abc, abc.;
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:213)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:117)
  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:220)
  at org.apache.spark.sql.Dataset.col(Dataset.scala:1246)
  at org.apache.spark.sql.DataFrameNaFunctions.org$apache$spark$sql$DataFrameNaFunctions$$fillCol(DataFrameNaFunctions.scala:443)
  at org.apache.spark.sql.DataFrameNaFunctions$$anonfun$7.apply(DataFrameNaFunctions.scala:500)
  at org.apache.spark.sql.DataFrameNaFunctions$$anonfun$7.apply(DataFrameNaFunctions.scala:492)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.sql.DataFrameNaFunctions.fillValue(DataFrameNaFunctions.scala:492)
  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:171)
  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:155)
  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:134)
  ... 54 elided{noformat}
 ",,cloud_fan,imback82,petertoth,sandeshyapuram,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28897,,,,,,,,SPARK-30065,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 18 12:56:34 UTC 2021,,,,,,,,,,"0|z08lns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Nov/19 09:36;sandeshyapuram;I've raised it as a bug because I feel fill.na(0) needs to fill 0 regardless of duplicate column names.

[~cloud_fan] Thoughts;;;","16/Nov/19 06:22;cloud_fan;seems like another self-join bug. [~imback82] can you take a look?;;;","16/Nov/19 06:32;imback82;Sure. I will take a look.;;;","16/Nov/19 16:07;sandeshyapuram;[~imback82] This happens even for a normal join:
{noformat}
val p1 = spark.sparkContext.parallelize(Seq((1,2),(3,4),(5,6))).toDF(""nums"", ""abc"")
val p2 = spark.sparkContext.parallelize(Seq((1,2),(3,4),(5,6))).toDF(""nums"", ""abc"")
p1.join(p2, Seq(""nums""), ""left"")
.na.fill(0).show
{noformat};;;","17/Nov/19 00:27;imback82;{code:java}
scala> p1.join(p2, Seq(""nums"")).printSchema
root
 |-- nums: integer (nullable = false)
 |-- abc: integer (nullable = false)
 |-- abc: integer (nullable = false)
{code}

Note that `fill` takes in column _names_. Thus, the following is ambiguous.

{code:java}
scala> p1.join(p2, Seq(""nums"")).na.fill(0, Seq(""abc""))
org.apache.spark.sql.AnalysisException: Reference 'abc' is ambiguous, could be: abc, abc.;
  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:240)
{code}

[~cloud_fan], I think this is an expected behavior. What do you think?;;;","18/Nov/19 11:07;cloud_fan;When we do `.fill(0, Seq(""abc""))`, I think we should report error. But for `.fill(0)`, we want to apply on all columns, which should not do name lookup?;;;","19/Nov/19 00:23;imback82;Got it. Thanks.;;;","19/Nov/19 07:10;imback82;I also noticed that `drop()` behaves the same. I will work on that if this PR gets merged.;;;","25/Nov/19 16:20;cloud_fan;Issue resolved by pull request 26593
[https://github.com/apache/spark/pull/26593];;;","15/Jan/21 14:17;petertoth;[~imback82], [~cloud_fan], due this change `fill` started to throw exception when `cols` contains a column that can't be resolved. I wonder if this behaviour change of `fill` is desired or it is more like bug. I'm happy to open fix PR if you think this side effect is unintended.;;;","18/Jan/21 02:41;imback82;[~petertoth] Could you share the example and the behavior change? Are you referring to something like the following:

{code:java}
scala> Seq(1).toDF(""i"").na.fill(0, Seq(""j""))
org.apache.spark.sql.AnalysisException: Cannot resolve column name ""j"" among (i)
{code}
, which seems fine to me.;;;","18/Jan/21 07:57;petertoth;[~imback82], yes, that's a good example. `fill` didn't throw any exception before this ticket. ;;;","18/Jan/21 12:56;cloud_fan;I think failing makes more sense than silently ignore it.;;;",,,,,,,,,,,
New interval string parser parse '.111 seconds' to null ,SPARK-29888,13268211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,14/Nov/19 07:07,15/Nov/19 05:37,13/Jul/23 08:49,15/Nov/19 05:34,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Current string to interval cast logic does not support i.e. cast('.111 second' as interval) which will fail in SIGN state and return null, actually, it is 00:00:00.111. 


{code:java}
These are the results of the master branch.

-- !query 63
select interval '.111 seconds'
-- !query 63 schema
struct<0.111 seconds:interval>
-- !query 63 output
0.111 seconds


-- !query 64
select cast('.111 seconds' as interval)
-- !query 64 schema
struct<CAST(.111 seconds AS INTERVAL):interval>
-- !query 64 output
NULL
{code}
",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 05:34:42 UTC 2019,,,,,,,,,,"0|z08lfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Nov/19 05:34;cloud_fan;Issue resolved by pull request 26514
[https://github.com/apache/spark/pull/26514];;;",,,,,,,,,,,,,,,,,,,,,,,
Avoid to use deprecated pyarrow.open_stream API in Spark 2.4.x,SPARK-29875,13267979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,13/Nov/19 10:10,12/Dec/22 17:51,13/Jul/23 08:49,13/Nov/19 17:51,2.4.4,,,,,,,,,,2.4.5,,,PySpark,SQL,,,,0,,,,"In Spark 2.4.x, if we use PyArrow higher then 0.12.0, it shows a bunch of warnings as below:

{code}
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf(""integer"", PandasUDFType.SCALAR)  # doctest: +SKIP
def add_one(x):
    return x + 1

spark.range(100).select(add_one(""id"")).collect()
{code}

{code}
UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
/usr/local/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream
  warnings.warn(""pyarrow.open_stream is deprecated, please use ""
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26566,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 13 17:51:05 UTC 2019,,,,,,,,,,"0|z08jzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Nov/19 17:51;dongjoon;Issue resolved by pull request 26501
[https://github.com/apache/spark/pull/26501];;;",,,,,,,,,,,,,,,,,,,,,,,
Optimize Dataset.isEmpty(),SPARK-29874,13267955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,13/Nov/19 09:05,21/Nov/19 10:50,13/Jul/23 08:49,21/Nov/19 10:50,2.4.0,3.0.0,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,angerszhuuu,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 21 10:50:37 UTC 2019,,,,,,,,,,"0|z08jug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Nov/19 10:50;cloud_fan;Issue resolved by pull request 26500
[https://github.com/apache/spark/pull/26500];;;",,,,,,,,,,,,,,,,,,,,,,,
HiveMetastoreCatalog#convertToLogicalRelation throws AssertionError,SPARK-29869,13267895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,yumwang,yumwang,13/Nov/19 03:12,19/Nov/19 07:23,13/Jul/23 08:49,19/Nov/19 07:23,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"{noformat}
scala> spark.table(""hive_table"").show
java.lang.AssertionError: assertion failed
  at scala.Predef$.assert(Predef.scala:208)
  at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:261)
  at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
  at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:220)
  at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:207)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
  at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:207)
  at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:191)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:130)
  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
  at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:49)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:127)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:119)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:119)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:168)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:162)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:122)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:98)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:98)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:146)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:66)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:63)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:55)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:86)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:585)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:581)
  ... 47 elided
{noformat}


It works if we disable {{spark.sql.hive.convertMetastoreParquet}} or recreate the table. We should find the root cause and fix this issue or make error message more meaningful. 
",,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/19 03:13;yumwang;SPARK-29869.jpg;https://issues.apache.org/jira/secure/attachment/12985673/SPARK-29869.jpg",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 07:23:06 UTC 2019,,,,,,,,,,"0|z08jh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Nov/19 07:23;cloud_fan;Issue resolved by pull request 26499
[https://github.com/apache/spark/pull/26499];;;",,,,,,,,,,,,,,,,,,,,,,,
lpad and rpad built in function not throw Exception for invalid len value,SPARK-29854,13267611,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,abhishek.akg,abhishek.akg,12/Nov/19 05:41,12/Dec/22 18:11,13/Jul/23 08:49,22/May/20 23:50,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Spark Returns Empty String)

{code}
0: jdbc:hive2://10.18.19.208:23040/default> SELECT lpad('hihhhhhhhhhhhhhhhhhhhhhhh', 500000000000000000000000, '????????????');
 +----------------------------------------------------+
|lpad(hihhhhhhhhhhhhhhhhhhhhhhh, CAST(500000000000000000000000 AS INT), ????????????)|

+----------------------------------------------------+


+----------------------------------------------------+


Hive:


SELECT lpad('hihhhhhhhhhhhhhhhhhhhhhhh', 500000000000000000000000, '????????????');
 Error: Error while compiling statement: FAILED: SemanticException [Error 10016]: Line 1:67 Argument type mismatch ''????????????'': lpad only takes INT/SHORT/BYTE types as 2-ths argument, got DECIMAL (state=42000,code=10016)


PostgreSQL


function lpad(unknown, numeric, unknown) does not exist


 

Expected output:

In Spark also it should throw Exception like Hive
{code}
 ",,abhishek.akg,Ankitraj,apachespark,maropu,sathyaprakashg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 23:50:26 UTC 2020,,,,,,,,,,"0|z08hq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Nov/19 05:43;Ankitraj;i will raise PR for this.;;;","12/Nov/19 11:38;gurwls223;[~abhishek.akg] Can you please use \{code\} ... \{code\} block.;;;","12/Nov/19 11:39;gurwls223;Other DMBSes have rpad and lpad too. Can you check?;;;","13/Nov/19 04:27;abhishek.akg;[~hyukjin.kwon]: Hi I checked in Hive PostgreSQL : It gives Excpetion
In my SQL It gives NULL
mysql> SELECT lpad('hihhhhhhhhhhhhhhhhhhhhhhh', 500000000000000000000000, '????????????');
+-----------------------------------------------------------------------------+
| lpad('hihhhhhhhhhhhhhhhhhhhhhhh', 500000000000000000000000, '????????????') |
+-----------------------------------------------------------------------------+
| NULL                                                                        |
+-----------------------------------------------------------------------------+
1 row in set, 3 warnings (0.00 sec)

mysql> SELECT rpad('hihhhhhhhhhhhhhhhhhhhhhhh', 500000000000000000000000, '????????????');
+-----------------------------------------------------------------------------+
| rpad('hihhhhhhhhhhhhhhhhhhhhhhh', 500000000000000000000000, '????????????') |
+-----------------------------------------------------------------------------+
| NULL                                                                        |
+-----------------------------------------------------------------------------+
1 row in set, 3 warnings (0.00 sec);;;","12/Apr/20 08:10;sathyaprakashg;In Spark 3.0, you can change the behaviour by using  configuration *spark.sql.ansi.enabled* ( _When `spark.sql.ansi.enabled` is set to `true`, Spark SQL follows the standard in basic behaviours (e.g., arithmetic operations, type conversion, SQL functions and SQL parsing)_)

If spark.sql.ansi.enabled=false, which is default, then spark does not throw any arithmetic error like overflowException. For the given sql query, it basically executes below statement to cast the input value to int. Since returned value is negative, passing negative value to lpad/rpad gives empty string as output for the sql query mentioned in this issue
{code:java}
scala> BigDecimal(""500000000000000000000000"").longValue.toInt
res7: Int = -796917760{code}
if you set spark.sql.ansi.enabled=true, then it throws below error.  
{code:java}
java.lang.ArithmeticException: Casting 500000000000000000000000 to int causes overflow
  at org.apache.spark.sql.types.Decimal.overflowException(Decimal.scala:254)
  at org.apache.spark.sql.types.Decimal.roundToInt(Decimal.scala:317)
  at org.apache.spark.sql.types.DecimalExactNumeric$.toInt(numerics.scala:183)
  at org.apache.spark.sql.types.DecimalExactNumeric$.toInt(numerics.scala:182)
  at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToInt$13(Cast.scala:518)
  at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToInt$13$adapted(Cast.scala:518)
  at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeEval(Cast.scala:808)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:461)
  at org.apache.spark.sql.catalyst.expressions.TernaryExpression.eval(Expression.scala:686)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:457)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:52){code};;;","22/May/20 00:09;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28604;;;","22/May/20 23:50;maropu;Resolved by [https://github.com/apache/spark/pull/28604];;;",,,,,,,,,,,,,,,,,
sort-merge-join an empty table should not memory leak,SPARK-29850,13267497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Nov/19 14:55,12/Nov/19 17:14,13/Jul/23 08:49,12/Nov/19 17:14,2.3.4,2.4.4,3.0.0,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 12 17:14:33 UTC 2019,,,,,,,,,,"0|z08h0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Nov/19 17:14;cloud_fan;Issue resolved by pull request 26471
[https://github.com/apache/spark/pull/26471];;;",,,,,,,,,,,,,,,,,,,,,,,
Cast error when there are spaces between signs and values,SPARK-29822,13267335,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,10/Nov/19 08:31,11/Nov/19 13:55,13/Jul/23 08:49,11/Nov/19 13:55,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"With the latest string to literal optimization, some interval strings can not be cast when there are some spaces between signs and unit values.

How to reproduce, 
{code:java}
select cast(v as interval) from values ('+ 1 second') t(v);
select cast(v as interval) from values ('- 1 second') t(v);
{code}
 ",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 11 13:55:07 UTC 2019,,,,,,,,,,"0|z08g0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Nov/19 08:35;Qin Yao;[https://github.com/apache/spark/pull/26449];;;","11/Nov/19 13:55;cloud_fan;Issue resolved by pull request 26449
[https://github.com/apache/spark/pull/26449];;;",,,,,,,,,,,,,,,,,,,,,,
Rewrite non-correlated EXISTS subquery use ScalaSubquery to optimize perf,SPARK-29800,13267074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,08/Nov/19 10:25,06/Jan/20 14:55,13/Jul/23 08:49,06/Jan/20 14:55,2.4.0,3.0.0,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,angerszhuuu,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 14:55:41 UTC 2020,,,,,,,,,,"0|z08eeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Nov/19 10:27;angerszhuuu;raise pr soon;;;","06/Jan/20 14:55;cloud_fan;Issue resolved by pull request 26437
[https://github.com/apache/spark/pull/26437];;;",,,,,,,,,,,,,,,,,,,,,,
Infers bytes as binary type in Python 3 at PySpark,SPARK-29798,13267005,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,08/Nov/19 02:46,12/Dec/22 17:50,13/Jul/23 08:49,08/Nov/19 20:11,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"Currently, PySpark cannot infer {{bytes}} type in Python 3. This should be accepted as binary type. See https://github.com/apache/spark/pull/25749",,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 08 20:11:16 UTC 2019,,,,,,,,,,"0|z08dzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Nov/19 20:11;bryanc;Issue resolved by pull request 26432
[https://github.com/apache/spark/pull/26432];;;",,,,,,,,,,,,,,,,,,,,,,,
HiveExternalCatalogVersionsSuite` should ignore preview release,SPARK-29796,13266882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,07/Nov/19 18:13,07/Nov/19 23:25,13/Jul/23 08:49,07/Nov/19 18:29,2.4.5,3.0.0,,,,,,,,,2.4.5,3.0.0,,SQL,Tests,,,,0,,,,"This issue to exclude the `preview` release to recover `HiveExternalCatalogVersionsSuite`. Currently, new preview release breaks `branch-2.4` PRBuilder since yesterday. New release (especially `preview`) should not affect `branch-2.4`.
- https://github.com/apache/spark/pull/26417 (Failed 4 times)

{code}
scala> scala.io.Source.fromURL(""https://dist.apache.org/repos/dist/release/spark/"").mkString.split(""\n"").filter(_.contains(""""""<li><a href=""spark-"""""")).map(""""""<a href=""spark-(\d.\d.\d)/"">"""""".r.findFirstMatchIn(_).get.group(1))
java.util.NoSuchElementException: None.get
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 18:29:09 UTC 2019,,,,,,,,,,"0|z08d80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Nov/19 18:29;dongjoon;Issue resolved by pull request 26428
[https://github.com/apache/spark/pull/26428];;;",,,,,,,,,,,,,,,,,,,,,,,
should not parse the bucket column name again when creating v2 tables,SPARK-29789,13266830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Nov/19 13:33,13/Nov/19 00:06,13/Jul/23 08:49,12/Nov/19 20:26,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-11-07 13:33:31.0,,,,,,,,,,"0|z08cwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Override SBT Jackson-databind dependency like Maven,SPARK-29781,13266690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,06/Nov/19 23:01,07/Nov/19 21:27,13/Jul/23 08:49,07/Nov/19 21:25,2.4.4,,,,,,,,,,2.4.5,,,Build,,,,,0,,,,This is `branch-2.4` only issue.,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 21:25:26 UTC 2019,,,,,,,,,,"0|z08c1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Nov/19 21:25;dongjoon;Issue resolved by pull request 26417
[https://github.com/apache/spark/pull/26417];;;",,,,,,,,,,,,,,,,,,,,,,,
saveAsTable append mode is not passing writer options,SPARK-29778,13266663,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,SpaceRangerWes,brkyvz,brkyvz,06/Nov/19 19:58,13/Nov/19 22:13,13/Jul/23 08:49,13/Nov/19 22:11,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,There was an oversight where AppendData is not getting the WriterOptions in saveAsTable. [https://github.com/apache/spark/blob/782992c7ed652400e33bc4b1da04c8155b7b3866/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L530],,brkyvz,dongjoon,dyqer,SpaceRangerWes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 13 22:11:51 UTC 2019,,,,,,,,,,"0|z08bvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Nov/19 03:00;dyqer;Does v1 table support append? just checking;;;","12/Nov/19 04:05;SpaceRangerWes;Looks like my PR linked right up. How handy!

 

The test works by creating a custom query listener so that I can gather the {{LogicalPlan}} and assert the proper {{writeOptions.}};;;","13/Nov/19 22:11;dongjoon;Issue resolved by pull request 26474
[https://github.com/apache/spark/pull/26474];;;",,,,,,,,,,,,,,,,,,,,,
SparkR::cleanClosure aggressively removes a function required by user function,SPARK-29777,13266661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,falaki,06/Nov/19 19:41,12/Dec/22 18:10,13/Jul/23 08:49,19/Nov/19 00:05,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,,,,,,3.0.0,,,SparkR,,,,,0,,,,"Following code block reproduces the issue:
{code}
df <- createDataFrame(data.frame(x=1))
f1 <- function(x) x + 1
f2 <- function(x) f1(x) + 2

dapplyCollect(df, function(x) { f1(x); f2(x) })
{code}

We get following error message:

{code}
org.apache.spark.SparkException: R computation failed with
 Error in f1(x) : could not find function ""f1""
Calls: compute -> computeFunc -> f2
{code}

Compare that to this code block with succeeds:
{code}
dapplyCollect(df, function(x) { f2(x) })
{code}",,falaki,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 00:05:25 UTC 2019,,,,,,,,,,"0|z08buw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Nov/19 00:05;gurwls223;Issue resolved by pull request 26429
[https://github.com/apache/spark/pull/26429];;;",,,,,,,,,,,,,,,,,,,,,,,
Unable to process empty ORC files in Hive Table using Spark SQL,SPARK-29773,13266553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,aermakov,aermakov,06/Nov/19 11:02,12/Dec/22 18:11,13/Jul/23 08:49,22/Jun/20 15:21,2.3.1,,,,,,,,,,2.4.4,,,SQL,,,,,1,,,,"Unable to process empty ORC files in Hive Table using Spark SQL. It seems that a problem with class org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits()

Stack trace:
{code:java}
19/10/30 22:29:54 ERROR SparkSQLDriver: Failed in [select distinct _tech_load_dt from dl_raw.tpaccsieee_ut_data_address]
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange hashpartitioning(_tech_load_dt#1374, 200)
+- *(1) HashAggregate(keys=[_tech_load_dt#1374], functions=[], output=[_tech_load_dt#1374])
   +- HiveTableScan [_tech_load_dt#1374], HiveTableRelation `dl_raw`.`tpaccsieee_ut_data_address`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [address#1307, address_9zp#1308, address_adm#1309, address_md#1310, adress_doc#1311, building#1312, change_date_addr_el#1313, change_date_okato#1314, change_date_окато#1315, city#1316, city_id#1317, cnv_cont_id#1318, code_intercity#1319, code_kladr#1320, code_plan1#1321, date_act#1322, date_change#1323, date_prz_incorrect_code_kladr#1324, date_record#1325, district#1326, district_id#1327, etaj#1328, e_plan#1329, fax#1330, ... 44 more fields]        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)
        at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:294)
        at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
        at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:364)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:272)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: serious problem
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:91)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:318)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
        ... 38 more
Caused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 0
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1016)
        ... 75 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 0
        at java.util.Collections$EmptyList.get(Collections.java:4454)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:927)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)holder
{code}","Centos 7, Spark 2.3.1, Hive 2.3.0",aermakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 15:20:43 UTC 2020,,,,,,,,,,"0|z08b6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Nov/19 12:09;gurwls223;2.3.x is EOL releases. Can you try out in higher versions?;;;","06/Nov/19 12:30;aermakov;We will try on 2.4.4;;;","11/Nov/19 17:13;gurwls223;ping [~aermakov], have you tried this out?;;;","22/Jun/20 15:20;aermakov;This issue has been resolved for Spark 2.4.4;;;",,,,,,,,,,,,,,,,,,,,
nondeterministic expression fails column pruning,SPARK-29768,13266469,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,yucai,yucai,06/Nov/19 02:31,03/Jan/20 13:49,13/Jul/23 08:49,27/Nov/19 08:03,2.4.4,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"nondeterministic expression like monotonically_increasing_id fails column pruning

{code}
    spark.range(10).selectExpr(""id as key"", ""id * 2 as value"").
      write.format(""parquet"").save(""/tmp/source"")
    spark.range(10).selectExpr(""id as key"", ""id * 3 as s1"", ""id * 5 as s2"").
      write.format(""parquet"").save(""/tmp/target"")

    val sourceDF = spark.read.parquet(""/tmp/source"")
    val targetDF = spark.read.parquet(""/tmp/target"").
      withColumn(""row_id"", monotonically_increasing_id())
    sourceDF.join(targetDF, ""key"").select(""key"", ""row_id"").explain()
{code}

Spark reads all columns from targetDF, but actually, we only need `key` column.
{code}
scala>     sourceDF.join(targetDF, ""key"").select(""key"", ""row_id"").explain()
== Physical Plan ==
*(2) Project [key#78L, row_id#88L]
+- *(2) BroadcastHashJoin [key#78L], [key#82L], Inner, BuildLeft
   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))
   :  +- *(1) Project [key#78L]
   :     +- *(1) Filter isnotnull(key#78L)
   :        +- *(1) FileScan parquet [key#78L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/source], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint>
   +- *(2) Filter isnotnull(key#82L)
      +- *(2) Project [key#82L, monotonically_increasing_id() AS row_id#88L]
         +- *(2) FileScan parquet [key#82L,s1#83L,s2#84L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/target], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<key:bigint,s1:bigint,s2:bigint>
{code}",,cloud_fan,petertoth,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 08:03:59 UTC 2019,,,,,,,,,,"0|z08ao8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"06/Nov/19 02:34;yucai;[~smilegator] [~wenchen], is it an issue or work as desgin?;;;","27/Nov/19 08:03;cloud_fan;Issue resolved by pull request 26629
[https://github.com/apache/spark/pull/26629];;;",,,,,,,,,,,,,,,,,,,,,,
json_tuple truncates fields,SPARK-29758,13266313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,Bytsko,Bytsko,05/Nov/19 11:26,21/Nov/19 00:59,13/Jul/23 08:49,20/Nov/19 07:33,2.3.0,2.4.4,,,,,,,,,2.4.5,,,SQL,,,,,0,,,,"`json_tuple` has inconsistent behaviour with `from_json` - but only if json string is longer than 2700 characters or so.

This can be reproduced in spark-shell and on cluster, but not in scalatest, for some reason.

{code}
import org.apache.spark.sql.functions.{from_json, json_tuple}
import org.apache.spark.sql.types._
val counterstring = ""*3*5*7*9*12*15*18*21*24*27*30*33*36*39*42*45*48*51*54*57*60*63*66*69*72*75*78*81*84*87*90*93*96*99*103*107*111*115*119*123*127*131*135*139*143*147*151*155*159*163*167*171*175*179*183*187*191*195*199*203*207*211*215*219*223*227*231*235*239*243*247*251*255*259*263*267*271*275*279*283*287*291*295*299*303*307*311*315*319*323*327*331*335*339*343*347*351*355*359*363*367*371*375*379*383*387*391*395*399*403*407*411*415*419*423*427*431*435*439*443*447*451*455*459*463*467*471*475*479*483*487*491*495*499*503*507*511*515*519*523*527*531*535*539*543*547*551*555*559*563*567*571*575*579*583*587*591*595*599*603*607*611*615*619*623*627*631*635*639*643*647*651*655*659*663*667*671*675*679*683*687*691*695*699*703*707*711*715*719*723*727*731*735*739*743*747*751*755*759*763*767*771*775*779*783*787*791*795*799*803*807*811*815*819*823*827*831*835*839*843*847*851*855*859*863*867*871*875*879*883*887*891*895*899*903*907*911*915*919*923*927*931*935*939*943*947*951*955*959*963*967*971*975*979*983*987*991*995*1000*1005*1010*1015*1020*1025*1030*1035*1040*1045*1050*1055*1060*1065*1070*1075*1080*1085*1090*1095*1100*1105*1110*1115*1120*1125*1130*1135*1140*1145*1150*1155*1160*1165*1170*1175*1180*1185*1190*1195*1200*1205*1210*1215*1220*1225*1230*1235*1240*1245*1250*1255*1260*1265*1270*1275*1280*1285*1290*1295*1300*1305*1310*1315*1320*1325*1330*1335*1340*1345*1350*1355*1360*1365*1370*1375*1380*1385*1390*1395*1400*1405*1410*1415*1420*1425*1430*1435*1440*1445*1450*1455*1460*1465*1470*1475*1480*1485*1490*1495*1500*1505*1510*1515*1520*1525*1530*1535*1540*1545*1550*1555*1560*1565*1570*1575*1580*1585*1590*1595*1600*1605*1610*1615*1620*1625*1630*1635*1640*1645*1650*1655*1660*1665*1670*1675*1680*1685*1690*1695*1700*1705*1710*1715*1720*1725*1730*1735*1740*1745*1750*1755*1760*1765*1770*1775*1780*1785*1790*1795*1800*1805*1810*1815*1820*1825*1830*1835*1840*1845*1850*1855*1860*1865*1870*1875*1880*1885*1890*1895*1900*1905*1910*1915*1920*1925*1930*1935*1940*1945*1950*1955*1960*1965*1970*1975*1980*1985*1990*1995*2000*2005*2010*2015*2020*2025*2030*2035*2040*2045*2050*2055*2060*2065*2070*2075*2080*2085*2090*2095*2100*2105*2110*2115*2120*2125*2130*2135*2140*2145*2150*2155*2160*2165*2170*2175*2180*2185*2190*2195*2200*2205*2210*2215*2220*2225*2230*2235*2240*2245*2250*2255*2260*2265*2270*2275*2280*2285*2290*2295*2300*2305*2310*2315*2320*2325*2330*2335*2340*2345*2350*2355*2360*2365*2370*2375*2380*2385*2390*2395*2400*2405*2410*2415*2420*2425*2430*2435*2440*2445*2450*2455*2460*2465*2470*2475*2480*2485*2490*2495*2500*2505*2510*2515*2520*2525*2530*2535*2540*2545*2550*2555*2560*2565*2570*2575*2580*2585*2590*2595*2600*2605*2610*2615*2620*2625*2630*2635*2640*2645*2650*2655*2660*2665*2670*2675*2680*2685*2690*2695*2700*2705*2710*2715*2720*2725*2730*2735*2740*2745*2750*2755*2760*2765*2770*2775*2780*2785*2790*2795*2800*""
val json_tuple_result = Seq(s""""""{""test"":""$counterstring""}"""""").toDF(""json"")
  .withColumn(""result"", json_tuple('json, ""test""))
  .select('result)
  .as[String].head.length
val from_json_result = Seq(s""""""{""test"":""$counterstring""}"""""").toDF(""json"")
  .withColumn(""parsed"", from_json('json, StructType(Seq(StructField(""test"", StringType)))))
  .withColumn(""result"", $""parsed.test"")
  .select('result)
  .as[String].head.length

scala> json_tuple_result
res62: Int = 2791

scala> from_json_result
res63: Int = 2800

{code}

Result is influenced by the total length of the json string at the moment of parsing:
{code}
val json_tuple_result_with_prefix = Seq(s""""""{""prefix"": ""dummy"", ""test"":""$counterstring""}"""""").toDF(""json"")
  .withColumn(""result"", json_tuple('json, ""test""))
  .select('result)
  .as[String].head.length

scala> json_tuple_result_with_prefix
res64: Int = 2772
{code}","EMR 5.15.0 (Spark 2.3.0) And MacBook Pro (Mojave 10.14.3, Spark 2.4.4)
Jdk 8, Scala 2.11.12",Bytsko,cloud_fan,clubnikon,javier_ivanov,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29978,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 07:33:31 UTC 2019,,,,,,,,,,"0|z089pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Nov/19 17:54;maxgekk;I have reproduced the issue on 2.4. The problem is in Jackson core 2.6.7. It was fixed by https://github.com/FasterXML/jackson-core/commit/554f8db0f940b2a53f974852a2af194739d65200#diff-7990edc67621822770cdc62e12d933d4R647-R650 in the version 2.7.7. We could try to back port this https://github.com/apache/spark/pull/21596 on 2.4. [~hyukjin.kwon] WDYT? ;;;","17/Nov/19 18:06;maxgekk;Another solution is to disable this optimization: [https://github.com/apache/spark/blob/v2.4.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/jsonExpressions.scala#L475-L478];;;","20/Nov/19 07:33;cloud_fan;Issue resolved by pull request 26563
[https://github.com/apache/spark/pull/26563];;;",,,,,,,,,,,,,,,,,,,,,
ClassCastException occurs when reading events from SHS,SPARK-29755,13266286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,05/Nov/19 09:05,12/Nov/19 01:20,13/Jul/23 08:49,11/Nov/19 23:49,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"Looks like SPARK-28869 triggered a technical issue on jackson-scala: https://github.com/FasterXML/jackson-module-scala/wiki/FAQ#deserializing-optionint-and-other-primitive-challenges

{noformat}
19/11/05 17:59:23 INFO FsHistoryProvider: Leasing disk manager space for app app-20191105152223-0000 / None...
19/11/05 17:59:23 INFO FsHistoryProvider: Parsing /apps/spark/eventlogs/app-20191105152223-0000 to re-build UI...
19/11/05 17:59:24 ERROR FsHistoryProvider: Exception in checking for event log updates
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
	at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)
	at org.apache.spark.deploy.history.FsHistoryProvider.shouldReloadLog(FsHistoryProvider.scala:585)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkForLogs$6(FsHistoryProvider.scala:458)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkForLogs$6$adapted(FsHistoryProvider.scala:444)
	at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:256)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:255)
	at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:249)
	at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)
	at scala.collection.TraversableLike.filter(TraversableLike.scala:347)
	at scala.collection.TraversableLike.filter$(TraversableLike.scala:347)
	at scala.collection.AbstractTraversable.filter(Traversable.scala:108)
	at org.apache.spark.deploy.history.FsHistoryProvider.checkForLogs(FsHistoryProvider.scala:444)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$startPolling$3(FsHistoryProvider.scala:267)
	at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:1302)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$getRunner$1(FsHistoryProvider.scala:190)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}",,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 11 23:49:40 UTC 2019,,,,,,,,,,"0|z089jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Nov/19 09:05;kabhwan;Working on the patch.;;;","11/Nov/19 23:49;vanzin;Issue resolved by pull request 26397
[https://github.com/apache/spark/pull/26397];;;",,,,,,,,,,,,,,,,,,,,,,
Remove sorting of fields in PySpark SQL Row creation,SPARK-29748,13266194,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,04/Nov/19 21:40,14/Feb/20 20:12,13/Jul/23 08:49,10/Jan/20 22:39,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,1,release-notes,,,"Currently, when a PySpark Row is created with keyword arguments, the fields are sorted alphabetically. This has created a lot of confusion with users because it is not obvious (although it is stated in the pydocs) that they will be sorted alphabetically, and then an error can occur later when applying a schema and the field order does not match.

The original reason for sorting fields is because kwargs in python < 3.6 are not guaranteed to be in the same order that they were entered. Sorting alphabetically would ensure a consistent order.  Matters are further complicated with the flag {{__from_dict__}} that allows the {{Row}} fields to to be referenced by name when made by kwargs, but this flag is not serialized with the Row and leads to inconsistent behavior.

This JIRA proposes that any sorting of the Fields is removed. Users with Python 3.6+ creating Rows with kwargs can continue to do so since Python will ensure the order is the same as entered. Users with Python < 3.6 will have to create Rows with an OrderedDict or by using the Row class as a factory (explained in the pydoc).  If kwargs are used, an error will be raised or based on a conf setting it can fall back to a LegacyRow that will sort the fields as before. This LegacyRow will be immediately deprecated and removed once support for Python < 3.6 is dropped.",,bryanc,jhereth,mauzhang,nikita.glashenko,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27712,SPARK-24915,SPARK-27939,SPARK-22232,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 10 22:39:39 UTC 2020,,,,,,,,,,"0|z088z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Nov/19 21:11;zero323;While this is a step in the right direction I think it justifies a broader discussion about {{Row}} purpose, API, and behavior guarantees. Especially if we're going to introduce diverging implementations, with {{Row}} and {{LegacyRow}}.

Over the years Spark code have accumulated a lot of conflicting behaviors and special cases related to {{Row}}:
 * Sometimes {{Row}} are reordered (subject of this JIRA), sometimes are not.
 * Sometimes there are treated as ordered products ({{tuples}}), sometimes as unordered dictionaries.
 * We provide efficient access only by position, but the primary access method is by name.
 * etc.

Some of the unusual properties, are well documented (but still confusing), other are not. For example objects that are indistinguishable using public API

 
{code:python}
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

a = Row(x1=1, x2=""foo"")
b = Row(""x1"", ""x2"")(1, ""foo"")

a == b
# True 
type(a) == type(b)
#True

list(a.__fields__) == list(b.__fields__)  # Not really public, but just to make a point
{code}
cannot be substituted in practice.
{code:python}
schema = StructType([
    StructField(""x2"", StringType()),
    StructField(""x1"", IntegerType())])

spark.createDataFrame([a], schema)                                                                                                                                              
# DataFrame[x2: string, x1: int]

spark.createDataFrame([b], schema) 
# TypeError                                 Traceback (most recent call last)
# ...
# TypeError: field x1: IntegerType can not accept object 'foo' in type <class 'str'>
{code}
To make things even worse the primary (while I don't have hard data here, but it is common both in the internal API as well as the code I've seen in the wild) access method - by name - is _O(M)_  where is the width of schema.

So if we're going to modify the core behavior (sorting) it makes sense to rethink the whole design.

Since the schema is carried around with each object and pass over the wire we might as well convert {{Row}} into a proxy of {{OrderedDict}} getting something around these lines:
{code:python}
import sys
from collections import OrderedDict

class Row:
    slots = [""_store""]

    def __init__(self, *args, **kwargs):
        if args and kwargs:
            raise ValueError(""Can not use both args ""
                             ""and kwargs to create Row"")
        if args:
            self._store = OrderedDict.fromkeys(args)
        else:
            self._store = OrderedDict(kwargs)

    def __getattr__(self, x):
        return self._store[x]

    def __getitem__(self, x):
        if isinstance(x, int):
            return list(self._store.values())[x]
        else:
            return self._store[x]

    def __iter__(self):
        return iter(self._store.values())

    def __repr__(self):
        return ""Row({})"".format("", "".join(
	    ""{}={}"".format(k, v) for k, v in self._store.items()
	))

    def __len__(self):
        return len(self._store)

    def __call__(self, *args):
        if len(args) > len(self):
            raise ValueError(""Can not create Row with fields %s, expected %d values ""
                             ""but got %s"" % (self, len(self), args))

        self._store.update(zip(self._store.keys(), args))
        return self

    def __eq__(self, other):
        return isinstance(other, Row) and self._store == other._store

    @property
    def _fields(self):
        return self._store.keys()

    @staticmethod
    def _conv(obj):
        if isinstance(obj, Row):
            return obj.asDict(True)
        elif isinstance(obj, list):
            return [conv(o) for o in obj]
        elif isinstance(obj, dict):
            return dict((k, conv(v)) for k, v in obj.items())
        else:
            return obj

    def asDict(self, recursive=False):
        if recursive:
            result = OrderedDict.fromkeys(self._fields)
            for key in self._fields:
                result[key] = Row._conv(self._store[key])
            return result
        else:
            return self._store

    @classmethod
    def  from_dict(cls, d):
        if sys.version_info >= (3, 6):
            if not(isinstance(d, dict)):
                raise ValueError(
                    ""from_dict requires dict but got {}"".format(
                        type(d)))

        else:
            if not(isinstance(d, OrderedDict)):
                raise ValueError(
                    ""from_dict requires collections.OrderedDict {}"".format(
                        type(d)))
        return cls(**d)
{code}
If we're committed to {{Row}} being a {{tuple}} (with _O(1)_ by index access) we could actually try to hack {{namedtuple}}:

{code:python}
from collections import namedtuple
import hashlib
import json

class Row:
    def __new__(cls, *args, **kwargs):
        if args and kwargs:
            raise ValueError(""Can not use both args ""
                             ""and kwargs to create Row"")
        if args:
            return _SchemaRegistry.schema(tuple(args))
        else:
            return _SchemaRegistry.make(tuple(kwargs.keys()), kwargs.values())

class _SchemaRegistry:
    registry = {}
    @classmethod
    def schema(cls, fields):
        if fields in cls.registry:
            return cls.registry[fields]
        else:
            m = hashlib.md5()
            m.update(json.dumps(fields).encode())
            suffix = m.hexdigest()

            reducer = lambda self: (cls.make, (self._fields, tuple(self)))
            # TODO Add recursive case
            def asDict = lambda self: self._asdict()

            schema = type(
                ""Row"",
                (namedtuple(""Row_{}"".format(suffix), fields), Row),
                {""__reduce__"": reducer, ""asDict"": asDict})
            cls.registry[fields] = schema  # Idempotent so we don't need lock
            return schema

    @classmethod
    def make(cls, fields, values):
        return cls.schema(fields)(*values)
{code}

and get efficient name and index access for free.;;;","14/Nov/19 23:24;bryanc;Thanks for discussing [~zero323] . The goal here is to only remove the sorting of fields, which causes all kinds of weird inconsistencies like in your above example. I'd prefer to leave efficient field access for another time. Since Row is a subclass of tuple, accessing fields by name has never been efficient and I don't want to change the fundamental design here. The only reason to introduce LegacyRow (which will be deprecated) is to maintain backward compatibility with existing code that expects fields to be sorted.;;;","15/Nov/19 05:47;jhereth;[~bryanc] With simply removing sorting we change the semantics, e.g. `Row(a=1, b=2) != Row(b=2, a=1)` (opposed to what we currently have.

Also, there might be problems if data was written with Spark pre-change and read after the change.

Adding workarounds (if possible) will make the code very complex.

I think [~zero323] was thinking about changes for the upcoming 3.0?;;;","15/Nov/19 12:28;zero323;[~jhereth]

{quote}With simply removing sorting we change the semantics, e.g. `Row(a=1, b=2) != Row(b=2, a=1)` (opposed to what we currently have.{quote}

It is even more messy. At the moment we adhere to {{tuple}} semantics so {{Row(a=1, b=2) == Row(y=1, z=2)}}. That might be acceptable (namedtuples use the same approach, but I think we should state that explicitly).

{quote}I think Maciej Szymkiewicz was thinking about changes for the upcoming 3.0?{quote}

Indeed.

[~bryanc]

Let me clarify things - I am not suggesting that any of these changes should be implemented here. Instead I think we should have clear picture what {{Row}} suppose to be (not only in terms of API, but also intended applications) before we decide on a concrete solution. That's particularly important because we already have special cases that were introduced specifically to target {{**kwargs}} and sorting behavior.

That being said, if we want to discuss this case in isolation

*  Introducing {{LegacyRow}} seems to make little sense if implementation of {{Row}} stays the same otherwise.  Sorting or not, depending on the config, should be enough.
* {quote} Users with Python < 3.6 will have to create Rows with an OrderedDict or by using the Row class as a factory (explained in the pydoc).  {quote}   I don't think we should introduce such behavior now, when 3.5 is deprecated. Having yet another way to initialize {{Row}} will be confusing at best (and introduce new problems  when using complex structures). Furthermore we already have one mechanism that provides ordered behavior independent of version.

Instead I'd suggest we:

* Make legacy behavior the only option for Python < 3.6. 
* For Python 3.6 let's introduce legacy sorting mechanism (keeping only single {{Row}}) class, enabled by default and deprecated. 

    
;;;","19/Nov/19 19:06;bryanc;[~zero323] and [~jhereth] this is targeted for Spark 3.0 and I agree, the behavior of Row should be very well defined to avoid any further confusion.

bq. Introducing {{LegacyRow}} seems to make little sense if implementation of {{Row}} stays the same otherwise. Sorting or not, depending on the config, should be enough.

LegacyRow isn't meant to be public and the user will not be aware of it. The reasons for it are to separate different implementations and make for a clean removal in the future without affecting the standard Row class. Having a separate implementation will make it easier to debug and diagnose problems - I don't want to get in the situation where a Row could sort fields or not, and then getting bug reports not knowing which way it was configured.

bq. I don't think we should introduce such behavior now, when 3.5 is deprecated. Having yet another way to initialize Row will be confusing at best 

That's reasonable. I'm not crazy about an option for OrderedDict as input, but I think users of Python < 3.6 should have a way to create a Row with ordered fields other than the 2-step process in the pydoc. We can explore other options for this.

bq. Make legacy behavior the only option for Python < 3.6.

I don't think we should have 2 very different behaviors that are chosen based on your Python verison. The user should be aware of what is happening and need to make the decision to use the legacy sorting. Some users will not know this, then upgrade their Python version and see Rows breaking. We should allow users with Python < 3.6 to make Rows with ordered fields and then be able to upgrade Python version without breaking their Spark app.

bq. For Python 3.6 let's introduce legacy sorting mechanism (keeping only single Row) class, enabled by default and deprecated.

Yeah, I'm not sure if we should enable the legacy sorting as default or not, what do others think?
 ;;;","03/Dec/19 20:51;bryanc;[~zero323] I made some updates to the PR with remove the _LegacyRow and option for OrderedDict, and also like you suggested for Python 3.6 will automatically fall back to legacy behavior of sorting and print a warning to the user.;;;","10/Jan/20 22:39;bryanc;Issue resolved by pull request 26496
[https://github.com/apache/spark/pull/26496];;;",,,,,,,,,,,,,,,,,
sample should set needCopyResult to true if its child is,SPARK-29743,13266134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,cloud_fan,cloud_fan,04/Nov/19 15:00,02/Mar/20 22:06,13/Jul/23 08:49,04/Nov/19 19:03,2.3.0,2.3.4,2.4.4,3.0.0,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,correctness,,,,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 04 19:03:17 UTC 2019,,,,,,,,,,"0|z088ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Nov/19 19:03;dongjoon;Issue resolved by pull request 26387
[https://github.com/apache/spark/pull/26387];;;",,,,,,,,,,,,,,,,,,,,,,,
dev/lint-java cann't check all code we will use,SPARK-29742,13266117,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,04/Nov/19 13:24,04/Nov/19 17:09,13/Jul/23 08:49,04/Nov/19 17:09,3.0.0,,,,,,,,,,3.0.0,,,Build,,,,,0,,,,`dev/lint-java` cann't cover all code we will use,,angerszhuuu,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 04 17:09:08 UTC 2019,,,,,,,,,,"0|z088i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Nov/19 17:09;dongjoon;Issue resolved by pull request 26385
[https://github.com/apache/spark/pull/26385];;;",,,,,,,,,,,,,,,,,,,,,,,
Non reversed keywords should be able to be used in high order functions,SPARK-29722,13265877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,02/Nov/19 08:56,12/Dec/22 18:10,13/Jul/23 08:49,04/Nov/19 05:52,2.4.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Non reversed keywords should able to using in high order functions.

select transform(ys, left -> left * left) as v from nested;
select transform(ys, (y, i) -> y + i) as v from nested;

left and cost are non-reversed keywords, currently, the above SQL syntaxes will fail.",,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 04 05:52:56 UTC 2019,,,,,,,,,,"0|z0870o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Nov/19 05:52;gurwls223;Issue resolved by pull request 26366
[https://github.com/apache/spark/pull/26366];;;",,,,,,,,,,,,,,,,,,,,,,,
Estimator fit method fails to copy params (in PySpark),SPARK-29691,13265630,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,JohnHBauer,JohnHBauer,JohnHBauer,31/Oct/19 20:35,19/Nov/19 22:18,13/Jul/23 08:49,19/Nov/19 22:16,2.4.4,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"Estimator `fit` method is supposed to copy a dictionary of params, overwriting the estimator's previous values, before fitting the model. However, the parameter values are not updated.  This was observed in PySpark, but may be present in the Java objects, as the PySpark code appears to be functioning correctly.   (The copy method that interacts with Java is actually implemented in Params.)

For example, this prints

Before: 0.8
After: 0.8

but After should be 0.75

{code:python}
from pyspark.ml.classification import LogisticRegression

# Load training data
training = spark \
    .read \
    .format(""libsvm"") \
    .load(""data/mllib/sample_multiclass_classification_data.txt"")

lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
print(""Before:"", lr.getOrDefault(""elasticNetParam""))

# Fit the model, but with an updated parameter setting:
lrModel = lr.fit(training, params={""elasticNetParam"" : 0.75})

print(""After:"", lr.getOrDefault(""elasticNetParam""))
{code}",,bryanc,huaxingao,JohnHBauer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 22:16:09 UTC 2019,,,,,,,,,,"0|z085hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Nov/19 22:31;huaxingao;I checked the doc and implementation. The Estimator fits the model using the passed in optional params instead of the embedded params, but it doesn't overwrite the estimator's embedded params values. In your case, the estimator uses 0.75 to fit the model, but it still keeps 0.8 for it's own elasticNetParam. If you get the model's parameters, it should have 0.75 for elasticNetParam. This seems to work as designed. 

 
{code:java}
# Fit the model, but with an updated parameter setting:
lrModel = lr.fit(training, params= {lor.elasticNetParam : 0.75})
print(""After:"", lrModel.getOrDefault(""elasticNetParam"")) # print 0.75
{code}
 ;;;","04/Nov/19 15:47;JohnHBauer;I was using this in the context of an MLflow Hyperparameter search. None of the model outputs changed whatsoever when fit(params=...) was used.  Updated example:

{code:python}
from pyspark.ml.classification import LogisticRegression

# Load training data
training = spark \
    .read \
    .format(""libsvm"") \
    .load(""data/mllib/sample_multiclass_classification_data.txt"")

def printStats(lrModel, title):
    trainingSummary = lrModel.summary
    print(title)
    print(lrModel.explainParam(""elasticNetParam""))
    print(""Accuracy:"", trainingSummary.accuracy)
    print(""fMeasure:"", trainingSummary.weightedFMeasure())
    print("""")

lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
lrModel = lr.fit(training)
printStats(lrModel, ""elasticNetParam = 0.8, set through __init__"")

lrModel1 = lr.fit(training, params={""elasticNetParam"": 0.3})
printStats(lrModel1, """"""elasticNetParam still 0.8, after trying to set through fit(..., params={""elasticNetParam"" : 0.3}"""""")

lr03 = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.3)
lrModel03 = lr03.fit(training)
printStats(lrModel03, """"""Correct results for elasticNetParam = 0.3, set through __init__"""""")
{code}
Output:

{noformat}
elasticNetParam = 0.8, set through __init__
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.8)
Accuracy: 0.82
fMeasure: 0.8007300232766211

elasticNetParam still 0.8, after trying to set through fit(..., params={""elasticNetParam"" : 0.3}
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.8)
Accuracy: 0.82
fMeasure: 0.8007300232766211

Correct results for elasticNetParam = 0.3, set through __init__
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.3)
Accuracy: 0.8933333333333333
fMeasure: 0.8922558922558923
{noformat}
;;;","04/Nov/19 19:14;huaxingao;Could you please change this line 
{code:java}
lrModel1 = lr.fit(training, params={""elasticNetParam"": 0.3}){code}
to 
{code:java}
lrModel1 = lr.fit(training, params={lr.elasticNetParam : 0.3}){code}
and try again?

 ;;;","04/Nov/19 19:27;JohnHBauer;OK that works.  I worked with fit doing a grid search some time ago, and don't remember it working like this.  I will check some of my earlier projects to see if my memory fails me...;;;","05/Nov/19 18:10;JohnHBauer;I wonder if it would make sense to do this:

{code:python}
    def _copyValues(self, to, extra=None):
        """"""
        Copies param values from this instance to another instance for
        params shared by them.

        :param to: the target instance
        :param extra: extra params to be copied
        :return: the target instance with param values copied
        """"""
        paramMap = self._paramMap.copy()
        if extra is not None:
            paramMap.update(extra)
        for param in self.params:
            # copy default params
            if param in self._defaultParamMap and to.hasParam(param.name):
                to._defaultParamMap[to.getParam(param.name)] = self._defaultParamMap[param]
            # copy explicitly set params
            if param in paramMap and to.hasParam(param.name):
                to._set(**{param.name: paramMap[param]})
            # allow extra to update parameters on self by name,
            # without having to call getParam first
            elif self.hasParam(param):
                to._set(**{param: paramMap[param]})
            else:
                pass
        return to
{code}
This should allow:
{code:python}
lr.fit(df, extra={""elasticNetParam"": 0.3})
{code}
to produce the same result as:
{code:python}
lr.fit(df, extra={lr.getParam(""elasticNetParam""): 0.3})
{code};;;","05/Nov/19 18:20;bryanc;[~JohnHBauer] I'm not sure we should extend the API to accept parameter names too, but it should definitely check that values  in {{extra}} are an instance of {{Param}} and raise an error if not. Would that be ok with you and could you do a PR for this?;;;","05/Nov/19 18:51;JohnHBauer;Yes, I can do that.  An error message suggesting a call to getParam would get people on track.  (I think that extending the API to include parameter names as above could be done safely, with a check that they could be bound to self, and an additional check in Pipeline.fit to prevent them being broadcast across a pipeline.);;;","14/Nov/19 18:41;JohnHBauer;[[SPARK-29691] ensure Param objects are valid in fit, transform|https://github.com/apache/spark/pull/26527];;;","19/Nov/19 22:16;bryanc;Issue resolved by pull request 26527
[https://github.com/apache/spark/pull/26527];;;",,,,,,,,,,,,,,,
Job failed due to executor failures all available nodes are blacklisted,SPARK-29683,13265491,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,uncleGen,uncleGen,31/Oct/19 09:35,01/Nov/22 03:08,13/Jul/23 08:49,01/Nov/22 03:08,3.0.0,,,,,,,,,,3.1.1,,,Spark Core,YARN,,,,4,,,,"My streaming job will fail *due to executor failures all available nodes are blacklisted*. This exception is thrown only when all node is blacklisted:
{code:java}
def isAllNodeBlacklisted: Boolean = currentBlacklistedYarnNodes.size >= numClusterNodes

val allBlacklistedNodes = excludeNodes ++ schedulerBlacklist ++ allocatorBlacklist.keySet
{code}
After diving into the code, I found some critical conditions not be handled properly:
 - unchecked `excludeNodes`: it comes from user config. If not set properly, it may lead to ""currentBlacklistedYarnNodes.size >= numClusterNodes"". For example, we may set some nodes not in Yarn cluster.
{code:java}
excludeNodes = (invalid1, invalid2, invalid3)
clusterNodes = (valid1, valid2)
{code}

 - `numClusterNodes` may equals 0: When HA Yarn failover, it will take some time for all NodeManagers to register ResourceManager again. In this case, `numClusterNode` may equals 0 or some other number, and Spark driver failed.
 - too strong condition check: Spark driver will fail as long as ""currentBlacklistedYarnNodes.size >= numClusterNodes"". This condition should not indicate a unrecovered fatal. For example, there are some NodeManagers restarting. So we can give some waiting time before job failed.",,apachespark,attilapiros,djanand,dongwook,Elixir Kook,Huibo Peng,jonathak,pjoneswork,sidshar,Steven Rand,uncleGen,,,,,,,,,,,,,,,,,,,,,,SPARK-16630,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 01 00:23:35 UTC 2022,,,,,,,,,,"0|z084mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Nov/19 03:12;Steven Rand;We're experiencing this as well during HA YARN failover.;;;","01/Jun/20 14:47;apachespark;User 'cnZach' has created a pull request for this issue:
https://github.com/apache/spark/pull/28606;;;","30/Oct/20 22:09;dongwook;I agree with Genmao, these logics that added by SPARK-16630 is too strong condition to make application fail.

 
{code:java}
def isAllNodeBlacklisted: Boolean = currentBlacklistedYarnNodes.size >= numClusterNodes
val allBlacklistedNodes = excludeNodes ++ schedulerBlacklist ++ allocatorBlacklist.keySet
{code}
 

I think the above logic would work only for partial failure or intermittent issue that numClusterNodes isn't changed, in case of a scheduler or allocator failure become permanent failure that ResourceManager has to remove from its pool which numClusterNodes is changed, the above logic could fail.

e.g) let's say a cluster has 2 NodeManagers(numClusterNodes = 2), and one NodeManager(N1) has the some issues that cause scheduling failures which ends up increasing schedulerBlacklist.size to 1, and later N1 can't recover from ResourceManager's perspective due to a hardware failure or decommissioned by operator or any other ways, in this case numClusterNodes becomes 1 which makes isAllNodeBlacklisted true, even if there is still 1 NodeManager available and ""spark.yarn.blacklist.executor.launch.blacklisting.enabled"" set to false

Particularly in cloud environment, resizing of cluster happens all the times, for long-running spark application with many resize operations of cluster, schedulerBlacklist.size could keep increasing while numClusterNodes keep fluctuated, in addition even if currentBlacklistedYarnNodes.size >= numClusterNodes is true case, there could be new nodes would be added quickly. 

I found [e70df2cea46f71461d8d401a420e946f999862c1|https://github.com/apache/spark/commit/e70df2cea46f71461d8d401a420e946f999862c1] was added to handle the case of numClusterNode = 0.

However for other cases as mentioned in this JIRA, I think just removing the following part from [ApplicationMaster|https://github.com/apache/spark/blob/branch-2.4/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L535-L538] would more make sense, because isAllNodeBlackListed doesn't necessarily mean running application needs to fail.

 
{code:java}
} else if (allocator.isAllNodeBlacklisted) {
 finish(FinalApplicationStatus.FAILED,
 ApplicationMaster.EXIT_MAX_EXECUTOR_FAILURES,
 ""Due to executor failures all available nodes are blacklisted"")
{code}
 

Or at least the above condition should apply as optional with ""spark.yarn.blacklist.executor.launch.blacklisting.enabled"" or some new configuration because SPARK-16630 added as optional but the above logic impact regardless of any configuration.

I wonder other's opinion about this.

 ;;;","14/Jun/21 16:22;djanand;Facing the same issue with 3.0.1 with streaming:

{noformat}
Application Report :
Application-Id : application_123934893489289_0001
Application-Name : yyyyy
Application-Type : SPARK
User : livy
Queue : default
Application Priority : 0
Start-Time : 1622842590806
Finish-Time : 1623111223883
Progress : 100%
State : FINISHED
Final-State : FAILED
Tracking-URL : ip-xx.ec2.internal:18080/history/application_1123934893489289_0001/3
RPC Port : 36535
AM Host : ip-10-160-98-55.ec2.internal
Aggregate Resource Allocation : 41388024201 MB-seconds, 2854390 vcore-seconds
Aggregate Resource Preempted : 0 MB-seconds, 0 vcore-seconds
Log Aggregation Status : TIME_OUT
Diagnostics : Due to executor failures all available nodes are blacklisted
Unmanaged Application : false
Application Node Label Expression : <Not set>
AM container Node Label Expression : <DEFAULT_PARTITION>
TimeoutType : LIFETIME	ExpiryTime : UNLIMITED	RemainingTime : -1seconds
{noformat}

Is there a workaround or any updates on this?
;;;","03/Jan/22 10:48;apachespark;User 'sungpeo' has created a pull request for this issue:
https://github.com/apache/spark/pull/35089;;;","03/Jan/22 10:49;apachespark;User 'sungpeo' has created a pull request for this issue:
https://github.com/apache/spark/pull/35089;;;","01/Nov/22 00:23;attilapiros;[~srowen] I think we can close this as this commit solved the issue:
https://github.com/apache/spark/commit/e70df2cea46f71461d8d401a420e946f999862c1

What do you think?;;;",,,,,,,,,,,,,,,,,
Failure when resolving conflicting references in Join:,SPARK-29682,13265482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,sandeshyapuram,sandeshyapuram,31/Oct/19 08:59,14/Nov/19 07:35,13/Jul/23 08:49,14/Nov/19 07:00,2.1.3,2.2.3,2.3.4,2.4.3,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"When I try to self join a parentDf with multiple childDf say childDf1 ... ... 

where childDfs are derived after a cube or rollup and are filtered based on group bys,

I get and error 

{{Failure when resolving conflicting references in Join: }}

This shows a long error message which is quite unreadable. On the other hand, if I replace cube or rollup with old groupBy, it works without issues.

 

*Sample code:* 
{code:java}
val numsDF = sc.parallelize(Seq(1,2,3,4,5,6)).toDF(""nums"")


val cubeDF = numsDF
    .cube(""nums"")
    .agg(
        max(lit(0)).as(""agcol""),
        grouping_id().as(""gid"")
    )
    
val group0 = cubeDF.filter(col(""gid"") <=> lit(0))
val group1 = cubeDF.filter(col(""gid"") <=> lit(1))

cubeDF.printSchema
group0.printSchema
group1.printSchema


//Recreating cubeDf
cubeDF.select(""nums"").distinct
    .join(group0, Seq(""nums""), ""inner"")
    .join(group1, Seq(""nums""), ""inner"")
    .show
{code}
*Sample output:*
{code:java}
numsDF: org.apache.spark.sql.DataFrame = [nums: int]
cubeDF: org.apache.spark.sql.DataFrame = [nums: int, agcol: int ... 1 more field]
group0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [nums: int, agcol: int ... 1 more field]
group1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [nums: int, agcol: int ... 1 more field]
root
 |-- nums: integer (nullable = true)
 |-- agcol: integer (nullable = true)
 |-- gid: integer (nullable = false)
root
 |-- nums: integer (nullable = true)
 |-- agcol: integer (nullable = true)
 |-- gid: integer (nullable = false)
root
 |-- nums: integer (nullable = true)
 |-- agcol: integer (nullable = true)
 |-- gid: integer (nullable = false)
org.apache.spark.sql.AnalysisException:
Failure when resolving conflicting references in Join:
'Join Inner
:- Deduplicate [nums#220]
:  +- Project [nums#220]
:     +- Aggregate [nums#220, spark_grouping_id#218], [nums#220, max(0) AS agcol#216, spark_grouping_id#218 AS gid#217]
:        +- Expand [List(nums#212, nums#219, 0), List(nums#212, null, 1)], [nums#212, nums#220, spark_grouping_id#218]
:           +- Project [nums#212, nums#212 AS nums#219]
:              +- Project [value#210 AS nums#212]
:                 +- SerializeFromObject [input[0, int, false] AS value#210]
:                    +- ExternalRDD [obj#209]
+- Filter (gid#217 <=> 0)
   +- Aggregate [nums#220, spark_grouping_id#218], [nums#220, max(0) AS agcol#216, spark_grouping_id#218 AS gid#217]
      +- Expand [List(nums#212, nums#219, 0), List(nums#212, null, 1)], [nums#212, nums#220, spark_grouping_id#218]
         +- Project [nums#212, nums#212 AS nums#219]
            +- Project [value#210 AS nums#212]
               +- SerializeFromObject [input[0, int, false] AS value#210]
                  +- ExternalRDD [obj#209]
Conflicting attributes: nums#220
;;
'Join Inner
:- Deduplicate [nums#220]
:  +- Project [nums#220]
:     +- Aggregate [nums#220, spark_grouping_id#218], [nums#220, max(0) AS agcol#216, spark_grouping_id#218 AS gid#217]
:        +- Expand [List(nums#212, nums#219, 0), List(nums#212, null, 1)], [nums#212, nums#220, spark_grouping_id#218]
:           +- Project [nums#212, nums#212 AS nums#219]
:              +- Project [value#210 AS nums#212]
:                 +- SerializeFromObject [input[0, int, false] AS value#210]
:                    +- ExternalRDD [obj#209]
+- Filter (gid#217 <=> 0)
   +- Aggregate [nums#220, spark_grouping_id#218], [nums#220, max(0) AS agcol#216, spark_grouping_id#218 AS gid#217]
      +- Expand [List(nums#212, nums#219, 0), List(nums#212, null, 1)], [nums#212, nums#220, spark_grouping_id#218]
         +- Project [nums#212, nums#212 AS nums#219]
            +- Project [value#210 AS nums#212]
               +- SerializeFromObject [input[0, int, false] AS value#210]
                  +- ExternalRDD [obj#209]
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:96)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:335)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:96)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:109)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:202)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:106)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:68)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)
  at org.apache.spark.sql.Dataset.join(Dataset.scala:939)
  ... 46 elided
{code}",,cloud_fan,dongjoon,imback82,sandeshyapuram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 14 07:35:37 UTC 2019,,,,,,,,,,"0|z084kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 09:07;sandeshyapuram;I have reproduced this in spark-submit as well as cubole notebooks.

I'm not sure how I can provide you with a self reproducer;;;","31/Oct/19 10:32;cloud_fan;[~imback82] do you want to look into it?;;;","31/Oct/19 15:54;imback82;Sure, I will look into this. Thanks for pinging me.;;;","02/Nov/19 03:26;sandeshyapuram;[~imback82]  & [~cloud_fan] Currently I've worked my way around by renaming every column in the dataframes to perform joins and that works.

Let me know if you have a better workaround to deal with it.;;;","05/Nov/19 02:50;imback82;[~sandeshyapuram] Glad that you found the workaround. I am still looking into this. I will update this thread when I find more about it. Thanks for the patience.;;;","14/Nov/19 07:00;dongjoon;This is resolved via https://github.com/apache/spark/pull/26441;;;","14/Nov/19 07:35;sandeshyapuram;Thanks!;;;",,,,,,,,,,,,,,,,,
"Spark Application UI- environment tab field ""value"" sort is not working ",SPARK-29681,13265478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prakharjain09,jobitmathew,jobitmathew,31/Oct/19 08:43,26/Nov/19 09:59,13/Jul/23 08:49,24/Nov/19 02:09,2.4.4,3.0.0,,,,,,,,,3.0.0,,,Web UI,,,,,0,,,,"Spark Application UI-

In environment tab, field ""value"" sort is not working if we do sort in ascending or descending order.

!Ascend.png|width=854,height=475!

 

!DESCEND.png|width=854,height=470!

 ",,dongjoon,jobitmathew,prakharjain09,rakson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 08:57;jobitmathew;Ascend.png;https://issues.apache.org/jira/secure/attachment/12984449/Ascend.png","31/Oct/19 08:58;jobitmathew;DESCEND.png;https://issues.apache.org/jira/secure/attachment/12984450/DESCEND.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 24 02:09:56 UTC 2019,,,,,,,,,,"0|z084k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 08:45;rakson;I will work on this one
;;;","31/Oct/19 09:16;jobitmathew;Issue observed in Spark Properties,System Properties.So please check in all properties tables;;;","24/Nov/19 02:09;dongjoon;Issue resolved by pull request 26638
[https://github.com/apache/spark/pull/26638];;;",,,,,,,,,,,,,,,,,,,,,
Release script fail to publish release under dry run mode,SPARK-29666,13265359,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,30/Oct/19 18:17,30/Oct/19 23:16,13/Jul/23 08:49,30/Oct/19 22:01,3.0.0,,,,,,,,,,2.4.5,3.0.0,,Build,,,,,0,,,,"`release-build.sh` fail to publish release under dry run mode with the following error message:
{code}
/opt/spark-rm/release-build.sh: line 429: pushd: spark-repo-g4MBm/org/apache/spark: No such file or directory
{code}",,dongjoon,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 22:01:08 UTC 2019,,,,,,,,,,"0|z083tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Oct/19 22:01;dongjoon;Issue resolved by pull request 26329
[https://github.com/apache/spark/pull/26329];;;",,,,,,,,,,,,,,,,,,,,,,,
Column.getItem behavior is not consistent with Scala version,SPARK-29664,13265342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,30/Oct/19 16:57,12/Dec/22 18:10,13/Jul/23 08:49,01/Nov/19 03:26,3.0.0,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"In PySpark, Column.getItem's behavior is different from the Scala version.

For example,
In PySpark:
{code:python}
df = spark.range(2)
map_col = create_map(lit(0), lit(100), lit(1), lit(200))
df.withColumn(""mapped"", map_col.getItem(col('id'))).show()
# +---+------+
# | id|mapped|
# +---+------+
# |  0|   100|
# |  1|   200|
# +---+------+
{code}

In Scala:
{code:scala}
val df = spark.range(2)
val map_col = map(lit(0), lit(100), lit(1), lit(200))
// The following getItem results in the following exception, which is the right behavior:
// java.lang.RuntimeException: Unsupported literal type class org.apache.spark.sql.Column id
//  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:78)
//  at org.apache.spark.sql.Column.getItem(Column.scala:856)
//  ... 49 elided
df.withColumn(""mapped"", map_col.getItem(col(""id""))).show


// You have to use apply() to match with PySpark's behavior.
df.withColumn(""mapped"", map_col(col(""id""))).show
// +---+------+
// | id|mapped|
// +---+------+
// |  0|   100|
// |  1|   200|
// +---+------+
{code}

Looking at the code for Scala implementation, PySpark's behavior is incorrect since the argument to getItem becomes `Literal`.",,apachespark,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 07:46:06 UTC 2020,,,,,,,,,,"0|z083ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Oct/19 16:59;imback82;[~hyukjin.kwon], is there a reason why getItem's behavior is not consistent? If this behavior is a bug, I will prepare a PR shortly.;;;","31/Oct/19 04:48;gurwls223;Can we fix Python side to match with Scala side?;;;","31/Oct/19 04:49;gurwls223;We will have to update migration guide at https://github.com/apache/spark/blob/master/docs/pyspark-migration-guide.md and show the workaround ({{df[...]}}) in the docstring of {{getItem}} in PySpark.;;;","31/Oct/19 04:50;imback82;OK, will do. Thanks!;;;","01/Nov/19 03:26;gurwls223;Issue resolved by pull request 26351
[https://github.com/apache/spark/pull/26351];;;","27/Jul/20 07:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28327;;;",,,,,,,,,,,,,,,,,,
Fix MICROS_PER_MONTH in IntervalUtils,SPARK-29653,13265245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,30/Oct/19 09:35,30/Oct/19 15:09,13/Jul/23 08:49,30/Oct/19 15:09,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"- final val MICROS_PER_MONTH: Long = DAYS_PER_MONTH * DateTimeUtils.SECONDS_PER_DAY
+ final val MICROS_PER_MONTH: Long = DAYS_PER_MONTH * DateTimeUtils.MICROS_PER_DAY",,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 15:09:42 UTC 2019,,,,,,,,,,"0|z08348:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Oct/19 15:09;dongjoon;Issue resolved by pull request 26321
[https://github.com/apache/spark/pull/26321];;;",,,,,,,,,,,,,,,,,,,,,,,
Incorrect parsing of interval seconds fraction,SPARK-29651,13265220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,30/Oct/19 07:47,08/Nov/19 03:53,13/Jul/23 08:49,31/Oct/19 01:22,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,,,,,,2.4.5,3.0.0,,SQL,,,,,0,correctness,,,"* The fractional part of interval seconds unit is incorrectly parsed if the number of digits is less than 9, for example:
{code}
spark-sql> select interval '10.123456 seconds';
interval 10 seconds 123 microseconds
{code}
The result must be *interval 10 seconds 123 milliseconds 456 microseconds*

* If the seconds unit of an interval is negative, it is incorrectly converted to `CalendarInterval`, for example:
{code}
spark-sql> select interval '-10.123456789 seconds';
interval -9 seconds -876 milliseconds -544 microseconds
{code}
Taking into account truncation to microseconds, the result must be *interval -10 seconds -123 milliseconds -456 microseconds*",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 01:22:02 UTC 2019,,,,,,,,,,"0|z082yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 01:22;cloud_fan;Issue resolved by pull request 26313
[https://github.com/apache/spark/pull/26313];;;",,,,,,,,,,,,,,,,,,,,,,,
Use Python 3.7 in GitHub Action to recover lint-python,SPARK-29647,13265201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,30/Oct/19 05:53,12/Dec/22 18:10,13/Jul/23 08:49,30/Oct/19 06:28,2.4.5,,,,,,,,,,2.4.5,,,PySpark,Tests,,,,0,,,,"`branch-2.4` seems incompatible with Python 3.7.
Currently, GitHub Action on `branch-2.4` is broken.
This issue aims to recover the GitHub Action `lint-python` first.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 06:28:23 UTC 2019,,,,,,,,,,"0|z082ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Oct/19 06:28;gurwls223;Issue resolved by pull request 26308
[https://github.com/apache/spark/pull/26308];;;",,,,,,,,,,,,,,,,,,,,,,,
ContinuousMemoryStream throws error on String type,SPARK-29642,13265135,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,29/Oct/19 22:10,06/Nov/19 20:17,13/Jul/23 08:49,06/Nov/19 18:37,3.0.0,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"While we can set String as a generic type of ContinuousMemoryStream, it doesn't work really because it doesn't convert String to UTFString and accessing it from Row interface would throw error.

We should encode the input and convert the input to Row properly.",,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 18:37:33 UTC 2019,,,,,,,,,,"0|z082fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Nov/19 18:37;vanzin;Issue resolved by pull request 26300
[https://github.com/apache/spark/pull/26300];;;",,,,,,,,,,,,,,,,,,,,,,,
SHS Endpoint /applications/<app_id>/jobs/ doesn't include description,SPARK-29637,13265003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,29/Oct/19 10:29,29/Oct/19 19:06,13/Jul/23 08:49,29/Oct/19 19:06,2.3.4,2.4.4,3.0.0,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,1,,,,"Starting from Spark 2.3, the SHS REST API endpoint /applications/<app_id>/jobs/  is not including description in the JobData returned. This is not the case until Spark 2.2.

Steps to reproduce:
 * Open spark-shell
{code:java}
scala> sc.setJobGroup(""test"", ""job"", false); 
scala> val foo = sc.textFile(""/user/foo.txt"");
foo: org.apache.spark.rdd.RDD[String] = /user/foo.txt MapPartitionsRDD[1] at textFile at <console>:24
scala> foo.foreach(println);
{code}

 * Access end REST API [http://SHS-host:port/api/v1/applications/|http://shs-host:port/api/v1/applications/]<app-id>/jobs/
 * REST API of Spark 2.3 and above will not return description",,gsomogyi,richardatcloudera,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 19:06:44 UTC 2019,,,,,,,,,,"0|z081mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Oct/19 10:32;gsomogyi;I'm working on this.;;;","29/Oct/19 19:06;vanzin;Issue resolved by pull request 26295
[https://github.com/apache/spark/pull/26295];;;",,,,,,,,,,,,,,,,,,,,,,
array_contains should allow column instances in PySpark,SPARK-29627,13264940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,gurwls223,,29/Oct/19 03:16,12/Dec/22 18:10,13/Jul/23 08:49,30/Oct/19 00:47,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"Scala API works well with column instances:

{code}
import org.apache.spark.sql.functions._
val df = Seq(Array(""a"", ""b"", ""c""), Array.empty[String]).toDF(""data"")
df.select(array_contains($""data"", lit(""a""))).collect()
{code}

{code}
Array[org.apache.spark.sql.Row] = Array([true], [false])
{code}

However, seems PySpark one doesn't:

{code}
from pyspark.sql.functions import array_contains, lit
df = spark.createDataFrame([([""a"", ""b"", ""c""],), ([],)], ['data'])
df.select(array_contains(df.data, lit(""a""))).show()
{code}

{code}
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""/.../spark/python/pyspark/sql/functions.py"", line 1950, in array_contains
 return Column(sc._jvm.functions.array_contains(_to_java_column(col), value))
 File ""/.../spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1277, in __call__
 File ""/.../spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1241, in _build_args
 File ""/.../spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1228, in _get_args
 File ""/.../spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_collections.py"", line 500, in convert
 File ""/.../spark/python/pyspark/sql/column.py"", line 344, in __iter__
 raise TypeError(""Column is not iterable"")
TypeError: Column is not iterable
{code}

We should let it allow",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 00:47:41 UTC 2019,,,,,,,,,,"0|z0818g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Oct/19 00:47;gurwls223;Issue resolved by pull request 26288
[https://github.com/apache/spark/pull/26288];;;",,,,,,,,,,,,,,,,,,,,,,,
"Jenkins fails with ""Python versions prior to 2.7 are not supported.""",SPARK-29624,13264867,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,shaneknapp,dongjoon,dongjoon,28/Oct/19 17:40,12/Dec/22 18:10,13/Jul/23 08:49,28/Oct/19 19:23,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,"- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112777/console
- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112779/console

{code}
...
+ ./dev/run-tests-jenkins
Python versions prior to 2.7 are not supported.
Build step 'Execute shell' marked build as failure
{code}",,dongjoon,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 01:48:41 UTC 2019,,,,,,,,,,"0|z080s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Oct/19 17:41;dongjoon;Hi, [~shaneknapp].
Jenkins server seems to be changed. Could you take a look at this issue?
cc [~cloud_fan] and [~hyukjin.kwon];;;","28/Oct/19 17:46;shaneknapp;nothing changed...  the PATH env vars for each worker got borked during the downtime.

i'll need to restart jenkins, and will send a note to the list about this.;;;","28/Oct/19 18:13;shaneknapp;alright, i triggered a job and checked the console, and the restart seemed to fix the PATH variable.

both of these above builds failed on amp-jenkins-worker-03, so i'll keep an eye on that worker.;;;","28/Oct/19 19:12;dongjoon;Thank you so much!;;;","28/Oct/19 19:22;shaneknapp;alright, a pull request build has successfully made it past this check:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112790/

resolving now.;;;","29/Oct/19 01:48;gurwls223;Awesome [~shaneknapp]!;;;",,,,,,,,,,,,,,,,,,
UnsafeKVExternalSorterSuite failure on bigendian system,SPARK-29620,13264807,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,salamani,salamani,28/Oct/19 13:15,27/Mar/20 11:04,13/Jul/23 08:49,27/Mar/20 11:04,2.4.4,,,,,,,,,,,,,Spark Shell,,,,,0,,,,"{code}
spark/sql/core# ../../build/mvn -Dtest=none -DwildcardSuites=org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite test
{code}

{code}
UnsafeKVExternalSorterSuite:
12:24:24.305 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
- kv sorting key schema [] and value schema [] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (4) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [int] and value schema [] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (20) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [] and value schema [int] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (20) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [int] and value schema [float,float,double,string,float] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (2732) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [double,string,string,int,float,string,string] and value schema [double,int,string,int,double,string,double] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (4548) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [int,string,float,int,int,string] and value schema [double,float,float,string,string,double,float,float,float,float] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (2764) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [string,double,int,int,string,string] and value schema [int,int,string,float,float,double,double] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (2796) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [int,float,float,int,float,float,int,int,float,int] and value schema [double,float,float,double] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (132) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting key schema [double,int,string,double,float,float] and value schema [float,string] *** FAILED ***
 java.lang.AssertionError: sizeInBytes (396) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter$1.apply(UnsafeKVExternalSorterSuite.scala:86)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- kv sorting with records that exceed page size *** FAILED ***
 java.lang.AssertionError: sizeInBytes (36) should be a multiple of 8
 at org.apache.spark.sql.catalyst.expressions.UnsafeRow.pointTo(UnsafeRow.java:168)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator.next(UnsafeKVExternalSorter.java:297)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.org$apache$spark$sql$execution$UnsafeKVExternalSorterSuite$$testKVSorter(UnsafeKVExternalSorterSuite.scala:145)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$2.apply$mcV$sp(UnsafeKVExternalSorterSuite.scala:201)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$2.apply(UnsafeKVExternalSorterSuite.scala:183)
 at org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite$$anonfun$2.apply(UnsafeKVExternalSorterSuite.scala:183)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
 at org.scalatest.Transformer.apply(Transformer.scala:22)
 at org.scalatest.Transformer.apply(Transformer.scala:20)
 ...
- SPARK-23376: Create UnsafeKVExternalSorter with BytesToByteMap having duplicated keys
12:24:32.974 WARN org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite:

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.execution.UnsafeKVExternalSorterSuite, thread names: MemoryMXBean notification dispatcher, process reaper, org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner =====

Run completed in 25 seconds, 347 milliseconds.
Total number of tests run: 11
Suites: completed 2, aborted 0
Tests: succeeded 1, failed 10, canceled 0, ignored 0, pending 0
*** 10 TESTS FAILED ***
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:10 min
[INFO] Finished at: 2019-10-28T12:24:33Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:1.0:test (test) on project spark-sql_2.11: There are test failures -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
{code}",,salamani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 11:04:52 UTC 2020,,,,,,,,,,"0|z080ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Mar/20 11:04;salamani;Passed rerun in new system;;;",,,,,,,,,,,,,,,,,,,,,,,
Failure of DateTimeUtilsSuite and TimestampFormatterSuite,SPARK-29614,13264664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,27/Oct/19 09:59,28/Oct/19 17:48,13/Jul/23 08:49,27/Oct/19 20:49,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"* https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-3.2/653/
* https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112721/testReport/",,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 27 20:49:12 UTC 2019,,,,,,,,,,"0|z07zj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Oct/19 20:49;dongjoon;Issue resolved by pull request 26273
[https://github.com/apache/spark/pull/26273];;;",,,,,,,,,,,,,,,,,,,,,,,
SessionState is initialized with isolated classloader for Hive if spark.sql.hive.metastore.jars is being set,SPARK-29604,13264478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,25/Oct/19 12:24,03/Dec/19 23:53,13/Jul/23 08:49,30/Oct/19 08:10,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"I've observed the issue that external listeners cannot be loaded properly when we run spark-sql with ""spark.sql.hive.metastore.jars"" configuration being used.

{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1102)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:154)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:153)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:153)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:150)
	at org.apache.spark.sql.SparkSession$$anonfun$1$$anonfun$apply$2.apply(SparkSession.scala:104)
	at org.apache.spark.sql.SparkSession$$anonfun$1$$anonfun$apply$2.apply(SparkSession.scala:104)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.sql.SparkSession$$anonfun$1.apply(SparkSession.scala:104)
	at org.apache.spark.sql.SparkSession$$anonfun$1.apply(SparkSession.scala:103)
	at org.apache.spark.sql.internal.SQLConf$.get(SQLConf.scala:149)
	at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$client(HiveClientImpl.scala:282)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:247)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:246)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:296)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:386)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:53)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:315)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:847)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:922)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:931)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.SparkException: Exception when registering StreamingQueryListener
	at org.apache.spark.sql.streaming.StreamingQueryManager.<init>(StreamingQueryManager.scala:70)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.streamingQueryManager(BaseSessionStateBuilder.scala:260)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:296)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1099)
	... 40 more
Caused by: java.lang.ClassNotFoundException: com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:193)
	at org.apache.spark.util.Utils$$anonfun$loadExtensions$1.apply(Utils.scala:2640)
	at org.apache.spark.util.Utils$$anonfun$loadExtensions$1.apply(Utils.scala:2638)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.util.Utils$.loadExtensions(Utils.scala:2638)
	at org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$1.apply(StreamingQueryManager.scala:62)
	at org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$1.apply(StreamingQueryManager.scala:61)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.sql.streaming.StreamingQueryManager.<init>(StreamingQueryManager.scala:61)
	... 43 more
 
{noformat}

",,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 15:30:09 UTC 2019,,,,,,,,,,"0|z07yds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Oct/19 12:25;kabhwan;I've figured out the root cause and have a patch. Will submit a patch soon. I may need some more time to craft a relevant test.;;;","30/Oct/19 08:10;dongjoon;This is resolved via https://github.com/apache/spark/pull/26258;;;","30/Oct/19 09:39;dongjoon;This lands at `branch-2.4` via https://github.com/apache/spark/pull/26316 .;;;","30/Oct/19 09:44;dongjoon;BTW, [~kabhwan]. Could you check the old version (at least `2.3.x`) and update `Affects Version/s:`, too?;;;","30/Oct/19 12:57;kabhwan;I think it doesn't apply to branch-2.3 as the root issue is more alike lazy initialization of streaming query listeners and there's no configuration for registering streaming query listeners in Spark 2.3. (only API);;;","30/Oct/19 17:35;dongjoon;Thank you so much for confirming, [~kabhwan]!

The newly added test case seems to be flaky in `SBT Hadoop 3.2` build. Could you check that?

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-3.2/676/testReport/org.apache.spark.sql.hive.thriftserver/SparkSQLEnvSuite/SPARK_29604_external_listeners_should_be_initialized_with_Spark_classloader/history/;;;","30/Oct/19 22:20;kabhwan;[~dongjoon]
Do we have any annotation/trait to ""isolate"" running test suite? I'm suspecting session, or listeners in session is being modified from other tests running concurrently.;;;","31/Oct/19 06:37;dongjoon;Unfortunately, I don't know~ 

The session theory makes sense to me.

In addition to that, according to the log, if some test fails, this test suite seems to fail together.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-3.2/682/testReport/junit/org.apache.spark.sql.hive.thriftserver/SparkSQLEnvSuite/SPARK_29604_external_listeners_should_be_initialized_with_Spark_classloader/history/

please see `SBT Hadoop-3.2` Jenkins job.;;;","31/Oct/19 07:45;kabhwan;I've manually ran the test suite locally (single run) and it passed 3 times sequentially.

{code}
$ java -version
openjdk version ""11.0.2"" 2019-01-15
OpenJDK Runtime Environment 18.9 (build 11.0.2+9)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)
{code}

{code}
$ build/sbt ""hive-thriftserver/testOnly *.SparkSQLEnvSuite"" -Phadoop-3.2 -Phive-thriftserver
...
[info] SparkSQLEnvSuite:
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/jlim/WorkArea/ScalaProjects/spark/common/unsafe/target/scala-2.12/spark-unsafe_2.12-3.0.0-SNAPSHOT.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[info] - SPARK-29604 external listeners should be initialized with Spark classloader (2 minutes, 26 seconds)
[info] ScalaTest
[info] Run completed in 2 minutes, 30 seconds.
[info] Total number of tests run: 1
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 1, Failed 0, Errors 0, Passed 1
[success] Total time: 319 s, completed Oct 31, 2019, 4:30:34 PM
{code}

Maybe I should add this suite to `testsWhichShouldRunInTheirOwnDedicatedJvm` - I cannot find any other way to isolate the test suite.;;;","31/Oct/19 15:30;dongjoon;Thank you for keeping working on this.
Yes. It passed locally. That's the reason why I didn't revert this patch until now.
But, we are on 3.0.0-preview voting. In the worst case, we need to revert this.
;;;",,,,,,,,,,,,,,
array_contains built in function is not backward compatible in 3.0,SPARK-29600,13264419,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aman_omer,abhishek.akg,abhishek.akg,25/Oct/19 06:28,12/Dec/22 18:10,13/Jul/23 08:49,17/Dec/19 17:31,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"{code}
SELECT array_contains(array(0,0.1,0.2,0.3,0.5,0.02,0.033), .2); 
{code}

throws Exception in 3.0 where as in 2.3.2 is working fine.

Spark 3.0 output:

{code}
0: jdbc:hive2://10.18.19.208:23040/default> SELECT array_contains(array(0,0.1,0.2,0.3,0.5,0.02,0.033), .2);
 Error: org.apache.spark.sql.AnalysisException: cannot resolve 'array_contains(array(CAST(0 AS DECIMAL(13,3)), CAST(0.1BD AS DECIMAL(13,3)), CAST(0.2BD AS DECIMAL(13,3)), CAST(0.3BD AS DECIMAL(13,3)), CAST(0.5BD AS DECIMAL(13,3)), CAST(0.02BD AS DECIMAL(13,3)), CAST(0.033BD AS DECIMAL(13,3))), 0.2BD)' due to data type mismatch: Input to function array_contains should have been array followed by a value with same element type, but it's [array<decimal(13,3)>, decimal(1,1)].; line 1 pos 7;
 'Project [unresolvedalias(array_contains(array(cast(0 as decimal(13,3)), cast(0.1 as decimal(13,3)), cast(0.2 as decimal(13,3)), cast(0.3 as decimal(13,3)), cast(0.5 as decimal(13,3)), cast(0.02 as decimal(13,3)), cast(0.033 as decimal(13,3))), 0.2), None)]
{code}

Spark 2.3.2 output

{code}
0: jdbc:hive2://10.18.18.214:23040/default> SELECT array_contains(array(0,0.1,0.2,0.3,0.5,0.02,0.033), .2);
|array_contains(array(CAST(0 AS DECIMAL(13,3)), CAST(0.1 AS DECIMAL(13,3)), CAST(0.2 AS DECIMAL(13,3)), CAST(0.3 AS DECIMAL(13,3)), CAST(0.5 AS DECIMAL(13,3)), CAST(0.02 AS DECIMAL(13,3)), CAST(0.033 AS DECIMAL(13,3))), CAST(0.2 AS DECIMAL(13,3)))|
|true|

1 row selected (0.18 seconds)
{code}

 

 ",,abhishek.akg,aman_omer,cloud_fan,ksunitha,Udbhav Agrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 17:37:01 UTC 2019,,,,,,,,,,"0|z07y0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Oct/19 06:29;Udbhav Agrawal;I will check this issue
;;;","28/Oct/19 12:25;gurwls223;Please go ahead but please just don't copy and paste. Narrow down the problem with analysis otherwise no one can investigate.;;;","05/Nov/19 11:59;gurwls223;[~Udbhav Agrawal]did you made some progresses on this?;;;","05/Nov/19 14:28;Udbhav Agrawal;Yes [~hyukjin.kwon] ,
Hi [~abhishek.akg] please check there was a behavior change in array_contains function, which has been documented in [sql-migration-guide|https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md#upgrading-from-spark-sql-23-to-24]  ;;;","07/Nov/19 04:15;abhishek.akg;[~Udbhav Agrawal]: I gone through the migration guide and it is more talking about the ""In Spark 2.4, AnalysisException is thrown since integer type can not be promoted to string type in a loss-less manner."" which is not the case in the above query.;;;","18/Nov/19 06:50;Udbhav Agrawal;Hi [~hyukjin.kwon], this failure is because we cannot cast the literal to array type after the above behavior change. For example:
array(0.1,0.2,0.33) is type decimal(2,2) and literal 0.1 and 0.2 is also changed to decimal(2,2) but if we check 0.2 which actually is of type decimal(1,1) this query fails its data type doesn't match with array's data type.;;;","17/Dec/19 17:31;cloud_fan;Issue resolved by pull request 26811
[https://github.com/apache/spark/pull/26811];;;","17/Dec/19 17:37;aman_omer;ID: aman_omer
[~cloud_fan];;;",,,,,,,,,,,,,,,,
Unify the behavior of pyspark.TaskContext with spark core,SPARK-29582,13264161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coneyliu,coneyliu,coneyliu,24/Oct/19 02:49,12/Dec/22 18:10,13/Jul/23 08:49,31/Oct/19 04:11,2.4.4,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"In Spark Core, there is a `TaskContext` object which is a singleton. We set a task context instance which can be TaskContext or BarrierTaskContext before the task function startup, and unset it to none after the function end. So we can both get TaskContext and BarrierTaskContext with the object. How we can only get the BarrierTaskContext with `BarrierTaskContext`, we will get `None` if we get it by `TaskContext.get` in a barrier stage.

 

In this patch, we unify the behavior of TaskContext for pyspark with Spark core. This is useful when people switch from normal code to barrier code, and only need a little update.",,coneyliu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 04:11:55 UTC 2019,,,,,,,,,,"0|z07wfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 04:11;gurwls223;Issue resolved by pull request 26239
[https://github.com/apache/spark/pull/26239];;;",,,,,,,,,,,,,,,,,,,,,,,
Add kerberos debug messages for Kafka secure tests,SPARK-29580,13264142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,dongjoon,dongjoon,23/Oct/19 23:24,28/Oct/19 08:55,13/Jul/23 08:49,25/Oct/19 21:12,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,"- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112562/testReport/org.apache.spark.sql.kafka010/KafkaDelegationTokenSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

{code}
sbt.ForkMain$ForkError: org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:407)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:55)
	at org.apache.spark.sql.kafka010.KafkaTestUtils.setupEmbeddedKafkaServer(KafkaTestUtils.scala:227)
	at org.apache.spark.sql.kafka010.KafkaTestUtils.setup(KafkaTestUtils.scala:249)
	at org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.beforeAll(KafkaDelegationTokenSuite.scala:49)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Server not found in Kerberos database (7) - Server not found in Kerberos database
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:160)
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:146)
	at org.apache.kafka.common.network.ChannelBuilders.clientChannelBuilder(ChannelBuilders.java:67)
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:99)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:382)
	... 16 more
Caused by: sbt.ForkMain$ForkError: javax.security.auth.login.LoginException: Server not found in Kerberos database (7) - Server not found in Kerberos database
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:804)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
	at org.apache.kafka.common.security.authenticator.AbstractLogin.login(AbstractLogin.java:60)
	at org.apache.kafka.common.security.kerberos.KerberosLogin.login(KerberosLogin.java:103)
	at org.apache.kafka.common.security.authenticator.LoginManager.<init>(LoginManager.java:61)
	at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:104)
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:149)
	... 20 more
Caused by: sbt.ForkMain$ForkError: sun.security.krb5.KrbException: Server not found in Kerberos database (7) - Server not found in Kerberos database
	at sun.security.krb5.KrbAsRep.<init>(KrbAsRep.java:82)
	at sun.security.krb5.KrbAsReqBuilder.send(KrbAsReqBuilder.java:316)
	at sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:361)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:776)
	... 37 more
Caused by: sbt.ForkMain$ForkError: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
	at sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
	at sun.security.krb5.internal.ASRep.init(ASRep.java:64)
	at sun.security.krb5.internal.ASRep.<init>(ASRep.java:59)
	at sun.security.krb5.KrbAsRep.<init>(KrbAsRep.java:60)
	... 40 more
{code}",,dongjoon,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29027,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 28 08:55:52 UTC 2019,,,,,,,,,,"0|z07wb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Oct/19 23:24;dongjoon;This is a different failure from SPARK-29027.;;;","23/Oct/19 23:26;dongjoon;Hi, [~gsomogyi].
Could you take a look at this failure?;;;","24/Oct/19 15:14;gsomogyi;[~dongjoon] Started to look...
Is it an intermittent problem or comes constantly under some circumstances?;;;","24/Oct/19 16:35;dongjoon;Thank you. I don't know yet, but it's difficult to track when the failure happens at the Suite Selector level, `_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_`.;;;","25/Oct/19 07:39;gsomogyi;I think `It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector` is because the issue happens in `beforeAll`.
 I've tried to reproduce it but no luck until now. Additionally I've taken a look at the jenkins logs but doesn't contain why this happened.
 I think we should add further debug log information in a PR and then
 * trying to reproduce it further
 * wait on jenkins and take a look at the logs when happens again;;;","25/Oct/19 18:02;dongjoon;Thank you for taking a look, [~gsomogyi]. +1 for waiting for the next failure. If that doesn't happen frequently, it's okay to leave this AS-IS status. Your time is precious.;;;","25/Oct/19 21:12;dongjoon;Issue resolved by pull request 26252
[https://github.com/apache/spark/pull/26252];;;","25/Oct/19 21:13;dongjoon;We will open a new JIRA if this happens again. For now, the PR is the best effort we can do.;;;","28/Oct/19 08:55;gsomogyi;[~dongjoon] thanks for taking care, ping me if reproduced and I'll continue the analysis...;;;",,,,,,,,,,,,,,,
"JDK 1.8.0_232 timezone updates cause ""Kwajalein"" test failures again",SPARK-29578,13264129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,23/Oct/19 22:24,30/Jan/20 01:21,13/Jul/23 08:49,24/Oct/19 13:30,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Tests,,,,,0,,,,"I have a report that tests fail in JDK 1.8.0_232 because of timezone changes in (I believe) tzdata2018i or later, per https://www.oracle.com/technetwork/java/javase/tzdata-versions-138805.html:

{{*** FAILED *** with 8634 did not equal 8633 Round trip of 8633 did not work in tz}}

See also https://issues.apache.org/jira/browse/SPARK-24950

I say ""I've heard"" because I can't get this version easily on my Mac. However the fix is so inconsequential that I think we can just make it, to allow this additional variation just as before.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24950,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 01:21:32 UTC 2020,,,,,,,,,,"0|z07w88:",9223372036854775807,,,,,,,,,,,,,2.4.5,3.0.0,,,,,,,,"24/Oct/19 13:30;srowen;Issue resolved by pull request 26236
[https://github.com/apache/spark/pull/26236];;;","30/Jan/20 01:21;dongjoon;This lands at `branch-2.4` via [https://github.com/apache/spark/pull/27386] .;;;",,,,,,,,,,,,,,,,,,,,,,
spark with user provided hadoop doesn't work on kubernetes,SPARK-29574,13264046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sshakeri,wesolows,wesolows,23/Oct/19 15:28,31/Oct/20 21:19,13/Jul/23 08:49,16/Dec/19 18:12,2.4.4,,,,,,,,,,2.4.8,3.0.0,,Kubernetes,Spark Core,,,,0,,,,"When spark-submit is run with image built with ""hadoop free"" spark and user provided hadoop it fails on kubernetes (hadoop libraries are not on spark's classpath). 

I downloaded spark [Pre-built with user-provided Apache Hadoop|https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-without-hadoop.tgz]. 

I created docker image with usage of [docker-image-tool.sh|[https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh]]. 

 

Based on this image (2.4.4-without-hadoop)

I created another one with Dockerfile
{code:java}
FROM spark-py:2.4.4-without-hadoop
ENV SPARK_HOME=/opt/spark/
# This is needed for newer kubernetes versions
ADD https://repo1.maven.org/maven2/io/fabric8/kubernetes-client/4.6.1/kubernetes-client-4.6.1.jar $SPARK_HOME/jars
COPY spark-env.sh /opt/spark/conf/spark-env.sh
RUN chmod +x /opt/spark/conf/spark-env.sh
RUN wget -qO- https://www-eu.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz | tar xz  -C /opt/
ENV HADOOP_HOME=/opt/hadoop-3.2.1
ENV PATH=${HADOOP_HOME}/bin:${PATH}

{code}
Contents of spark-env.sh:
{code:java}
#!/usr/bin/env bash
export SPARK_DIST_CLASSPATH=$(hadoop classpath):$HADOOP_HOME/share/hadoop/tools/lib/*
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
{code}
spark-submit run with image crated this way fails since spark-env.sh is overwritten by [volume created when pod starts|https://github.com/apache/spark/blob/master/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L108]

As quick workaround I tried to modify [entrypoint script|https://github.com/apache/spark/blob/ea8b5df47476fe66b63bd7f7bcd15acfb80bde78/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh] to run spark-env.sh during startup and moving spark-env.sh to a different directory. 
 Driver starts without issues in this setup however, evethough SPARK_DIST_CLASSPATH is set executor is constantly failing:
{code:java}
PS C:\Sandbox\projekty\roboticdrive-analytics\components\docker-images\spark-rda> kubectl logs rda-script-1571835692837-exec-12
++ id -u
+ myuid=0
++ id -g
+ mygid=0
+ set +e
++ getent passwd 0
+ uidentry=root:x:0:0:root:/root:/bin/ash
+ set -e
+ '[' -z root:x:0:0:root:/root:/bin/ash ']'
+ source /opt/spark-env.sh
+++ hadoop classpath
++ export 'SPARK_DIST_CLASSPATH=/opt/hadoop-3.2.1/etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/*:/opt/hadoop-3.2.1/share/hadoop/common/*:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/opt/hadoop-3.2.1/share/hadoop/hdfs/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/*:/opt/hadoop-3.2.1/share/hadoo++ SPARK_DIST_CLASSPATH='/opt/hadoop-3.2.1/etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/*:/opt/hadoop-3.2.1/share/hadoop/common/*:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/opt/hadoop-3.2.1/share/hadoop/hdfs/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/*:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/*:/opt/hadoop-3.2.1/share/hadoop/yarn/*:/opt/hadoop-3.2.1/share/hadoop/tools/lib/*'
++ export LD_LIBRARY_PATH=/opt/hadoop-3.2.1/lib/native
++ LD_LIBRARY_PATH=/opt/hadoop-3.2.1/lib/native
++ echo 'SPARK_DIST_CLASSPATH=/opt/hadoop-3.2.1/etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/*:/opt/hadoop-3.2.1/share/hadoop/common/*:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/opt/hadoop-3.2.1/share/hadoop/hdfs/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/*:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/*:/opt/hadoop-3.2.1/share/hadoop/yarn/*:/opt/hadoop-3.2.1/share/hadoop/tools/lib/*'
SPARK_DIST_CLASSPATH=/opt/hadoop-3.2.1/etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/*:/opt/hadoop-3.2.1/share/hadoop/common/*:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/opt/hadoop-3.2.1/share/hadoop/hdfs/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/opt/hadoop-3.2.1/share/hadoop/mapreduce/*:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/*:/opt/hadoop-3.2.1/share/hadoop/yarn/*:/opt/hadoop-3.2.1/share/hadoop/tools/lib/*
++ echo LD_LIBRARY_PATH=/opt/hadoop-3.2.1/lib/native
+ SPARK_K8S_CMD=executor
LD_LIBRARY_PATH=/opt/hadoop-3.2.1/lib/native
+ case ""$SPARK_K8S_CMD"" in
+ shift 1
+ SPARK_CLASSPATH=':/opt/spark//jars/*'
+ env
+ sed 's/[^=]*=\(.*\)/\1/g'
+ sort -t_ -k4 -n
+ grep SPARK_JAVA_OPT_
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -n '' ']'
+ PYSPARK_ARGS=
+ '[' -n '' ']'
+ R_ARGS=
+ '[' -n '' ']'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ case ""$SPARK_K8S_CMD"" in
+ CMD=(${JAVA_HOME}/bin/java ""${SPARK_EXECUTOR_JAVA_OPTS[@]}"" -Xms$SPARK_EXECUTOR_MEMORY -Xmx$SPARK_EXECUTOR_MEMORY -cp ""$SPARK_CLASSPATH"" org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url $SPARK_DRIVER_URL --executor-id $SPARK_EXECUTOR_ID --cores $SPARK_EXECUTOR_CORES --app-id $SPARK_APPLICATION_ID --hostname $SPARK_EXECUTOR_POD_IP)
+ exec /sbin/tini -s -- /usr/lib/jvm/java-1.8-openjdk/bin/java -Xms3g -Xmx3g -cp ':/opt/spark//jars/*' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@rda-script-1571835692837-driver-svc.default.svc:7078 --executor-id 12 --cores 1 --app-id spark-33382c27389c4b289d79c06d5f631819 --hostname 10.244.2.24
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:186)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:281)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 3 more
{code}",,apachespark,drboyer,vanzin,wesolows,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29882,SPARK-33271,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 31 17:21:52 UTC 2020,,,,,,,,,,"0|z07vps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Oct/19 15:18;wesolows;I investigated the executor issue. It doesn't handle SPARK_DIST_CLASSPATH environment variable because in kubernetes it is simply  {color:#172b4d}org.apache.spark.executor.CoarseGrainedExecutorBackend invoked that does not respect it. For executor to ""see"" user provided hadoop dependencies I modified entrypoint script so in case of SPARK_K8S_CMD executor it would specify classpath with $SPARK_DIST_CLASSPATH{color}
{code:java}
...
executor)
  CMD=(
    ${JAVA_HOME}/bin/java
    ""${SPARK_EXECUTOR_JAVA_OPTS[@]}""
    -Xms$SPARK_EXECUTOR_MEMORY
    -Xmx$SPARK_EXECUTOR_MEMORY
    -cp ""$SPARK_CLASSPATH:$SPARK_DIST_CLASSPATH""
    org.apache.spark.executor.CoarseGrainedExecutorBackend
    --driver-url $SPARK_DRIVER_URL
    --executor-id $SPARK_EXECUTOR_ID
    --cores $SPARK_EXECUTOR_CORES
    --app-id $SPARK_APPLICATION_ID
    --hostname $SPARK_EXECUTOR_POD_IP
  ) {code}
So there are two problems:

Driver doesn't see environment variables from $SPARK_HOME/conf/spark-env.sh  because this gets hidden by mounted config map, and executor doesn't take into account $SPARK_DIST_CLASSPATH at all. ;;;","16/Dec/19 18:12;vanzin;Issue resolved by pull request 26493
[https://github.com/apache/spark/pull/26493];;;","01/Apr/20 19:51;drboyer;Will this or can this change be backported to future versions of 2.4? Doing so would mean that I won't have to manually patch or fork the entrypoint.sh file in my docker images.

 

It's unclear to me if this introduces a backwards-incompatible change or not.;;;","31/Oct/20 17:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30214;;;",,,,,,,,,,,,,,,,,,,,
Fix UT in  AllExecutionsPageSuite class,SPARK-29571,13264032,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ankitraj,Ankitraj,Ankitraj,23/Oct/19 14:16,12/Dec/22 18:10,13/Jul/23 08:49,24/Oct/19 06:58,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,sorting should be successful UT in class AllExecutionsPageSuite failing due to invalid assert condition. Needs to handle this.,,Ankitraj,ankitrajboudh,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 18:30:18 UTC 2019,,,,,,,,,,"0|z07vmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Oct/19 14:16;Ankitraj;i will raise the PR soon;;;","23/Oct/19 19:03;shahid;Could you clarify which UT is failing?;;;","23/Oct/19 20:50;ankitrajboudh;Assert condition is wrong in AllExecutionsPageSuite.scala for testname : ""

sorting should be successful"" , if IllegalArgumentException will occurs then also unit test will pass (actually it should fail.)

 ;;;","24/Oct/19 06:58;gurwls223;Issue resolved by pull request 26234
[https://github.com/apache/spark/pull/26234];;;","06/Nov/19 18:30;Ankitraj;Issue got fixed.;;;",,,,,,,,,,,,,,,,,,,
doc build fails with `/api/scala/lib/jquery.js` doesn't exist,SPARK-29569,13263993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,jiangxb1987,jiangxb1987,23/Oct/19 10:26,12/Dec/22 18:10,13/Jul/23 08:49,23/Oct/19 13:24,3.0.0,,,,,,,,,,3.0.0,,,Build,Documentation,,,,0,,,,"Run `jekyll build` under `./spark/docs`, the command fail with the following error message:
{code}
Making directory api/scala
cp -r ../target/scala-2.12/unidoc/. api/scala
Making directory api/java
cp -r ../target/javaunidoc/. api/java
Updating JavaDoc files for badge post-processing
Copying jquery.js from Scala API to Java API for page post-processing of badges
jekyll 3.8.6 | Error:  No such file or directory @ rb_sysopen - ./api/scala/lib/jquery.js
{code}

This error only happens on master branch, the command works on branch-2.4",,dongjoon,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/19 11:27;Screen Shot 2019-10-23 at 8.25.01 PM.png;https://issues.apache.org/jira/secure/attachment/12983829/Screen+Shot+2019-10-23+at+8.25.01+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 23:49:50 UTC 2019,,,,,,,,,,"0|z07ve0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Oct/19 10:27;jiangxb1987;[~sowen][~dongjoon] Can you take a look at this issue?;;;","23/Oct/19 11:28;gurwls223;I attached the ScalaDoc output from the current master. Seems like, at some point, the documentation style became completely different.;;;","23/Oct/19 11:42;gurwls223;This seems to start to happen after Scala 2.12 upgrade. It seems pretty critical since it's unable to generate the doc ...;;;","23/Oct/19 13:24;jiangxb1987;Fixed by https://github.com/apache/spark/pull/26228;;;","23/Oct/19 17:39;dongjoon;Sorry for being late to the party! Thank you for swift fixing, [~hyukjin.kwon]!
I saw new `3.0.0-preview-rc1` tag. Now, it's ready for vote? :);;;","23/Oct/19 18:58;jiangxb1987;[~dongjoon] Not yet, the release script is still failing, Wenchen and I are investigating more.;;;","23/Oct/19 23:49;dongjoon;Oh.. Too bad. Got it. Thank you for update, [~jiangxb1987].;;;",,,,,,,,,,,,,,,,,
Add typesafe bintray repo for sbt-mima-plugin,SPARK-29560,13263856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,22/Oct/19 18:11,23/Oct/19 05:06,13/Jul/23 08:49,22/Oct/19 23:34,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Build,,,,,0,,,,"GitHub Action detects the following from yesterday (Oct 21, 2019).

- `branch-2.4`: `sbt-mima-plugin:0.1.17` is missing.
- `master`: `sbt-mima-plugin:0.3.0` is missing.

These versions of `sbt-mima-plugin` seems to be removed from the old repo. We need to change the repo location or upgrade this.

{code}
~/A/spark-merge:branch-2.4$ rm -rf ~/.ivy2/

~/A/spark-merge:branch-2.4$ build/sbt scalastyle test:scalastyle
Using /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
Attempting to fetch sbt
Launching sbt from build/sbt-launch-0.13.17.jar
[info] Loading project definition from /Users/dongjoon/APACHE/spark-merge/project
[info] Updating {file:/Users/dongjoon/APACHE/spark-merge/project/}spark-merge-build...
[info] Resolving com.typesafe#sbt-mima-plugin;0.1.17 ...
[warn] 	module not found: com.typesafe#sbt-mima-plugin;0.1.17
[warn] ==== typesafe-ivy-releases: tried
[warn]   https://repo.typesafe.com/typesafe/ivy-releases/com.typesafe/sbt-mima-plugin/scala_2.10/sbt_0.13/0.1.17/ivys/ivy.xml
[warn] ==== sbt-plugin-releases: tried
[warn]   https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.typesafe/sbt-mima-plugin/scala_2.10/sbt_0.13/0.1.17/ivys/ivy.xml
[warn] ==== local: tried
[warn]   /Users/dongjoon/.ivy2/local/com.typesafe/sbt-mima-plugin/scala_2.10/sbt_0.13/0.1.17/ivys/ivy.xml
[warn] ==== public: tried
[warn]   https://repo1.maven.org/maven2/com/typesafe/sbt-mima-plugin_2.10_0.13/0.1.17/sbt-mima-plugin-0.1.17.pom
[warn] ==== local-preloaded-ivy: tried
[warn]   /Users/dongjoon/.sbt/preloaded/com.typesafe/sbt-mima-plugin/0.1.17/ivys/ivy.xml
[warn] ==== local-preloaded: tried
[warn]   file:////Users/dongjoon/.sbt/preloaded/com/typesafe/sbt-mima-plugin_2.10_0.13/0.1.17/sbt-mima-plugin-0.1.17.pom

...
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	::          UNRESOLVED DEPENDENCIES         ::
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	:: com.typesafe#sbt-mima-plugin;0.1.17: not found
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn]
[warn] 	Note: Some unresolved dependencies have extra attributes.  Check that these dependencies exist with the requested attributes.
[warn] 		com.typesafe:sbt-mima-plugin:0.1.17 (scalaVersion=2.10, sbtVersion=0.13)
[warn]
[warn] 	Note: Unresolved dependencies path:
[warn] 		com.typesafe:sbt-mima-plugin:0.1.17 (scalaVersion=2.10, sbtVersion=0.13) (/Users/dongjoon/APACHE/spark-merge/project/plugins.sbt#L18-19)
[warn] 		  +- default:spark-merge-build:0.1-SNAPSHOT (scalaVersion=2.10, sbtVersion=0.13)
sbt.ResolveException: unresolved dependency: com.typesafe#sbt-mima-plugin;0.1.17: not found
{code}

This breaks our Jenkins in `branch-2.4` now.
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.4-test-sbt-hadoop-2.6/611/console",,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 23:34:22 UTC 2019,,,,,,,,,,"0|z07ujk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Oct/19 18:12;dongjoon;Although this is not an Apache Spark issue, but we are affected. (cc [~srowen] and [~hyukjin.kwon]);;;","22/Oct/19 20:41;srowen;Hm. I note that 0.3.0 is the last version that works with sbt 0.13, so we need to find 0.3.0.

It does seem to have disappeared; I presume it was previously at
https://dl.bintray.com/sbt/sbt-plugin-releases/com.typesafe/sbt-mima-plugin/scala_2.10/sbt_0.13/
or under
https://dl.bintray.com/typesafe/ivy-releases/com.typesafe/

It looks like it is still here:
https://dl.bintray.com/typesafe/sbt-plugins/com.typesafe/sbt-mima-plugin/scala_2.10/sbt_0.13/0.3.0/

... so maybe it's a question of adding a new repo for plugin resolution? I don't know how to do that off the top of my head, anyone know SBT better? :);;;","22/Oct/19 21:28;dongjoon;Yes. It does. I'm trying to fix this because this starts to break our Jenkins, too.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.4-test-sbt-hadoop-2.6/611/console;;;","22/Oct/19 21:30;dongjoon;I raised the priority to `Blocker` because Jenkins is broken. We need to recover this as soon as possible to protect the branches from the upcoming commits.;;;","22/Oct/19 23:34;dongjoon;Issue resolved by pull request 26217
[https://github.com/apache/spark/pull/26217];;;",,,,,,,,,,,,,,,,,,,
ResolveTables and ResolveRelations should be order-insensitive,SPARK-29558,13263830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,22/Oct/19 16:10,22/Nov/19 18:05,13/Jul/23 08:49,21/Nov/19 17:48,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,cloud_fan,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 21 18:00:13 UTC 2019,,,,,,,,,,"0|z07uds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Nov/19 18:00;rdblue;Thanks for fixing this, [~cloud_fan]!;;;",,,,,,,,,,,,,,,,,,,,,,,
Avoid including path in error response from REST submission server,SPARK-29556,13263814,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,22/Oct/19 14:05,22/Oct/19 21:23,13/Jul/23 08:49,22/Oct/19 21:15,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"I'm not sure if it's possible to exploit, but, the following code in RESTSubmissionServer's ErrorServlet.service is a little risky as it includes user-supplied path input in the error response. We don't want to let a link determine what's in the resulting HTML.

{code}
val path = request.getPathInfo
...
var msg =
      parts match {
        ...
        case _ =>
          // never reached
          s""Malformed path $path.""
      }
    msg += s"" Please submit requests through http://[host]:[port]/$serverVersion/submissions/...""
    val error = handleError(msg)
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 21:15:37 UTC 2019,,,,,,,,,,"0|z07ua8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Oct/19 21:15;dongjoon;Issue resolved by pull request 26211
[https://github.com/apache/spark/pull/26211];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix the flaky test failed in AdaptiveQueryExecSuite # multiple joins,SPARK-29552,13263776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jk_Self,Jk_Self,Jk_Self,22/Oct/19 11:17,23/Oct/19 20:38,13/Jul/23 08:49,23/Oct/19 17:20,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"AQE will optimize the logical plan once there is query stage finished. So for inner join, when two join side is all small to be the build side. The planner of converting logical plan to physical plan will select the build side as BuildRight if right side finished firstly or BuildLeft if left side finished firstly. In some case, when BuildRight or BuildLeft may introduce additional exchange to the parent node. The revert approach in OptimizeLocalShuffleReader rule may be too conservative, which revert all the local shuffle reader when introduce additional exchange not  revert the local shuffle reader introduced shuffle.  It may be expense to only revert the local shuffle reader introduced shuffle. The workaround is to apply the OptimizeLocalShuffleReader rule again when creating new query stage to further optimize the sub tree shuffle reader to local shuffle reader.",,cloud_fan,Jk_Self,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29538,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 17:20:57 UTC 2019,,,,,,,,,,"0|z07u1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Oct/19 17:20;cloud_fan;Issue resolved by pull request 26207
[https://github.com/apache/spark/pull/26207];;;",,,,,,,,,,,,,,,,,,,,,,,
SparkSession.sql() method parse process not under current sparksession's conf,SPARK-29530,13263465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,21/Oct/19 06:10,25/Oct/19 02:29,13/Jul/23 08:49,22/Oct/19 02:50,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,SparkSession.sql() method parse process not under current sparksession's conf,angerszhuuu,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28527,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 02:50:26 UTC 2019,,,,,,,,,,"0|z07s4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Oct/19 02:50;cloud_fan;Issue resolved by pull request 26187
[https://github.com/apache/spark/pull/26187];;;",,,,,,,,,,,,,,,,,,,,,,,
Incorrect checking of negative intervals,SPARK-29520,13263342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,19/Oct/19 17:14,31/Oct/19 07:35,13/Jul/23 08:49,31/Oct/19 07:35,2.4.4,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"An interval is negative when its duration is negative. The following code checks interval incorrectly:
* https://github.com/apache/spark/blob/f302c2ee6203de36e966fcc58917af4847dff7f2/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala#L163
* https://github.com/apache/spark/blob/d841b33ba3a9b0504597dbccd4b0d11fa810abf3/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L734",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 07:35:53 UTC 2019,,,,,,,,,,"0|z07rdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 07:35;cloud_fan;Issue resolved by pull request 26177
[https://github.com/apache/spark/pull/26177];;;",,,,,,,,,,,,,,,,,,,,,,,
desc extended <table name> <column name> is case sensitive,SPARK-29505,13263021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pavithraramachandran,abhishek.akg,abhishek.akg,18/Oct/19 07:00,24/Dec/19 23:59,13/Jul/23 08:49,24/Dec/19 23:59,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"{code}
create table customer(id int, name String, *CName String*, address String, city String, pin int, country String);
insert into customer values(1,'Alfred','Maria','Obere Str 57','Berlin',12209,'Germany');
insert into customer values(2,'Ana','trujilo','Adva de la','Maxico D.F.',05021,'Maxico');
insert into customer values(3,'Antonio','Antonio Moreno','Mataderos 2312','Maxico D.F.',05023,'Maxico');

analyze table customer compute statistics for columns cname; – *Success( Though cname is not as CName)*

desc extended customer cname; – Failed

jdbc:hive2://10.18.19.208:23040/default> desc extended customer *cname;*
+-----------------+-------------+
| info_name | info_value |
+-----------------+-------------+
| col_name | cname |
| data_type | string |
| comment | NULL |
| min | NULL |
| max | NULL |
| num_nulls | NULL |
| distinct_count | NULL |
| avg_col_len | NULL |
| max_col_len | NULL |
| histogram | NULL |
+-----------------+----------
{code}
 

But 

{code}
desc extended customer CName; – SUCCESS

0: jdbc:hive2://10.18.19.208:23040/default> desc extended customer *CName;*
+-----------------+-------------+
| info_name | info_value |
+-----------------+-------------+
| col_name | CName |
| data_type | string |
| comment | NULL |
| min | NULL |
| max | NULL |
| num_nulls | 0 |
| distinct_count | 3 |
| avg_col_len | 9 |
| max_col_len | 14 |
| histogram | NULL |
+-----------------+-------------+
 {code}

 ",,abhishek.akg,maropu,pavithraramachandran,shivusondur@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 24 23:59:21 UTC 2019,,,,,,,,,,"0|z07pv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Dec/19 08:58;pavithraramachandran;i will work on this;;;","24/Dec/19 23:59;maropu;Resolved by [https://github.com/apache/spark/pull/26927|https://github.com/apache/spark/pull/26927#issuecomment-568774416];;;",,,,,,,,,,,,,,,,,,,,,,
MapObjects doesn't copy Unsafe data when nested under Safe data,SPARK-29503,13262915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,aaronlewism,aaronlewism,17/Oct/19 17:50,02/Mar/20 21:49,13/Jul/23 08:49,23/Oct/19 17:07,2.1.1,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,3.0.0,,,SQL,,,,,0,correctness,,,"In order for MapObjects to operate safely, it checks to see if the result of the mapping function is an Unsafe type (UnsafeRow, UnsafeArrayData, UnsafeMapData) and performs a copy before writing it into MapObjects' output array. This is to protect against expressions which re-use the same native memory buffer to represent its result across evaluations; if the copy wasn't here, all results would be pointing to the same native buffer and would represent the last result written to the buffer. However, MapObjects misses this needed copy if the Unsafe data is nested below some safe structure, for instance a GenericArrrayData whose elements are all UnsafeRows. In this scenario, all elements of the GenericArrayData will be pointing to the same native UnsafeRow buffer which will hold the last value written to it.

 

Right now, this bug seems to only occur when a `ProjectExec` goes down the `execute` path, as opposed to WholeStageCodegen's `produce` and `consume` path.

 

Example Reproduction Code:
{code:scala}
import org.apache.spark.sql.catalyst.expressions.objects.MapObjects
import org.apache.spark.sql.catalyst.expressions.CreateArray
import org.apache.spark.sql.catalyst.expressions.Expression
import org.apache.spark.sql.functions.{array, struct}
import org.apache.spark.sql.Column
import org.apache.spark.sql.types.ArrayType

// For the purpose of demonstration, we need to disable WholeStage codegen
spark.conf.set(""spark.sql.codegen.wholeStage"", ""false"")

val exampleDS = spark.sparkContext.parallelize(Seq(Seq(1, 2, 3))).toDF(""items"")

// Trivial example: Nest unsafe struct inside safe array
// items: Seq[Int] => items.map{item => Seq(Struct(item))}
val result = exampleDS.select(    
    new Column(MapObjects(
        {item: Expression => array(struct(new Column(item))).expr},
        $""items"".expr,
        exampleDS.schema(""items"").dataType.asInstanceOf[ArrayType].elementType
    )) as ""items""
)

result.show(10, false)
{code}
 

Actual Output:
{code:java}
+---------------------------------------------------------+
|items                                                    |
+---------------------------------------------------------+
|[WrappedArray([3]), WrappedArray([3]), WrappedArray([3])]|
+---------------------------------------------------------+
{code}
 

Expected Output:
{code:java}
+---------------------------------------------------------+
|items                                                    |
+---------------------------------------------------------+
|[WrappedArray([1]), WrappedArray([2]), WrappedArray([3])]|
+---------------------------------------------------------+
{code}
 

We've confirmed that the bug exists on version 2.1.1 as well as on master (which I assume corresponds to version 3.0.0?)

 ",,aaronlewism,cloud_fan,kabhwan,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 17:07:54 UTC 2019,,,,,,,,,,"0|z07p7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Oct/19 06:56;kabhwan;Thanks for reporting the issue in super detailed information! I've submitted a PR based on your observation. Please take a look.;;;","23/Oct/19 17:07;cloud_fan;Issue resolved by pull request 26173
[https://github.com/apache/spark/pull/26173];;;",,,,,,,,,,,,,,,,,,,,,,
typed interval expression should fail for invalid format,SPARK-29502,13262903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Oct/19 16:37,18/Oct/19 23:24,13/Jul/23 08:49,18/Oct/19 23:13,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 23:13:04 UTC 2019,,,,,,,,,,"0|z07p4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Oct/19 23:13;yumwang;Issue resolved by pull request 26151
[https://github.com/apache/spark/pull/26151];;;",,,,,,,,,,,,,,,,,,,,,,,
CatalogTable to HiveTable should not change the table's ownership,SPARK-29498,13262809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,Qin Yao,yumwang,17/Oct/19 09:17,29/Apr/20 02:21,13/Jul/23 08:49,21/Oct/19 07:54,2.3.4,2.4.4,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"How to reproduce:

{code:scala}
  test(""CatalogTable to HiveTable should not change the table's ownership"") {
    val catalog = newBasicCatalog()
      val identifier = TableIdentifier(""test_table_owner"", Some(""default""))
      val owner = ""Apache Spark""
      val newTable = CatalogTable(
        identifier,
        tableType = CatalogTableType.EXTERNAL,
        storage = CatalogStorageFormat(
          locationUri = None,
          inputFormat = None,
          outputFormat = None,
          serde = None,
          compressed = false,
          properties = Map.empty),
        owner = owner,
        schema = new StructType().add(""i"", ""int""),
        provider = Some(""hive""))

    catalog.createTable(newTable, false)
    assert(catalog.getTable(""default"", ""test_table_owner"").owner === owner)
  }
{code}


{noformat}
[info] - CatalogTable to HiveTable should not change the table's ownership *** FAILED *** (267 milliseconds)
[info]   ""[yumwang]"" did not equal ""[Apache Spark]"" (HiveExternalCatalogSuite.scala:136)
{noformat}
",,cloud_fan,Qin Yao,yumwang,,,,,,,,,,,,,,,,,,,,,SPARK-29405,,,,SPARK-30261,,,,,,,,,,,,,,,,,SPARK-30261,,,"17/Oct/19 10:00;yumwang;image-2019-10-17-18-00-38-101.png;https://issues.apache.org/jira/secure/attachment/12983256/image-2019-10-17-18-00-38-101.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 21 07:54:38 UTC 2019,,,,,,,,,,"0|z07ok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Oct/19 09:27;yumwang;I'm working on.;;;","17/Oct/19 09:37;Qin Yao;This means the table first creator != first owner?;;;","17/Oct/19 10:01;yumwang;It changes the table's owner:

!image-2019-10-17-18-00-38-101.png!;;;","17/Oct/19 11:14;Qin Yao;I mean create table syntax has no parameter like owner, it should be the creator;;;","17/Oct/19 11:15;Qin Yao;Your case seems not to be a public api to users;;;","21/Oct/19 07:54;cloud_fan;Issue resolved by pull request 26160
[https://github.com/apache/spark/pull/26160];;;",,,,,,,,,,,,,,,,,,
ArrayOutOfBoundsException when converting from string to timestamp,SPARK-29494,13262688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rahulsmahadev,rahulsmahadev,rahulsmahadev,16/Oct/19 18:11,19/Oct/19 23:06,13/Jul/23 08:49,18/Oct/19 21:46,2.4.4,,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"In a couple of scenarios while converting from String to Timestamp `

DateTimeUtils.stringToTimestamp` throws an array out of bounds exception if there is trailing spaces or ':'. The behavior of this method requires it to return `None` in case the format of the string is incorrect.",,rahulsmahadev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 21:46:47 UTC 2019,,,,,,,,,,"0|z07nt4:",9223372036854775807,,,,,zsxwing,,,,,,,,,,,,,,,,,"18/Oct/19 21:46;srowen;Issue resolved by pull request 26143
[https://github.com/apache/spark/pull/26143];;;",,,,,,,,,,,,,,,,,,,,,,,
Reset 'WritableColumnVector' in 'RowToColumnarExec',SPARK-29490,13262547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rongma,rongma,rongma,16/Oct/19 07:56,26/Oct/19 06:13,13/Jul/23 08:49,26/Oct/19 06:12,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"When converting {{Iterator[InternalRow]}} to {{Iterator[ColumnarBatch]}}, the vectors used to create a new {{ColumnarBatch}} should be reset in the iterator's ""next()"" method.",,carsonwang,dongjoon,rongma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 26 06:12:26 UTC 2019,,,,,,,,,,"0|z07mxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Oct/19 17:29;dongjoon;Hi, [~rongma]. Thank you for reporting.
Does this only exist in 3.0.0?;;;","26/Oct/19 06:12;dongjoon;Issue resolved by pull request 26137
[https://github.com/apache/spark/pull/26137];;;",,,,,,,,,,,,,,,,,,,,,,
"In Web UI, stage page has js error when sort table.",SPARK-29488,13262541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cjn082030,cjn082030,cjn082030,16/Oct/19 07:45,23/Oct/19 01:07,13/Jul/23 08:49,22/Oct/19 13:58,2.3.2,2.4.4,,,,,,,,,3.0.0,,,Web UI,,,,,0,,,,"In Web UI, follow the steps below, get js error ""Uncaught TypeError: Failed to execute 'removeChild' on 'Node': parameter 1 is not of type 'Node'."".
 # Click ""Summary Metrics..."" 's tablehead ""Min""
 # Click ""Aggregated Metrics by Executor"" 's tablehead ""Task Time""
 # Click ""Summary Metrics..."" 's tablehead ""Min""（the same as step 1.）

  !image-2019-10-16-15-47-25-212.png!

 ",,cjn082030,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/19 07:47;cjn082030;image-2019-10-16-15-47-25-212.png;https://issues.apache.org/jira/secure/attachment/12983135/image-2019-10-16-15-47-25-212.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 01:05:17 UTC 2019,,,,,,,,,,"0|z07mwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Oct/19 13:58;srowen;Issue resolved by pull request 26136
[https://github.com/apache/spark/pull/26136];;;","23/Oct/19 01:05;cjn082030;Thank you  [~srowen]  for review.;;;",,,,,,,,,,,,,,,,,,,,,,
Floating point literals produce incorrect SQL,SPARK-29468,13262226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,joseph.torres,joseph.torres,14/Oct/19 20:40,24/Oct/19 04:44,13/Jul/23 08:49,16/Oct/19 13:08,2.4.4,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"A FLOAT literal 1.2345 returns SQL `CAST(1.2345 AS FLOAT)`. For very small values this doesn't work; `CAST(1e-44 AS FLOAT)` for example doesn't parse, because the parser tries to squeeze the numeric literal 1e-44 into a DECIMAL(38).",,cloud_fan,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 13:08:52 UTC 2019,,,,,,,,,,"0|z07kyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Oct/19 13:08;cloud_fan;Issue resolved by pull request 26114
[https://github.com/apache/spark/pull/26114];;;",,,,,,,,,,,,,,,,,,,,,,,
"[SS] In streaming aggregation, metric for output rows is not measured in append mode",SPARK-29450,13261984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,13/Oct/19 07:11,12/Dec/22 18:10,13/Jul/23 08:49,19/Dec/19 09:24,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,Structured Streaming,,,,,0,,,,"The bug is reported in dev. mailing list. Credit to [~jlaskowski]. Quoting here:
 
{quote}
I've just noticed that the number of output rows metric of StateStoreSaveExec physical operator does not seem to be measured for Append output mode. In other words, whatever happens before or after StateStoreSaveExec operator the metric is always 0.
 
It is measured for the other modes - Complete and Update.
{quote}

https://lists.apache.org/thread.html/eec318f7ff84c700bffc8bdb7c95c0dcaffb2494ac7ccd7a7c7c6588@%3Cdev.spark.apache.org%3E
",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 09:24:39 UTC 2019,,,,,,,,,,"0|z07jgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Dec/19 09:24;gurwls223;Issue resolved by pull request 26104
[https://github.com/apache/spark/pull/26104];;;",,,,,,,,,,,,,,,,,,,,,,,
Set `default` mode should override the existing mode,SPARK-29442,13261842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,11/Oct/19 17:16,14/Oct/19 20:49,13/Jul/23 08:49,14/Oct/19 20:15,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 14 20:15:55 UTC 2019,,,,,,,,,,"0|z07il4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Oct/19 20:15;dongjoon;This is resolved via https://github.com/apache/spark/pull/26094;;;",,,,,,,,,,,,,,,,,,,,,,,
Failed to get state store in stream-stream join,SPARK-29438,13261766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kabhwan,uncleGen,uncleGen,11/Oct/19 10:35,14/Apr/20 04:27,13/Jul/23 08:49,31/Jan/20 04:22,2.4.4,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"Now, Spark use the `TaskPartitionId` to determine the StateStore path.
{code:java}
TaskPartitionId   \ 
StateStoreVersion  --> StoreProviderId -> StateStore
StateStoreName    /  
{code}
In spark stages, the task partition id is determined by the number of tasks. As we said the StateStore file path depends on the task partition id. So if stream-stream join task partition id is changed against last batch, it will get wrong StateStore data or fail with non-exist StateStore data. In some corner cases, it happened. Following is a sample pseudocode:
{code:java}
val df3 = streamDf1.join(streamDf2)
val df5 = streamDf3.join(batchDf4)
val df = df3.union(df5)
df.writeStream...start()
{code}
A simplified DAG like this:
{code:java}
DataSourceV2Scan   Scan Relation     DataSourceV2Scan   DataSourceV2Scan
 (streamDf3)            |               (streamDf1)        (streamDf2)
     |                  |                   |                 |
  Exchange(200)      Exchange(200)       Exchange(200)     Exchange(200)
     |                  |                   |                 | 
   Sort                Sort                 |                 |
     \                  /                   \                 /
      \                /                     \               /
        SortMergeJoin                    StreamingSymmetricHashJoin
                     \                 /
                       \             /
                         \         /
                            Union
{code}
Stream-Steam join task Id will start from 200 to 399 as they are in the same stage with `SortMergeJoin`. But when there is no new incoming data in `streamDf3` in some batch, it will generate a empty LocalRelation, and then the SortMergeJoin will be replaced with a BroadcastHashJoin. In this case, Stream-Steam join task Id will start from 1 to 200. Finally, it will get wrong StateStore path through TaskPartitionId, and failed with error reading state store delta file.
{code:java}
LocalTableScan   Scan Relation     DataSourceV2Scan   DataSourceV2Scan
     |                  |                   |                 |
BroadcastExchange       |              Exchange(200)     Exchange(200)
     |                  |                   |                 | 
     |                  |                   |                 |
      \                /                     \               /
       \             /                        \             /
      BroadcastHashJoin                 StreamingSymmetricHashJoin
                     \                 /
                       \             /
                         \         /
                            Union
{code}
In my job, I closed the auto BroadcastJoin feature (set spark.sql.autoBroadcastJoinThreshold=-1) to walk around this bug. We should make the StateStore path determinate but not depends on TaskPartitionId.",,kabhwan,tdas,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 31 04:22:56 UTC 2020,,,,,,,,,,"0|z07i48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Oct/19 10:56;uncleGen;There are several optional alternatives to resolve this issue:
 * Adding some rules to make the stream-stream join task partition id more determinate. In above cases, we can reorder the LogicalPlan in Union, i.e. making`StreamingSymmetricHashJoin` prior to `SortMergeJoin/BroadcastHashJoin`. 
 * As said in desc, the StateStore path should not depend on the TaskPartitionId. We may get the StateStore path from StateStoreCoordinator. But this may increase some RPC load in Driver.
 * Dynamically disable the `autoBroadcastJoin` in some rules.;;;","11/Oct/19 13:10;kabhwan;Could you point out the codebase where you are referring? AFAIK it is determined by a pair of (operatorId, partitionId), not a pair of (taskId, partitionId). If there's a chance operatorId  changes, that would be pretty serious bug as state store will load completely wrong state.;;;","11/Oct/19 13:43;uncleGen;[~kabhwan] Yes, the whole statestore path is `CheckpointRoot/operatorId/taskPartitionId/version/`. The `taskPartitionId` may change in some corner case in subsequent batch, that is what I mean.;;;","11/Oct/19 13:49;kabhwan;Could you please link the actual code block from Github repo?

* current master branch
https://github.com/apache/spark/blob/6390f02f9fba059ec5d089a68c8d758aca35c9cd/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala#L267-L280

* current branch-2.4 branch
https://github.com/apache/spark/blob/4f46e8f804cba6d845116cb7daf9b4c682e6a0f1/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala#L267-L280

* current branch-2.3 branch
https://github.com/apache/spark/blob/75cc3b2da9ee0b51ecf0f13169f2b634e36a60c4/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala#L265-L278

using taskId in the path is completely new to me so would like to confirm.;;;","17/Oct/19 05:47;kabhwan;Any updates here?;;;","21/Oct/19 06:30;kabhwan;This would be pretty much easier to reproduce: make left side of union changing its number of partitions at any chance, then right side of union (stream-stream join, no need to be outer join) would throw error.

The chance of having number of partitions changed from left side of union is pretty easy - for example, Kafka streaming source. It doesn't guarantee fixed number of partitions, which means the number of partitions can be changed between batches.

It doesn't seem to hit an edge-case - fairly easy to reproduce, so priority of 'critical' seems OK to me, and even I think it should be a 'blocker' as the query can be crashed at any time.;;;","31/Jan/20 04:22;tdas;Issue resolved by pull request 26162
[https://github.com/apache/spark/pull/26162];;;",,,,,,,,,,,,,,,,,
Spark 3 doesnt work with older shuffle service,SPARK-29435,13261659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,koert,koert,10/Oct/19 22:44,17/May/20 18:30,13/Jul/23 08:49,14/Oct/19 16:22,3.0.0,,,,,,,,,,3.0.0,,,Shuffle,Spark Core,,,,0,,,,"SPARK-27665 introduced a change to the shuffle protocol. It also introduced a setting spark.shuffle.useOldFetchProtocol which would allow spark 3 to run with old shuffle service.

However i have not gotten that to work. I have been testing with Spark 3 master (from Sept 26) and shuffle service from Spark 2.4.1 in yarn.

The errors i see are for example on EMR:
{code}
Error occurred while fetching local blocks
java.nio.file.NoSuchFileException: /mnt1/yarn/usercache/hadoop/appcache/application_1570697024032_0058/blockmgr-d1d009b1-1c95-4e2a-9a71-0ff20078b9a8/38/shuffle_0_0_0.index
{code}

And on CDH5:
{code}
org.apache.spark.shuffle.FetchFailedException: /data/9/hadoop/nm/usercache/koert/appcache/application_1568061697664_8250/blockmgr-57f28014-cdf2-431e-8e11-447ba5c2b2f2/0b/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:596)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:511)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:67)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:850)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:850)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /data/9/hadoop/nm/usercache/koert/appcache/application_1568061697664_8250/blockmgr-57f28014-cdf2-431e-8e11-447ba5c2b2f2/0b/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:204)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:551)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:349)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:391)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:161)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:60)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:172)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	... 11 more
{code}","Spark 3 from Sept 26, commit 8beb736a00b004f97de7fcdf9ff09388d80fc548
Spark 2.4.1 shuffle service in yarn ",cloud_fan,glenn.strycker@gmail.com,koert,sandeep.katta2007,Steven Rand,vanzin,viirya,XuanYuan,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27665,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 14 16:22:35 UTC 2019,,,,,,,,,,"0|z07hgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Oct/19 22:50;vanzin;I think you have to set {{spark.shuffle.useOldFetchProtocol=true}} with 3.0 for that to work.;;;","10/Oct/19 22:53;koert;[~vanzin] sorry i should have been more clear, i did set spark.shuffle.useOldFetchProtocol=true and i could not get it to work;;;","11/Oct/19 03:19;koert;actually, it doesnt matter if i use spark 2 or spark 3 shuffle service.
it doesnt matter if i use dynamic allocation or not.
as soon as i set spark.shuffle.useOldFetchProtocol=true i get these errors, always.
;;;","11/Oct/19 14:54;sandeep.katta2007;Ya I am even able to reproduce it, I am looking into this issue. ;;;","12/Oct/19 03:01;sandeep.katta2007;[~koert] can you please apply this [patch|https://github.com/apache/spark/pull/26095] and check ?;;;","12/Oct/19 20:22;koert;i checked the patch and it works with dynamic execution using spark 3 shuffle service and using spark 2 shuffle service;;;","13/Oct/19 03:13;sandeep.katta2007;cc [~cloud_fan] [~XuanYuan] this patched is tested by [~koert] please help to review patch;;;","13/Oct/19 09:43;XuanYuan;Great thanks for the reporting and fix by [~koert] and [~sandeep.katta2007].

As I comment in [#26095|https://github.com/apache/spark/pull/26095], the root cause is that shuffle id in the reader side and the writer side are inconsistent while useOldFetchProtocl=false, so we need to fix this in the shuffle writer side, not change the shuffle reader side. I'll keep tracking this problem and watching the PR.;;;","13/Oct/19 09:53;sandeep.katta2007;[~XuanYuan] thank you for your comments, I will update the PR as per this changes;;;","14/Oct/19 16:22;cloud_fan;Issue resolved by pull request 26095
[https://github.com/apache/spark/pull/26095];;;",,,,,,,,,,,,,,
Web UI Stages table tooltip correction,SPARK-29433,13261650,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,planga82,planga82,planga82,10/Oct/19 21:27,12/Oct/19 15:34,13/Jul/23 08:49,12/Oct/19 15:34,3.0.0,,,,,,,,,,3.0.0,,,Web UI,,,,,0,,,,"In the Web UI, Stages table, the tool tip of Input and output column are not corrrect.

Actual tooltip messages: 
 * Bytes and records read from Hadoop or from Spark storage.
 * Bytes and records written to Hadoop.

In this column we are only showing bytes, not records

More information at the pull request",,planga82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 12 15:34:47 UTC 2019,,,,,,,,,,"0|z07heg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Oct/19 15:34;srowen;Issue resolved by pull request 26084
[https://github.com/apache/spark/pull/26084];;;",,,,,,,,,,,,,,,,,,,,,,,
Seq.toDS / spark.createDataset(Seq) is not thread-safe,SPARK-29419,13261424,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,09/Oct/19 20:22,12/Dec/22 18:10,13/Jul/23 08:49,02/Mar/20 01:20,2.0.2,2.1.3,2.2.3,2.3.4,2.4.0,3.0.0,,,,,2.4.6,3.0.0,,SQL,,,,,0,correctness,,,"The {{Seq.toDS}} and {{spark.createDataset(Seq)}} code is not thread-safe: if the caller-supplied {{Encoder}} is used in multiple threads then {{createDataset}}'s usage of the encoder may lead to incorrect answers because the Encoder's internal mutable state will be updated by from multiple threads.

Here is an example demonstrating the problem:
{code:java}
import org.apache.spark.sql._

val enc = implicitly[Encoder[(Int, Int)]]

val datasets = (1 to 100).par.map { _ =>
  val pairs = (1 to 100).map(x => (x, x))
  spark.createDataset(pairs)(enc)
}

datasets.reduce(_ union _).collect().foreach {
  pair => require(pair._1 == pair._2, s""Pair elements are mismatched: $pair"")
}{code}
Due to the thread-safety issue, the above example results in the creation of corrupted records where different input records' fields are co-mingled.

This bug is similar to SPARK-22355, a related problem in {{Dataset.collect()}} (fixed in Spark 2.2.1+).

Fortunately, this has a simple one-line fix (copy the encoder); I'll submit a patch for this shortly.",,dongjoon,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28456,,,,SPARK-22355,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 01:20:30 UTC 2020,,,,,,,,,,"0|z07g08:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"13/Jan/20 05:33;dongjoon;Hi, [~joshrosen]. If this is `correctness`, is this `Bug` instead of `Improvement`?;;;","29/Jan/20 23:26;dongjoon;I switched this to `Bug` because this is marked as `correctness` as a blocker for 3.0.0.;;;","02/Mar/20 01:20;gurwls223;Fixed in https://github.com/apache/spark/pull/26076;;;",,,,,,,,,,,,,,,,,,,,,
HasOutputCol param isSet() property is not preserved after persistence,SPARK-29414,13261347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,borys.biletskyy,borys.biletskyy,09/Oct/19 14:42,25/Oct/19 17:37,13/Jul/23 08:49,25/Oct/19 17:37,2.3.2,,,,,,,,,,2.4.4,,,ML,PySpark,,,,0,,,,"HasOutputCol param isSet() property is not preserved after saving and loading using DefaultParamsReadable and DefaultParamsWritable.
{code:java}
import pytest
from pyspark import keyword_only
from pyspark.ml import Model
from pyspark.sql import DataFrame
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
from pyspark.ml.param.shared import HasInputCol, HasOutputCol
from pyspark.sql.functions import *


class HasOutputColTester(Model,
                         HasInputCol,
                         HasOutputCol,
                         DefaultParamsReadable,
                         DefaultParamsWritable
                         ):
    @keyword_only
    def __init__(self, inputCol: str = None, outputCol: str = None):
        super(HasOutputColTester, self).__init__()
        kwargs = self._input_kwargs
        self.setParams(**kwargs)

    @keyword_only
    def setParams(self, inputCol: str = None, outputCol: str = None):
        kwargs = self._input_kwargs
        self._set(**kwargs)
        return self

    def _transform(self, data: DataFrame) -> DataFrame:
        return data


class TestHasInputColParam(object):
    def test_persist_input_col_set(self, spark, temp_dir):
        path = temp_dir + '/test_model'
        model = HasOutputColTester()
        assert not model.isDefined(model.inputCol)
        assert not model.isSet(model.inputCol)

        assert model.isDefined(model.outputCol)
        assert not model.isSet(model.outputCol)
        model.write().overwrite().save(path)

        loaded_model: HasOutputColTester = HasOutputColTester.load(path)
        assert not loaded_model.isDefined(model.inputCol)
        assert not loaded_model.isSet(model.inputCol)

        assert loaded_model.isDefined(model.outputCol)
        assert not loaded_model.isSet(model.outputCol)  # AssertionError: assert not True
{code}",,borys.biletskyy,bryanc,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 25 17:37:07 UTC 2019,,,,,,,,,,"0|z07fj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Oct/19 16:06;huaxingao;I somehow can't reproduce the problem. In my test, isSet returns false for loaded_model. Could you please try 2.4?;;;","25/Oct/19 15:46;borys.biletskyy;The test passes in v. 2.4.4 and persistence works as expected.;;;","25/Oct/19 17:37;bryanc;Thanks [~borys.biletskyy], I'll mark this as resolved for 2.4.4 then.;;;",,,,,,,,,,,,,,,,,,,,,
Alter table / Insert statements should not change a table's ownership,SPARK-29405,13261261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,09/Oct/19 08:17,25/Oct/19 02:43,13/Jul/23 08:49,18/Oct/19 08:24,2.3.4,2.4.4,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"When executing 'insert into/overwrite ...' DML, or 'alter table set tblproperties ...'  DDL, spark would change the ownership of the table the one who runs the spark application.",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,SPARK-29498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 08:24:26 UTC 2019,,,,,,,,,,"0|z07f00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Oct/19 08:24;cloud_fan;Issue resolved by pull request 26068
[https://github.com/apache/spark/pull/26068];;;",,,,,,,,,,,,,,,,,,,,,,,
"SHOW FUNCTIONS don't show '!=', '<>' , 'between', 'case'",SPARK-29379,13261035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,08/Oct/19 05:58,20/Oct/19 06:05,13/Jul/23 08:49,20/Oct/19 06:05,2.4.0,3.0.0,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"SHOW FUNCTIONS don't show '!=', '<>' , 'between', 'case'",,angerszhuuu,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 20 06:05:07 UTC 2019,,,,,,,,,,"0|z07dm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Oct/19 06:05;cloud_fan;Issue resolved by pull request 26053
[https://github.com/apache/spark/pull/26053];;;",,,,,,,,,,,,,,,,,,,,,,,
DataSourceV2: Commands should not submit a spark job.,SPARK-29373,13260930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,imback82,imback82,imback82,07/Oct/19 16:55,09/Oct/19 03:45,13/Jul/23 08:49,09/Oct/19 03:45,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"DataSourceV2 Exec classes (ShowTablesExec, ShowNamespacesExec, etc.) all extend LeafExecNode. This results in running a job when executeCollect() is called. This breaks the previous behavior [SPARK-19650|https://issues.apache.org/jira/browse/SPARK-19650].

A new command physical operator will be introduced form which all V2 Exec classes derive to avoid running a job.",,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 09 03:45:18 UTC 2019,,,,,,,,,,"0|z07cyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Oct/19 03:45;cloud_fan;Issue resolved by pull request 26048
[https://github.com/apache/spark/pull/26048];;;",,,,,,,,,,,,,,,,,,,,,,,
Subqueries created for DPP are not printed in EXPLAIN FORMATTED,SPARK-29366,13260803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,07/Oct/19 00:54,14/Oct/19 05:59,13/Jul/23 08:49,08/Oct/19 06:39,2.4.4,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,The subquery expressions introduced by DPP are not printed in the newer explain.,,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-10-07 00:54:56.0,,,,,,,,,,"0|z07c6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better exception handling in SQLQueryTestSuite and ThriftServerQueryTestSuite,SPARK-29359,13260625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,petertoth,petertoth,petertoth,04/Oct/19 19:11,13/Oct/19 08:15,13/Jul/23 08:49,13/Oct/19 05:18,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,"SQLQueryTestSuite and ThriftServerQueryTestSuite should have the same exception handling to avoid issues like this:
{noformat}
  Expected ""[Recursion level limit 100 reached but query has not exhausted, try increasing spark.sql.cte.recursion.level.limit
  org.apache.spark.SparkException]"", but got ""[org.apache.spark.SparkException
  Recursion level limit 100 reached but query has not exhausted, try increasing spark.sql.cte.recursion.level.limit]""
{noformat}",,petertoth,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28527,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 13 05:18:34 UTC 2019,,,,,,,,,,"0|z07b2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Oct/19 05:18;yumwang;Issue resolved by pull request 26028
[https://github.com/apache/spark/pull/26028];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix the flaky test in DataFrameSuite,SPARK-29357,13260546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,XuanYuan,XuanYuan,XuanYuan,04/Oct/19 09:54,05/Oct/19 02:40,13/Jul/23 08:49,04/Oct/19 17:13,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,Fix the test `SPARK-25159: json schema inference should only trigger one job` by changing to use AtomicLong instead of a var that will not always be updated.,,dongjoon,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 04 17:13:39 UTC 2019,,,,,,,,,,"0|z07alc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Oct/19 17:13;dongjoon;This is resolved via https://github.com/apache/spark/pull/26020;;;",,,,,,,,,,,,,,,,,,,,,,,
AlterTableAlterColumnStatement should fallback to v1 AlterTableChangeColumnCommand,SPARK-29353,13260488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,cloud_fan,cloud_fan,04/Oct/19 02:39,04/Nov/19 07:57,13/Jul/23 08:49,04/Nov/19 07:56,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,cloud_fan,imback82,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 04 07:56:47 UTC 2019,,,,,,,,,,"0|z07a8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Nov/19 07:56;cloud_fan;Issue resolved by pull request 26354
[https://github.com/apache/spark/pull/26354];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix BroadcastExchange reuse in Dynamic Partition Pruning,SPARK-29350,13260336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,03/Oct/19 16:28,03/Oct/19 23:12,13/Jul/23 08:49,03/Oct/19 23:12,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Dynamic partition pruning filters are added as an in-subquery containing a {{BroadcastExchange}} in a broadcast hash join. To ensure this new {{BroadcastExchange}} can be reused, we need to make the {{ReuseExchange}} rule visit in-subquery nodes.",,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-10-03 16:28:41.0,,,,,,,,,,"0|z079ow:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The implementation of QuantileSummaries.merge  does not guarantee that the relativeError will be respected ,SPARK-29336,13260183,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sitegui,sitegui,sitegui,02/Oct/19 20:31,11/Apr/20 19:57,13/Jul/23 08:49,08/Oct/19 13:12,2.4.3,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Hello Spark maintainers,

I was experimenting with my own implementation of the [space-efficient quantile algorithm|http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf] in another language and I was using the Spark's one as a reference.

In my analysis, I believe to have found an issue with the {{merge()}} logic. Here is some simple Scala code that reproduces the issue I've found:

 
{code:java}
var values = (1 to 100).toArray
val all_quantiles = values.indices.map(i => (i+1).toDouble / values.length).toArray
for (n <- 0 until 5) {
  var df = spark.sparkContext.makeRDD(values).toDF(""value"").repartition(5)
  val all_answers = df.stat.approxQuantile(""value"", all_quantiles, 0.1)
  val all_answered_ranks = all_answers.map(ans => values.indexOf(ans)).toArray
  val error = all_answered_ranks.zipWithIndex.map({ case (answer, expected) => Math.abs(expected - answer) }).toArray
  val max_error = error.max
  print(max_error + ""\n"")
}
{code}
I query for all possible quantiles in a 100-element array with a desired 10% max error. In this scenario, one would expect to observe a maximum error of 10 ranks or less (10% of 100). However, the output I observe is:

 
{noformat}
16
12
10
11
17{noformat}
The variance is probably due to non-deterministic operations behind the scenes, but irrelevant to the core cause. (and sorry for my Scala, I'm not used to it)

Interestingly enough, if I change from five to one partition the code works as expected and gives 10 every time. This seems to point to some problem at the [merge logic|https://github.com/apache/spark/blob/51d6ba7490eaac32fc33b8996fdf06b747884a54/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/QuantileSummaries.scala#L153-L171]

The original authors ([~clockfly] and [~cloud_fan] for what I could dig from the history) suggest the published paper is not clear on how that should be done and, honestly, I was not confident in the current approach either.

I've found SPARK-21184 that reports the same problem, but it was unfortunately closed with no fix applied.

In my external implementation I believe to have found a sound way to implement the merge method. [Here is my take in Rust, if relevant|https://github.com/sitegui/space-efficient-quantile/blob/188c74638c9840e5f47d6c6326b2886d47b149bc/src/modified_gk/summary.rs#L162-L218]
I'd be really glad to add unit tests and contribute my implementation adapted to Scala.
 I'd love to hear your opinion on the matter.
Best regards

 

 ",,cloud_fan,siddarthan,sitegui,viirya,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29325,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 11 19:57:33 UTC 2020,,,,,,,,,,"0|z078qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Oct/19 21:40;sitegui;I've added a new test case that reproduces the problem here: https://github.com/sitegui/spark/commit/fa123cf289c47ceeb6f84278ae028e5a46a85bf0

The problem is triggered specially when the merging summaries had seen an uneven number of samples.

I've managed to reproduce for exact splits as well, however that requires a larger number of samples.

I'm current working at a forked branch and will try to create a PR that fixes the issue in the following days.;;;","07/Oct/19 11:27;cloud_fan;This algorithm was implemented in Spark many years ago: https://github.com/apache/spark/pull/6042

cc [~viirya] do you have any clue about the reported problem?;;;","07/Oct/19 15:57;viirya;It is long time ago, but from the comment seems the merging algorithm is not clear in the paper. The fix in the proposed PR looks promising.;;;","07/Oct/19 20:12;viirya;Actually I found the merging is changed by others since I implemented. Anyway, the fix I think can solve this.;;;","08/Oct/19 13:12;srowen;Issue resolved by pull request 26029
[https://github.com/apache/spark/pull/26029];;;","11/Apr/20 19:57;siddarthan;We just upgraded from Spark 2.4.5 to 3.0.0-preview and hit an issue that seems related to this change.

We were getting correctly computed quantiles on a dataframe with 200 partitions. After this change the results changed. When I run it with quantiles [0.8, 0.9, 1.0] I get the same values for 0.9 and 1.0. With spark 2.4.5 I get different values for 0.9 and 1.0. In fact, substantially different values. Relative error = 0.000001 and the results for 2.4.5 match the results when computing quantiles with pandas.

I have not been able to debug further yet but this seems like a pretty serious regression.;;;",,,,,,,,,,,,,,,,,,
History server is stuck reading incomplete event log file compressed with zstd,SPARK-29322,13260029,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,02/Oct/19 02:49,03/Oct/19 13:31,13/Jul/23 08:49,03/Oct/19 03:49,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,1,,,,"While working on SPARK-28869, I've discovered the issue that reading inprogress event log file on zstd compression could lead the thread being stuck. I just experimented with Spark History Server and observed same issue. I'll attach the jstack files.

This is very easy to reproduce: setting configuration as below

- spark.eventLog.enabled=true
- spark.eventLog.compress=true
- spark.eventLog.compression.codec=zstd

and start Spark application. While the application is running, load the application in SHS webpage. It may succeed to replay the event log, but high likely it will be stuck and loading page will be also stuck.

Only listing the thread stack trace being stuck across jstack files:

{code}
2019-10-02 11:32:36
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

""qtp2072313080-30"" #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97c60> (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked <0x00000007b5f97b58> (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97af8> (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked <0x00000007b5f97ac0> (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5cd3bd0> (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked <0x00000007b5cd3de0> (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)


2019-10-02 11:33:13
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

""qtp2072313080-30"" #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97c60> (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked <0x00000007b5f97b58> (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97af8> (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked <0x00000007b5f97ac0> (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5cd3bd0> (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked <0x00000007b5cd3de0> (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)


2019-10-02 11:34:00
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

""qtp2072313080-30"" #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97c60> (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked <0x00000007b5f97b58> (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97af8> (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked <0x00000007b5f97ac0> (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5cd3bd0> (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked <0x00000007b5cd3de0> (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)


2019-10-02 11:38:33
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

""qtp2072313080-30"" #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97c60> (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked <0x00000007b5f97b58> (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5f97af8> (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked <0x00000007b5f97ac0> (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked <0x00000007b5cd3bd0> (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked <0x00000007b5f94a00> (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked <0x00000007b5cd3de0> (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.Thread.run(Thread.java:748)
{code}",,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26283,,,,,,,,,,,,"02/Oct/19 02:50;kabhwan;history-server-1.jstack;https://issues.apache.org/jira/secure/attachment/12981940/history-server-1.jstack","02/Oct/19 02:50;kabhwan;history-server-2.jstack;https://issues.apache.org/jira/secure/attachment/12981941/history-server-2.jstack","02/Oct/19 02:50;kabhwan;history-server-3.jstack;https://issues.apache.org/jira/secure/attachment/12981942/history-server-3.jstack","02/Oct/19 02:50;kabhwan;history-server-4.jstack;https://issues.apache.org/jira/secure/attachment/12981943/history-server-4.jstack",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 03 03:49:08 UTC 2019,,,,,,,,,,"0|z077sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Oct/19 02:54;kabhwan;FYI, thread being stuck was finished immediately when I stopped the application.;;;","02/Oct/19 03:22;kabhwan;Just initiated discussion on dev. mailing list to see which approach is preferred to deal with this.

https://lists.apache.org/thread.html/24f2c371a5859f2a31f305ef42c8aaed1b1c6add2239d6cf97f57203@%3Cdev.spark.apache.org%3E;;;","02/Oct/19 04:51;dongjoon;Can we file and link an upstream issue Hadoop or ZSTD here? BTW, thanks!;;;","02/Oct/19 04:57;dongjoon;Since this is ZSTD, you are using `hadoop-3.2` profile on the master branch with Hadoop 3.2.0, right? In addition to that, please update the reproducible procedure into the JIRA description. The content on the mailing list would be a good candidate.;;;","02/Oct/19 05:00;dongjoon;Also, cc [~dbtsai];;;","02/Oct/19 05:09;kabhwan;{quote}
Since this is ZSTD, you are using `hadoop-3.2` profile on the master branch with Hadoop 3.2.0, right?
{quote}

Spark 2.x also has support for zstd, as well as I have been working without enabling Hadoop-3.2 for SPARK-28869 so if I'm not missing anything, I don't think it's related to `hadoop-3.2` profile.

{quote}
please update the reproducible procedure into the JIRA description.
{quote}

Thanks for guiding! I'll update the description accordingly.;;;","02/Oct/19 05:34;dongjoon;-AFAIK, we are using Hadoop ZStandardCodec which is added by HADOOP-13578 at 2.9.0 and 3.0.0.- My bad. I checked the code path. You're right. I was confused with Parquet in SQL module.;;;","03/Oct/19 03:49;dongjoon;Issue resolved by pull request 25996
[https://github.com/apache/spark/pull/25996];;;",,,,,,,,,,,,,,,,
ProgressReporter.extractStateOperatorMetrics should not overwrite updated as 0 when it actually runs a batch even with no data,SPARK-29314,13259861,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,01/Oct/19 09:52,30/Jul/20 05:39,13/Jul/23 08:49,09/Apr/20 00:01,2.4.4,3.0.0,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"SPARK-24156 brought the ability to run a batch without actual data to enable fast state cleanup as well as emit evicted outputs without waiting actual data to come.

This breaks some assumption on `ProgressReporter.extractStateOperatorMetrics`. See comment in source code:

{code:java}
// lastExecution could belong to one of the previous triggers if `!hasNewData`.
// Walking the plan again should be inexpensive.
{code}

and newNumRowsUpdated is replaced to 0 if hasNewData is false. It makes sense if we copy progress from previous execution (which means no batch is run for this time), but after SPARK-24156 the precondition is broken. 

Spark should still replace the value of newNumRowsUpdated with 0 if there's no batch being run and it needs to copy the old value from previous execution, but it shouldn't touch the value if it runs a batch for no data.",,brkyvz,kabhwan,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 05:39:01 UTC 2020,,,,,,,,,,"0|z076rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Apr/20 00:01;brkyvz;Resolved by [https://github.com/apache/spark/pull/25987];;;","29/Jul/20 14:07;sandeep.katta2007;[~kabhwan] [~brkyvz] this is required to backport to 2.4 branch;;;","30/Jul/20 05:31;kabhwan;[~sandeep.katta2007]
Did you encounter some issue regarding to this? We also need to port back SPARK-31278 as well to port this back, so it's not trivial to do.;;;","30/Jul/20 05:39;sandeep.katta2007;ya my bad, not required to backport;;;",,,,,,,,,,,,,,,,,,,,
dynamic partition overwrite with speculation enabled,SPARK-29302,13259642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,duripeng,hzfeiwang,hzfeiwang,30/Sep/19 11:12,25/Nov/20 14:41,13/Jul/23 08:49,25/Nov/20 12:52,2.4.4,,,,,,,,,,3.1.0,,,SQL,,,,,2,,,,"Now, for a dynamic partition overwrite operation,  the filename of a task output is determinable.

So, if speculation is enabled,  would a task conflict with  its relative speculation task?

Would the two tasks concurrent write a same file?
",,apachespark,cloud_fan,dangdangdang,dongjoon,hzfeiwang,jonathak,koert,mridulm80,Tagar,viirya,ZaishengDai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/19 08:28;hzfeiwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12982544/screenshot-1.png","09/Oct/19 08:27;hzfeiwang;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/12982543/screenshot-2.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 12:52:31 UTC 2020,,,,,,,,,,"0|z075ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Sep/19 23:28;viirya;FileCommitProtocol will assign a filename based on task id, isn't? Should not a speculation task have a different task id?;;;","30/Sep/19 23:29;viirya;Can you share a reproducer? or this sounds more like a question instead of a bug.;;;","01/Oct/19 02:14;hzfeiwang;You can add the code below into FileFormatWriterSuite.
{code:java}
  test(""SPARK-29302: for dynamic partition overwrite, a task will concurrent write a same file"" +
    "" with its relative speculation task"") {
    withTempDir { f =>
      val jobId = SparkHadoopWriterUtils.createJobID(new Date(), 1)
      val taskId = new TaskID(jobId, TaskType.MAP, 1)
      val taskAttemptId0 = new TaskAttemptID(taskId, 0)
      val taskAttemptId1 = new TaskAttemptID(taskId, 1)

      val taskAttemptContext0: TaskAttemptContext = {
        // Set up the configuration object
        val hadoopConf = new Configuration();
        hadoopConf.set(""mapreduce.job.id"", jobId.toString)
        hadoopConf.set(""mapreduce.task.id"", taskAttemptId0.getTaskID.toString)
        hadoopConf.set(""mapreduce.task.attempt.id"", taskAttemptId0.toString)
        hadoopConf.setBoolean(""mapreduce.task.ismap"", true)
        hadoopConf.setInt(""mapreduce.task.partition"", 0)

        new TaskAttemptContextImpl(hadoopConf, taskAttemptId0)
      }

      val taskAttemptContext1: TaskAttemptContext = {
        // Set up the configuration object
        val hadoopConf = new Configuration();
        hadoopConf.set(""mapreduce.job.id"", jobId.toString)
        hadoopConf.set(""mapreduce.task.id"", taskAttemptId1.getTaskID.toString)
        hadoopConf.set(""mapreduce.task.attempt.id"", taskAttemptId1.toString)
        hadoopConf.setBoolean(""mapreduce.task.ismap"", true)
        hadoopConf.setInt(""mapreduce.task.partition"", 0)

        new TaskAttemptContextImpl(hadoopConf, taskAttemptId1)
      }

      val committer = new HadoopMapReduceCommitProtocol(jobId.toString, f.getAbsolutePath)
      val tf0 = committer.newTaskTempFile(taskAttemptContext0, Some(f.getAbsolutePath), ""ext"")
      val tf1 = committer.newTaskTempFile(taskAttemptContext1, Some(f.getAbsolutePath), ""ext"")
      assert(tf0 == tf1)
    }
{code}
;;;","01/Oct/19 02:24;hzfeiwang;For dynamic partition overwrite, when executing a task, a determinable path would be specified.

In the reproduce suite above, I create two task attempt context with same task id and different attempt id.
And specify a output dir for   newTaskTempFile method.;;;","01/Oct/19 02:52;viirya;Will a speculation task have the same JobID as previous task like above test? 
;;;","01/Oct/19 02:59;hzfeiwang;Yes, they are in a same stage, so they have a same jobId.;;;","01/Oct/19 04:08;viirya;No, I meant this:
{code:java}
val jobId = SparkHadoopWriterUtils.createJobID(new Date(), 1)
{code}

For each writing job, in setupJob, HadoopMapReduceCommitProtocol will create a JobID. This JobID will be used getFilename. A speculation task will have a different writing job, the JobID should be different?;;;","04/Oct/19 03:50;viirya;If this is not an issue, we should close it.;;;","09/Oct/19 08:29;hzfeiwang;Sorry for the late reply, I was on my National Day holiday for the past eight days. 
I just made a simple jobId in the UT above.
In fact, it was created by a jobIdInstant.
And for  the tasks of a same job, they are same.
So, I think this is still an issue.
 !screenshot-1.png! 
 !screenshot-2.png! 

;;;","09/Oct/19 08:43;hzfeiwang;cc [~cloud_fan];;;","11/Oct/19 06:49;dangdangdang;Hi all, I have also encountered this issue and have a fix still under test.

When we use the dynamic overwrite, we actually use same staging directory for speculative tasks. Meanwhile, FileOutputComitter actually did nothing when commit task.;;;","11/Oct/19 06:56;dangdangdang;This bug can be easily reproduced by inserting into a partitioned DataSource table (would not reproduced if using a Hive table) with spark.sql.sources.partitionOverwriteMode=dynamic and spark.speculation=true.;;;","11/Oct/19 07:46;hzfeiwang;[~dangdangdang]

Hi, I have thought a simple solution.

We just need make the file name of a task be unique.
And the OutputCommitCoordinator would decide which task file can be committed.

But I don't have an appropriate unit test.;;;","11/Oct/19 10:40;dangdangdang;Hi [~hzfeiwang], thanks for your contribution.(y)

I just took a look at your PR, the solution is quite different from how I solved this issue, so I created a PR as an alternative solution which use FileOutputCommitter to avoid writing collision.

Meanwhile, I think current tests in org.apache.spark.sql.sources.InsertSuite is enough for this case.;;;","26/Mar/20 19:16;koert;i believe we are seeing this issue. it shows up in particular when pre-emption is turned on and we are using dynamic partition overwrite. pre-emption kills tasks, they get restarted, and then they fail again because the output directory already exists (so task throws FileAlreadyExistsException). as a result entire job fails.

so i dont think this is just a speculative execution issue. this is a general issue with dynamic partition overwrite not being able to recover from task failure.;;;","17/Apr/20 23:02;mridulm80;I agree with [~feiwang], it looks like newTaskTempFile is not robust to speculative execution and task failures when dynamicPartitionOverwrite is enabled IMO.
This will need to be fixed - it is currently using the same path irrespective of which attempt it is.;;;","03/Jul/20 03:07;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/28989;;;","05/Jul/20 20:02;apachespark;User 'WinkerDu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29000;;;","05/Jul/20 20:03;apachespark;User 'WinkerDu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29000;;;","27/Jul/20 11:45;apachespark;User 'WinkerDu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29260;;;","27/Jul/20 11:46;apachespark;User 'WinkerDu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29260;;;","25/Nov/20 12:52;cloud_fan;Issue resolved by pull request 29000
[https://github.com/apache/spark/pull/29000];;;",,
Duplicate result when dropping partition of an external table and then overwriting,SPARK-29295,13259560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,hzfeiwang,hzfeiwang,30/Sep/19 01:19,09/Jun/20 07:31,13/Jul/23 08:49,18/Oct/19 08:37,2.2.3,2.3.4,2.4.5,,,,,,,,2.4.6,3.0.0,,SQL,,,,,1,correctness,,,"When we drop a partition of a external table and then overwrite it, if we set CONVERT_METASTORE_PARQUET=true(default value), it will overwrite this partition.
But when we set CONVERT_METASTORE_PARQUET=false, it will give duplicate result.

Here is a reproduce code below(you can add it into SQLQuerySuite in hive module):

{code:java}
  test(""spark gives duplicate result when dropping a partition of an external partitioned table"" +
    "" firstly and they overwrite it"") {
    withTable(""test"") {
      withTempDir { f =>
        sql(""create external table test(id int) partitioned by (name string) stored as "" +
          s""parquet location '${f.getAbsolutePath}'"")

        withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> false.toString) {
          sql(""insert overwrite table test partition(name='n1') select 1"")
          sql(""ALTER TABLE test DROP PARTITION(name='n1')"")
          sql(""insert overwrite table test partition(name='n1') select 2"")
          checkAnswer( sql(""select id from test where name = 'n1' order by id""),
            Array(Row(1), Row(2)))
        }

        withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> true.toString) {
          sql(""insert overwrite table test partition(name='n1') select 1"")
          sql(""ALTER TABLE test DROP PARTITION(name='n1')"")
          sql(""insert overwrite table test partition(name='n1') select 2"")
          checkAnswer( sql(""select id from test where name = 'n1' order by id""),
            Array(Row(2)))
        }
      }
    }
  }
{code}

{code}
create external table test(id int) partitioned by (name string) stored as parquet location '/tmp/p';
set spark.sql.hive.convertMetastoreParquet=false;
insert overwrite table test partition(name='n1') select 1;
ALTER TABLE test DROP PARTITION(name='n1');
insert overwrite table test partition(name='n1') select 2;
select id from test where name = 'n1' order by id;
{code}",,angerszhuuu,apachespark,cloud_fan,dongjoon,hzfeiwang,mauzhang,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-18702,,,,SPARK-25271,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 07:31:46 UTC 2020,,,,,,,,,,"0|z074wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Sep/19 01:32;hzfeiwang;When we set CONVERT_METASTORE_PARQUET=true, it will use InsertIntoHadoopFsRelationCommand to process this statement.

When we set CONVERT_METASTORE_PARQUET=false, it will use InsertIntoHiveTable.;;;","30/Sep/19 01:38;hzfeiwang;cc [~cloud_fan] [~viirya];;;","30/Sep/19 07:52;viirya;Thanks for reporting this. Will take a look.;;;","30/Sep/19 08:42;hzfeiwang;relative hive issue, https://issues.apache.org/jira/browse/HIVE-17063;;;","30/Sep/19 15:28;viirya;I traced code last night, and yes it looks like a Hive bug. https://jira.apache.org/jira/browse/HIVE-18702;;;","01/Oct/19 02:27;hzfeiwang;Yes, how can we resolve this issue?
Only by upgrading the hive version?;;;","01/Oct/19 02:53;viirya;I created a PR for this issue.;;;","12/Oct/19 03:26;angerszhuuu; This probelm seems start from hive 1.2

I test in our env hive-1.1, won't have this problem.;;;","18/Oct/19 08:37;cloud_fan;Issue resolved by pull request 25979
[https://github.com/apache/spark/pull/25979];;;","11/Mar/20 17:43;dongjoon;I marked this as a `correctness` issue.;;;","11/Mar/20 17:43;dongjoon;Hi, [~viirya]. Could you make a backport against branch-2.4?;;;","11/Mar/20 18:05;dongjoon;I confirmed that Apache Spark 2.1.3 and older versions have no problem.;;;","12/Mar/20 10:01;dongjoon;This lands to `branch-2.4` via https://github.com/apache/spark/pull/27887 .;;;","09/Jun/20 07:31;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/28765;;;",,,,,,,,,,
"Error message is hidden when query from JDBC, especially enabled adaptive execution",SPARK-29283,13259482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,29/Sep/19 06:33,17/Oct/19 02:55,13/Jul/23 08:49,17/Oct/19 02:54,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"When adaptive execution is enabled, the Spark users who connected from JDBC always get adaptive execution error whatever the under root cause is. It's very confused. We have to check the driver log to find out why.
{code}
0: jdbc:hive2://localhost:10000> SELECT * FROM testData join testData2 ON key = v;
SELECT * FROM testData join testData2 ON key = v;
Error: Error running query: org.apache.spark.SparkException: Adaptive execution failed due to stage materialization failures. (state=,code=0)
0: jdbc:hive2://localhost:10000> 
{code}

For example, a job queried from JDBC failed due to HDFS missing block. User still get the error message {{Adaptive execution failed due to stage materialization failures}}.",,cltlfcjin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 17 02:54:16 UTC 2019,,,,,,,,,,"0|z074fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Oct/19 02:54;yumwang;Issue resolved by pull request 25960
https://github.com/apache/spark/pull/25960;;;",,,,,,,,,,,,,,,,,,,,,,,
Examples in Like/RLike doesn't consider the default value of spark.sql.parser.escapedStringLiterals,SPARK-29281,13259411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,28/Sep/19 06:46,12/Dec/22 18:11,13/Jul/23 08:49,28/Sep/19 18:06,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Please take a look at example of `LIKE`:

[https://github.com/apache/spark/blob/d72f39897b00d0bbd7a4db9de281a1256fcf908d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala#L97-L106]

If spark.sql.parser.escapedStringLiterals=false, then it should fail as there's `\U` in pattern (spark.sql.parser.escapedStringLiterals=false by default) but it doesn't fail.
{quote}The escape character is '\'. If an escape character precedes a special symbol or another
 escape character, the following character is matched literally. It is invalid to escape
 any other character.
{quote}
For the query
{code:java}
SET spark.sql.parser.escapedStringLiterals=false;
SELECT '%SystemDrive%\Users\John' like '\%SystemDrive\%\Users%'; {code}
SQL parser removes single `\` (not sure that is intended) so the expressions of Like are constructed as following:
{code:java}
 LIKE - left `%SystemDrive%UsersJohn` / right `\%SystemDrive\%Users%`{code}
which are no longer having origin intention. 

Same happens on RLike example:
{code:java}
SET spark.sql.parser.escapedStringLiterals=false;
SELECT '%SystemDrive%\Users\John' rlike '%SystemDrive%\Users.*'; {code}
{code:java}
RLIKE - left `%SystemDrive%UsersJohn` / right `%SystemDrive%Users.*`{code}
 ",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 28 18:06:17 UTC 2019,,,,,,,,,,"0|z073zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Sep/19 18:06;gurwls223;Issue resolved by pull request 25957
[https://github.com/apache/spark/pull/25957];;;",,,,,,,,,,,,,,,,,,,,,,,
DataSourceV2: Add early filter and projection pushdown,SPARK-29277,13259396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,28/Sep/19 01:19,31/Oct/19 20:25,13/Jul/23 08:49,31/Oct/19 15:26,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,Spark uses optimizer rules that need stats before conversion to physical plan. DataSourceV2 should support early pushdown for those rules.,,dongjoon,rdblue,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 15:26:08 UTC 2019,,,,,,,,,,"0|z073w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Oct/19 01:08;rdblue;Fixed by #25955.;;;","31/Oct/19 06:17;dongjoon;This is reverted due to `SBT with Hadoop 3.2` profile failure.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-3.2/681/

For the detail, https://github.com/apache/spark/pull/25955#issuecomment-548230386 .;;;","31/Oct/19 15:26;dongjoon;Issue resolved by pull request 26341
[https://github.com/apache/spark/pull/26341];;;",,,,,,,,,,,,,,,,,,,,,
availableSlots in scheduler can change before being checked by barrier taskset,SPARK-29263,13259118,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,26/Sep/19 19:04,17/May/20 17:48,13/Jul/23 08:49,27/Sep/19 18:37,2.4.0,,,,,,,,,,3.0.0,,,Scheduler,Spark Core,,,,0,,,,"availableSlots are computed before the loop in resourceOffer, but they change in every iteration",,jiangxb1987,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 18:37:15 UTC 2019,,,,,,,,,,"0|z07268:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Sep/19 18:37;jiangxb1987;Resolved by https://github.com/apache/spark/pull/25946;;;",,,,,,,,,,,,,,,,,,,,,,,
DataFrameWriterV2 should not allow setting table properties for existing tables,SPARK-29249,13258907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,25/Sep/19 22:41,26/Sep/19 16:11,13/Jul/23 08:49,26/Sep/19 05:51,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"tableProperty should return CreateTableWriter, not DataFrameWriterV2.",,cloud_fan,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 26 05:51:54 UTC 2019,,,,,,,,,,"0|z070vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Sep/19 05:51;cloud_fan;Issue resolved by pull request 25931
[https://github.com/apache/spark/pull/25931];;;",,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException on TaskCompletionListener during releasing of memory blocks,SPARK-29244,13258764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,Tradunsky,Tradunsky,25/Sep/19 10:34,01/Oct/19 20:27,13/Jul/23 08:49,01/Oct/19 18:51,2.4.0,,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"At the end of task completion an exception happened:

{code:java}

19/09/25 09:03:58 ERROR TaskContextImpl: Error in TaskCompletionListener19/09/25 09:03:58 ERROR TaskContextImpl: Error in TaskCompletionListenerjava.lang.ArrayIndexOutOfBoundsException: -3 at org.apache.spark.memory.TaskMemoryManager.freePage(TaskMemoryManager.java:333) at org.apache.spark.memory.MemoryConsumer.freePage(MemoryConsumer.java:130) at org.apache.spark.memory.MemoryConsumer.freeArray(MemoryConsumer.java:108) at org.apache.spark.unsafe.map.BytesToBytesMap.free(BytesToBytesMap.java:803) at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.free(UnsafeFixedWidthAggregationMap.java:225) at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.lambda$new$0(UnsafeFixedWidthAggregationMap.java:111) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

{code}

 

Important to note, that before this one, there was OOM of allocating some pages. It looks like everything related to each other, but on OOM the whole flow goes abnormally, so no resources are fried correctly.

{code:java}

java.lang.NullPointerExceptionjava.lang.NullPointerException at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getMemoryUsage(UnsafeInMemorySorter.java:208) at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getMemoryUsage(UnsafeExternalSorter.java:249) at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.updatePeakMemoryUsed(UnsafeExternalSorter.java:253) at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.freeMemory(UnsafeExternalSorter.java:296) at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.cleanupResources(UnsafeExternalSorter.java:328) at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.lambda$new$0(UnsafeExternalSorter.java:178) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

{code}

 

This is must be something with job planning, but taking so many exceptions into account doesn't make things easier. Would be happy to provide more details.","Release label:emr-5.20.0
Hadoop distribution:Amazon 2.8.5
Applications:Livy 0.5.0, Spark 2.4.0",sandeep.katta2007,Tradunsky,viirya,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/19 10:35;Tradunsky;executor_oom.txt;https://issues.apache.org/jira/secure/attachment/12981311/executor_oom.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 01 20:27:51 UTC 2019,,,,,,,,,,"0|z06zzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Oct/19 18:51;viirya;Issue resolved by pull request 25953
[https://github.com/apache/spark/pull/25953];;;","01/Oct/19 20:27;Tradunsky;Thank you! ;;;",,,,,,,,,,,,,,,,,,,,,,
PySpark 2.4 about sql function 'element_at' param 'extraction',SPARK-29240,13258726,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gurwls223,ChuboChaser,ChuboChaser,25/Sep/19 08:27,12/Dec/22 17:34,13/Jul/23 08:49,27/Sep/19 18:06,2.4.0,,,,,,,,,,2.4.5,3.0.0,,PySpark,,,,,0,,,,"I was trying to translate {color:#FF0000}Scala{color} into {color:#FF0000}python{color} with {color:#FF0000}PySpark 2.4.0{color} .Codes below aims to extract col '{color:#FF0000}list{color}' value using col '{color:#FF0000}num{color}' as index.

 
{code:java}
x = spark.createDataFrame([((1,2,3),1),((4,5,6),2),((7,8,9),3)],['list','num'])
x.show(){code}
 
||list||num||
|[1,2,3]|1|
|[4,5,6]|2|
|[7,8,9]|3|

I suppose to use new func '{color:#FF0000}element_at{color}' in 2.4.0 .But it gives an error:
{code:java}
x.withColumn('aa',F.element_at('list',x.num.cast('int')))
{code}
_TypeError: Column is not iterable_

 

Finally ,I have to use {color:#FF0000}udf{color} to solve this problem.

But in Scala ,it is ok when the second param '{color:#FF0000}extraction{color}' in func '{color:#FF0000}element_at{color}' is a col name with int type: 
{code:java}
//Scala
val y = x.withColumn(""aa"",element_at('list,'num.cast(""int"")))
y.show(){code}
||list||num|| aa||
|[1,2,3]|1| 1|
|[4,5,6] |2 |5 |
|[7,8,9] |3 |9 |

 I hope it could be fixed in latest version.",,ChuboChaser,dongjoon,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 18:06:28 UTC 2019,,,,,,,,,,"0|z06zr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Sep/19 18:06;dongjoon;This is resolved via https://github.com/apache/spark/pull/25950;;;",,,,,,,,,,,,,,,,,,,,,,,
Access 'executorDataMap' out of 'DriverEndpoint' should be protected by lock,SPARK-29236,13258671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coneyliu,coneyliu,coneyliu,25/Sep/19 02:01,26/Sep/19 01:06,13/Jul/23 08:49,25/Sep/19 14:40,2.4.4,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"Just as the comments:

> 

// Accessing `executorDataMap` in `DriverEndpoint.receive/receiveAndReply` doesn't need any
// protection. But accessing `executorDataMap` out of `DriverEndpoint.receive/receiveAndReply`
// must be protected by `CoarseGrainedSchedulerBackend.this`. Besides, `executorDataMap` should
// only be modified in `DriverEndpoint.receive/receiveAndReply` with protection by
// `CoarseGrainedSchedulerBackend.this`.

 

`executorDataMap` is not threadsafe, it should be protected by lock when accessing it out of `DriverEndpoint`",,cloud_fan,coneyliu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 25 14:40:46 UTC 2019,,,,,,,,,,"0|z06zew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Sep/19 14:40;cloud_fan;Issue resolved by pull request 25922
[https://github.com/apache/spark/pull/25922];;;",,,,,,,,,,,,,,,,,,,,,,,
CrossValidatorModel.avgMetrics disappears after model is written/read again,SPARK-29235,13258639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,mcbedford21,mcbedford21,24/Sep/19 20:03,20/Oct/19 09:20,13/Jul/23 08:49,19/Oct/19 20:24,2.4.1,2.4.3,,,,,,,,,3.0.0,,,ML,,,,,0,,,," 
 Right after a CrossValidatorModel is trained, it has avgMetrics.  After the model is written to disk and read later, it no longer has avgMetrics.  To reproduce:

{{from pyspark.ml.tuning import CrossValidator, CrossValidatorModel}}

{{cv = CrossValidator(...) #fill with params}}

{{cvModel = cv.fit(trainDF) #given dataframe with training data}}

{{print(cvModel.avgMetrics) #prints a nonempty list as expected}}

{{cvModel.write().save({color:#172b4d}""/tmp/model""{color})}}

{{cvModel2 = CrossValidatorModel.read().load({color:#172b4d}""/tmp/model""{color})}}

{{print(cvModel2.avgMetrics) #BUG - prints an empty list}}","Databricks cluster:

{
    ""num_workers"": 4,
    ""cluster_name"": ""mabedfor-test-classfix"",
    ""spark_version"": ""5.3.x-cpu-ml-scala2.11"",
    ""spark_conf"": {
        ""spark.databricks.delta.preview.enabled"": ""true""
    },
    ""node_type_id"": ""Standard_DS12_v2"",
    ""driver_node_type_id"": ""Standard_DS12_v2"",
    ""ssh_public_keys"": [],
    ""custom_tags"": {},
    ""spark_env_vars"": {
        ""PYSPARK_PYTHON"": ""/databricks/python3/bin/python3""
    },
    ""autotermination_minutes"": 120,
    ""enable_elastic_disk"": true,
    ""cluster_source"": ""UI"",
    ""init_scripts"": [],
    ""cluster_id"": ""0722-165622-calls746""
}",mcbedford21,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 19 20:24:18 UTC 2019,,,,,,,,,,"0|z06z7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Sep/19 05:49;shahid;I would like to analyze the issue.;;;","19/Oct/19 20:24;srowen;Issue resolved by pull request 26038
[https://github.com/apache/spark/pull/26038];;;",,,,,,,,,,,,,,,,,,,,,,
RandomForestRegressionModel does not update the parameter maps of the DecisionTreeRegressionModels underneath,SPARK-29232,13258623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,jiaqig,jiaqig,24/Sep/19 18:41,22/Oct/19 14:48,13/Jul/23 08:49,22/Oct/19 09:52,2.4.0,,,,,,,,,,3.0.0,,,ML,,,,,0,,,,"We trained a RandomForestRegressionModel, and tried to access the trees. Even though the DecisionTreeRegressionModel is correctly built with the proper parameters from random forest, the parameter map is not updated, and still contains only the default value. 

For example, if a RandomForestRegressor was trained with maxDepth of 12, then accessing the tree information, extractParamMap still returns the default values, with maxDepth=5. Calling the depth itself of DecisionTreeRegressionModel returns the correct value of 12 though.

This creates issues when we want to access each individual tree and build the trees back up for the random forest estimator.",,aman_omer,jiaqig,podongfeng,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 09:52:45 UTC 2019,,,,,,,,,,"0|z06z48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Sep/19 12:44;aman_omer;I would like to analyse this. 

Thanks [~jiaqig];;;","27/Sep/19 10:09;aman_omer;I used some examples for RF regression but can't use model.extractParamMap(). I am stuck here.

[~jiaqig] [~shahid] any lead to reproduce this bug will be helpful. 

Thanks;;;","27/Sep/19 19:20;jiaqig;[~aman_omer], here is [an example from the Spark documentation|[https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression]].
{code:java}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.feature.VectorIndexer
import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}

// Load and parse the data file, converting it to a DataFrame.
val data = spark.read.format(""libsvm"").load(""data/mllib/sample_libsvm_data.txt"")

// Set maxCategories so features with > 4 distinct values are treated as continuous.
val featureIndexer = new VectorIndexer()
  .setInputCol(""features"")
  .setOutputCol(""indexedFeatures"")
  .setMaxCategories(4)
  .fit(data)

// Split the data into training and test sets (30% held out for testing).
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))

// Train a RandomForest model.
val rf = new RandomForestRegressor()
  .setNumTrees(5)
  .setMaxDepth(10)
  .setLabelCol(""label"")
  .setFeaturesCol(""indexedFeatures"")

// Chain indexer and forest in a Pipeline.
val pipeline = new Pipeline()
  .setStages(Array(featureIndexer, rf))

// Train model. This also runs the indexer.
val model = pipeline.fit(trainingData)

// Make predictions.
val predictions = model.transform(testData)

// Select example rows to display.
predictions.select(""prediction"", ""label"", ""features"").show(5)

// Select (prediction, true label) and compute test error.
val evaluator = new RegressionEvaluator()
  .setLabelCol(""label"")
  .setPredictionCol(""prediction"")
  .setMetricName(""rmse"")
val rmse = evaluator.evaluate(predictions)
println(s""Root Mean Squared Error (RMSE) on test data = $rmse"")
val rfModel = model.stages(1).asInstanceOf[RandomForestRegressionModel]
println(s""Learned regression forest model:\n ${rfModel.toDebugString}"")
{code}
This gives you a random forest model called rfModel. I modified the max depth to 10 for the trees. 
{code:java}
rfModel.extractParamMap()
// Printout
res23: org.apache.spark.ml.param.ParamMap = { rfr_8197914ca605-cacheNodeIds: false, rfr_8197914ca605-checkpointInterval: 10, rfr_8197914ca605-featureSubsetStrategy: auto, rfr_8197914ca605-featuresCol: indexedFeatures, rfr_8197914ca605-impurity: variance, rfr_8197914ca605-labelCol: label, rfr_8197914ca605-maxBins: 32, rfr_8197914ca605-maxDepth: 10, rfr_8197914ca605-maxMemoryInMB: 256, rfr_8197914ca605-minInfoGain: 0.0, rfr_8197914ca605-minInstancesPerNode: 1, rfr_8197914ca605-numTrees: 5, rfr_8197914ca605-predictionCol: prediction, rfr_8197914ca605-seed: 235498149, rfr_8197914ca605-subsamplingRate: 1.0 }
{code}
As you can see the maxDepth here is correct. However, if we were to check the parameter map of the trees.
{code:java}
rfModel.trees(0).extractParamMap()
// Printout
res22: org.apache.spark.ml.param.ParamMap = { dtr_bfcfc13f1334-cacheNodeIds: false, dtr_bfcfc13f1334-checkpointInterval: 10, dtr_bfcfc13f1334-featuresCol: features, dtr_bfcfc13f1334-impurity: variance, dtr_bfcfc13f1334-labelCol: label, dtr_bfcfc13f1334-maxBins: 32, dtr_bfcfc13f1334-maxDepth: 5, dtr_bfcfc13f1334-maxMemoryInMB: 256, dtr_bfcfc13f1334-minInfoGain: 0.0, dtr_bfcfc13f1334-minInstancesPerNode: 1, dtr_bfcfc13f1334-predictionCol: prediction, dtr_bfcfc13f1334-seed: 1366634793 }
{code}
The max depth stays at the default value 5. In fact, parameter maps of individual trees will only give the default decision tree values.;;;","30/Sep/19 12:14;aman_omer;Thnaks [~jiaqig] ;;;","22/Oct/19 09:52;podongfeng;Issue resolved by pull request 26154
[https://github.com/apache/spark/pull/26154];;;",,,,,,,,,,,,,,,,,,,
Fix NullPointerException in the test class ProcfsMetricsGetterSuite,SPARK-29230,13258573,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sev7e0,sev7e0,sev7e0,24/Sep/19 14:53,25/Sep/19 06:41,13/Jul/23 08:49,24/Sep/19 21:13,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,Tests,,,,0,,,,"When I use `ProcfsMetricsGetterSuite` testing, always throw out `java.lang.NullPointerException`. 

I think there is a problem with locating `new ProcfsMetricsGetter`, which will lead to `SparkEnv` not being initialized in time. 

This leads to `java.lang.NullPointerException` when the method is executed.",,dongjoon,sev7e0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 25 06:41:10 UTC 2019,,,,,,,,,,"0|z06yt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Sep/19 21:13;dongjoon;This is resolved via https://github.com/apache/spark/pull/25918;;;","25/Sep/19 06:41;sev7e0;Work completed!;;;",,,,,,,,,,,,,,,,,,,,,,
Change the additional remote repository in IsolatedClientLoader to google minor,SPARK-29229,13258543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,24/Sep/19 12:26,25/Sep/19 01:50,13/Jul/23 08:49,24/Sep/19 17:15,2.4.4,3.0.0,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"The repository currently used is ""[http://www.datanucleus.org/downloads/maven2]"", which is no longer maintained. This will sometimes cause downloading failure and make hive test cases flaky.",,cloud_fan,dongjoon,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29175,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 24 17:56:16 UTC 2019,,,,,,,,,,"0|z06ymg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Sep/19 17:15;cloud_fan;Issue resolved by pull request 25915
[https://github.com/apache/spark/pull/25915];;;","24/Sep/19 17:56;dongjoon;This is backported to `branch-2.4` via https://github.com/apache/spark/commit/5267e6e5bdc189a5ae876c726d96038b3ac3db66;;;",,,,,,,,,,,,,,,,,,,,,,
Increase `Show Additional Metrics` checkbox width in StagePage,SPARK-29218,13258366,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,23/Sep/19 17:40,24/Sep/19 06:57,13/Jul/23 08:49,24/Sep/19 06:57,3.0.0,,,,,,,,,,3.0.0,,,Web UI,,,,,0,,,,"Widths of checkboxes in StagePage are not proper and labels are wrapped. It looks weird.

------
This happens always in two cases.
1. The default font size is big.
2. When the user increase font size.","I've noticed this issue occurs to at least following environments.

 

Firefox 67.01 and 69.00 on Pop!_OS, an Ubuntu based OS 19.04.

Firefox 69.01 on Windows10.

Chrome 77.0.3865.90 on Windows10.",dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/19 17:41;sarutak;before-modified1.png;https://issues.apache.org/jira/secure/attachment/12981091/before-modified1.png","23/Sep/19 17:41;sarutak;before-modified2.png;https://issues.apache.org/jira/secure/attachment/12981090/before-modified2.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 24 06:57:54 UTC 2019,,,,,,,,,,"0|z06xj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Sep/19 06:57;dongjoon;Issue resolved by pull request 25905
[https://github.com/apache/spark/pull/25905];;;",,,,,,,,,,,,,,,,,,,,,,,
Make it consistent when get notnull output and generate null checks in FilterExec,SPARK-29213,13258253,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wang-shuo,wang-shuo,wang-shuo,23/Sep/19 09:32,27/Sep/19 07:40,13/Jul/23 08:49,27/Sep/19 07:39,2.4.4,,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"Currently the behavior of getting output and generating null checks in FilterExec is different. Thus some nullable attribute could be treated as not nullable by mistake.

In FilterExec.ouput, an attribute is marked as nullable or not by finding its `exprId` in notNullAttributes:
{code:java}
a.nullable && notNullAttributes.contains(a.exprId)
{code}
But in FilterExec.doConsume,  a `nullCheck` is generated or not for an attribute is decided by whether there is semantic equal not null predicate:
{code:java}
val nullChecks = c.references.map { r => val idx = notNullPreds.indexWhere { n => n.asInstanceOf[IsNotNull].child.semanticEquals(r)} if (idx != -1 && !generatedIsNotNullChecks(idx)) { generatedIsNotNullChecks(idx) = true // Use the child's output. The nullability is what the child produced. genPredicate(notNullPreds(idx), input, child.output) } else { """" } }.mkString(""\n"").trim
{code}
 

NPE will happen when run the SQL below:
{code:java}
sql(""create table table1(x string)"")
sql(""create table table2(x bigint)"")
sql(""create table table3(x string)"")
sql(""insert into table2 select null as x"")
sql(
  """"""
    |select t1.x
    |from (
    |    select x from table1) t1
    |left join (
    |    select x from (
    |        select x from table2
    |        union all
    |        select substr(x,5) x from table3
    |    ) a
    |    where length(x)>0
    |) t3
    |on t1.x=t3.x
  """""".stripMargin).collect()
{code}
 

NPE Exception:
{code:java}
java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:40)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:135)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:94)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:127)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:449)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:452)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

{code}
 

the generated code:
{code:java}
== Subtree 4 / 5 ==
*(2) Project [cast(x#7L as string) AS x#9]
+- *(2) Filter ((length(cast(x#7L as string)) > 0) AND isnotnull(cast(x#7L as string)))
   +- Scan hive default.table2 [x#7L], HiveTableRelation `default`.`table2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [x#7L]


Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( inputadapter_input_0.hasNext()) {
/* 027 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 028 */
/* 029 */       do {
/* 030 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */         long inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */         -1L : (inputadapter_row_0.getLong(0));
/* 033 */
/* 034 */         boolean filter_isNull_2 = inputadapter_isNull_0;
/* 035 */         UTF8String filter_value_2 = null;
/* 036 */         if (!inputadapter_isNull_0) {
/* 037 */           filter_value_2 = UTF8String.fromString(String.valueOf(inputadapter_value_0));
/* 038 */         }
/* 039 */         int filter_value_1 = -1;
/* 040 */         filter_value_1 = (filter_value_2).numChars();
/* 041 */
/* 042 */         boolean filter_value_0 = false;
/* 043 */         filter_value_0 = filter_value_1 > 0;
/* 044 */         if (!filter_value_0) continue;
/* 045 */
/* 046 */         boolean filter_isNull_6 = inputadapter_isNull_0;
/* 047 */         UTF8String filter_value_6 = null;
/* 048 */         if (!inputadapter_isNull_0) {
/* 049 */           filter_value_6 = UTF8String.fromString(String.valueOf(inputadapter_value_0));
/* 050 */         }
/* 051 */         if (!(!filter_isNull_6)) continue;
/* 052 */
/* 053 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 054 */
/* 055 */         boolean project_isNull_0 = false;
/* 056 */         UTF8String project_value_0 = null;
/* 057 */         if (!false) {
/* 058 */           project_value_0 = UTF8String.fromString(String.valueOf(inputadapter_value_0));
/* 059 */         }
/* 060 */         filter_mutableStateArray_0[1].reset();
/* 061 */
/* 062 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 063 */
/* 064 */         if (project_isNull_0) {
/* 065 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 066 */         } else {
/* 067 */           filter_mutableStateArray_0[1].write(0, project_value_0);
/* 068 */         }
/* 069 */         append((filter_mutableStateArray_0[1].getRow()));
/* 070 */
/* 071 */       } while(false);
/* 072 */       if (shouldStop()) return;
/* 073 */     }
/* 074 */   }
/* 075 */
/* 076 */ }

{code}
 ",,cloud_fan,wang-shuo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 07:39:14 UTC 2019,,,,,,,,,,"0|z06wu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Sep/19 07:39;cloud_fan;Issue resolved by pull request 25902
[https://github.com/apache/spark/pull/25902];;;",,,,,,,,,,,,,,,,,,,,,,,
--driver-java-options are not passed to driver process in yarn client mode,SPARK-29202,13258133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,sandeep.katta2007,sandeep.katta2007,22/Sep/19 06:03,26/Sep/19 22:51,13/Jul/23 08:49,26/Sep/19 22:38,3.0.0,,,,,,,,,,3.0.0,,,Deploy,,,,,0,,,,"Run the below command 

./bin/spark-sql --master yarn --driver-java-options=""-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5555""

 
In Spark 2.3.3

/opt/softwares/Java/jdk1.8.0_211/bin/java -cp /opt/BigdataTools/spark-2.3.3-bin-hadoop2.7/conf/:/opt/BigdataTools/spark-2.3.3-bin-hadoop2.7/jars/*:/opt/BigdataTools/hadoop-3.2.0/etc/hadoop/ -Xmx1g -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5555 org.apache.spark.deploy.SparkSubmit --master yarn --conf spark.driver.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5555 --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver spark-internal

 
In Spark 3.0

/opt/softwares/Java/jdk1.8.0_211/bin/java -cp /opt/apache/git/sparkSourceCode/spark/conf/:/opt/apache/git/sparkSourceCode/spark/assembly/target/scala-2.12/jars/*:/opt/BigdataTools/hadoop-3.2.0/etc/hadoop/ org.apache.spark.deploy.SparkSubmit --master yarn --conf spark.driver.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5556 --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver spark-internal


We can see that java options are not passed to driver process in spark3

 ",,dongjoon,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 26 22:38:34 UTC 2019,,,,,,,,,,"0|z06w3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Sep/19 22:38;dongjoon;This is resolved via https://github.com/apache/spark/pull/25889;;;",,,,,,,,,,,,,,,,,,,,,,,
toPandas gets wrong dtypes when applied on empty DF,SPARK-29188,13257901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dlindelof,radcheb,radcheb,20/Sep/19 09:36,12/Dec/22 18:10,13/Jul/23 08:49,12/Dec/19 11:51,2.0.0,2.4.4,,,,,,,,,3.0.0,,,PySpark,SQL,,,,1,,,,"When calling toPandas from an empty dataframe, all dtypes are set to `object`.
{code:python}
spark_df = spark.createDataFrame([(10, ""Emy"", datetime.today() ), (11, ""Bob"", datetime.today())], [""age"", ""name"", ""date""])

spark.createDataFrame(spark.sparkContext.emptyRDD(), schema=spark_df.schema).toPandas().dtypes 
{code}
Result: 
{code:bash}
age     object
name    object
date    object
dtype: object
{code}
 

While it gets the correct types when converting the entire dataframe (or at least with 1 line of data) to pandas:
{code:python}
spark_df = spark.createDataFrame([(10, ""Emy"", datetime.today() ), (11, ""Bob"", datetime.today())], [""age"", ""name"", ""date""]) 

spark_df.limit(1).toPandas().dtypes 
{code}
 Result:
{code:bash}
age              int64
name            object
date    datetime64[ns]
dtype: object
{code}
 

Is this intended ? Why toPandas does not rely on the Spark DataFrame Schema ?",">> uname -a

Linux XXXXXXXXXXXXXXXX 4.14.104-95.84.amzn2.x86_64 #1 SMP Sat Mar 2 00:40:20 UTC 2019 x86_64 GNU/Linux

>> python

Python 3.6.7 | packaged by conda-forge | (default, Jul 2 2019, 02:18:42)
[GCC 7.3.0] on linux

>> conda list
...
openjdk   8.0.192   h1de35cc_1003       conda-forge
pandas    0.25.1      py36h86efe34_0    conda-forge
py4j         0.10.7      py_1                           conda-forge
pyspark   2.4.4       py_0                          conda-forge
....",bryanc,holden,radcheb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30537,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 11:51:52 UTC 2019,,,,,,,,,,"0|z06uo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Dec/19 11:51;gurwls223;Issue resolved by pull request 26747
[https://github.com/apache/spark/pull/26747];;;",,,,,,,,,,,,,,,,,,,,,,,
Zombie tasks prevents executor from releasing when task exceeds maxResultSize,SPARK-29177,13257611,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,19/Sep/19 08:41,23/Sep/19 14:52,13/Jul/23 08:49,23/Sep/19 11:47,2.3.4,2.4.4,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"When we fetch results from executors and found the total size has exceeded the maxResultSize configured, Spark will simply abort the stage and all dependent jobs. But the task triggered this is actually successful, but never post out `TaskEnd` event, as a result it will never be removed from `CoarseGrainedSchedulerBackend`. If dynamic allocation is enabled, there will be zombie executor(s) remaining in resource manager, it will never die until application ends.",,adrian-wang,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 23 11:47:28 UTC 2019,,,,,,,,,,"0|z06svs:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"23/Sep/19 11:47;cloud_fan;Issue resolved by pull request 25850
[https://github.com/apache/spark/pull/25850];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix some exception issue of explain commands,SPARK-29172,13257592,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,T.Komiyama,T.Komiyama,T.Komiyama,19/Sep/19 07:45,29/Sep/19 15:42,13/Jul/23 08:49,29/Sep/19 15:42,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"The behaviors of run commands during exception handling are different depends on explain command.

See the attachments. ",,sarutak,T.Komiyama,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/19 07:46;T.Komiyama;cost.png;https://issues.apache.org/jira/secure/attachment/12980681/cost.png","19/Sep/19 07:46;T.Komiyama;extemded.png;https://issues.apache.org/jira/secure/attachment/12980682/extemded.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 29 15:42:19 UTC 2019,,,,,,,,,,"0|z06srk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Sep/19 05:08;sarutak;This was accidentally closed because the issue number included in the title of PR #25848 was wrong. What should have been closed is SPARK-29178.

So I'll reopen this.;;;","29/Sep/19 15:42;srowen;Issue resolved by pull request 25848
[https://github.com/apache/spark/pull/25848];;;",,,,,,,,,,,,,,,,,,,,,,
Event log file is written without specific charset which should be ideally UTF-8,SPARK-29160,13257529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kabhwan,kabhwan,18/Sep/19 22:51,12/Dec/22 18:10,13/Jul/23 08:49,21/Sep/19 15:00,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"This issue is from observation by [~vanzin] : [https://github.com/apache/spark/pull/25670#discussion_r325383512]

Quoting his comment here:

{quote}
This is a long standing bug in the original code, but this should be explicitly setting the charset to UTF-8 (using new PrintWriter(new OutputStreamWriter(...)).

The reader side should too, although doing that now could potentially break old logs... we should open a bug for this.
{quote}

While EventLoggingListener writes to UTF-8 properly when converting to byte[] before writing, it doesn't deal with charset in logEvent().

It should be fixed, but as Marcelo said, we also need to be aware of potential broken of reading old logs.",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-38411,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 21 15:00:05 UTC 2019,,,,,,,,,,"0|z06sdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 22:53;kabhwan;While I just added 3.0.0 as Affected Version, all versions we support might be affected.;;;","18/Sep/19 23:04;kabhwan;I'll raise a patch today. It might need to have config to ""use default charset"" when end users suffer from reading old log, but then writer should still use default charset to let reader reads both old log and new log, so it's somewhat messed up. I'll not apply this to the PR, but will add a comment instead.;;;","21/Sep/19 15:00;gurwls223;Issue resolved by pull request 25845
[https://github.com/apache/spark/pull/25845];;;",,,,,,,,,,,,,,,,,,,,,
DataSourceV2: Add DataFrameWriterV2 to Python API,SPARK-29157,13257508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,rdblue,rdblue,18/Sep/19 21:10,12/Dec/22 18:10,13/Jul/23 08:49,20/Jul/20 01:43,3.0.0,,,,,,,,,,3.1.0,,,PySpark,SQL,,,,0,,,,"After SPARK-28612 is committed, we need to add the corresponding PySpark API.",,bryanc,idomi,imback82,rdblue,,,,,,,,,,,,,,,,,,SPARK-28612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 01:43:41 UTC 2020,,,,,,,,,,"0|z06s8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Sep/19 13:34;idomi;It looks like https://issues.apache.org/jira/browse/SPARK-28612 was resolved.

 

What there is to do here?

Ido;;;","03/May/20 04:13;gurwls223;SPARK-28612 implemented Scala/Java APIs. This ticket targets to add Python side APIs.;;;","20/Jul/20 01:43;gurwls223;Issue resolved by pull request 27331
[https://github.com/apache/spark/pull/27331];;;",,,,,,,,,,,,,,,,,,,,,
Spark Executor Plugin API shutdown is not proper when dynamic allocation enabled,SPARK-29152,13257376,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rakson,jobitmathew,jobitmathew,18/Sep/19 13:07,02/Jun/21 16:29,13/Jul/23 08:49,10/Dec/19 22:24,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,,,3.0.0,,,Spark Core,,,,,0,,,,"*Issue Description*

Spark Executor Plugin API *shutdown handling is not proper*, when dynamic allocation enabled .Plugin's shutdown method is not processed when dynamic allocation is enabled and *executors become dead* after inactive time.

*Test Precondition*
1. Create a plugin and make a jar named SparkExecutorplugin.jar

import org.apache.spark.ExecutorPlugin;
public class ExecutoTest1 implements ExecutorPlugin{
    public void init(){
        System.out.println(""Executor Plugin Initialised."");
    }

    public void shutdown(){
        System.out.println(""Executor plugin closed successfully."");
    }
}

2. Create the  jars with the same and put it in folder /spark/examples/jars

*Test Steps*

1. launch bin/spark-sql with dynamic allocation enabled

./spark-sql --master yarn --conf spark.executor.plugins=ExecutoTest1  --jars /opt/HA/C10/install/spark/spark/examples/jars/SparkExecutorPlugin.jar --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.initialExecutors=2 --conf spark.dynamicAllocation.minExecutors=1

2 create a table , insert the data and select * from tablename
3.Check the spark UI Jobs tab/SQL tab
4. Check all Executors(executor tab will give all executors details) application log file for Executor plugin Initialization and Shutdown messages or operations.
Example /yarn/logdir/application_1567156749079_0025/container_e02_1567156749079_0025_01_000005/ stdout

5. Wait for the executor to be dead after the inactive time and check the same container log 
6. Kill the spark sql and check the container log  for executor plugin shutdown.

*Expect Output*

1. Job should be success. Create table ,insert and select query should be success.

2.While running query All Executors  log should contain the executor plugin Init messages or operations.
""Executor Plugin Initialised.

3.Once the executors are dead ,shutdown message should be there in log file.
“ Executor plugin closed successfully.

4.Once the sql application closed ,shutdown message should be there in log.
“ Executor plugin closed successfully"". 


*Actual Output*

Shutdown message is not called when executor is dead after inactive time.

*Observation*
Without dynamic allocation Executor plugin is working fine. But after enabling dynamic allocation,Executor shutdown is not processed.


",,jobitmathew,rakson,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35610,,,,,,SPARK-24918,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 22:24:46 UTC 2019,,,,,,,,,,"0|z06rfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 13:12;rakson;I will look into this issue.;;;","06/Dec/19 09:35;jobitmathew;Reopening the Jira, as the issue exists in the master branch also.;;;","10/Dec/19 22:24;vanzin;Issue resolved by pull request 26810
[https://github.com/apache/spark/pull/26810];;;",,,,,,,,,,,,,,,,,,,,,
"Spark SQL cannot handle ""NOT IN"" condition when using ""JOIN""",SPARK-29145,13257345,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,angerszhuuu,caidezhi655,caidezhi655,18/Sep/19 10:31,11/May/21 07:46,13/Jul/23 08:49,24/Oct/19 12:57,2.1.3,2.2.3,2.3.4,2.4.4,,,,,,,3.0.0,,,SQL,,,,,0,,,,"sample sql: 

{code}
spark.range(10).createOrReplaceTempView(""A"")
spark.range(10).createOrReplaceTempView(""B"")
spark.range(10).createOrReplaceTempView(""C"")
sql(""""""select * from A inner join B on A.id=B.id and A.id NOT IN (select id from C)"""""")
{code}
 
{code}
org.apache.spark.sql.AnalysisException: Table or view not found: C; line 1 pos 74;
'Project [*]
+- 'Join Inner, ((id#0L = id#2L) AND NOT id#0L IN (list#6 []))
   :  +- 'Project ['id]
   :     +- 'UnresolvedRelation [C]
   :- SubqueryAlias `a`
   :  +- Range (0, 10, step=1, splits=Some(12))
   +- SubqueryAlias `b`
      +- Range (0, 10, step=1, splits=Some(12))

  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:94)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:89)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:155)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:154)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:154)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:89)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:86)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:120)
...
{code}
 ",,angerszhuuu,apachespark,caidezhi655,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 07:46:07 UTC 2021,,,,,,,,,,"0|z06r8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Sep/19 07:59;angerszhuuu;maybe we should just use:
{code:java}
sql(""""""select * from A inner join B where A.id=B.id and A.id NOT IN (select id from C)"""""")
{code};;;","19/Sep/19 08:37;caidezhi655;[~angerszhuuu] we have workarounds,but using ""on"" when ""join"" tables is the SQL standard, so i report the issue to the community.
 Workarounds:
{code:java}
1. sql(""""""select * from A inner join B where A.id=B.id and A.id NOT IN (select id from C)""""""){code}
{code:java}
2. sql(""""""select * from A inner join B on A.id=B.id left outer join C on A.id=C.id where C.id is null""""""){code}
 ;;;","19/Sep/19 15:31;angerszhuuu;Currently,

IN/EXISTS/JOIN predicate sub-queries can only be used in Filter/DeleteFromTable. 

JOIN .. ON won't build a Filter LogicalPlan

JOIN .. WHERE will build a Filter LogicalPlan;;;","19/Sep/19 16:05;angerszhuuu;[~caidezhi655]

Make a  PR for support  it, But I don't know if it will cause other problem, need run all test case .;;;","24/Oct/19 12:57;maropu;Resolved by [https://github.com/apache/spark/pull/25854|https://github.com/apache/spark/pull/25854#];;;","11/May/21 07:45;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32499;;;","11/May/21 07:46;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32499;;;",,,,,,,,,,,,,,,,,
Binarizer handle sparse vectors incorrectly with negative threshold,SPARK-29144,13257339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,18/Sep/19 09:59,23/Sep/19 06:50,13/Jul/23 08:49,21/Sep/19 00:23,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,,,,,,3.0.0,,,ML,,,,,0,,,,"the process on sparse vector is wrong if thread<0:
{code:java}
scala> val data = Seq((0, Vectors.sparse(3, Array(1), Array(0.5))), (1, Vectors.dense(Array(0.0, 0.5, 0.0))))
data: Seq[(Int, org.apache.spark.ml.linalg.Vector)] = List((0,(3,[1],[0.5])), (1,[0.0,0.5,0.0]))

scala> val df = data.toDF(""id"", ""feature"")
df: org.apache.spark.sql.DataFrame = [id: int, feature: vector]

scala> val binarizer: Binarizer = new Binarizer().setInputCol(""feature"").setOutputCol(""binarized_feature"").setThreshold(-0.5)
binarizer: org.apache.spark.ml.feature.Binarizer = binarizer_1c07ac2ae3c8

scala> binarizer.transform(df).show()
+---+-------------+-----------------+
| id|      feature|binarized_feature|
+---+-------------+-----------------+
|  0|(3,[1],[0.5])|    [0.0,1.0,0.0]|
|  1|[0.0,0.5,0.0]|    [1.0,1.0,1.0]|
+---+-------------+-----------------+
{code}
expected outputs of the above two input vectors should be the same.

 

To deal with sparse vectors with threshold < 0, we have two options:

1, return 1 for non-active items, but this will convert sparse vectors to dense ones

2, throw an exception like what Scikit-Learn's [Binarizer|https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html] does:
{code:java}
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.preprocessing import Binarizer

row = np.array([0, 0, 1, 2, 2, 2])
col = np.array([0, 2, 2, 0, 1, 2])
data = np.array([1, 2, 3, 4, 5, 6])
a = csr_matrix((data, (row, col)), shape=(3, 3))
binarizer = Binarizer(threshold=-1.0)
binarizer.transform(a)
Traceback (most recent call last):  File ""<ipython-input-24-7e12ab26b3ed>"", line 1, in <module>
    binarizer.transform(a)  File ""/home/zrf/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py"", line 1874, in transform
    return binarize(X, threshold=self.threshold, copy=copy)  File ""/home/zrf/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py"", line 1774, in binarize
    raise ValueError('Cannot binarize a sparse matrix with threshold 'ValueError: Cannot binarize a sparse matrix with threshold < 0 {code}
 ",,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 21 00:23:04 UTC 2019,,,,,,,,,,"0|z06r7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 10:04;podongfeng;I prefer option 2, and will send a PR for this.;;;","21/Sep/19 00:23;srowen;Issue resolved by pull request 25829
[https://github.com/apache/spark/pull/25829];;;",,,,,,,,,,,,,,,,,,,,,,
"Flaky test: org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite.randomized aggregation test - [with partial + unsafe, with distinct] - with grouping keys",SPARK-29140,13257286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,18/Sep/19 05:44,21/Sep/19 08:07,13/Jul/23 08:49,21/Sep/19 07:31,3.0.0,,,,,,,,,,3.0.0,,,SQL,Tests,,,,0,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110840/testReport/]
{code:java}
sbt.ForkMain$ForkError: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange rangepartitioning(group#100846 ASC NULLS FIRST, 5), true, [id=#110477]
+- *(3) HashAggregate(keys=[(id#100836 % 10)#100858], functions=[max(c1#100838), count(distinct c3#100840)], output=[group#100846, max(c1)#100852, count(c3)#100853L])
   +- Exchange hashpartitioning((id#100836 % 10)#100858, 5), true, [id=#110473]
      +- *(2) HashAggregate(keys=[(id#100836 % 10)#100858], functions=[merge_max(c1#100838), partial_count(distinct c3#100840)], output=[(id#100836 % 10)#100858, max#100860, count#100863L])
         +- *(2) HashAggregate(keys=[(id#100836 % 10)#100858, c3#100840], functions=[merge_max(c1#100838)], output=[(id#100836 % 10)#100858, c3#100840, max#100860])
            +- Exchange hashpartitioning((id#100836 % 10)#100858, c3#100840, 5), true, [id=#110468]
               +- *(1) HashAggregate(keys=[(id#100836 % 10) AS (id#100836 % 10)#100858, c3#100840], functions=[partial_max(c1#100838)], output=[(id#100836 % 10)#100858, c3#100840, max#100860])
                  +- *(1) Project [id#100836, c1#100838, c3#100840]
                     +- *(1) Scan ExistingRDD[id#100836,c0#100837,c1#100838,c2#100839,c3#100840]

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:524)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:452)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:451)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:495)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:124)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:329)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:378)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3382)
	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2740)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3372)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3368)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2740)
	at org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite.$anonfun$makeRandomizedTests$8(ObjectHashAggregateSuite.scala:352)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:47)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:31)
	at org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(ObjectHashAggregateSuite.scala:37)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:231)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:229)
	at org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite.withSQLConf(ObjectHashAggregateSuite.scala:37)
	at org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite.$anonfun$makeRandomizedTests$7(ObjectHashAggregateSuite.scala:339)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange hashpartitioning((id#100836 % 10)#100858, 5), true, [id=#110473]
+- *(2) HashAggregate(keys=[(id#100836 % 10)#100858], functions=[merge_max(c1#100838), partial_count(distinct c3#100840)], output=[(id#100836 % 10)#100858, max#100860, count#100863L])
   +- *(2) HashAggregate(keys=[(id#100836 % 10)#100858, c3#100840], functions=[merge_max(c1#100838)], output=[(id#100836 % 10)#100858, c3#100840, max#100860])
      +- Exchange hashpartitioning((id#100836 % 10)#100858, c3#100840, 5), true, [id=#110468]
         +- *(1) HashAggregate(keys=[(id#100836 % 10) AS (id#100836 % 10)#100858, c3#100840], functions=[partial_max(c1#100838)], output=[(id#100836 % 10)#100858, c3#100840, max#100860])
            +- *(1) Project [id#100836, c1#100838, c3#100840]
               +- *(1) Scan ExistingRDD[id#100836,c0#100837,c1#100838,c2#100839,c3#100840]

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:524)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:452)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:451)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:495)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:160)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:717)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:74)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.createShuffledRDD(ShuffleExchangeExec.scala:82)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:93)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 81 more
Caused by: sbt.ForkMain$ForkError: java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 380, Column 38: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 380, Column 38: IDENTIFIER expected instead of '['
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1314)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:692)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:691)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:74)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.createShuffledRDD(ShuffleExchangeExec.scala:82)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:93)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 105 more
Caused by: sbt.ForkMain$ForkError: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 380, Column 38: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 380, Column 38: IDENTIFIER expected instead of '['
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1380)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1456)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1453)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 125 more {code}",,kabhwan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 21 07:31:22 UTC 2019,,,,,,,,,,"0|z06qvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 09:26;kabhwan;I'm looking into this.;;;","18/Sep/19 09:31;kabhwan;Found the root cause. Will craft a patch.;;;","21/Sep/19 07:31;maropu;Resolved by https://github.com/apache/spark/pull/25830;;;",,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.SparkContextSuite.test gpu driver resource files and discovery under local-cluster mode,SPARK-29139,13257285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,18/Sep/19 05:43,07/Oct/19 05:04,13/Jul/23 08:49,07/Oct/19 02:50,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,Tests,,,,0,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110759/testReport/]
{code:java}
sbt.ForkMain$ForkError: java.util.concurrent.TimeoutException: Can't find 1 executors before 10000 milliseconds elapsed
	at org.apache.spark.TestUtils$.waitUntilExecutorsUp(TestUtils.scala:293)
	at org.apache.spark.SparkContextSuite.$anonfun$new$82(SparkContextSuite.scala:793)
	at org.apache.spark.SparkContextSuite.$anonfun$new$82$adapted(SparkContextSuite.scala:772)
	at org.apache.spark.SparkFunSuite.withTempDir(SparkFunSuite.scala:161)
	at org.apache.spark.SparkContextSuite.$anonfun$new$81(SparkContextSuite.scala:772)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748) {code}",,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28467,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 07 05:04:03 UTC 2019,,,,,,,,,,"0|z06qvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Sep/19 04:23;kabhwan;Looks like the build was super slow at that moment:
|[test driver discovery under local-cluster mode|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110759/testReport/org.apache.spark/SparkContextSuite/test_driver_discovery_under_local_cluster_mode]|15 sec|Failed|
|[test gpu driver resource files and discovery under local-cluster mode|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110759/testReport/org.apache.spark/SparkContextSuite/test_gpu_driver_resource_files_and_discovery_under_local_cluster_mode]|10 sec|Failed|
|[test resource scheduling under local-cluster mode|https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110759/testReport/org.apache.spark/SparkContextSuite/test_resource_scheduling_under_local_cluster_mode]|31 sec|Passed|

Though ""test resource scheduling under local-cluster mode"" was successful, it has been elapsed mostly under 10 secs, even under 20 secs for longest around 5 pages of history.

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110759/testReport/junit/org.apache.spark/SparkContextSuite/test_resource_scheduling_under_local_cluster_mode/history]

Other tests should have pretty higher timeout like it to handle such kind of slowness.;;;","06/Oct/19 23:16;kabhwan;[~dongjoon]
Somehow this wasn't marked as resolved when PR was merged. Could you handle this? Thanks in advance!;;;","07/Oct/19 02:50;dongjoon;This is resolved via https://github.com/apache/spark/pull/25864;;;","07/Oct/19 02:50;dongjoon;Sure. Sorry for the delay, [~kabhwan].;;;","07/Oct/19 05:04;kabhwan;No problem! I was trying to resolve by myself and realized I can't change assignee so needed to ask for help. Thanks for the quick turnaround!;;;",,,,,,,,,,,,,,,,,,,
Flaky test: pyspark.mllib.tests.test_streaming_algorithms.StreamingLogisticRegressionWithSGDTests.test_parameter_accuracy,SPARK-29138,13257283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,kabhwan,kabhwan,18/Sep/19 05:42,12/Dec/22 18:10,13/Jul/23 08:49,01/Feb/20 06:40,3.0.0,,,,,,,,,,3.0.0,,,MLlib,Tests,,,,1,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110686/testReport/]
{code:java}
Traceback (most recent call last):
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 266, in test_parameter_accuracy
    self._eventually(condition, catch_assertions=True)
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 74, in _eventually
    raise lastValue
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 65, in _eventually
    lastValue = condition()
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 263, in condition
    self.assertAlmostEqual(rel, 0.1, 1)
AssertionError: 0.17619737864096185 != 0.1 within 1 places {code}",,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 01 06:40:57 UTC 2020,,,,,,,,,,"0|z06quw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Sep/19 00:42;kabhwan;[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/111220/testReport/]

0.24354595657120295 != 0.1 within 1 places;;;","01/Feb/20 01:47;dongjoon;I saw this in three places (2 independent PRs and `Hadoop 2.7+Hive1.2` Jenkins job).
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7-hive-1.2/lastCompletedBuild/testReport/pyspark.mllib.tests.test_streaming_algorithms/StreamingLogisticRegressionWithSGDTests/test_parameter_accuracy/

cc [~tgraves]. This is the one we are hitting today. When I try this in JDK11 environment, this is not reproduced. So, the flakiness is not about JDK version.

https://github.com/apache/spark/pull/27424;;;","01/Feb/20 06:40;gurwls223;Issue resolved by pull request 27424
[https://github.com/apache/spark/pull/27424];;;",,,,,,,,,,,,,,,,,,,,,
Flaky test: pyspark.mllib.tests.test_streaming_algorithms.StreamingLinearRegressionWithTests.test_train_prediction,SPARK-29137,13257281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,kabhwan,kabhwan,18/Sep/19 05:41,12/Dec/22 18:10,13/Jul/23 08:49,02/Jun/20 05:04,3.0.0,,,,,,,,,,3.0.0,,,MLlib,Tests,,,,0,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110686/testReport/]
{code:java}
Traceback (most recent call last):
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 503, in test_train_prediction
    self._eventually(condition)
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 69, in _eventually
    lastValue = condition()
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 498, in condition
    self.assertGreater(errors[1] - errors[-1], 2)
AssertionError: 1.672640157855923 not greater than 2 {code}",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 05:04:28 UTC 2020,,,,,,,,,,"0|z06qug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Apr/20 06:54;kabhwan;Still valid on latest master (3.1.0-SNAPSHOT).

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/121229

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/121231

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/121232
;;;","27/May/20 05:05;kabhwan;Still valid on latest master.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123144/consoleFull

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123146/testReport/

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123141/testReport/

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123142/testReport/

Would we need to disable the test for now?;;;","02/Jun/20 03:51;gurwls223;It became much more flaky .. let me try to increase the timeout for now.;;;","02/Jun/20 03:56;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28701;;;","02/Jun/20 05:04;dongjoon;Issue resolved by pull request 28701
[https://github.com/apache/spark/pull/28701];;;",,,,,,,,,,,,,,,,,,,
"Add a Python, Pandas and PyArrow versions in clue at SQL query tests",SPARK-29127,13257241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,brkyvz,18/Sep/19 00:29,12/Dec/22 17:51,13/Jul/23 08:49,16/Nov/19 02:47,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"Once Python test cases is failed in integrated UDF test cases, it's difficult to find out the version informations. See https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/113828/testReport/org.apache.spark.sql/SQLQueryTestSuite/sql___Scalar_Pandas_UDF/ as an example

It might be better to add the version information.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 16 02:47:05 UTC 2019,,,,,,,,,,"0|z06qlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Nov/19 02:42;dongjoon;Hi, [~brkyvz] and [~hyukjin.kwon]. 
Sorry, but I'll switch the both JIRA issue IDs due to the following.
- https://github.com/apache/spark/commit/7720781695d47fe0375f6e1150f6981b886686bd;;;","16/Nov/19 02:47;dongjoon;This is resolved via https://github.com/apache/spark/pull/26538;;;",,,,,,,,,,,,,,,,,,,,,,
Expose more details when ApplicationMaster reporter faces a fatal exception,SPARK-29112,13257017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,17/Sep/19 03:12,17/May/20 18:13,13/Jul/23 08:49,18/Sep/19 06:13,2.4.4,3.0.0,,,,,,,,,3.0.0,,,Spark Core,YARN,,,,0,,,,"In {{ApplicationMaster.Reporter}} thread, fatal exception information is swallowed. It's better to expose.
A thrift server was shutdown due to some fatal exception but no useful information from log.
{code}
19/09/16 06:59:54,498 INFO [Reporter] yarn.ApplicationMaster:54 : Final app status: FAILED, exitCode: 12, (reason: Exception was thrown 1 time(s) from Reporter thread.)
19/09/16 06:59:54,500 ERROR [Driver] thriftserver.HiveThriftServer2:91 : Error starting HiveThriftServer2
java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:160)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:708)
{code}",,cltlfcjin,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 18 06:13:34 UTC 2019,,,,,,,,,,"0|z06p8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 06:13;jerryshao;Issue resolved by pull request 25810
https://github.com/apache/spark/pull/25810;;;",,,,,,,,,,,,,,,,,,,,,,,
SHS may delete driver log file of in progress application,SPARK-29105,13256991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,17/Sep/19 00:02,18/Sep/19 16:12,13/Jul/23 08:49,18/Sep/19 16:12,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"There's an issue with how the SHS cleans driver logs that is similar to the problem of event logs: because the file size is not updated when you write to it, the SHS fails to detect activity and thus may delete the file while it's still being written to.

SPARK-24787 added a workaround in the SHS so that it can detect that situation for in-progress apps, replacing the previous solution which was too slow for event logs.

But that doesn't work for driver logs because they do not follow the same pattern (different file names for in-progress files), and thus would require the SHS to open the driver log files on every scan, which is expensive.

The old approach (using the {{hsync}} API) seems to be a good match for the driver logs, though, which don't slow down the listener bus like event logs do.",,gsomogyi,roczei,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 18 16:12:19 UTC 2019,,,,,,,,,,"0|z06p2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 16:12;vanzin;Issue resolved by pull request 25819
[https://github.com/apache/spark/pull/25819];;;",,,,,,,,,,,,,,,,,,,,,,,
CSV datasource returns incorrect .count() from file with malformed records,SPARK-29101,13256960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sandeep.katta2007,stwhit,stwhit,16/Sep/19 19:38,12/Dec/22 18:10,13/Jul/23 08:49,18/Sep/19 14:34,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,,,,2.4.5,3.0.0,,SQL,,,,,1,correctness,,,"Spark 2.4 introduced a change to the way csv files are read.  See [Upgrading From Spark SQL 2.3 to 2.4|https://spark.apache.org/docs/2.4.0/sql-migration-guide-upgrade.html#upgrading-from-spark-sql-23-to-24] for more details.

In that document, it states: _To restore the previous behavior, set spark.sql.csv.parser.columnPruning.enabled to false._

I am configuring Spark 2.4.4 as such, yet I'm still getting results inconsistent with pre-2.4.  For example:

Consider this file (fruit.csv).  Notice it contains a header record, 3 valid records, and one malformed record.

{noformat}
fruit,color,price,quantity
apple,red,1,3
banana,yellow,2,4
orange,orange,3,5
xxx
{noformat}
 
With Spark 2.1.1, if I call .count() on a DataFrame created from this file (using option DROPMALFORMED), ""3"" is returned.

{noformat}
(using Spark 2.1.1)
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").count
19/09/16 14:28:01 WARN CSVRelation: Dropping malformed line: xxx
res1: Long = 3
{noformat}

With Spark 2.4.4, I set the ""spark.sql.csv.parser.columnPruning.enabled"" option to false to restore the pre-2.4 behavior for handling malformed records, then call .count() and ""4"" is returned.

{noformat}
(using spark 2.4.4)
scala> spark.conf.set(""spark.sql.csv.parser.columnPruning.enabled"", false)
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").count
res1: Long = 4
{noformat}

So, using the *spark.sql.csv.parser.columnPruning.enabled* option did not actually restore previous behavior.

How can I, using Spark 2.4+, get a count of the records in a .csv which excludes malformed records?
",,dongjoon,stwhit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 19 22:25:55 UTC 2019,,,,,,,,,,"0|z06ovs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Sep/19 15:48;sandeep.katta2007;I found the reason, soon I will raise PR for this;;;","18/Sep/19 14:34;gurwls223;Issue resolved by pull request 25820
[https://github.com/apache/spark/pull/25820];;;","19/Sep/19 22:25;dongjoon;This is backported to branch-2.4 for Apache Spark 2.4.5 via https://github.com/apache/spark/pull/25843;;;",,,,,,,,,,,,,,,,,,,,,
Codegen with switch in InSet expression causes compilation error,SPARK-29100,13256947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,16/Sep/19 18:47,17/Sep/19 04:02,13/Jul/23 08:49,17/Sep/19 03:06,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"SPARK-26205 adds an optimization to InSet that generates Java switch condition for certain cases. When the given set is empty, it is possibly that codegen causes compilation error:

 

[info] - SPARK-29100: InSet with empty input set *** FAILED *** (58 milliseconds)                                      

[info]   Code generation of input[0, int, true] INSET () failed:                                                                        

[info]   org.codehaus.janino.InternalCompilerException: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling ""GeneratedClass"" in ""generated.java"": Compiling ""apply(java.lang.Object _i)""; apply(java.lang.Object _i): Operand stack inconsistent at offset 45: Previous size 0, now 1                                                                                                           

[info]   org.codehaus.janino.InternalCompilerException: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling ""GeneratedClass"" in ""generated.java"": Compiling ""apply(java.lang.Object _i)""; apply(java.lang.Object _i): Operand stack inconsistent at offset 45: Previous size 0, now 1                                                                                                           

[info]         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1308)                                                                                        

[info]         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1386)               

[info]         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1383)",,cloud_fan,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26205,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 17 03:06:52 UTC 2019,,,,,,,,,,"0|z06osw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Sep/19 03:06;cloud_fan;Issue resolved by pull request 25806
[https://github.com/apache/spark/pull/25806];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark driver cannot start with only delegation tokens,SPARK-29082,13256645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,13/Sep/19 23:38,27/Sep/19 15:09,13/Jul/23 08:49,24/Sep/19 18:12,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"If you start a Spark application with just delegation tokens, it fails. For example, from an Oozie launch, you see things like this (line numbers may be different):

{noformat}
No child hadoop job is executed.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.oozie.action.hadoop.LauncherAM.runActionMain(LauncherAM.java:410)
        at org.apache.oozie.action.hadoop.LauncherAM.access$300(LauncherAM.java:55)
        at org.apache.oozie.action.hadoop.LauncherAM$2.run(LauncherAM.java:223)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.oozie.action.hadoop.LauncherAM.run(LauncherAM.java:217)
        at org.apache.oozie.action.hadoop.LauncherAM$1.run(LauncherAM.java:153)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.oozie.action.hadoop.LauncherAM.main(LauncherAM.java:141)
Caused by: org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: hrt_qa javax.security.auth.login.LoginException: Unable to obtain password from user

        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1847)
        at org.apache.hadoop.security.UserGroupInformation.getUGIFromTicketCache(UserGroupInformation.java:616)
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.doLogin(HadoopDelegationTokenManager.scala:276)
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainDelegationTokens(HadoopDelegationTokenManager.scala:140)
        at org.apache.spark.deploy.yarn.Client.setupSecurityToken(Client.scala:305)
        at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:1057)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:179)
        at org.apache.spark.deploy.yarn.Client.run(Client.scala:1178)
        at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1584)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:860)
{noformat}
",,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 24 18:12:39 UTC 2019,,,,,,,,,,"0|z06mxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Sep/19 20:30;vanzin;Issue resolved by pull request 25805
[https://github.com/apache/spark/pull/25805];;;","20/Sep/19 14:15;srowen;Reopened because we had to revert it for now;;;","24/Sep/19 18:12;dongjoon;Issue resolved by pull request 25901
[https://github.com/apache/spark/pull/25901];;;",,,,,,,,,,,,,,,,,,,,,
Properly track shuffle write time with refactor,SPARK-29072,13256444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mcheah,mcheah,mcheah,13/Sep/19 01:02,17/May/20 18:30,13/Jul/23 08:49,16/Sep/19 14:09,3.0.0,,,,,,,,,,3.0.0,,,Shuffle,Spark Core,,,,0,,,,"From SPARK-28209, SPARK-28570, and SPARK-28571, we used the new shuffle writer plugin API across all the shuffle writers. However, we accidentally lost time tracking metrics for shuffle writes in the process, particularly for UnsafeShuffleWriter when writing with streams (without transferTo), as well as the SortShuffleWriter.",,irashid,mcheah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 16 14:09:01 UTC 2019,,,,,,,,,,"0|z06lp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Sep/19 14:09;irashid;Issue resolved by pull request 25780
[https://github.com/apache/spark/pull/25780];;;",,,,,,,,,,,,,,,,,,,,,,,
Add V1_BATCH_WRITE to the TableCapabilityChecks in the Analyzer,SPARK-29062,13256233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,12/Sep/19 00:45,20/Sep/19 14:05,13/Jul/23 08:49,20/Sep/19 14:05,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,Currently the checks in the Analyzer require that V2 Tables have BATCH_WRITE defined for all tables that have V1 Write fallbacks. This is confusing as these tables may not have the V2 writer interface implemented yet.,,brkyvz,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 20 14:05:55 UTC 2019,,,,,,,,,,"0|z06ke8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Sep/19 14:05;cloud_fan;Issue resolved by pull request 25767
[https://github.com/apache/spark/pull/25767];;;",,,,,,,,,,,,,,,,,,,,,,,
ThriftServerSessionPage displays 1970/01/01 for queries that are not finished and not closed,SPARK-29056,13256142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,11/Sep/19 15:21,13/Sep/19 16:44,13/Jul/23 08:49,13/Sep/19 16:14,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Spark UI ODBC/JDBC tab session page displays 1970/01/01 (timestamp 0) as finish/close time for queries that haven't finished yet.

!image-2019-09-11-17-21-52-771.png!",,juliuszsompolski,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 13 16:14:26 UTC 2019,,,,,,,,,,"0|z06ju0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Sep/19 16:14;yumwang;Issue resolved by pull request 25762
[https://github.com/apache/spark/pull/25762];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark UI storage memory increasing overtime,SPARK-29055,13256113,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,Geopap,Geopap,11/Sep/19 13:13,16/Mar/20 07:24,13/Jul/23 08:49,01/Oct/19 16:49,2.3.3,,,,,,,,,,2.4.5,3.0.0,,Block Manager,Spark Core,,,,1,,,,"I used Spark 2.1.1 and I upgraded into new versions. After Spark version 2.3.3,  I observed from Spark UI that the driver memory is{color:#ff0000} increasing continuously.{color}

In more detail, the driver memory and executors memory have the same used memory storage and after each iteration the storage memory is increasing. You can reproduce this behavior by running the following snippet code. The following example, is very simple, without any dataframe persistence, but the memory consumption is not stable as it was in former Spark versions (Specifically until Spark 2.3.2).

Also, I tested with Spark streaming and structured streaming API and I had the same behavior. I tested with an existing application which reads from Kafka source and do some aggregations, persist dataframes and then unpersist them. The persist and unpersist it works correct, I see the dataframes in the storage tab in Spark UI and after the unpersist, all dataframe have removed. But, after the unpersist the executors memory is not zero, BUT has the same value with the driver memory. This behavior also affects the application performance because the memory of the executors is increasing as the driver increasing and after a while the persisted dataframes are not fit in the executors memory and  I have spill to disk.

Another error which I had after a long running, was {color:#ff0000}java.lang.OutOfMemoryError: GC overhead limit exceeded, but I don't know if its relevant with the above behavior or not.{color}

 

*HOW TO REPRODUCE THIS BEHAVIOR:*

Create a very simple application(streaming count_file.py) in order to reproduce this behavior. This application reads CSV files from a directory, count the rows and then remove the processed files.
{code:java}
import time
import os

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T

target_dir = ""...""

spark=SparkSession.builder.appName(""DataframeCount"").getOrCreate()

while True:
    for f in os.listdir(target_dir):
        df = spark.read.load(target_dir + f, format=""csv"")
        print(""Number of records: {0}"".format(df.count()))
        time.sleep(15){code}
Submit code:
{code:java}
spark-submit 
--master spark://xxx.xxx.xx.xxx
--deploy-mode client
--executor-memory 4g
--executor-cores 3
streaming count_file.py
{code}
 

*TESTED CASES WITH THE SAME BEHAVIOUR:*
 * I tested with default settings (spark-defaults.conf)
 * Add spark.cleaner.periodicGC.interval 1min (or less)
 * {{Turn spark.cleaner.referenceTracking.blocking}}=false
 * Run the application in cluster mode
 * Increase/decrease the resources of the executors and driver
 * I tested with extraJavaOptions in driver and executor -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12
  

*DEPENDENCIES*
 * Operation system: Ubuntu 16.04.3 LTS
 * Java: jdk1.8.0_131 (tested also with jdk1.8.0_221)
 * Python: Python 2.7.12

 

*NOTE:* In Spark 2.1.1 the driver memory consumption (Storage Memory tab) was extremely low and after the run of ContextCleaner and BlockManager the memory was decreasing.",,bigicecream,Geopap,jkleckner,kabhwan,maxgekk,sandeep.katta2007,toopt4,viirya,,,,,,,,,,,,,,,SPARK-29321,,,,,,,,,,,,,,,,,,,,,,SPARK-29301,SPARK-27648,,"16/Sep/19 12:58;Geopap;test_csvs.zip;https://issues.apache.org/jira/secure/attachment/12980412/test_csvs.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Tue Oct 01 19:34:10 UTC 2019,,,,,,,,,,"0|z06jnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Sep/19 11:55;maxgekk;[~Geopap] Can you provide the csv files?;;;","16/Sep/19 13:00;Geopap;[~maxgekk] I have uploaded a zip with few test csvs which you can use to reproduce the error.;;;","27/Sep/19 22:28;jkleckner;[~Geopap] How quickly does the memory grow?;;;","28/Sep/19 14:02;Geopap;[~jkleckner] its grows few KB after each action. I have tested with different applications with different inputs, and the growth was the same; it is independent of the input data volume. Maybe this growth is caused from metadata that have not be cleared successfully.;;;","29/Sep/19 06:03;kabhwan;[~Geopap]

Could you reproduce against latest Spark 2.4.4? Even better if you could reproduce it against latest master branch.

How long (many jobs) you've run the query when you encounter the memory issue?

Is this issue same with SPARK-27648?;;;","29/Sep/19 13:43;Geopap;[~kabhwan]

I have tested also with the latest Spark 2.4.4, and I had the same behavior.

Initially, I tested with production applications in a testing environment. I run them for a lot of hours, but the memory storage didn't have any decrease.

Due to the production applications are based on Spark streaming and structured streaming, it is difficult someone else to reproduce the same environment,  so I created a simple application (snippet code).

You can run the snippet code (with spark version <=2.3.2) and observe the storage memory of the executors and the driver; then run the same code with 2.3.3 or newer and observe the difference. I think the starting point is to reproduce the same behavior with a very simple application like the application in the snippet code.

In my view the problem is not related with the Spark API (batch, spark streaming, structured streaming etc) but it is something more fundamental in Spark versions. The issue SPARK-27648 it looks the same, but I think the problem is not related with the structured streaming (Initially, I faced it with a structured streaming application, but then I understand that it was not the structured streaming API);;;","29/Sep/19 13:48;Geopap;[~yy3b2007com]

You have created the SPARK-27648   issue. I think my issue is the same with yours, can you take a look to this issue if its similar to yours?;;;","29/Sep/19 14:02;kabhwan;[~Geopap]

Could you take memory dump for multiple of times which there are huge difference of memory usages? How long does your minimized reproducer run before showing outstanding difference? If you could provide more information it would save bunch of hours from contributors who are interested on this.;;;","30/Sep/19 07:16;kabhwan;[~Geopap]

I guess you're hitting SPARK-29301. Please take a look at the description, and if you don't mind please try out with the patch.

Btw, thanks for providing simple reproducer!;;;","30/Sep/19 18:14;Geopap;[~kabhwan]

Good news!! I run the snippet code with your patch and the Spark looks to works right now!!

I have a question about the PR description; You said ""The storage memory in metrics in AppStatusListener is now out of sync which lets end users easy to confuse as memory leak is happening."". Do you mean that it was not a leak, but a wrong in the storage memory in UI?

Because, the memory of the JVM (MEM%) in the system was also increasing before your patch, and after your correction the JVM memory is also stable.;;;","01/Oct/19 01:45;kabhwan;I haven't observed increased memory usage even without the patch. jvisualvm showed the graph of heap memory usage which looked OK. Could you attach jvisualvm or other profiler tool and take a look at heap memory usage on driver?;;;","01/Oct/19 10:56;Geopap; 
[~kabhwan]
First of all thank for your time about this issue!!
As I described in the initial description, after a long time run, I start having spill to disk. So, maybe the bug in the UI memory usage report which is look to be corrected with ([GitHub Pull Request #25973|https://github.com/apache/spark/pull/25973]), there is another hidden bug.
 
I run the code in the snippet (I test it without any sleeping time, in order to see the results faster) and I have recorded the JVM memory usage for approximately 1 hour between Spark 2.4.4 and your branch with your patch.
Spark JVM memory with Spark 2.4.4:
||Time||RES||SHR||MEM%||
|1min|{color:#de350b}1349{color}|32724|1.5|
|3min|{color:#de350b}1936{color}|32724|2.2|
|5min|{color:#de350b}2506{color}|32724|2.6|
|7min|{color:#de350b}2564{color}|32724|2.7|
|9min|{color:#de350b}2584{color}|32724|2.7|
|11min|{color:#de350b}2585{color}|32724|2.7|
|13min|{color:#de350b}2592{color}|32724|2.7|
|15min|{color:#de350b}2591{color}|32724|2.7|
|17min|{color:#de350b}2591{color}|32724|2.7|
|30min|{color:#de350b}2600{color}|32724|2.7|
|1h|{color:#de350b}2618{color}|32724|2.7|

 

Spark JVM memory with Spark patch([GitHub Pull Request #25973|https://github.com/apache/spark/pull/25973])

 
||Time||RES||SHR||MEM%||
|1min|{color:#de350b}1134{color}|25380|1.4|
|3min|{color:#de350b}1520{color}|25380|1.6|
|5min|{color:#de350b}1570{color}|25380|1.6|
|7min|{color:#de350b}1598{color}|25380|1.7|
|9min|{color:#de350b}1613{color}|25380|1.7|
|11min|{color:#de350b}1616{color}|25380|1.7|
|15min|{color:#de350b}1620{color}|25380|1.7|
|17min|{color:#de350b}1625{color}|25380|1.7|
|30min|{color:#de350b}1629{color}|25380|1.7|
|1h|{color:#de350b}1660{color}|25380|1.7|

 

As you can see the RES memory is slightly increasing in both cases overtime. Also, when I tested with a real streaming application in a testing env after hours, the persisted dataframes overflows the memory and spill to disk.

* You can easily reproduce the above behavior, by running the snippet code (I prefer to run without any sleeping delay) and track the JVM memory with top or htop command.

 

 

 ;;;","01/Oct/19 18:32;jkleckner;[~vanzin] It seems from the previous comment that it is not resolved by PR #25973 as the memory continues to grow.

 

[~Geopap] I recorded an internal tech talk about how to connect/grab heap dump/analyze with jvisualvm if you are interested here: [https://www.youtube.com/channel/UCA81uPFG3aqo2X1YgZRAoEg]



 ;;;","01/Oct/19 18:46;viirya;Should we update the title of this ticket? It is not real memory leak in Spark, right?;;;","01/Oct/19 19:34;kabhwan;Looks like I can update the title and description, but then the information of ""possible memory leak"" is lost.
I could also clone the issue but it changes the reporter to me (as I don't have privilege to modify reporter) which is not desired.

[~Geopap] [~vanzin] [~viirya] Could anyone clone this issue? After cloning I'll update this issue accordingly.;;;",,,,,,,,,
Sort does not work on some columns,SPARK-29053,13256091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,aman_omer,jobitmathew,jobitmathew,11/Sep/19 10:20,23/Sep/19 15:09,13/Jul/23 08:49,21/Sep/19 12:37,2.4.3,,,,,,,,,,2.4.5,3.0.0,,Web UI,,,,,0,,,,"Spark Thrift JDBC/ODBC Server application UI, *Sorting* is not working for *Duration* and *Execution time* fields.

*Test Steps*
 1.Install spark
 2.Start Spark beeline
 3.Submit some SQL queries
 4.Close some spark applications
 5.Check the Spark Web UI JDBC/ODBC Server TAB.

*Issue:*
 *Sorting [ascending or descending]* based on *Duration* and *Execution time* is not proper in *JDBC/ODBC Server TAB*. 
 Issue there in *Session Statistics* & *SQL Statistics* session tables .Please check it.

Screenshots are attached.

!Duration_1.png|width=826,height=410!

!ExecutionTime_1.png|width=823,height=407!

 ",,aman_omer,jobitmathew,rakson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28599,,,,,,,,"20/Sep/19 07:08;aman_omer;Duration_1.png;https://issues.apache.org/jira/secure/attachment/12980850/Duration_1.png","20/Sep/19 07:09;aman_omer;ExecutionTime_1.png;https://issues.apache.org/jira/secure/attachment/12980851/ExecutionTime_1.png","11/Sep/19 10:24;jobitmathew;Sort Icon.png;https://issues.apache.org/jira/secure/attachment/12980067/Sort+Icon.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 21 13:30:02 UTC 2019,,,,,,,,,,"0|z06jio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Sep/19 10:22;rakson;I will work on this one.;;;","21/Sep/19 12:37;srowen;Resolved by https://github.com/apache/spark/pull/25855;;;","21/Sep/19 13:30;aman_omer;PR for branch 2.4 [https://github.com/apache/spark/pull/25855];;;",,,,,,,,,,,,,,,,,,,,,
Possible NPE on SQLConf.get when SparkContext is stopping in another thread,SPARK-29046,13256031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,11/Sep/19 06:29,12/Dec/22 18:10,13/Jul/23 08:49,15/Sep/19 18:05,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"We encountered NPE in listener code which deals with query plan - and according to the stack trace below, only possible case of NPE is SparkContext._dagScheduler being null, which is only possible while stopping SparkContext (unless null is set from outside).

 
{code:java}
19/09/11 00:22:24 INFO server.AbstractConnector: Stopped Spark@49d8c117{HTTP/1.1,[http/1.1]}{0.0.0.0:0}19/09/11 00:22:24 INFO server.AbstractConnector: Stopped Spark@49d8c117{HTTP/1.1,[http/1.1]}{0.0.0.0:0}19/09/11 00:22:24 INFO ui.SparkUI: Stopped Spark web UI at http://....:3277019/09/11 00:22:24 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors19/09/11 00:22:24 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down19/09/11 00:22:24 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices(serviceOption=None, services=List(), started=false)19/09/11 00:22:24 WARN sql.SparkExecutionPlanProcessor: Caught exception during parsing eventjava.lang.NullPointerException at org.apache.spark.sql.internal.SQLConf$$anonfun$15.apply(SQLConf.scala:133) at org.apache.spark.sql.internal.SQLConf$$anonfun$15.apply(SQLConf.scala:133) at scala.Option.map(Option.scala:146) at org.apache.spark.sql.internal.SQLConf$.get(SQLConf.scala:133) at org.apache.spark.sql.types.StructType.simpleString(StructType.scala:352) at com.hortonworks.spark.atlas.types.internal$.sparkTableToEntity(internal.scala:102) at com.hortonworks.spark.atlas.types.AtlasEntityUtils$class.tableToEntity(AtlasEntityUtils.scala:62) at com.hortonworks.spark.atlas.sql.CommandsHarvester$.tableToEntity(CommandsHarvester.scala:45) at com.hortonworks.spark.atlas.sql.CommandsHarvester$$anonfun$com$hortonworks$spark$atlas$sql$CommandsHarvester$$discoverInputsEntities$1.apply(CommandsHarvester.scala:240) at com.hortonworks.spark.atlas.sql.CommandsHarvester$$anonfun$com$hortonworks$spark$atlas$sql$CommandsHarvester$$discoverInputsEntities$1.apply(CommandsHarvester.scala:239) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241) at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104) at com.hortonworks.spark.atlas.sql.CommandsHarvester$.com$hortonworks$spark$atlas$sql$CommandsHarvester$$discoverInputsEntities(CommandsHarvester.scala:239) at com.hortonworks.spark.atlas.sql.CommandsHarvester$CreateDataSourceTableAsSelectHarvester$.harvest(CommandsHarvester.scala:104) at com.hortonworks.spark.atlas.sql.SparkExecutionPlanProcessor$$anonfun$2.apply(SparkExecutionPlanProcessor.scala:138) at com.hortonworks.spark.atlas.sql.SparkExecutionPlanProcessor$$anonfun$2.apply(SparkExecutionPlanProcessor.scala:89) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241) at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104) at com.hortonworks.spark.atlas.sql.SparkExecutionPlanProcessor.process(SparkExecutionPlanProcessor.scala:89) at com.hortonworks.spark.atlas.sql.SparkExecutionPlanProcessor.process(SparkExecutionPlanProcessor.scala:63) at com.hortonworks.spark.atlas.AbstractEventProcessor$$anonfun$eventProcess$1.apply(AbstractEventProcessor.scala:72) at com.hortonworks.spark.atlas.AbstractEventProcessor$$anonfun$eventProcess$1.apply(AbstractEventProcessor.scala:71) at scala.Option.foreach(Option.scala:257) at com.hortonworks.spark.atlas.AbstractEventProcessor.eventProcess(AbstractEventProcessor.scala:71) at com.hortonworks.spark.atlas.AbstractEventProcessor$$anon$1.run(AbstractEventProcessor.scala:38)19/09/11 00:22:24 WARN sql.SparkCatalogEventProcessor: Caught exception during parsing eventjava.lang.NullPointerException at org.apache.spark.sql.internal.SQLConf$$anonfun$15.apply(SQLConf.scala:133) at org.apache.spark.sql.internal.SQLConf$$anonfun$15.apply(SQLConf.scala:133) at scala.Option.map(Option.scala:146) at org.apache.spark.sql.internal.SQLConf$.get(SQLConf.scala:133) at org.apache.spark.sql.types.StructType.simpleString(StructType.scala:352) at com.hortonworks.spark.atlas.types.internal$.sparkTableToEntity(internal.scala:102) at com.hortonworks.spark.atlas.types.AtlasEntityUtils$class.sparkTableToEntity(AtlasEntityUtils.scala:69) at com.hortonworks.spark.atlas.sql.SparkCatalogEventProcessor.sparkTableToEntity(SparkCatalogEventProcessor.scala:28) at com.hortonworks.spark.atlas.sql.SparkCatalogEventProcessor.process(SparkCatalogEventProcessor.scala:80) at com.hortonworks.spark.atlas.sql.SparkCatalogEventProcessor.process(SparkCatalogEventProcessor.scala:28) at com.hortonworks.spark.atlas.AbstractEventProcessor$$anonfun$eventProcess$1.apply(AbstractEventProcessor.scala:72) at com.hortonworks.spark.atlas.AbstractEventProcessor$$anonfun$eventProcess$1.apply(AbstractEventProcessor.scala:71) at scala.Option.foreach(Option.scala:257) at com.hortonworks.spark.atlas.AbstractEventProcessor.eventProcess(AbstractEventProcessor.scala:71) at com.hortonworks.spark.atlas.AbstractEventProcessor$$anon$1.run(AbstractEventProcessor.scala:38)19/09/11 00:22:24 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!19/09/11 00:22:24 INFO memory.MemoryStore: MemoryStore cleared19/09/11 00:22:24 INFO storage.BlockManager: BlockManager stopped19/09/11 00:22:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped19/09/11 00:22:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!19/09/11 00:22:24 INFO spark.SparkContext: Successfully stopped SparkContext {code}",,dongjoon,kabhwan,Tradunsky,,,,,,,,,,,,,,,,,,,,,,,SPARK-31698,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 08:34:24 UTC 2020,,,,,,,,,,"0|z06j5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Sep/19 06:29;kabhwan;Will raise a patch soon.;;;","12/Sep/19 02:16;gurwls223;Issue resolved by pull request 25753
[https://github.com/apache/spark/pull/25753];;;","14/Sep/19 13:04;dongjoon;This is reverted in order to recover Jenkins jobs.;;;","15/Sep/19 18:05;dongjoon;Issue resolved by pull request 25790
[https://github.com/apache/spark/pull/25790];;;","13/May/20 08:09;Tradunsky;[~kabhwan] Do you know a lower version of spark which does not have this issue? Maybe Spark 2.3.2? 

 ;;;","13/May/20 08:34;kabhwan;Sorry I don't know. Also worth noting that Spark 2.3 version line was EOLed AFAIK.;;;",,,,,,,,,,,,,,,,,,
Test failed due to table already exists in SQLMetricsSuite,SPARK-29045,13256014,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,11/Sep/19 02:15,14/Sep/19 21:55,13/Jul/23 08:49,12/Sep/19 06:06,2.3.0,2.4.0,3.0.0,,,,,,,,2.4.5,3.0.0,,SQL,Tests,,,,0,,,,"In method {{SQLMetricsTestUtils.testMetricsDynamicPartition()}}, there is a CREATE TABLE sentence without {{withTable}} block. It causes test failure if use same table name in other unit tests.",,cltlfcjin,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21878,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 14 21:55:46 UTC 2019,,,,,,,,,,"0|z06j1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Sep/19 06:06;yumwang;Issue resolved by pull request 25752
[https://github.com/apache/spark/pull/25752];;;","14/Sep/19 21:55;dongjoon;This is backported to `branch-2.4` via https://github.com/apache/spark/commit/339b0f2a0c4043fca9cca52797936c8654910fc9;;;",,,,,,,,,,,,,,,,,,,,,,
[History Server]Only one replay thread of FsHistoryProvider work because of straggler,SPARK-29043,13256012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hzfeiwang,hzfeiwang,hzfeiwang,11/Sep/19 01:56,02/May/22 05:27,13/Jul/23 08:49,16/Dec/19 22:46,2.4.4,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"As shown in the attachment, we set spark.history.fs.numReplayThreads=30 for spark history server.
However, there is only one replay thread work because of straggler.

Let's check the code.
https://github.com/apache/spark/blob/7f36cd2aa5e066a807d498b8c51645b136f08a75/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L509-L547

There is a synchronous operation for all replay tasks.",,hzfeiwang,kabhwan,mauzhang,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-39083,,,,,,,,,,,,"11/Sep/19 07:09;hzfeiwang;image-2019-09-11-15-09-22-912.png;https://issues.apache.org/jira/secure/attachment/12980033/image-2019-09-11-15-09-22-912.png","11/Sep/19 07:10;hzfeiwang;image-2019-09-11-15-10-25-326.png;https://issues.apache.org/jira/secure/attachment/12980032/image-2019-09-11-15-10-25-326.png","11/Sep/19 02:01;hzfeiwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12980012/screenshot-1.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 16 22:46:21 UTC 2019,,,,,,,,,,"0|z06j14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Sep/19 02:25;hzfeiwang;I think it is better to replay logs asynchronously.;;;","11/Sep/19 02:39;kabhwan;It's asynchronous for replaying logs: it's synchronous for waiting for replaying logs to be finished, but it wouldn't matter much. So you may want to post full thread dump to show what other threads are doing when one thread is running for replaying logs. If it really doesn't work concurrently, there might be some other place being locked.;;;","11/Sep/19 07:08;hzfeiwang;[~kabhwan] Other replay threads are waiting for the straggler replay thread.
 !image-2019-09-11-15-10-25-326.png! 
 !image-2019-09-11-15-09-22-912.png! 
I think we can set a status for replaying log into application listing, such as Processing and Completed.

When we checking logs to replay, we could filter the logs which are processing to prevent been replayed repeatedly.;;;","11/Sep/19 07:48;kabhwan;Hmm... it would be better for us if you can elaborate what you have been struggling about, as the code is intended to wait for all tasks being finished, as commented in code. So we may need to be careful on trying to change it, and may need to find alternative approach to resolve the problem you're encountering.

To understand the problem:
 * How long ""spark.history.fs.update.interval"" has been set?
 * How many applications are reloaded per each call of checkForLogs?
 * How big the event log for each application is?
 * Do you think the improvements planned for event log help?
 ** [http://issues.apache.org/jira/browse/SPARK-28867]
 ** https://issues.apache.org/jira/browse/SPARK-28594

 ;;;","11/Sep/19 08:41;hzfeiwang;[~kabhwan]
* How long ""spark.history.fs.update.interval"" has been set?    20s
* How many applications are reloaded per each call of checkForLogs?   50000+
* How big the event log for each application is?    there maybe many large logs.

I think SPARK-28594 is more helpful for our case.;;;","11/Sep/19 12:45;kabhwan;50000+! I'm very surprised to hear that, as it means 50000+ of files are stored in same directory and being listed via SHS, and 50000+ of UI objects are loaded and rendered in SHS (one JVM).

I'd be appreciated if you can review design doc for SPARK-28594 to see whether it helps your case, and participate code review. Thanks!;;;","15/Sep/19 08:58;hzfeiwang;Thanks, [~kabhwan]. ;;;","16/Dec/19 22:46;vanzin;Issue resolved by pull request 25797
[https://github.com/apache/spark/pull/25797];;;",,,,,,,,,,,,,,,,
Sampling-based RDD with unordered input should be INDETERMINATE,SPARK-29042,13256010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,11/Sep/19 01:15,12/Dec/22 18:10,13/Jul/23 08:49,13/Sep/19 21:07,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,Spark Core,,,,,0,correctness,,,"We have found and fixed the correctness issue when RDD output is INDETERMINATE. One missing part is sampling-based RDD. This kind of RDDs is order sensitive to its input. A sampling-based RDD with unordered input, should be INDETERMINATE.",,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23243,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 09 01:31:07 UTC 2020,,,,,,,,,,"0|z06j0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Sep/19 21:07;viirya;Issue resolved by pull request 25751
[https://github.com/apache/spark/pull/25751];;;","18/Sep/19 20:16;viirya;[~hyukjin.kwon] Am I setting the fix versions and affects version correct after backport? Can you take a look? Thanks.

 ;;;","18/Sep/19 22:22;gurwls223;Usually I only set ""Fix Version/s"" as that's what the merge script does.
But I think it can be legitimate to set ""Affects Version/s"" too.;;;","09/Feb/20 01:31;dongjoon;Hi, [~viirya]. Could you update the `Affected Version` by checking at least `2.4.4` and `2.3.4`?;;;",,,,,,,,,,,,,,,,,,,,
Allow createDataFrame to accept bytes as binary type,SPARK-29041,13255994,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,10/Sep/19 23:09,12/Dec/22 18:10,13/Jul/23 08:49,11/Sep/19 23:52,2.4.4,3.0.0,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"{code}
spark.createDataFrame([[b""abcd""]], ""col binary"")
{code}

simply fails as below:

in Python 3

{code}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/session.py"", line 787, in createDataFrame
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/.../spark/python/pyspark/sql/session.py"", line 442, in _createFromLocal
    data = list(data)
  File ""/.../spark/python/pyspark/sql/session.py"", line 769, in prepare
    verify_func(obj)
  File ""/.../forked/spark/python/pyspark/sql/types.py"", line 1403, in verify
    verify_value(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1384, in verify_struct
    verifier(v)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1403, in verify
    verify_value(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1397, in verify_default
    verify_acceptable_types(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1282, in verify_acceptable_types
    % (dataType, obj, type(obj))))
TypeError: field col: BinaryType can not accept object b'abcd' in type <class 'bytes'>
{code}

in Python 2:

{code}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/session.py"", line 787, in createDataFrame
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/.../spark/python/pyspark/sql/session.py"", line 442, in _createFromLocal
    data = list(data)
  File ""/.../spark/python/pyspark/sql/session.py"", line 769, in prepare
    verify_func(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1403, in verify
    verify_value(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1384, in verify_struct
    verifier(v)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1403, in verify
    verify_value(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1397, in verify_default
    verify_acceptable_types(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1282, in verify_acceptable_types
    % (dataType, obj, type(obj))))
TypeError: field col: BinaryType can not accept object 'abcd' in type <type 'str'>
{code}

{{bytes}} should also be able to accepted as binary type",,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 20:16:14 UTC 2019,,,,,,,,,,"0|z06ix4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Sep/19 23:52;gurwls223;Issue resolved by pull request 25749
[https://github.com/apache/spark/pull/25749];;;","30/Sep/19 19:49;Tagar;Thank you [~hyukjin.kwon]

Our users say this issue exists in 2.3 too.. could it be possible to apply that patch to 2.3 branch as well?

 ;;;","11/Oct/19 06:24;gurwls223;It was discussed to not backport. See the discussion in the PR itself.
;;;","16/Oct/19 20:16;Tagar;[~hyukjin.kwon] thanks for getting back on this .. I see discussion in the PR regarding Python 2 and Python 3,

but no discussion regarding applying that patch to Spark 2.3... what do I miss? 

Thanks.;;;",,,,,,,,,,,,,,,,,,,,
PhysicalOperation.collectProjectsAndFilters should use AttributeMap while substituting aliases,SPARK-29029,13255768,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nikitakonda,nikitakonda,nikitakonda,10/Sep/19 00:47,20/Nov/19 04:07,13/Jul/23 08:49,20/Nov/19 04:03,2.3.0,,,,,,,,,,3.0.0,,,Optimizer,SQL,,,,0,,,,"We have a specific use case where in we are trying insert a custom logical operator in our logical plan to avoid some of the Spark’s optimization rules. However, we remove this logical operator as part of custom optimization rule before we send this to SparkStrategies.

However, we are hitting issue in the following scenario:

Analyzed plan:
{code:java}
[1] Project [userid#0]
+- [2] SubqueryAlias tmp6
   +- [3] Project [videoid#47L, avebitrate#2, userid#0]
      +- [4] Filter NOT (videoid#47L = cast(30 as bigint))
         +- [5] SubqueryAlias tmp5
            +- [6] CustomBarrier
               +- [7] Project [videoid#47L, avebitrate#2, userid#0]
                  +- [8] Filter (avebitrate#2 < 10)
                     +- [9] SubqueryAlias tmp3
                        +- [10] Project [avebitrate#2, factorial(videoid#1) AS videoid#47L, userid#0]
                           +- [11] SubqueryAlias tmp2
                              +- [12] Project [userid#0, videoid#1, avebitrate#2]
                                 +- [13] SubqueryAlias tmp1
                                    +- [14] Project [userid#0, videoid#1, avebitrate#2]
                                       +- [15] SubqueryAlias views
                                          +- [16] Relation[userid#0,videoid#1,avebitrate#2] 
{code}
 

Optimized Plan:
{code:java}
[1] Project [userid#0]
+- [2] Filter (isnotnull(videoid#47L) && NOT (videoid#47L = 30))
   +- [3] Project [factorial(videoid#1) AS videoid#47L, userid#0]
      +- [4] Filter (isnotnull(avebitrate#2) && (avebitrate#2 < 10))
         +- [5] Relation[userid#0,videoid#1,avebitrate#2]
{code}
 
 When this plan is passed into *PhysicalOperation* in *DataSourceStrategy*, the collectProjectsAndFilters collects filters as List[[+AttributeReference(""videoid#47L""), AttributeReference(""avebitrate#2"")]+|#47L), AttributeReference(avebitrate#2)]. However, at this stage the base relation only has videoid#1 and hence it throws exception saying *key not found: videoid#47L.*

 On looking further, noticed that the alias map in *PhysicalOperation.substitute* does have the entry with key *videoid#47L* -> Aliases Map((videoid#47L, factorial(videoid#1))). However, the substitute alias is not substituting the expression for alias videoid#47L because they differ in qualifier parameter.
 Attribute key in Alias: AttributeReference(""videoid"", LongType, nullable = true)(ExprId(47, _), *""None""*)
 Attribute in Filter condition: AttributeReference(""videoid"", LongType, nullable = true)(ExprId(47, _), *""Some(tmp5)""*)

Both differ only in the qualifier, however for alias map if we use AttributeMap instead of Map[Attribute, Expression], we can get rid of the above issue. ",,dongjoon,nikitakonda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 04:03:07 UTC 2019,,,,,,,,,,"0|z06hiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Nov/19 04:03;dongjoon;Issue resolved by pull request 25761
[https://github.com/apache/spark/pull/25761];;;",,,,,,,,,,,,,,,,,,,,,,,
KafkaDelegationTokenSuite fails,SPARK-29027,13255753,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,koert,koert,09/Sep/19 22:40,12/Dec/22 18:10,13/Jul/23 08:49,17/Sep/19 22:30,3.0.0,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"i am seeing consistent failure of KafkaDelegationTokenSuite on master

{code}
JsonUtilsSuite:
- parsing partitions
- parsing partitionOffsets
KafkaDelegationTokenSuite:
javax.security.sasl.SaslException: Failure to initialize security context [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)]
	at com.sun.security.sasl.gsskerb.GssKrb5Server.<init>(GssKrb5Server.java:125)
	at com.sun.security.sasl.gsskerb.FactoryImpl.createSaslServer(FactoryImpl.java:85)
	at javax.security.sasl.Sasl.createSaslServer(Sasl.java:524)
	at org.apache.zookeeper.server.ZooKeeperSaslServer$1.run(ZooKeeperSaslServer.java:118)
	at org.apache.zookeeper.server.ZooKeeperSaslServer$1.run(ZooKeeperSaslServer.java:114)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.zookeeper.server.ZooKeeperSaslServer.createSaslServer(ZooKeeperSaslServer.java:114)
	at org.apache.zookeeper.server.ZooKeeperSaslServer.<init>(ZooKeeperSaslServer.java:48)
	at org.apache.zookeeper.server.NIOServerCnxn.<init>(NIOServerCnxn.java:100)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.createConnection(NIOServerCnxnFactory.java:156)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:197)
	at java.lang.Thread.run(Thread.java:748)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)
	at sun.security.jgss.krb5.Krb5AcceptCredential.getInstance(Krb5AcceptCredential.java:87)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:127)
	at sun.security.jgss.GSSManagerImpl.getCredentialElement(GSSManagerImpl.java:193)
	at sun.security.jgss.GSSCredentialImpl.add(GSSCredentialImpl.java:427)
	at sun.security.jgss.GSSCredentialImpl.<init>(GSSCredentialImpl.java:62)
	at sun.security.jgss.GSSManagerImpl.createCredential(GSSManagerImpl.java:154)
	at com.sun.security.sasl.gsskerb.GssKrb5Server.<init>(GssKrb5Server.java:108)
	... 12 more
org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite *** ABORTED ***
  org.I0Itec.zkclient.exception.ZkAuthFailedException: Authentication failure
  at org.I0Itec.zkclient.ZkClient.waitForKeeperState(ZkClient.java:947)
  at org.I0Itec.zkclient.ZkClient.waitUntilConnected(ZkClient.java:924)
  at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:1231)
  at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:157)
  at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:131)
  at kafka.utils.ZkUtils$.createZkClientAndConnection(ZkUtils.scala:93)
  at kafka.utils.ZkUtils$.apply(ZkUtils.scala:75)
  at org.apache.spark.sql.kafka010.KafkaTestUtils.setupEmbeddedZookeeper(KafkaTestUtils.scala:202)
  at org.apache.spark.sql.kafka010.KafkaTestUtils.setup(KafkaTestUtils.scala:243)
  at org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.beforeAll(KafkaDelegationTokenSuite.scala:49)
  ...
KafkaSourceOffsetSuite:
- comparison {""t"":{""0"":1}} <=> {""t"":{""0"":2}}
- comparison {""t"":{""1"":0,""0"":1}} <=> {""t"":{""1"":1,""0"":2}}
- comparison {""t"":{""0"":1},""T"":{""0"":0}} <=> {""t"":{""0"":2},""T"":{""0"":1}}
- comparison {""t"":{""0"":1}} <=> {""t"":{""1"":1,""0"":2}}
- comparison {""t"":{""0"":1}} <=> {""t"":{""1"":3,""0"":2}}
- basic serialization - deserialization
- OffsetSeqLog serialization - deserialization
- read Spark 2.1.0 offset format
{code}

{code}
[INFO] Reactor Summary for Spark Project Parent POM 3.0.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  4.178 s]
[INFO] Spark Project Tags ................................. SUCCESS [  9.373 s]
[INFO] Spark Project Sketch ............................... SUCCESS [ 24.586 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  5.456 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 49.819 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  6.096 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 14.714 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  5.277 s]
[INFO] Spark Project Core ................................. SUCCESS [32:58 min]
[INFO] Spark Project ML Local Library ..................... SUCCESS [ 41.076 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:51 min]
[INFO] Spark Project Streaming ............................ SUCCESS [06:43 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [15:04 min]
[INFO] Spark Project SQL .................................. SUCCESS [  01:32 h]
[INFO] Spark Project ML Library ........................... SUCCESS [26:48 min]
[INFO] Spark Project Tools ................................ SUCCESS [  7.830 s]
[INFO] Spark Project Hive ................................. SUCCESS [  01:00 h]
[INFO] Spark Project Graph API ............................ SUCCESS [  3.378 s]
[INFO] Spark Project Cypher ............................... SUCCESS [  3.672 s]
[INFO] Spark Project Graph ................................ SUCCESS [  3.615 s]
[INFO] Spark Project REPL ................................. SUCCESS [02:11 min]
[INFO] Spark Project Assembly ............................. SUCCESS [  3.058 s]
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SUCCESS [ 24.208 s]
[INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [01:46 min]
[INFO] Kafka 0.10+ Source for Structured Streaming ........ FAILURE [06:15 min]
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [  2.467 s]
[INFO] Spark Avro ......................................... SUCCESS [01:47 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  04:11 h
[INFO] Finished at: 2019-09-08T02:49:07-04:00
[INFO] ------------------------------------------------------------------------
{code}
","{code}
commit 6378d4bc06cd1bb1a209bd5fb63d10ef52d75eb4
Author: Sean Owen <sean.owen@databricks.com>
Date:   Mon Sep 9 10:19:40 2019 -0500
{code}

Ubuntu 16.04 with OpenJDK 1.8 (1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10)",gsomogyi,kabhwan,koert,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29580,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 17 22:30:42 UTC 2019,,,,,,,,,,"0|z06hfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Sep/19 03:02;gurwls223;Seems like this is passing correctly in Jenkins. Would you be willing to investigate further? which command did you use to run the tests?;;;","10/Sep/19 03:24;kabhwan;The test passed in my local dev. MacOS 10.14.2, JDK 1.8.0_191-b12

I've run below command to avoid running all tests.
{code:java}
mvn clean install -DskipTests
mvn clean install -rf :spark-sql-kafka-0-10_2.12 {code}
Which profile(s) you enable when running tests?

cc. [~gsomogyi];;;","10/Sep/19 08:28;gsomogyi;[~kabhwan] thanks for pinging. I know of this because I've suggested on the original PR to open this jira.
Apart from jenkins runs (which are passing) yesterday I've started this test in a loop with sbt and maven as well but until now haven't failed.

What I can think of:
* The environment is significantly different from my Mac and from PR builder
* The code is not vanilla Spark and has some downstream changes

All in all as suggested exact environment description + debug logs would help.
;;;","10/Sep/19 08:34;gsomogyi;[~koert] are you guys using vanilla Spark or the code contains some downstream changes?;;;","10/Sep/19 08:39;gsomogyi;Hmmm, based on the reactor summary you've provided I see downstream changes.;;;","10/Sep/19 12:11;koert;hey the command i run is:
mvn clean test -fae

i am not aware of downstream changes. where/how do you see that in reactor summary?
in so far i know this is spark master. to be sure i will do new clone of repo.;;;","10/Sep/19 12:19;koert;i am running test on my work laptop. it has kerberos client installed (e.g. i can kinit, klist, kdestroy on it). i get the same error on other laptop (ubuntu 18) and one of our build servers. they also have kerberos client installed. 
;;;","10/Sep/19 13:07;koert;i am going to try running tests on a virtual machine to try to isolate what the issue could be in environment;;;","10/Sep/19 13:37;gsomogyi;{quote}where/how do you see that in reactor summary?{quote}
I thought I've seen additional project in the summary but revisited and it's not true.

I've double checked my Mac and there I've also kerberos client installed.
;;;","10/Sep/19 14:52;koert;i tried doing tests in a virtual machine and they pass
so its something in my environment (or really in all our corporate laptops and servers) but i have no idea what it could be right now;;;","10/Sep/19 17:49;koert;[~gsomogyi] do you use any services that require open ports perhaps? i am thinking it could be firewall issue, or host to ip mapping?;;;","11/Sep/19 01:59;kabhwan;[~koert]

Please try to mv krb5.conf to other and run the test again. If it works, please find ""EXAMPLE.COM"" is defined as realm in krb5.conf, as MiniKdc seems to use it for default configuration.;;;","11/Sep/19 07:40;gsomogyi;[~koert] Zookeeper opens a port where ZKClient tries to connect. That said it would be good to see debug log from this suite. It may contain lot of useful info...;;;","11/Sep/19 15:04;gsomogyi;I've tried to create a krb5.conf file which contains various things but not able to make the test fail. [~koert] please attach something to proceed.
;;;","11/Sep/19 15:08;gsomogyi;Can you give for example the output of this cmd:
{quote}[gaborsomogyi:~/spark/external/kafka-0-10-sql] master(+8/-2)+ ± mvn dependency:tree -Dverbose | grep zookeeper{quote}
;;;","11/Sep/19 15:12;koert;let me try to get debug logs;;;","11/Sep/19 15:58;koert;i renamed /etc/krb5.conf and it did not change anything. still same failure.

{code}
~/spark/external/kafka-0-10-sql$ mvn dependency:tree -Dverbose | grep zookeeper
[INFO] +- org.apache.zookeeper:zookeeper:jar:3.4.7:test
{code};;;","11/Sep/19 16:33;koert;i get same error in sbt i think, plus i find sbt a lot easier to handle :)
{code}
[info] KafkaDelegationTokenSuite:
[info] org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite *** ABORTED *** (10 seconds, 543 milliseconds)
[info]   org.I0Itec.zkclient.exception.ZkAuthFailedException: Authentication failure
[info]   at org.I0Itec.zkclient.ZkClient.waitForKeeperState(ZkClient.java:947)
[info]   at org.I0Itec.zkclient.ZkClient.waitUntilConnected(ZkClient.java:924)
[info]   at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:1231)
[info]   at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:157)
[info]   at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:131)
[info]   at kafka.utils.ZkUtils$.createZkClientAndConnection(ZkUtils.scala:93)
[info]   at kafka.utils.ZkUtils$.apply(ZkUtils.scala:75)
[info]   at org.apache.spark.sql.kafka010.KafkaTestUtils.setupEmbeddedZookeeper(KafkaTestUtils.scala:202)
[info]   at org.apache.spark.sql.kafka010.KafkaTestUtils.setup(KafkaTestUtils.scala:243)
[info]   at org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.beforeAll(KafkaDelegationTokenSuite.scala:49)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
org.apache.directory.api.ldap.model.exception.LdapOperationErrorException: /home/koert/src/spark/target/tmp/spark-dc223dd0-e499-4ccf-9600-c70e4706a909/1568218986864/partitions/system/1.3.6.1.4.1.18060.0.4.1.2.50.lg (No such file or directory)
	at org.apache.directory.server.core.partition.impl.btree.AbstractBTreePartition.modify(AbstractBTreePartition.java:1183)
	at org.apache.directory.server.core.shared.partition.DefaultPartitionNexus.sync(DefaultPartitionNexus.java:335)
	at org.apache.directory.server.core.DefaultDirectoryService.shutdown(DefaultDirectoryService.java:1299)
	at org.apache.directory.server.core.DefaultDirectoryService$1.run(DefaultDirectoryService.java:1230)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /home/koert/src/spark/target/tmp/spark-dc223dd0-e499-4ccf-9600-c70e4706a909/1568218986864/partitions/system/1.3.6.1.4.1.18060.0.4.1.2.50.lg (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:101)
	at jdbm.recman.TransactionManager.open(TransactionManager.java:209)
	at jdbm.recman.TransactionManager.synchronizeLogFromMemory(TransactionManager.java:202)
	at jdbm.recman.TransactionManager.synchronizeLog(TransactionManager.java:135)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmIndex.sync(JdbmIndex.java:698)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmPartition.sync(JdbmPartition.java:312)
	at org.apache.directory.server.core.partition.impl.btree.AbstractBTreePartition.modify(AbstractBTreePartition.java:1228)
	at org.apache.directory.server.core.partition.impl.btree.AbstractBTreePartition.modify(AbstractBTreePartition.java:1173)
	... 4 more
java.io.FileNotFoundException: /home/koert/src/spark/target/tmp/spark-dc223dd0-e499-4ccf-9600-c70e4706a909/1568218986864/partitions/example/1.3.6.1.4.1.18060.0.4.1.2.5.lg (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:101)
	at jdbm.recman.TransactionManager.open(TransactionManager.java:209)
	at jdbm.recman.TransactionManager.synchronizeLogFromMemory(TransactionManager.java:202)
	at jdbm.recman.TransactionManager.synchronizeLog(TransactionManager.java:135)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmIndex.sync(JdbmIndex.java:698)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmPartition.sync(JdbmPartition.java:312)
	at org.apache.directory.server.core.shared.partition.DefaultPartitionNexus.sync(DefaultPartitionNexus.java:353)
	at org.apache.directory.server.core.DefaultDirectoryService.shutdown(DefaultDirectoryService.java:1299)
	at org.apache.directory.server.core.DefaultDirectoryService$1.run(DefaultDirectoryService.java:1230)
	at java.lang.Thread.run(Thread.java:748)
java.io.FileNotFoundException: /home/koert/src/spark/target/tmp/spark-dc223dd0-e499-4ccf-9600-c70e4706a909/1568218986864/partitions/system/1.3.6.1.4.1.18060.0.4.1.2.5.lg (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:101)
	at jdbm.recman.TransactionManager.open(TransactionManager.java:209)
	at jdbm.recman.TransactionManager.synchronizeLogFromMemory(TransactionManager.java:202)
	at jdbm.recman.TransactionManager.synchronizeLog(TransactionManager.java:135)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmIndex.sync(JdbmIndex.java:698)
	at org.apache.directory.server.core.partition.impl.btree.jdbm.JdbmPartition.sync(JdbmPartition.java:312)
	at org.apache.directory.server.core.shared.partition.DefaultPartitionNexus.sync(DefaultPartitionNexus.java:353)
	at org.apache.directory.server.core.DefaultDirectoryService.shutdown(DefaultDirectoryService.java:1299)
	at org.apache.directory.server.core.DefaultDirectoryService$1.run(DefaultDirectoryService.java:1230)
	at java.lang.Thread.run(Thread.java:748)
[info] ScalaTest
[info] Run completed in 14 seconds, 392 milliseconds.
[info] Total number of tests run: 0
[info] Suites: completed 0, aborted 1
[info] Tests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0
[info] *** 1 SUITE ABORTED ***
[error] Error: Total 1, Failed 0, Errors 1, Passed 0
[error] Error during tests:
[error] 	org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite
[error] (sql-kafka-0-10/test:testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 52 s, completed Sep 11, 2019 12:23:22 PM
{code};;;","11/Sep/19 16:33;koert;[~gsomogyi] i can email you debug log file directly if thats ok. rather not post it publicly.;;;","11/Sep/19 17:15;gsomogyi;You can remove the sensitive parts or if you only trust me then fine but you loose the possibility of community knowledge. Maybe somebody would pinpoint the issue right away.;;;","11/Sep/19 18:28;koert;just for this one test debug logs is 62mb of kerberos and ldap stuff. its difficult to say whats sensitive and whats not.;;;","11/Sep/19 19:03;koert;[~gsomogyi] if you email me at koert at tresata dot com i can send logs;;;","12/Sep/19 11:36;gsomogyi;[~koert] I see your concern and sending a mail...;;;","17/Sep/19 22:30;vanzin;Issue resolved by pull request 25803
[https://github.com/apache/spark/pull/25803];;;"
SparkSQLCLI can not use 'ADD JAR' 's jar as Serde class,SPARK-29022,13255612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,09/Sep/19 11:43,01/Oct/19 15:10,13/Jul/23 08:49,01/Oct/19 15:09,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,3.0.0,,,SQL,,,,,0,,,,"Spark SQL CLI can't use class in jars add by SQL 'ADD JAR'
{code:java}
spark-sql> add jar /root/.m2/repository/org/apache/hive/hcatalog/hive-hcatalog-core/2.3.6/hive-hcatalog-core-2.3.6.jar;
ADD JAR /root/.m2/repository/org/apache/hive/hcatalog/hive-hcatalog-core/2.3.6/hive-hcatalog-core-2.3.6.jar
spark-sql> CREATE TABLE addJar(key string) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe';
spark-sql> select * from addJar;
19/09/07 03:06:54 ERROR SparkSQLDriver: Failed in [select * from addJar]
java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.data.JsonSerDe
	at org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializerClass(TableDesc.java:79)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.addColumnMetadataToConf(HiveTableScanExec.scala:123)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.hadoopConf$lzycompute(HiveTableScanExec.scala:101)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.hadoopConf(HiveTableScanExec.scala:98)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.hadoopReader$lzycompute(HiveTableScanExec.scala:110)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.hadoopReader(HiveTableScanExec.scala:105)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.$anonfun$doExecute$1(HiveTableScanExec.scala:188)
	at org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2488)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:188)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:329)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:378)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:408)
	at org.apache.spark.sql.execution.HiveResult$.hiveResultString(HiveResult.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.$anonfun$run$1(SparkSQLDriver.scala:65)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:367)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:272)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:179)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:202)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:89)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:999)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1008)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.data.JsonSerDe
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializerClass(TableDesc.java:76)
	... 38 more
OptionsAttachments{code}",,angerszhuuu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/19 05:54;angerszhuuu;image-2019-09-17-13-54-50-896.png;https://issues.apache.org/jira/secure/attachment/12980463/image-2019-09-17-13-54-50-896.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 01 15:09:53 UTC 2019,,,,,,,,,,"0|z06gk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Sep/19 04:55;yumwang;It works before Spark 2.0.;;;","17/Sep/19 05:57;angerszhuuu;Since after spark-2.0, when we call method HiveClientImpl#withHiveState, it will set back origin classLoader of HiveClientImpl's state:
{code:java}
def withHiveState[A](f: => A): A = retryLocked {
  val original = Thread.currentThread().getContextClassLoader
  val originalConfLoader = state.getConf.getClassLoader
  // The classloader in clientLoader could be changed after addJar, always use the latest
  // classloader. We explicitly set the context class loader since ""conf.setClassLoader"" does
  // not do that, and the Hive client libraries may need to load classes defined by the client's
  // class loader.
  Thread.currentThread().setContextClassLoader(clientLoader.classLoader)
  state.getConf.setClassLoader(clientLoader.classLoader)
  // Set the thread local metastore client to the client associated with this HiveClientImpl.
  Hive.set(client)
  // Replace conf in the thread local Hive with current conf
  Hive.get(conf)
  // setCurrentSessionState will use the classLoader associated
  // with the HiveConf in `state` to override the context class loader of the current
  // thread.
  shim.setCurrentSessionState(state)
  val ret = try f finally {
    state.getConf.setClassLoader(originalConfLoader)
    Thread.currentThread().setContextClassLoader(original)
    HiveCatalogMetrics.incrementHiveClientCalls(1)
  }
  ret
}
{code}
Before version 2.0, it will just set state's conf's classloader as clientLoader.classLoader, won't setback.

 
{code:java}
/**
 * Runs `f` with ThreadLocal session state and classloaders configured for this version of hive.
 */
def withHiveState[A](f: => A): A = retryLocked {
  val original = Thread.currentThread().getContextClassLoader
  // Set the thread local metastore client to the client associated with this HiveClientImpl.
  Hive.set(client)
  // The classloader in clientLoader could be changed after addJar, always use the latest
  // classloader
  state.getConf.setClassLoader(clientLoader.classLoader)
  // setCurrentSessionState will use the classLoader associated
  // with the HiveConf in `state` to override the context class loader of the current
  // thread.
  shim.setCurrentSessionState(state)
  val ret = try f finally {
    Thread.currentThread().setContextClassLoader(original)
  }
  ret
}
{code};;;","18/Sep/19 10:01;angerszhuuu;In spark-2.0, run with hive load by 'maven' will meet this error.;;;","01/Oct/19 15:09;srowen;Issue resolved by pull request 25729
[https://github.com/apache/spark/pull/25729];;;",,,,,,,,,,,,,,,,,,,,
"DataSourceV2: Clean up current, default, and session catalog uses",SPARK-29014,13255376,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,imback82,rdblue,rdblue,06/Sep/19 21:46,18/Oct/19 14:48,13/Jul/23 08:49,18/Oct/19 14:48,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,1,,,,"Catalog tracking in DSv2 has evolved since the initial changes went in. We need to make sure that handling is consistent across plans using the latest rules:
 * The _current_ catalog should be used when no catalog is specified
 * The _default_ catalog is the catalog _current_ is initialized to
 * If the _default_ catalog is not set, then it is the built-in Spark session catalog, which will be called `spark_catalog` (This is the v2 session catalog)",,cloud_fan,imback82,rdblue,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 14:48:12 UTC 2019,,,,,,,,,,"0|z06f3s:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"10/Sep/19 03:51;imback82;[~rdblue] / [~cloud_fan] did you start working on it? If not, I can take this. Please let me know.;;;","10/Sep/19 05:23;cloud_fan;I have a major refactor to solve this issue, so that we don't need to take care of current/default catalog in many places. I'll send out the PR today.;;;","10/Sep/19 21:02;rdblue;[~cloud_fan], why does this require a major refactor?

It would be best to keep the implementation of this as small as possible and not tie it to other work.;;;","11/Sep/19 16:45;cloud_fan;It doesn't require a major refactor but it's easier and cleaner to make this change with a refactor that centralizes the catalog/table lookup logic.;;;","18/Oct/19 14:48;cloud_fan;Issue resolved by pull request 26120
[https://github.com/apache/spark/pull/26120];;;",,,,,,,,,,,,,,,,,,,
Possible leak of SparkContext in tests / test suites initializing StreamingContext,SPARK-29007,13255217,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,06/Sep/19 09:49,13/Sep/19 20:32,13/Jul/23 08:49,11/Sep/19 17:35,3.0.0,,,,,,,,,,3.0.0,,,DStreams,MLlib,Spark Core,,,0,,,,"There're lots of tests creating StreamingContext with creating new SparkContext in its constructor, and we don't have enough guard to prevent leakage of SparkContext in test suites. Ideally we should ensure SparkContext is not leaked between test suites, even between tests if each test creates StreamingContext.

 

One of example for leakage is below:
{noformat}
[info] *** 4 SUITES ABORTED ***
[info] *** 131 TESTS FAILED ***
[error] Error: Total 418, Failed 131, Errors 4, Passed 283, Ignored 1
[error] Failed tests:
[error] 	org.apache.spark.streaming.scheduler.JobGeneratorSuite
[error] 	org.apache.spark.streaming.ReceiverInputDStreamSuite
[error] 	org.apache.spark.streaming.WindowOperationsSuite
[error] 	org.apache.spark.streaming.StreamingContextSuite
[error] 	org.apache.spark.streaming.scheduler.ReceiverTrackerSuite
[error] 	org.apache.spark.streaming.CheckpointSuite
[error] 	org.apache.spark.streaming.UISeleniumSuite
[error] 	org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite
[error] 	org.apache.spark.streaming.ReceiverSuite
[error] 	org.apache.spark.streaming.BasicOperationsSuite
[error] 	org.apache.spark.streaming.InputStreamsSuite
[error] Error during tests:
[error] 	org.apache.spark.streaming.MapWithStateSuite
[error] 	org.apache.spark.streaming.DStreamScopeSuite
[error] 	org.apache.spark.streaming.rdd.MapWithStateRDDSuite
[error] 	org.apache.spark.streaming.scheduler.InputInfoTrackerSuite
 {noformat}
{{}}
{noformat}
[info] JobGeneratorSuite:
[info] - SPARK-6222: Do not clear received block data too soon *** FAILED *** (2 milliseconds)
[info]   org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:
[info] org.apache.spark.SparkContext.<init>(SparkContext.scala:82)
[info] org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:851)
[info] org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:85)
[info] org.apache.spark.streaming.TestSuiteBase.setupStreams(TestSuiteBase.scala:317)
[info] org.apache.spark.streaming.TestSuiteBase.setupStreams$(TestSuiteBase.scala:311)
[info] org.apache.spark.streaming.CheckpointSuite.setupStreams(CheckpointSuite.scala:209)
[info] org.apache.spark.streaming.CheckpointSuite.$anonfun$new$3(CheckpointSuite.scala:258)
[info] scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info] org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] org.scalatest.Transformer.apply(Transformer.scala:22)
[info] org.scalatest.Transformer.apply(Transformer.scala:20)
[info] org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
[info] org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
[info] org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
[info] org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
[info] org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
[info] org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
[info] org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
[info]   at org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2512)
[info]   at scala.Option.foreach(Option.scala:274)
[info]   at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2509)
[info]   at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2586)
[info]   at org.apache.spark.SparkContext.<init>(SparkContext.scala:87)
[info]   at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:851)
[info]   at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:85)
[info]   at org.apache.spark.streaming.scheduler.JobGeneratorSuite.$anonfun$new$1(JobGeneratorSuite.scala:65)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
[info]   at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
[info]   at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
[info]   at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
[info]   at org.apache.spark.streaming.scheduler.JobGeneratorSuite.org$scalatest$BeforeAndAfter$$super$runTest(JobGeneratorSuite.scala:30)
[info]   at org.scalatest.BeforeAndAfter.runTest(BeforeAndAfter.scala:203)
[info]   at org.scalatest.BeforeAndAfter.runTest$(BeforeAndAfter.scala:192)
[info]   at org.apache.spark.streaming.scheduler.JobGeneratorSuite.runTest(JobGeneratorSuite.scala:30)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
[info]   at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
[info]   at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
[info]   at org.scalatest.Suite.run(Suite.scala:1147)
[info]   at org.scalatest.Suite.run$(Suite.scala:1129)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
[info]   at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
[info]   at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
[info]   at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.streaming.scheduler.JobGeneratorSuite.org$scalatest$BeforeAndAfter$$super$run(JobGeneratorSuite.scala:30)
[info]   at org.scalatest.BeforeAndAfter.run(BeforeAndAfter.scala:258)
[info]   at org.scalatest.BeforeAndAfter.run$(BeforeAndAfter.scala:256)
[info]   at org.apache.spark.streaming.scheduler.JobGeneratorSuite.run(JobGeneratorSuite.scala:30)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748) {noformat}
{{}}",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-09-06 09:49:56.0,,,,,,,,,,"0|z06e4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark history server startup hang due to deadlock,SPARK-29003,13255117,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shanyu,shanyu,shanyu,06/Sep/19 00:28,14/Sep/19 04:09,13/Jul/23 08:49,14/Sep/19 04:09,2.3.4,2.4.4,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"Occasionally when starting Spark History Server, the service process will hang before binding to the port so Spark History Server is not usable. One has to kill the process and start again. You can write a simple bash program to stop and start Spark History Server and you can reproduce this problem approximately 10% of time.

The problem is due to java.nio.file.FileSystems.getDefault() cause deadlock. This is what I collected with jstack:
{code:java}
""log-replay-executor-0"" #17 daemon prio=5 os_prio=0 tid=0x00007fca90028800 nid=0x6e8 in Object.wait() [0x00007fcaa9471000]
    java.lang.Thread.State: RUNNABLE 
    at java.nio.file.FileSystems.getDefault(FileSystems.java:176) 
    ... 
    at java.lang.Runtime.loadLibrary0(Runtime.java:870) - locked <0x00000000aaac1d40> (a java.lang.Runtime) 
    ... 
    at org.apache.spark.deploy.history.FsHistoryProvider.mergeApplicationListing(FsHistoryProvider.scala:698)

""main"" #1 prio=5 os_prio=0 tid=0x00007fcad8016800 nid=0x6d8 waiting for monitor entry [0x00007fcae146c000]
    java.lang.Thread.State: BLOCKED (on object monitor) 
    at java.lang.Runtime.loadLibrary0(Runtime.java:862) - waiting to lock <0x00000000aaac1d40> (a java.lang.Runtime) 
    ... 
    at java.nio.file.FileSystems.getDefault(FileSystems.java:176) 
    at java.io.File.toPath(File.java:2234) - locked <0x000000008699bb68> (a java.io.File) 
    ... 
    at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:365){code}
Basically ""main"" thread and ""log-replay-executor-0"" thread simultaneously calling java.nio,file.FileSystems.getDefault() and deadlocked. 

This is similar to the reported JDK bug:

[https://bugs.openjdk.java.net/browse/JDK-8037567]

The problem is that during Spark History Server startup, there are two things happening simultaneously that call into java.nio.file.FileSystems.getDefault():

1) start jetty server
 2) start ApplicationHistoryProvider (which reads files from HDFS)

We should do this two things sequentially instead of in parallel.

 ",,dongjoon,kabhwan,shanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/19 20:06;shanyu;sparkhistory-jstack.log;https://issues.apache.org/jira/secure/attachment/12979712/sparkhistory-jstack.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 14 04:09:37 UTC 2019,,,,,,,,,,"0|z06di8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Sep/19 02:38;kabhwan;Could you provide full of jstack? I guess unless you modify Spark it would be no line to redact. That would be much clearer to see the full picture.;;;","06/Sep/19 20:07;shanyu;Please see the full jstack attached.;;;","06/Sep/19 20:38;kabhwan;Thanks for providing jstack. Looks like it's known JDK issue but given the fixed version is too high I agree we may need to apply workaround on this.

[https://bugs.openjdk.java.net/browse/JDK-8194653]



 

Discussion: [http://mail.openjdk.java.net/pipermail/core-libs-dev/2018-January/050830.html];;;","14/Sep/19 04:09;dongjoon;Issue resolved by pull request 25705
[https://github.com/apache/spark/pull/25705];;;",,,,,,,,,,,,,,,,,,,,
[SQL] Decimal precision overflow when don't allow precision loss,SPARK-29000,13255048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hzfeiwang,hzfeiwang,hzfeiwang,05/Sep/19 16:54,09/Sep/19 08:44,13/Jul/23 08:49,09/Sep/19 05:51,2.4.4,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"When we set spark.sql.decimalOperations.allowPrecisionLoss=false.
For the sql below, the result will overflow and return null.
{code:java}
// Some comments here
select case when 1=2 then 1 else 100.000000000000000000000000 end * 1
{code}

However, this sql will return correct result.
{code:java}
// Some comments here
select case when 1=2 then 1 else 100.000000000000000000000000 end * 1.0
{code}
",,cloud_fan,fengchaoge,hzfeiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/19 17:26;hzfeiwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12979578/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 05:51:51 UTC 2019,,,,,,,,,,"0|z06d2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Sep/19 05:51;cloud_fan;Issue resolved by pull request 25701
[https://github.com/apache/spark/pull/25701];;;",,,,,,,,,,,,,,,,,,,,,,,
PySpark: Can't pass more than 256 arguments to a UDF,SPARK-28978,13254858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bago.amirbekian,j1m,j1m,04/Sep/19 20:17,09/Nov/19 03:21,13/Jul/23 08:49,09/Nov/19 03:21,2.3.2,2.4.0,2.4.4,,,,,,,,3.0.0,,,PySpark,,,,,0,koalas,mlflow,pyspark,"This code:

[https://github.com/apache/spark/blob/712874fa0937f0784f47740b127c3bab20da8569/python/pyspark/worker.py#L367-L379]

Creates Python lambdas that call UDF functions passing arguments singly, rather than using varargs.  For example: `lambda a: f(a[0], a[1], ...)`.

This fails when there are more than 256 arguments.

mlflow, when generating model predictions, uses an argument for each feature column.  I have a model with > 500 features.

I was able to easily hack around this by changing the generated lambdas to use varargs, as in `lambda a: f(*a)`. 

IDK why these lambdas were created the way they were.  Using varargs is much simpler and works fine in my testing.

 

 ",,bryanc,holden,j1m,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-09-04 20:17:43.0,,,,,,,,,,"0|z06bww:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException is thrown from EventLoggingListener,SPARK-28967,13254538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,04/Sep/19 02:35,06/Sep/19 19:57,13/Jul/23 08:49,06/Sep/19 14:08,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"While testing SPARK-28869 manually, I've found simple Structured Streaming query is continuously throwing ConcurrentModificationException from EventLoggingListener.

Stack trace follows:
{code:java}
19/09/04 09:48:49 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception19/09/04 09:48:49 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exceptionjava.util.ConcurrentModificationException at java.util.Hashtable$Enumerator.next(Hashtable.java:1387) at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424) at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420) at scala.collection.Iterator.foreach(Iterator.scala:941) at scala.collection.Iterator.foreach$(Iterator.scala:941) at scala.collection.AbstractIterator.foreach(Iterator.scala:1429) at scala.collection.IterableLike.foreach(IterableLike.scala:74) at scala.collection.IterableLike.foreach$(IterableLike.scala:73) at scala.collection.AbstractIterable.foreach(Iterable.scala:56) at scala.collection.TraversableLike.map(TraversableLike.scala:237) at scala.collection.TraversableLike.map$(TraversableLike.scala:230) at scala.collection.AbstractTraversable.map(Traversable.scala:108) at org.apache.spark.util.JsonProtocol$.mapToJson(JsonProtocol.scala:514) at org.apache.spark.util.JsonProtocol$.$anonfun$propertiesToJson$1(JsonProtocol.scala:520) at scala.Option.map(Option.scala:163) at org.apache.spark.util.JsonProtocol$.propertiesToJson(JsonProtocol.scala:519) at org.apache.spark.util.JsonProtocol$.jobStartToJson(JsonProtocol.scala:155) at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:79) at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:149) at org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:217) at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37) at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28) at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37) at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37) at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:99) at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:84) at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:102) at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:102) at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:97) at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:93) at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319) at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:93) {code}
 

It also occurs with current master branch.",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 06 14:08:51 UTC 2019,,,,,,,,,,"0|z06aso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Sep/19 02:36;kabhwan;Working on the fix. It should be simple fix, though I'm not sure I can add test easily.;;;","04/Sep/19 04:27;kabhwan;FYI, SPARK-26714 causes this bug.;;;","06/Sep/19 14:08;srowen;Resolved by https://github.com/apache/spark/pull/25672;;;",,,,,,,,,,,,,,,,,,,,,
saveAsTable doesn't pass in the data source information for V2 nodes,SPARK-28964,13254475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,03/Sep/19 20:37,05/Sep/19 07:29,13/Jul/23 08:49,05/Sep/19 07:29,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"We need to pass in the ""provider"" property in saveAsTable for V2 nodes in DataFrameWriter.",,brkyvz,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 05 07:29:57 UTC 2019,,,,,,,,,,"0|z06aeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Sep/19 07:29;cloud_fan;Issue resolved by pull request 25669
[https://github.com/apache/spark/pull/25669];;;",,,,,,,,,,,,,,,,,,,,,,,
Fall back to archive.apache.org to download Maven if mirrors don't have requested version,SPARK-28963,13254450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,03/Sep/19 18:54,12/Dec/22 18:11,13/Jul/23 08:49,04/Sep/19 04:11,3.0.0,,,,,,,,,,2.4.5,3.0.0,,Build,,,,,0,,,,"As described in https://github.com/apache/spark/pull/25665, {{build/mvn}} attempts to download Maven from the ASF mirrors. But these are only guaranteed to have the latest release in each branch. If older ones are removed, this script fails, and that's a problem for older releases at least.

We can try falling back to archive.apache.org in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31691,SPARK-31713,,,,,SPARK-28961,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 04 04:11:37 UTC 2019,,,,,,,,,,"0|z06a94:",9223372036854775807,,,,,,,,,,,,,2.4.5,3.0.0,,,,,,,,"04/Sep/19 04:11;gurwls223;Issue resolved by pull request 25667
[https://github.com/apache/spark/pull/25667];;;",,,,,,,,,,,,,,,,,,,,,,,
Status logging occurs on every state change but not at an interval for liveness.,SPARK-28947,13254162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,02/Sep/19 07:19,17/May/20 18:23,13/Jul/23 08:49,15/Oct/19 19:35,2.3.3,2.4.4,,,,,,,,,3.0.0,,,Kubernetes,Spark Core,,,,0,,,,The start method of `LoggingPodStatusWatcherImpl`  should be invoked,,Qin Yao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 15 19:35:18 UTC 2019,,,,,,,,,,"0|z068h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Oct/19 19:35;vanzin;Issue resolved by pull request 25648
[https://github.com/apache/spark/pull/25648];;;",,,,,,,,,,,,,,,,,,,,,,,
SQL configuration are not always propagated,SPARK-28939,13254102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mgaido,mgaido,01/Sep/19 07:11,08/Jun/20 16:30,13/Jul/23 08:49,09/Sep/19 13:21,2.3.4,2.4.4,,,,,,,,,2.4.5,3.0.0,,SQL,,,,,0,,,,"The SQL configurations are propagated to executors in order to be effective.
Unfortunately, in some cases, we are missing to propagate them, making them un-effective.

The problem happens every time {{rdd}} or {{queryExecution.toRdd}} are used. And this is pretty frequent in the codebase.

Please notice that there are 2 parts of this issue:
 - when a user directly uses those APIs
 - when Spark invokes them (eg. throughout the ML lib and other usages or the {{describe}} method on the {{Dataset}} class)

",,cloud_fan,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 13:21:59 UTC 2019,,,,,,,,,,"0|z0683s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Sep/19 13:21;cloud_fan;Issue resolved by pull request 25643
[https://github.com/apache/spark/pull/25643];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix couple of bugs in FsHistoryProviderSuite,SPARK-28931,13253959,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,30/Aug/19 14:57,04/Sep/19 18:06,13/Jul/23 08:49,04/Sep/19 17:07,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"There're some bugs reside on FsHistoryProviderSuite itself.
 # When creating log file via {{newLogFile}}, codec is ignored, leading to wrong file name. (No one tends to create test for test code, as well as the bug doesn't affect existing tests indeed, so not easy to catch.)
 # When writing events to log file via {{writeFile}}, metadata (in case of new format) gets written to file regardless of its codec, and the content is overwritten by another stream, hence no information for Spark version is available. It affects existing test, hence we have wrong expected value to workaround the bug.

Note that they're bugs on test code, non-test code works fine.",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-30 14:57:22.0,,,,,,,,,,"0|z06780:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark DESC FORMATTED TABLENAME information display issues,SPARK-28930,13253932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,S71955,jobitmathew,jobitmathew,30/Aug/19 13:04,12/Dec/22 18:10,13/Jul/23 08:49,18/Sep/19 03:55,2.4.3,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Spark DESC FORMATTED TABLENAME information display issues.Showing incorrect *Last Access time and* feeling some information displays can make it better.

Test steps:
 1. Open spark sql
 2. Create table with partition
 CREATE EXTERNAL TABLE IF NOT EXISTS employees_info_extended ( id INT, name STRING, usd_flag STRING, salary DOUBLE, deductions MAP<STRING, DOUBLE>, address STRING ) PARTITIONED BY (entrytime STRING) STORED AS TEXTFILE location 'hdfs://hacluster/user/sparkhive/warehouse';
 3. from spark sql check the table description
 desc formatted tablename;
 4. From scala shell check the table description
 sql(""desc formatted tablename"").show()

*Issue1:*
 If there is no comment for spark scala shell shows *""null"" in small letters* but all other places Hive beeline/Spark beeline/Spark SQL it is showing in *CAPITAL ""NULL*"". Better to show same in all places.

 
{code:java}
*scala>* sql(""desc formatted employees_info_extended"").show(false);
 +-----------------------------+---------------------------++-------
|col_name|data_type|*comment*|

+-----------------------------+---------------------------++-------
|id|int|*null*|
|name|string|*null*|
|usd_flag|string|*null*|
|salary|double|*null*|
|deductions|map<string,double>|*null*|
|address|string|null|
|entrytime|string|null|
| # Partition Information| | |
| # col_name|data_type|comment|
|entrytime|string|null|
| | | |
| # Detailed Table Information| | |
|Database|sparkdb__| |
|Table|employees_info_extended| |
|Owner|root| |

*|Created Time |Tue Aug 20 13:42:06 CST 2019| |*
 *|Last Access |Thu Jan 01 08:00:00 CST 1970| |*
|Created By|Spark 2.4.3| |
|Type|EXTERNAL| |
|Provider|hive| |

+-----------------------------+---------------------------++-------
 only showing top 20 rows

*scala>*
{code}
*Issue 2:*
 Spark SQL ""desc formatted tablename"" is not showing the header [# col_name,data_type,comment|#col_name,data_type,comment] in the top of the query result.But header is showing on top of partition description. For Better understanding show the header on Top of the query result.Other than in spark sql ,we are able to see the header like [# col_name,data_type,comment|#col_name,data_type,comment] in spark-beeline & hive beeline  .
{code:java}
*spark-sql>* desc formatted employees_info_extended1;
 id int *NULL*
 name string *NULL*
 usd_flag string NULL
 salary double NULL
 deductions map<string,double> NULL
 address string NULL
 entrytime string NULL
 * 
 ## Partition Information*
 ## col_name data_type comment*
 entrytime string *NULL*

 
 *spark-sql>*               |                                                                                 |                  |



{code}
 

*This is Scala shell showing the headers ""|col_name|data_type|*comment*| ""*

 

*scala>* sql(""desc formatted employees_info_extended"").show(false); +-----------------------------+---------------------------++------- *|col_name|data_type|*comment*|* +-----------------------------+---------------------------++------- |id|int|*null*| |name|string|*null*| |usd_flag|string|*null*|

 

*This is the spark-beeline which is showing the headers*

0: jdbc:hive2://10.186.60.158:23040/default> desc formatted employees; +-------------------------------+---------------------------------------------------------------------------------+------------------+--+ | *col_name | data_type | comment | +-*------------------------------+---------------------------------------------------------------------------------+------------------+--+ | name | string | Employee name | | salary | float | Employee salary | | | | |

 

*This is Hive Beeline showing headers*

*0: jdbc:hive2://10.18.98.147:21066/> desc formatted mytest1;*
INFO : Concurrency mode is disabled, not creating a lock manager
+-------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
*| col_name | data_type | comment |*
+-------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| # col_name | data_type | comment |
| col | array<string> | from deserializer |
| value | string 



*Issue 3:*
 I created the table on Aug 20.So it is showing created time correct .*But Last access time showing 1970 Jan 01*. It is not good to show Last access time earlier time than the created time.Better to show the correct date and time else show UNKNOWN.
 *[Created Time,Tue Aug 20 13:42:06 CST 2019,]*
 *[Last Access,Thu Jan 01 08:00:00 CST 1970,]*",,jobitmathew,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29099,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 18 03:55:50 UTC 2019,,,,,,,,,,"0|z06720:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Aug/19 13:46;S71955;@ [~jobitmathew] As i remember Issue 3 is already handled as part of SPARK-24812 some time back, need to recheck. other issues i will check and get back to you.

cc [~dongjoon] ;;;","18/Sep/19 03:55;gurwls223;Issue resolved by pull request 25720
[https://github.com/apache/spark/pull/25720];;;",,,,,,,,,,,,,,,,,,,,,,
"Spark jobs failing on latest versions of Kubernetes (1.15.3, 1.14.6, 1,13.10, 1.12.10, 1.11.10)",SPARK-28921,13253802,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andygrove,psschwei,psschwei,30/Aug/19 01:46,19/Oct/22 01:33,13/Jul/23 08:49,02/Sep/19 23:51,2.3.0,2.3.1,2.3.3,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,2.4.5,3.0.0,,Kubernetes,Spark Core,,,,2,,,,"Spark jobs are failing on latest versions of Kubernetes when jobs attempt to provision executor pods (jobs like Spark-Pi that do not launch executors run without a problem):

 

Here's an example error message:

 
{code:java}
19/08/30 01:29:09 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes.
19/08/30 01:29:09 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes.19/08/30 01:29:09 WARN WatchConnectionManager: Exec Failure: HTTP 403, Status: 403 - 
java.net.ProtocolException: Expected HTTP 101 response but was '403 Forbidden' 
    at okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:216) 
    at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:183) 
    at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141) 
    at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32) 
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 
    at java.lang.Thread.run(Thread.java:748)
{code}
 

Looks like the issue is caused by fixes for a recent CVE : 

CVE: [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-14809]

Fix: [https://github.com/fabric8io/kubernetes-client/pull/1669]

 

Looks like upgrading kubernetes-client to 4.4.2 would solve this issue.",,albertmichaelj,andygrove,dongjoon,fokko,jiangjian,jrg,jugosag,LiozN,psschwei,skonto,thesuperzapper,wesolows,,,,,,,,,,,,,,,,SPARK-28925,SPARK-28444,SPARK-29193,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 19 01:33:14 UTC 2022,,,,,,,,,,"0|z06694:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Aug/19 20:28;dongjoon;Could you make a PR with your test case, [~psschwei]?;;;","31/Aug/19 14:13;andygrove;Here's a PR to fix against master branch since it didn't automatically link to this JIRA: https://github.com/apache/spark/pull/25640;;;","31/Aug/19 22:31;dongjoon;[~psschwei] and [~andygrove]. 
BTW, do you know how many clusters are exposed those versions in the productions? Maybe, at least, in EKS/AKS/GKE since they are popular managed services.;;;","31/Aug/19 22:33;dongjoon;BTW, [~andygrove]. I tried to add your PR to this, but it seems to be already there, doesn't it? 25640?;;;","01/Sep/19 15:01;andygrove;[~dongjoon] we are seeing it on both of the EKS clusters where we are running Spark jobs. I imagine it affects all EKS clusters?

The versions we are using are 1.11.10 and 1.12.10 .. full version info:
{code:java}
Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.10-eks-7f15cc"", GitCommit:""7f15ccb4e58f112866f7ddcfebf563f199558488"", GitTreeState:""clean"", BuildDate:""2019-08-19T17:46:02Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""linux/amd64""} {code}
{code:java}
Server Version: version.Info{Major:""1"", Minor:""12+"", GitVersion:""v1.12.10-eks-825e5d"", GitCommit:""825e5de08cb05714f9b224cd6c47d9514df1d1a7"", GitTreeState:""clean"", BuildDate:""2019-08-18T03:58:32Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""linux/amd64""} {code};;;","02/Sep/19 23:51;dongjoon;Issue resolved by pull request 25640
[https://github.com/apache/spark/pull/25640];;;","03/Sep/19 10:24;skonto;[~andygrove] could you please clarify what do you mean when you say? ""jobs like Spark-Pi that do not launch executors run without a problem""

I run a pi job and it creates executors fine:

spark-pi-03afbd6cf6a72622-driver 1/1 Running 0 15s
spark-pi-03afbd6cf6a72622-exec-1 1/1 Running 0 7s
spark-pi-03afbd6cf6a72622-exec-2 1/1 Running 0 7s;;;","04/Sep/19 03:58;dongjoon;Please launch a new EKS cluster and build/publish master branch, [~skonto].
I tested this yesterday. This does not happen on the old clusters.;;;","05/Sep/19 21:31;LiozN;Hi [~dongjoon], [~skonto]

I just recently had the same issue and found out some details that might help.

looks like AWS rolls this update on their EKS clusters, it didn't happened at once.

The issue started at 30.08.2019 on one of our clusters, one of our workarounds was migrating to other new cluster - which worked for several hours and then started to fail with the same error.

I've noticed that ""fabric8-rbac"" clusterrolebinding was changed exactly when the issue started on the new cluster.

Maybe you can try creating a cluster and watch ""fabric8-rbac"" for changes.

 

I think you should add all spark versions up to 2.4.4 to the affected version list.

Compiling older version with the new K8s client solved the issue, but there's a problem with 2.4.0 which can't be compiled with the new client dependency.;;;","06/Sep/19 16:08;dongjoon;Hi, thank you for sharing. Of course, we know that. ;);;;","09/Sep/19 21:43;fokko;I can confirm that we're running into the same issue with an on-premise k8s cluster with RBAC enabled. After updating the kubernetes client to 4.4.2 everything works fine again.;;;","09/Sep/19 22:50;dongjoon;Thank you for confirmation, [~Fokko].;;;","10/Oct/19 18:35;albertmichaelj;Is there a timeline for this fix being present in a release version? This bug is really affecting us severely, and I've been waiting for 2.4.5 to drop. Is there a timeline in which we can expect some Spark release version to include the fix?;;;","10/Oct/19 19:20;psschwei;[~albertmichaelj] You can replace the `kubernetes-client-4.1.2.jar` (in the `jars/` directory) with the newer version (v 4.4.2).;;;","04/Nov/19 11:25;jrg;For anyone who lands here today, note that Google GKE just rolled out an update of their k8s masters to 1.3.11.gke5 and this broke our deployed Spark 2.4.x applications - preventing them creating new executor pods - for this very same reason (we were previously running with a master version of 1.13.7.gke24, and that worked fine.)

As a quick, tactical, low-risk fix, we've adopted the ""update the kubernetes-client JAR"" workaround.;;;","04/Nov/19 17:36;dongjoon;Thank you for sharing, [~jrg].;;;","04/Dec/19 07:02;jugosag;We are also observing this on two of our clusters, both set up with Rancher, one on Kubernetes version 1.15.5 the other on 1.14.6. Interestingly, on Minikube with version 1.15.4 it works.

Problem is that the Spark version which has the fix (2.4.5) has not been released yet and the 3.0.0 preview from Nov 6th has the same problem.

When will 2.4.5 be released?

 ;;;","24/Jan/20 01:03;thesuperzapper;It is not enough to replace the kuberntes-client.jar in your $SPARK_HOME/jars, you must also replace:
 * $SPARK_HOME/jars/kubernetes-client-*.jar
 * $SPARK_HOME/jars/kubernetes-model-common-*jar
 * $SPARK_HOME/jars/kubernetes-model-*.jar
 * $SPARK_HOME/jars/okhttp-*.jar
 * $SPARK_HOME/jars/okio-*.jar

With the versions specified in this PR:
 [https://github.com/apache/spark/commit/65c0a7812b472147c615fb4fe779da9d0a11ff18];;;","24/Jan/20 01:18;dongjoon;Thank you for updating, [~thesuperzapper]. What problem did you hit when you don't change the others?
BTW, 2.4.5 RC2 vote is coming.;;;","25/Jan/20 07:25;thesuperzapper;[~dongjoon], it's just very bad practice to not update all jars which depend on each other, so I never tried to only do one. However, I also remember reading people who said they encountered errors while only updating one, on other threads about this issue.;;;","25/Jan/20 10:25;dongjoon;Of course, I agree with you. I was just wondering the error type.;;;","19/Oct/22 01:33;jiangjian;[~thesuperzapper] Where can I get these two jar packages  （okhttp-*.jar，okio-*.jar）;;;",,
Jobs can hang because of race of RDD.dependencies,SPARK-28917,13253753,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,29/Aug/19 16:22,17/May/20 17:48,13/Jul/23 08:49,08/Oct/19 18:43,2.3.3,2.4.3,,,,,,,,,2.4.5,3.0.0,,Scheduler,Spark Core,,,,0,,,,"{{RDD.dependencies}} stores the precomputed cache value, but it is not thread-safe.  This can lead to a race where the value gets overwritten, but the DAGScheduler gets stuck in an inconsistent state.  In particular, this can happen when there is a race between the DAGScheduler event loop, and another thread (eg. a user thread, if there is multi-threaded job submission).


First, a job is submitted by the user, which then computes the result Stage and its parents:

https://github.com/apache/spark/blob/24655583f1cb5dae2e80bb572604fb4a9761ec07/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L983

Which eventually makes a call to {{rdd.dependencies}}:

https://github.com/apache/spark/blob/24655583f1cb5dae2e80bb572604fb4a9761ec07/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L519

At the same time, the user could also touch {{rdd.dependencies}} in another thread, which could overwrite the stored value because of the race.

Then the DAGScheduler checks the dependencies *again* later on in the job submission, via {{getMissingParentStages}}

https://github.com/apache/spark/blob/24655583f1cb5dae2e80bb572604fb4a9761ec07/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1025

Because it will find new dependencies, it will create entirely different stages.  Now the job has some orphaned stages which will never run.

One symptoms of this are seeing disjoint sets of stages in the ""Parents of final stage"" and the ""Missing parents"" messages on job submission (however this is not required).

(*EDIT*: Seeing repeated msgs ""Registering RDD X"" actually is just fine, it is not a symptom of a problem at all.  It just means the RDD is the *input* to multiple shuffles.)

{noformat}
[INFO] 2019-08-15 23:22:31,570 org.apache.spark.SparkContext logInfo - Starting job: count at XXX.scala:462
...
[INFO] 2019-08-15 23:22:31,573 org.apache.spark.scheduler.DAGScheduler logInfo - Registering RDD 14 (repartition at XXX.scala:421)
...
...
[INFO] 2019-08-15 23:22:31,582 org.apache.spark.scheduler.DAGScheduler logInfo - Got job 1 (count at XXX.scala:462) with 40 output partitions
[INFO] 2019-08-15 23:22:31,582 org.apache.spark.scheduler.DAGScheduler logInfo - Final stage: ResultStage 5 (count at XXX.scala:462)
[INFO] 2019-08-15 23:22:31,582 org.apache.spark.scheduler.DAGScheduler logInfo - Parents of final stage: List(ShuffleMapStage 4)
[INFO] 2019-08-15 23:22:31,599 org.apache.spark.scheduler.DAGScheduler logInfo - Registering RDD 14 (repartition at XXX.scala:421)
[INFO] 2019-08-15 23:22:31,599 org.apache.spark.scheduler.DAGScheduler logInfo - Missing parents: List(ShuffleMapStage 6)
{noformat}

Another symptom is only visible with DEBUG logs turned on for DAGScheduler -- you will calls to {{submitStage(Stage X)}} multiple times, followed by a different set of missing stages.  eg. here, we see stage 1 first is missing stage 0 as a dependency, and then later on its missing stage 23:

{noformat}
19/09/19 22:28:15 DEBUG scheduler.DAGScheduler: submitStage(ShuffleMapStage 1)
19/09/19 22:28:15 DEBUG scheduler.DAGScheduler: missing: List(ShuffleMapStage 0)
...
19/09/19 22:32:01 DEBUG scheduler.DAGScheduler: submitStage(ShuffleMapStage 1)
19/09/19 22:32:01 DEBUG scheduler.DAGScheduler: missing: List(ShuffleMapStage 23)
{noformat}

Note that there is a similar issue w/ {{rdd.partitions}}.  In particular for some RDDs, {{partitions}} references {{dependencies}} (eg. {{CoGroupedRDD}}).  

There is also an issue that {{rdd.storageLevel}} is read and cached in the scheduler, but it could be modified simultaneously by the user in another thread.   But, I can't see a way it could effect the scheduler.

*WORKAROUND*:
(a) call {{rdd.dependencies}} while you know that RDD is only getting touched by one thread (eg. in the thread that created it, or before you submit multiple jobs touching that RDD from other threads). Then that value will get cached.
(b) don't submit jobs from multiple threads.",,hzfeiwang,irashid,roczei,tgraves,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 08 18:43:45 UTC 2019,,,,,,,,,,"0|z065y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Aug/19 16:33;irashid;[~markhamstra] [~jiangxb1987] [~tgraves] [~Ngone51] would appreciate your thoughts on this.  I think the bug I've described above is pretty clear.  However, the part which I'm wondering about a bit more is whether there is more mutability in RDD that could cause problems.

For the case I have of this, I only know for sure that the user is calling {{rdd.cache()}} in another thread.  But I can't see how that would leave to the symptoms I describe above.  I don't know that they are doing anything in ther user thread which would touch {{rdd.dependencies}}, but I also don't have full visibility into everything they are doing, so this still seems like the best explanation to me.;;;","03/Sep/19 13:27;tgraves;So for this to happen, you essentially have to have that RDD referenced in 2 separate threads and the second thread to start operating on it before it has been materialized in the first thread, correct?

I see how it can change with a checkpoint, but like you say I'm not sure how the cache would cause this. Anyway you can see if they are checkpointing? ;;;","16/Sep/19 14:38;irashid;I finally got some more info about this case -- they are not using checkpointing, nor touching dependencies.  It seems things work consistently once they move the call to {{rdd.cache()}} before touching the RDD with multiple threads, so it could still be that caching alone is enough to mess this up somehow.

Just by coincidence, another entirely separate group is reporting what looks to be a very similar bug with submitting jobs from multiple threads.  Its not exactly the same, though -- it doesn't have the orphaned stages in the logs, but does have repeated RDD registration.;;;","08/Oct/19 18:43;vanzin;Issue resolved by pull request 25951
[https://github.com/apache/spark/pull/25951];;;",,,,,,,,,,,,,,,,,,,,
Generated SpecificSafeProjection.apply method grows beyond 64 KB when use  SparkSQL,SPARK-28916,13253744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,MOBIN,MOBIN,29/Aug/19 15:44,12/Dec/22 18:10,13/Jul/23 08:49,09/Sep/19 05:31,2.3.1,2.4.3,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Can be reproduced by the following steps：

1. Create a table with 5000 fields

2. val data=spark.sql(""select * from spark64kb limit 10"");

3. data.describe()

Then，The following error occurred
{code:java}
WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 0, localhost, executor 1): org.codehaus.janino.InternalCompilerException: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling ""GeneratedClass"": Code of method ""apply(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection"" grows beyond 64 KB
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1298)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1376)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1373)
  at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
  at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
  at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
  at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
  at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
  at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
  at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1238)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:143)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
  at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:385)
  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:96)
  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:95)
  at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:180)
  at org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:199)
  at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:40)
  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:86)
  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:121)
  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""GeneratedClass"": Code of method ""apply(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection"" grows beyond 64 KB
  at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
  at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
  at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
  at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:313)
  at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:235)
  at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
  at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$do

{code}",,cloud_fan,maropu,mgaido,MOBIN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 05:31:56 UTC 2019,,,,,,,,,,"0|z065wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Aug/19 04:27;gurwls223;Reproducer:

{code}
val df = spark.range(100).selectExpr((0 to 5000).map(i => s""id as field_$i""): _*)
df.createOrReplaceTempView(""spark64kb"")
val data = spark.sql(""select * from spark64kb limit 10"")
data.describe()
{code};;;","31/Aug/19 09:29;mgaido;Thanks for reporting this. I am checking it.;;;","31/Aug/19 15:46;mgaido;I think the problem is related to subexpression elimination. I've not been able to confirm since for some reasons I am not able to disable it, even though I set the config to false, it is performed anyway. Maybe I am missing something there. Anyway, you may try and set {{spark.sql.subexpressionElimination.enabled}} to {{false}}. Meanwhile I am working on a fix. Thanks.;;;","02/Sep/19 02:45;MOBIN;[~mgaido]thinks, spark.sql.subexpressionElimination.enabled parameter solved my problem;;;","02/Sep/19 08:30;cloud_fan;to double-check, this is just error message not actual exception, right?;;;","09/Sep/19 05:31;cloud_fan;Issue resolved by pull request 25642
[https://github.com/apache/spark/pull/25642];;;",,,,,,,,,,,,,,,,,,
MatchError exception in CheckpointWriteHandler,SPARK-28912,13253698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,avk1,avk1,avk1,29/Aug/19 11:42,12/Dec/22 18:11,13/Jul/23 08:49,07/Sep/19 00:56,2.3.0,2.3.2,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,,2.4.5,3.0.0,,Spark Core,,,,,0,,,,"Setting checkpoint directory name to ""checkpoint-"" plus some digits (e.g. ""checkpoint-01"") results in the following error:
{code:java}
Exception in thread ""pool-32-thread-1"" scala.MatchError: 0523a434-0daa-4ea6-a050-c4eb3c557d8c (of class java.lang.String) 
 at org.apache.spark.streaming.Checkpoint$.org$apache$spark$streaming$Checkpoint$$sortFunc$1(Checkpoint.scala:121) 
 at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:132) 
 at org.apache.spark.streaming.Checkpoint$$anonfun$getCheckpointFiles$1.apply(Checkpoint.scala:132) 
 at scala.math.Ordering$$anon$9.compare(Ordering.scala:200) 
 at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355) 
 at java.util.TimSort.sort(TimSort.java:234) 
 at java.util.Arrays.sort(Arrays.java:1438) 
 at scala.collection.SeqLike$class.sorted(SeqLike.scala:648) 
 at scala.collection.mutable.ArrayOps$ofRef.sorted(ArrayOps.scala:186) 
 at scala.collection.SeqLike$class.sortWith(SeqLike.scala:601) 
 at scala.collection.mutable.ArrayOps$ofRef.sortWith(ArrayOps.scala:186) 
 at org.apache.spark.streaming.Checkpoint$.getCheckpointFiles(Checkpoint.scala:132) 
 at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:262) 
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 
 at java.lang.Thread.run(Thread.java:748){code}",,avk1,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 07 00:56:04 UTC 2019,,,,,,,,,,"0|z065m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Aug/19 04:24;gurwls223;It might be much easier for other poeple write a test and investigate further if there's a self-contained reproducer, if you're not going to open a PR.;;;","02/Sep/19 06:31;gurwls223;ping [~avk1];;;","02/Sep/19 17:15;avk1;Steps to reproduce the error:
 # Start Hadoop in a pseudo-distributed mode.
 # In another terminal run command  {{nc -lk 9999}}
 # In the Spark shell execute the following statements:
{code:java}
scala> val ssc = new StreamingContext(sc, Seconds(30))
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@376fd14f

scala> ssc.checkpoint(""hdfs://localhost:9000/checkpoint-01"")   
        
scala> val lines = ssc.socketTextStream(""localhost"", 9999)
lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@39b7d031
  
scala> val words = lines.flatMap(_.split("" ""))
words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@637ae337   
                                                           
scala> val pairs = words.map(word => (word, 1))          
pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@523d07cc
                                                     
scala> val wordCounts = pairs.reduceByKey(_ + _)   
wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@3c62183b
                                               
scala> wordCounts.print()                    
                                           
scala> ssc.start()                       

scala> ssc.awaitTermination()
{code};;;","07/Sep/19 00:56;dongjoon;Issue resolved by pull request 25654
[https://github.com/apache/spark/pull/25654];;;",,,,,,,,,,,,,,,,,,,,
`bin/spark-submit --version` shows incorrect info,SPARK-28906,13253587,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,vanzin,dongjoon,28/Aug/19 23:25,12/Dec/22 18:10,13/Jul/23 08:49,11/Sep/19 13:13,2.3.1,2.3.2,2.3.3,2.3.4,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,2.4.5,3.0.0,,Project Infra,,,,,0,,,,"Since Spark 2.3.1, `spark-submit` shows a wrong information.
{code}
$ bin/spark-submit --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.3
      /_/

Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_222
Branch
Compiled by user  on 2019-02-04T13:00:46Z
Revision
Url
Type --help for more information.
{code}",,dongjoon,kiszk,shivusondur@gmail.com,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/19 00:20;shivusondur@gmail.com;image-2019-08-29-05-50-13-526.png;https://issues.apache.org/jira/secure/attachment/12978830/image-2019-08-29-05-50-13-526.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 11 13:13:43 UTC 2019,,,,,,,,,,"0|z064xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Aug/19 23:43;shivusondur@gmail.com;i am checking this issue;;;","29/Aug/19 00:21;shivusondur@gmail.com;[~vanzin]

I verified in the master branch,  i got below results

Please let me know what information is wrong  in the below snap?

!image-2019-08-29-05-50-13-526.png!

 ;;;","29/Aug/19 01:27;vanzin;The code is fine. This is a problem in the release scripts.;;;","30/Aug/19 04:38;gurwls223;cc [~kiszk] FYI;;;","30/Aug/19 08:32;kiszk;I attached output of 2.3.0 and 2.3.4 in one comment as below. Let me see the script, too.

```
$ spark-2.3.0-bin-hadoop2.6/bin/spark-submit --version
Welcome to
____ __
/ __/__ ___ _____/ /__
_\ \/ _ \/ _ `/ __/ '_/
/___/ .__/\_,_/_/ /_/\_\ version 2.3.0
/_/
 
Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_212
Branch master
Compiled by user sameera on 2018-02-22T19:24:38Z
Revision a0d7949896e70f427e7f3942ff340c9484ff0aab
Url git@github.com:sameeragarwal/spark.git
Type --help for more information.
$ spark-2.3.4-bin-hadoop2.6/bin/spark-submit --version
Welcome to
____ __
/ __/__ ___ _____/ /__
_\ \/ _ \/ _ `/ __/ '_/
/___/ .__/\_,_/_/ /_/\_\ version 2.3.4
/_/
 
Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_212
Branch 
Compiled by user on 2019-08-26T08:29:39Z
Revision 
Url 
Type --help for more information.
```;;;","30/Aug/19 10:47;kiszk;In {{jars/spark-core_2.11-2.3.*.jar}}, {{spark-version-info.properties}} exists. This file is different between 2.3.0 and 2.3.4.
This file is generated by `build/spark-build-info`.

{code}
$ cat spark-version-info.properties.230
version=2.3.0
user=sameera
revision=a0d7949896e70f427e7f3942ff340c9484ff0aab
branch=master
date=2018-02-22T19:24:38Z
url=git@github.com:sameeragarwal/spark.git
$ cat spark-version-info.properties.234
version=2.3.4
user=
revision=
branch=
date=2019-08-26T08:29:39Z
url=
{code};;;","30/Aug/19 21:16;kiszk;For user name, we have to pass {{USER}} environment variable to the docker container at the end of {{do-release-docker.sh}}. I created a patch to fix this.

For other information to be got by {{git}} command, {{spark-build-info}} script is not executed at the wrong directory (i.e. out of the cloned directory). My guess is the command is executed under the work directory. I did not creat a patch yet.;;;","02/Sep/19 03:31;kiszk;For the information on {{git}} comand, {{.git}} directory is deleted after {{git clone}} is executed. As a result, we cannot get infomration on {{git}} command. When I tentatively stop deleting {{.git}} directory, {{spark-version-info.properties}} can include the correct information like:
{code}
version=2.3.4
user=ishizaki
revision=8c6f8150f3c6298ff4e1c7e06028f12d7eaf0210
branch=HEAD
date=2019-09-02T02:31:25Z
url=https://gitbox.apache.org/repos/asf/spark.git
{code}
;;;","11/Sep/19 13:13;srowen;Issue resolved by pull request 25655
[https://github.com/apache/spark/pull/25655];;;",,,,,,,,,,,,,,,
Fix AWS JDK version conflict that breaks Pyspark Kinesis tests,SPARK-28903,13253548,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,28/Aug/19 17:11,09/Sep/19 12:43,13/Jul/23 08:49,31/Aug/19 15:30,2.4.3,3.0.0,,,,,,,,,2.4.5,3.0.0,,Structured Streaming,,,,,0,,,,"The Pyspark Kinesis tests are failing, at least in master:
{code}
======================================================================
ERROR: test_kinesis_stream (pyspark.streaming.tests.test_kinesis.KinesisStreamTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@2/python/pyspark/streaming/tests/test_kinesis.py"", line 44, in test_kinesis_stream
    kinesisTestUtils = self.ssc._jvm.org.apache.spark.streaming.kinesis.KinesisTestUtils(2)
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1554, in __call__
    answer, self._gateway_client, None, self._fqn)
  File ""/home/jenkins/workspace/SparkPullRequestBuilder@2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
Py4JJavaError: An error occurred while calling None.org.apache.spark.streaming.kinesis.KinesisTestUtils.
: java.lang.NoSuchMethodError: com.amazonaws.regions.Region.getAvailableEndpoints()Ljava/util/Collection;
	at org.apache.spark.streaming.kinesis.KinesisTestUtils$.$anonfun$getRegionNameByEndpoint$1(KinesisTestUtils.scala:211)
	at org.apache.spark.streaming.kinesis.KinesisTestUtils$.$anonfun$getRegionNameByEndpoint$1$adapted(KinesisTestUtils.scala:211)
	at scala.collection.Iterator.find(Iterator.scala:993)
	at scala.collection.Iterator.find$(Iterator.scala:990)
	at scala.collection.AbstractIterator.find(Iterator.scala:1429)
	at scala.collection.IterableLike.find(IterableLike.scala:81)
	at scala.collection.IterableLike.find$(IterableLike.scala:80)
	at scala.collection.AbstractIterable.find(Iterable.scala:56)
	at org.apache.spark.streaming.kinesis.KinesisTestUtils$.getRegionNameByEndpoint(KinesisTestUtils.scala:211)
	at org.apache.spark.streaming.kinesis.KinesisTestUtils.<init>(KinesisTestUtils.scala:46)
...
{code}

The non-Python Kinesis tests are fine though. It turns out that this is because Pyspark tests use the output of the Spark assembly, and it pulls in hadoop-cloud, which in turn pulls in an old AWS Java SDK.

Per [~stevel@apache.org], it seems like we can just resolve this by excluding the aws-java-sdk dependency. See the attached PR for some more detail about the debugging and other options.",,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 31 15:30:49 UTC 2019,,,,,,,,,,"0|z064ow:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"31/Aug/19 15:30;srowen;Issue resolved by pull request 25559
[https://github.com/apache/spark/pull/25559];;;",,,,,,,,,,,,,,,,,,,,,,,
Kubernetes DepsTestsSuite fails on OSX with minikube 1.3.1 due to formatting,SPARK-28886,13253314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holden,holden,holden,27/Aug/19 18:15,17/May/20 18:23,13/Jul/23 08:49,09/Sep/19 01:04,3.0.0,,,,,,,,,,3.0.0,,,Kubernetes,Spark Core,Tests,,,0,,,,"With minikube 1.3.1 on OSX the service discovery command returns an extra ""* "" which doesn't parse into a URL causing the DepsTestsSuite to fail.

 

I've got a fix just need to double check some stuff.",,eje,holden,,,,,,,,,,,,,,,,,,,SPARK-28953,,,,,,,SPARK-28953,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 01:04:33 UTC 2019,,,,,,,,,,"0|z0638w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Sep/19 01:04;srowen;Issue resolved by pull request 25599
[https://github.com/apache/spark/pull/25599];;;",,,,,,,,,,,,,,,,,,,,,,,
fallBackToHdfs should not support Hive partitioned table,SPARK-28876,13253036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,26/Aug/19 13:26,27/Aug/19 13:39,13/Jul/23 08:49,27/Aug/19 13:39,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,The current implementation is incorrect for external partitions and it is expensive to support partitioned table with external partitions.,,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 13:39:10 UTC 2019,,,,,,,,,,"0|z061j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Aug/19 13:39;cloud_fan;Issue resolved by pull request 25584
[https://github.com/apache/spark/pull/25584];;;",,,,,,,,,,,,,,,,,,,,,,,
Specify Jekyll version to 3.8.6 in release docker image,SPARK-28868,13252894,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,25/Aug/19 20:13,25/Aug/19 22:39,13/Jul/23 08:49,25/Aug/19 22:39,2.4.4,3.0.0,,,,,,,,,2.4.4,3.0.0,,Project Infra,,,,,0,,,,"Recently, Jekyll 4.0 is released and it dropped Ruby 2.3 support.

This breaks our release docker image build.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 25 22:39:19 UTC 2019,,,,,,,,,,"0|z060nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Aug/19 22:39;dongjoon;Issue resolved by pull request 25578
[https://github.com/apache/spark/pull/25578];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix typo in SQLConf FILE_COMRESSION_FACTOR,SPARK-28844,13252247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ZhangYao,ZhangYao,22/Aug/19 02:03,22/Aug/19 07:10,13/Jul/23 08:49,22/Aug/19 07:10,2.3.3,2.4.3,3.0.0,,,,,,,,2.3.4,2.4.4,3.0.0,SQL,,,,,0,,,,Fix the typo in SQLConf FILE_COMRESSION_FACTOR and change it to FILE_COMPRESSION_FACTOR,,dongjoon,sharangk,ZhangYao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 22 07:10:52 UTC 2019,,,,,,,,,,"0|z05wo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Aug/19 03:40;sharangk;I will work on this. ;;;","22/Aug/19 04:38;ZhangYao;[~sharangk] Hi, I have already do this.;;;","22/Aug/19 07:10;dongjoon;Issue resolved by pull request 25538
[https://github.com/apache/spark/pull/25538];;;",,,,,,,,,,,,,,,,,,,,,
ExecutorMonitor$Tracker NullPointerException,SPARK-28839,13252141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,yumwang,yumwang,21/Aug/19 14:13,12/Dec/22 17:35,13/Jul/23 08:49,23/Aug/19 19:46,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"
{noformat}
19/08/21 06:44:01 ERROR AsyncEventQueue: Listener ExecutorMonitor threw an exception
java.lang.NullPointerException
	at org.apache.spark.scheduler.dynalloc.ExecutorMonitor$Tracker.removeShuffle(ExecutorMonitor.scala:479)
	at org.apache.spark.scheduler.dynalloc.ExecutorMonitor.$anonfun$cleanupShuffle$2(ExecutorMonitor.scala:408)
	at org.apache.spark.scheduler.dynalloc.ExecutorMonitor.$anonfun$cleanupShuffle$2$adapted(ExecutorMonitor.scala:407)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.dynalloc.ExecutorMonitor.cleanupShuffle(ExecutorMonitor.scala:407)
	at org.apache.spark.scheduler.dynalloc.ExecutorMonitor.onOtherEvent(ExecutorMonitor.scala:351)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:82)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:99)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:84)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:102)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:102)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:97)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:93)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:93)
{noformat}
",,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 23 19:46:49 UTC 2019,,,,,,,,,,"0|z05w0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Aug/19 14:16;yumwang;[~vanzin] I will provide more information if you need it.;;;","23/Aug/19 19:46;vanzin;Issue resolved by pull request 25551
[https://github.com/apache/spark/pull/25551];;;",,,,,,,,,,,,,,,,,,,,,,
FrequentItems applies an incorrect schema to the resulting dataframe when nulls are present,SPARK-28818,13252040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mhawes,mhawes,mhawes,21/Aug/19 07:45,12/Dec/22 18:10,13/Jul/23 08:49,29/Aug/19 01:50,2.4.3,,,,,,,,,,2.4.7,3.0.0,,SQL,,,,,0,,,,"A trivially reproducible bug in the code for `FrequentItems`. The schema for the resulting arrays of frequent items is [hard coded|#L122]] to have non-nullable array elements:
{code:scala}
val outputCols = colInfo.map { v =>
StructField(v._1 + ""_freqItems"", ArrayType(v._2, false))
 }
 val schema = StructType(outputCols).toAttributes
 Dataset.ofRows(df.sparkSession, LocalRelation.fromExternalRows(schema, Seq(resultRow)))
{code}
 

However if the column contains frequent nulls then these nulls are included in the frequent items array. This results in various errors such as any attempt to `collect()` resulting in a null pointer exception:
{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.Builder().getOrCreate()
df = spark.createDataFrame([
    (1, 'a'),
    (2, None),
    (3, 'b'),
], schema=""id INTEGER, val STRING"")

rows = df.freqItems(df.columns).collect()
{code}
 Results in:
{code:java}
Traceback (most recent call last):                                              
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/bin/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py"", line 533, in collect
    sock_info = self._jdf.collectToPython()
  File ""/usr/local/bin/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/usr/local/bin/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/usr/local/bin/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o40.collectToPython.
: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.LocalTableScanExec$$anonfun$unsafeRows$1.apply(LocalTableScanExec.scala:44)
	at org.apache.spark.sql.execution.LocalTableScanExec$$anonfun$unsafeRows$1.apply(LocalTableScanExec.scala:44)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at org.apache.spark.sql.execution.LocalTableScanExec.unsafeRows$lzycompute(LocalTableScanExec.scala:44)
	at org.apache.spark.sql.execution.LocalTableScanExec.unsafeRows(LocalTableScanExec.scala:39)
	at org.apache.spark.sql.execution.LocalTableScanExec.executeCollect(LocalTableScanExec.scala:70)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
{code}
Unclear if the hardcoding is at fault or if the algorithm is actually designed to not return nulls even if they are frequent. In which case the hard coding would be appropriate. I'll put a PR in that assumes that the hardcoding is the bug unless people know otherwise?",,aman_omer,apachespark,mhawes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 02 10:28:11 UTC 2020,,,,,,,,,,"0|z05ve8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Aug/19 07:43;mhawes;[PR|https://github.com/apache/spark/pull/25575] created with tests to ensure it fixes the original issue.;;;","29/Aug/19 01:50;gurwls223;Issue resolved by pull request 25575
[https://github.com/apache/spark/pull/25575];;;","02/Aug/20 10:27;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29327;;;","02/Aug/20 10:28;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29327;;;",,,,,,,,,,,,,,,,,,,,
Delete the incorrect setWeightCol method in LinearSVCModel,SPARK-28780,13251785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,20/Aug/19 08:52,22/Aug/19 07:20,13/Jul/23 08:49,21/Aug/19 14:48,2.2.0,2.3.0,2.4.0,3.0.0,,,,,,,2.3.4,2.4.4,3.0.0,ML,,,,,0,release-notes,,,"1, the weightCol is only used in training, and should not be set in  LinearSVCModel;

2, the method 'def setWeightCol(value: Double): this.type = set(threshold, value)' is wrongly defined, since value should be a string and weightCol instead of threshold should be set.",,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"Spark 2.2 accidentally introduced the method LinearSVCModel.setWeightCol. This method works incorrectly, too. It is removed in 3.0.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 21 14:48:27 UTC 2019,,,,,,,,,,"0|z05tts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Aug/19 14:48;srowen;Issue resolved by pull request 25510
[https://github.com/apache/spark/pull/25510];;;",,,,,,,,,,,,,,,,,,,,,,,
Shuffle jobs fail due to incorrect advertised address when running in virtual network,SPARK-28778,13251717,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anton.kirillov,anton.kirillov,anton.kirillov,19/Aug/19 22:30,31/Aug/19 19:30,13/Jul/23 08:49,24/Aug/19 01:34,2.2.3,2.3.0,2.4.3,,,,,,,,2.4.5,3.0.0,,Mesos,,,,,0,Mesos,,,"When shuffle jobs are launched by Mesos in a virtual network, Mesos scheduler sets executor {{--hostname}} parameter to {{0.0.0.0}} in the case when {{spark.mesos.network.name}} is provided. This makes executors use {{0.0.0.0}} as their advertised address and, in the presence of shuffle, executors fail to fetch shuffle blocks from each other using {{0.0.0.0}} as the origin. When a virtual network is used the hostname or IP address is not known upfront and assigned to a container at its start time so the executor process needs to advertise the correct dynamically assigned address to be reachable by other executors.
h3.  

The bug described above prevents Mesos users from running any jobs which involve shuffle due to the inability of executors to fetch shuffle blocks because of incorrect advertised address when virtual network is used.",,anton.kirillov,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 24 01:36:13 UTC 2019,,,,,,,,,,"0|z05teo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Aug/19 01:34;dongjoon;This is resolved via https://github.com/apache/spark/pull/25500;;;","24/Aug/19 01:36;dongjoon;You are added to the Apache Spark contributor group. Thank you for your first contribution and welcome!;;;",,,,,,,,,,,,,,,,,,,,,,
SparkML MLWriter gets hadoop conf from spark context instead of session,SPARK-28776,13251701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,heleny,heleny,heleny,19/Aug/19 21:21,22/Aug/19 14:28,13/Jul/23 08:49,22/Aug/19 14:27,2.4.3,,,,,,,,,,3.0.0,,,MLlib,,,,,0,,,,"In handleOverwrite of MLWriter, the hadoop configuration of the spark context is used where as the hadoop configuration of the spark session's session state should be used instead. [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/util/ReadWrite.scala#L677]",,heleny,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 22 14:27:48 UTC 2019,,,,,,,,,,"0|z05tb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Aug/19 14:27;srowen;Issue resolved by pull request 25505
[https://github.com/apache/spark/pull/25505];;;",,,,,,,,,,,,,,,,,,,,,,,
DateTimeUtilsSuite fails for JDKs using the tzdata2018i or newer timezone database,SPARK-28775,13251687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,hvanhovell,hvanhovell,19/Aug/19 21:04,20/Aug/19 00:56,13/Jul/23 08:49,20/Aug/19 00:55,3.0.0,,,,,,,,,,2.3.4,2.4.4,3.0.0,SQL,Tests,,,,0,,,,"org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite 'daysToMillis and millisToDays'  test case fails because of an update in the timezone library: tzdata2018h. This retroactively changes a the value of a missing day for the Kwalalein atol. See for more information: https://bugs.openjdk.java.net/browse/JDK-8215981

Let's fix this by excluding both dates.",,dongjoon,hvanhovell,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 20 00:55:34 UTC 2019,,,,,,,,,,"0|z05t80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Aug/19 00:55;dongjoon;Issue resolved by pull request 25504
[https://github.com/apache/spark/pull/25504];;;",,,,,,,,,,,,,,,,,,,,,,,
ReusedExchangeExec cannot be columnar,SPARK-28774,13251641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,revans2,revans2,revans2,19/Aug/19 16:54,12/Dec/22 18:10,13/Jul/23 08:49,21/Aug/19 10:13,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,If a ShuffleExchangeExec is replaced with a columnar version and deduped to a ReusedExchangeExec it will fail because ReusedExchangeExec does not implement any of the columnar APIs.,,cloud_fan,revans2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 21 10:13:01 UTC 2019,,,,,,,,,,"0|z05sxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Aug/19 04:50;gurwls223;Please avoid to set target version which is usually reserved for committers.;;;","21/Aug/19 10:13;cloud_fan;Issue resolved by pull request 25499
[https://github.com/apache/spark/pull/25499];;;",,,,,,,,,,,,,,,,,,,,,,
Fix CRAN incoming feasibility warning on invalid URL,SPARK-28766,13251385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Aug/19 10:35,18/Aug/19 05:36,13/Jul/23 08:49,17/Aug/19 18:12,2.3.4,2.4.4,3.0.0,,,,,,,,2.3.4,2.4.4,3.0.0,SparkR,,,,,0,,,,"*BEFORE*
{code}
* checking CRAN incoming feasibility ... NOTE
Maintainer: ‘Shivaram Venkataraman <shivaram@cs.berkeley.edu>’

Found the following (possibly) invalid URLs:
  URL: https://wiki.apache.org/hadoop/HCFS (moved to https://cwiki.apache.org/confluence/display/hadoop/HCFS)
    From: man/spark.addFile.Rd
    Status: 404
    Message: Not Found
{code}

*AFTER*
{code}
* checking CRAN incoming feasibility ... Note_to_CRAN_maintainers
Maintainer: ‘Shivaram Venkataraman <shivaram@cs.berkeley.edu>’
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 17 18:12:28 UTC 2019,,,,,,,,,,"0|z05rh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Aug/19 18:12;dongjoon;Issue resolved by pull request 25483
[https://github.com/apache/spark/pull/25483];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky Tests: SparkThriftServerProtocolVersionsSuite.HIVE_CLI_SERVICE_PROTOCOL_V1 get binary type,SPARK-28763,13251329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dongjoon,dongjoon,16/Aug/19 20:35,22/Jan/20 22:02,13/Jul/23 08:49,22/Aug/19 03:35,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"{code}
org.apache.spark.sql.hive.thriftserver.SparkThriftServerProtocolVersionsSuite.HIVE_CLI_SERVICE_PROTOCOL_V1 get binary type

org.scalatest.exceptions.TestFailedException: ""[?]("" did not equal ""[�](""
{code}

 !Screen Shot 2019-08-16 at 1.34.23 PM.png|width=100%! 

",,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/19 20:36;dongjoon;Screen Shot 2019-08-16 at 1.34.23 PM.png;https://issues.apache.org/jira/secure/attachment/12977838/Screen+Shot+2019-08-16+at+1.34.23+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 22 03:35:22 UTC 2019,,,,,,,,,,"0|z05r4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Aug/19 20:40;dongjoon;Hi, [~yumwang]. This test suite seems to fail very frequently. As you see, it fails 4 times consequtively.
Could you take a look? If the fix is not trivial, we had better revert this first.
 
cc [~srowen];;;","22/Aug/19 03:35;yumwang;We fixed it by update environment. More details: https://github.com/apache/spark/pull/25480;;;",,,,,,,,,,,,,,,,,,,,,,
File table location should include both values of option `path` and `paths`,SPARK-28757,13251193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,16/Aug/19 08:28,16/Aug/19 14:32,13/Jul/23 08:49,16/Aug/19 14:29,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"In V1 implementation, file table location includes both values of option `path` and `paths`.
In the refactoring of https://github.com/apache/spark/pull/24025, the value of option `path` is ignored if ""paths"" are specified. We should make it consistent with V1.",,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 16 14:29:34 UTC 2019,,,,,,,,,,"0|z05qag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Aug/19 14:29;cloud_fan;Issue resolved by pull request 25473
[https://github.com/apache/spark/pull/25473];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix PySpark tests not to require kafka-0-8 in branch-2.4,SPARK-28749,13251098,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mattf,mattf,mattf,15/Aug/19 20:54,12/Dec/22 18:10,13/Jul/23 08:49,19/Aug/19 23:46,2.4.3,,,,,,,,,,2.4.5,,,PySpark,Tests,,,,0,,,,"As noted in SPARK-27550 we want to encourage testing of Spark 2.4.x with Scala-2.12, and kafka-0-8 does not support Scala-2.12.

Currently, the PySpark tests invoked by `python/run-tests` demand the presence of kafka-0-8 libraries. If not present, this failure message will be generated:
 {code}
Traceback (most recent call last):
 File ""/usr/lib64/python2.7/runpy.py"", line 174, in _run_module_as_main
 ""__main__"", fname, loader, pkg_name)
 File ""/usr/lib64/python2.7/runpy.py"", line 72, in _run_code
 exec code in run_globals
 File ""spark/python/pyspark/streaming/tests.py"", line 1579, in <module>
 kafka_assembly_jar = search_kafka_assembly_jar()
 File ""spark/python/pyspark/streaming/tests.py"", line 1524, in search_kafka_assembly_jar
 ""You need to build Spark with ""
 Exception: Failed to find Spark Streaming kafka assembly jar in spark/external/kafka-0-8-assembly. You need to build Spark with 'build/sbt -Pkafka-0-8 assembly/package streaming-kafka-0-8-assembly/assembly' or 'build/mvn -DskipTests -Pkafka-0-8 package' before running this test.

Had test failures in pyspark.streaming.tests with spark/py_virtenv/bin/python; see logs.
 Process exited with code 255
{code}

This change is only targeted at branch-2.4, as most kafka-0-8 related materials have been removed in master and this problem no longer occurs there.

PROPOSED SOLUTION

The proposed solution is to make the kafka-0-8 stream testing optional for pyspark testing, exactly the same as the Kinesis stream testing currently is, in file `python/pyspark/streaming/tests.py`. This is only a few lines of change.

Ideally it would be limited to when SPARK_SCALA_VERSION >= 2.12, but it turns out to be somewhat onerous to reliably obtain that value from within the python test env, and no other python test code currently does so. So my proposed solution simply makes the use of the kafka-0-8 profile optional, and leaves it to the tester to include it for Scala-2.11 test builds and exclude it for Scala-2.12 test builds.

PR will be available in a day or so.",,holden,mattf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 19 23:46:18 UTC 2019,,,,,,,,,,"0|z05ppc:",9223372036854775807,,,,,dongjoon,,,,,,,,,,,,,,,,,"16/Aug/19 04:39;gurwls223;Seems you can workaround by explicitly setting {{ENABLE_KAFKA_0_8_TESTS}}.;;;","16/Aug/19 05:55;mattf;Hi [~hyukjin.kwon], thanks for looking at the issue.  I did try that, but it doesn't work for the following reason:
In {{python/pyspark/streaming/tests.py}}
* {{ENABLE_KAFKA_0_8_TESTS}} is used to derive boolean {{are_kafka_tests_enabled}}
* The call to {{search_kafka_assembly_jar()}} is not guarded by the use of {{are_kafka_tests_enabled}}.
* And the Failure exception is thrown from {{search_kafka_assembly_jar()}}.

So to make {{ENABLE_KAFKA_0_8_TESTS}} to properly guard the call to {{search_kafka_assembly_jar()}} would be a similar bug fix.;;;","19/Aug/19 23:46;srowen;Issue resolved by pull request 25482
[https://github.com/apache/spark/pull/25482];;;",,,,,,,,,,,,,,,,,,,,,
Memory leaks after stopping of StreamingContext,SPARK-28709,13250487,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,choojoyq,choojoyq,choojoyq,13/Aug/19 09:50,15/Sep/19 09:16,13/Jul/23 08:49,26/Aug/19 14:31,2.4.3,,,,,,,,,,2.4.5,3.0.0,,DStreams,,,,,0,,,,"In my application spark streaming is restarted programmatically by stopping StreamingContext without stopping of SparkContext and creating/starting a new one . I use it for automatic detection of Kafka topic/partition changes and automatic failover in case of non fatal exceptions.

However i notice that after multiple restarts driver fails with OOM. During investigation of heap dump i figured out that StreamingContext object isn't cleared by GC after stopping.

There are several places which holds reference to it :
 # StreamingTab registers StreamingJobProgressListener which holds reference to Streaming Context directly to LiveListenerBus shared queue via ssc.sc.addSparkListener(listener) method invocation. However this listener isn't unregistered at stop method, moreover the same listener is registered via ssc.addStreamingListener(listener) one line above so i assume this listener could just be removed or at least unregistered.
 # json handlers (/streaming/json and /streaming/batch/json) aren't unregistered in SparkUI, while they hold reference to StreamingJobProgressListener. Basically the same issue affects all the pages, i assume that renderJsonHandler should be added to pageToHandlers cache on attachPage method invocation in order to unregistered it as well on detachPage.
 # SparkUi holds reference to StreamingJobProgressListener in the corresponding local variable which isn't cleared after stopping of StreamingContext.

After i applied these changes via reflection in my app OOM on driver side gone.

I will submit a pull request to fix the mentioned issues.",,choojoyq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29087,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 26 14:31:34 UTC 2019,,,,,,,,,,"0|z05lxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Aug/19 14:31;srowen;Issue resolved by pull request 25439
[https://github.com/apache/spark/pull/25439];;;",,,,,,,,,,,,,,,,,,,,,,,
Allow upcast from null type to any types,SPARK-28706,13250423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,13/Aug/19 05:34,13/Aug/19 12:42,13/Jul/23 08:49,13/Aug/19 12:42,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,It should be safe to upcast from null type to any types.,,cloud_fan,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 13 12:42:25 UTC 2019,,,,,,,,,,"0|z05ljk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Aug/19 12:42;cloud_fan;Issue resolved by pull request 25425
[https://github.com/apache/spark/pull/25425];;;",,,,,,,,,,,,,,,,,,,,,,,
Cache an indeterminate RDD could lead to incorrect result while stage rerun,SPARK-28699,13250283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,XuanYuan,XuanYuan,XuanYuan,12/Aug/19 13:16,16/Nov/19 14:48,13/Jul/23 08:49,21/Aug/19 18:03,2.3.3,2.4.3,3.0.0,,,,,,,,2.3.4,2.4.4,3.0.0,Spark Core,,,,,0,correctness,,,"It's another case for the indeterminate stage/RDD rerun while stage rerun happened.

We can reproduce this by the following code, thanks to Tyson for reporting this!
  
{code:scala}
import scala.sys.process._
import org.apache.spark.TaskContext

val res = spark.range(0, 10000 * 10000, 1).map{ x => (x % 1000, x)}
// kill an executor in the stage that performs repartition(239)
val df = res.repartition(113).cache.repartition(239).map { x =>
 if (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId < 1 && TaskContext.get.stageAttemptNumber == 0) {
 throw new Exception(""pkill -f -n java"".!!)
 }
 x
}

val r2 = df.distinct.count()
{code}",,dongjoon,kiszk,smilegator,tcondie,tgraves,viirya,XuanYuan,yumwang,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28845,,,,,,SPARK-23207,SPARK-23243,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 21 18:03:22 UTC 2019,,,,,,,,,,"0|z05kog:",9223372036854775807,,,,,,,,,,,,,2.3.4,2.4.4,,,,,,,,"12/Aug/19 14:24;XuanYuan;-The current [approach|https://github.com/apache/spark/pull/25420] just a bandage fix for returning the wrong answer.-

After further investigation, we found that this bug is nothing to do with cache operation. So we focus on the sort + shuffle self and finally found the root cause is about the wrong usage for radix sort.

In original logic, we open the radix sort only depends on the config, and use the radix for the binary data comparison. It’s maybe OK for the dataset only has one column which is numeric, but during this case, binary format after transform “map\{ x => (x%1000, x)}” operation can’t be sorted by radix sort.

After the fix in [https://github.com/apache/spark/pull/25491] all tests passed with the right answer.

Also, find a corner case of DAGScheduler during the test is fixed separately in [https://github.com/apache/spark/pull/25491].

After we finish the work of indeterminate stage rerunning(SPARK-25341), we can fix this by unpersisting the original RDD and rerunning the cached indeterminate stage. Gives a preview codebase [here|https://github.com/xuanyuanking/spark/tree/SPARK-28699-RERUN].;;;","19/Aug/19 22:23;dongjoon;Hi, [~XuanYuan].
Could you check old Spark versions and update `Affects Version/s:` of this JIRA issue?;;;","20/Aug/19 00:12;smilegator;Also cc [~kiszk] Let us wait for this before starting RC1 for 2.3;;;","20/Aug/19 01:56;kiszk;[~smilegator] Thank you for cc. I wait for fixing this.

I was in the middle of releasing RC1. Thus, there is already {{2.4.4-rc1}} tag in the [branch-2.3|https://github.com/apache/spark/tree/branch-2.3]. Should I remove this tag and release rc1? Or should I leave this tag and release rc2 at first?
;;;","20/Aug/19 03:53;dongjoon;? [~kiszk]. `2.4.4-rc1` is `branch-2.4` and mine. You should not remove that.
I guess you wanted to say `2.3.4-rc1` and `2.3.4-rc1` is not created yet.;;;","20/Aug/19 04:09;kiszk;[~dongjoon] Thank you for pointing out my typo. You are right. I should have said {2.3.4-rc1}.

Actually, while I was doing the following, it is not reflected at the repository yet!  After fixing this, let me restart the release process for {2.3.4-rc1}.

{code}
Release details:
BRANCH: branch-2.3
VERSION: 2.3.4
TAG: v2.3.4-rc1
{code};;;","20/Aug/19 04:13;dongjoon;:)

BTW, I updated this to `Blocker` according to [~smilegator]'s advice.;;;","20/Aug/19 04:15;XuanYuan;[~dongjoon] Sure, the affects version is spark-2.1 after 2.1.4, spark 2.2 after 2.2.3, spark 2.3 and spark 2.4, Jira field update is done.;;;","20/Aug/19 04:26;dongjoon;Thank you for the update, [~XuanYuan]!;;;","21/Aug/19 18:03;dongjoon;This is resolved via https://github.com/apache/spark/pull/25491;;;",,,,,,,,,,,,,,
Spark Yarn ResourceRequestHelper shouldn't lookup setResourceInformation is no resources specified,SPARK-28679,13249920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,abellina,tgraves,tgraves,09/Aug/19 15:17,17/May/20 18:13,13/Jul/23 08:49,26/Aug/19 19:01,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,YARN,,,,0,,,,"in the Spark Yarn ResourceRequestHelper it uses reflection to lookup setResourceInformation. We should skip that lookup if the resource Map is empty.

[https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ResourceRequestHelper.scala#L154]

 ",,abellina,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 26 19:01:06 UTC 2019,,,,,,,,,,"0|z05ig0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Aug/19 16:09;abellina;Working on it.;;;","26/Aug/19 19:01;vanzin;Issue resolved by pull request 25403
[https://github.com/apache/spark/pull/25403];;;",,,,,,,,,,,,,,,,,,,,,,
"""Select All"" checkbox in StagePage doesn't work properly",SPARK-28677,13249858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,09/Aug/19 09:59,10/Aug/19 21:55,13/Jul/23 08:49,10/Aug/19 21:51,3.0.0,,,,,,,,,,3.0.0,,,Web UI,,,,,0,,,,,"In StagePage, only the first optional column (Scheduler Delay, in this case) appears even though ""Select All"" checkbox is checked.",sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/19 09:59;sarutak;Screenshot from 2019-08-09 18-46-05.png;https://issues.apache.org/jira/secure/attachment/12977132/Screenshot+from+2019-08-09+18-46-05.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 10 21:51:49 UTC 2019,,,,,,,,,,"0|z05i28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Aug/19 21:51;srowen;Issue resolved by pull request 25397
[https://github.com/apache/spark/pull/25397];;;",,,,,,,,,,,,,,,,,,,,,,,
[UDF] dropping permanent function when a temporary function with the same name already exists giving wrong msg on dropping it again,SPARK-28671,13249812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pavithraramachandran,abhishek.akg,abhishek.akg,09/Aug/19 04:57,16/Aug/19 13:55,13/Jul/23 08:49,16/Aug/19 13:54,2.4.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Created jar and uploaded at hdfs path
1.	./hdfs dfs -put /opt/trash1/AddDoublesUDF.jar /user/user1/
2.	Launch beeline and created permanent function
CREATE FUNCTION addDoubles AS 'com.huawei.bigdata.hive.example.udf.AddDoublesUDF' using jar 'hdfs://hacluster/user/user1/AddDoublesUDF.jar';
3.	Perform select operation
jdbc:hive2://100.100.208.125:23040/default> SELECT addDoubles(1,2,3);
+------------------------------+--+
| default.addDoubles(1, 2, 3)  |
+------------------------------+--+
| 6.0                          |
+------------------------------+--+
1 row selected (0.111 seconds)
4.	Created temporary function as below
jdbc:hive2://100.100.208.125:23040/default> CREATE temporary FUNCTION addDoubles AS 'com.huawei.bigdata.hive.example.udf.AddDoublesUDF' using jar 'hdfs://hacluster/user/user1/AddDoublesUDF.jar';
5.	jdbc:hive2://100.100.208.125:23040/default> SELECT addDoubles(1,2,3);
+----------------------+--+
| addDoubles(1, 2, 3)  |
+----------------------+--+
| 6.0                  |
+----------------------+--+
1 row selected (0.088 seconds)
6.	Drop function
jdbc:hive2://100.100.208.125:23040/default> drop function addDoubles;
+---------+--+
| Result  |
+---------+--+
+---------+--+
7.	jdbc:hive2://100.100.208.125:23040/default> SELECT addDoubles(1,2,3); -- It is success
8.	Drop again Error thrown
jdbc:hive2://100.100.208.125:23040/default> drop function addDoubles;
Error: org.apache.spark.sql.catalyst.analysis.NoSuchFunctionException: Undefined function: 'default.addDoubles'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; (state=,code=0)

9.	Perform again select 
jdbc:hive2://100.100.208.125:23040/default>  SELECT addDoubles(1,2,3);
+----------------------+--+
| addDoubles(1, 2, 3)  |
+----------------------+--+
| 6.0                  |
	

Issue is why the Error msg shown is step 8 saying it is neither registered as permanent or temporary function where as it is registered as temporary function in step 4 that is why in step 9 select is returning result.
",,abhishek.akg,maropu,pavithraramachandran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 16 13:54:24 UTC 2019,,,,,,,,,,"0|z05hs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Aug/19 04:58;pavithraramachandran;i will work on this;;;","16/Aug/19 13:54;maropu;Resolved by [https://github.com/apache/spark/pull/25394];;;",,,,,,,,,,,,,,,,,,,,,,
Create Hive Partitioned Table without  specifying data type for  partition columns will success unexpectedly,SPARK-28662,13249665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lidinghao,lidinghao,lidinghao,08/Aug/19 13:38,20/Aug/19 06:42,13/Jul/23 08:49,20/Aug/19 06:38,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"*Case :*
Create Hive Partitioned Table without specifying data type for partition column will success unexpectly.
{code:java}
// create a hive table partition by b, but the data type of b isn't specified.
CREATE TABLE tbl(a int) PARTITIONED BY (b) STORED AS parquet
{code}
 

*Root Cause:*

In https://issues.apache.org/jira/browse/SPARK-26435 ,  PARTITIONED BY clause  are extended to support Hive CTAS as following:
{code:java}
// Before
(PARTITIONED BY '(' partitionColumns=colTypeList ')’

//After
(PARTITIONED BY '(' partitionColumns=colTypeList ‘)’|
PARTITIONED BY partitionColumnNames=identifierList) |

{code}
Create Table Statement like above case will pass the syntax check,  and recognized as (PARTITIONED BY partitionColumnNames=identifierList) 。

We should check this case in visitCreateHiveTable and give a explicit error message to user

 ",,cloud_fan,lidinghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 20 06:38:01 UTC 2019,,,,,,,,,,"0|z05gvc:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"20/Aug/19 06:38;cloud_fan;Issue resolved by pull request 25390
[https://github.com/apache/spark/pull/25390];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix currentContext Instance failed sometimes,SPARK-28657,13249620,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hongdongdong,hongdongdong,hongdongdong,08/Aug/19 09:40,09/Sep/19 23:45,13/Jul/23 08:49,09/Sep/19 23:04,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"When run spark on yarn, I got 
{code:java}
// java.lang.ClassCastException: org.apache.hadoop.ipc.CallerContext$Builder cannot be cast to scala.runtime.Nothing$ 
{code}
  !warn.jpg!

{{Utils.classForName return Class[Nothing], I think it should be defind as Class[_] to resolve this issue}}"," 

 ",hongdongdong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/19 10:26;hongdongdong;warn.jpg;https://issues.apache.org/jira/secure/attachment/12977020/warn.jpg",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 23:04:24 UTC 2019,,,,,,,,,,"0|z05glc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Sep/19 23:04;srowen;Issue resolved by pull request 25389
[https://github.com/apache/spark/pull/25389];;;",,,,,,,,,,,,,,,,,,,,,,,
Streaming file source doesn't change the schema to nullable automatically,SPARK-28651,13249529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,tom.magdanski,zsxwing,07/Aug/19 21:13,12/Dec/22 18:11,13/Jul/23 08:49,09/Aug/19 09:55,2.4.3,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,release-notes,,,"Right now, batch DataFrame always changes the schema to nullable automatically (See this line: https://github.com/apache/spark/blob/325bc8e9c6187a96b33a033fbb0145dfca619135/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L399).

However, streaming DataFrame's schema is read in this line https://github.com/apache/spark/blob/325bc8e9c6187a96b33a033fbb0145dfca619135/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L259 which doesn't change the schema to nullable automatically.

We should make streaming DataFrame consistent with batch.

It can cause corrupted parquet files due to the schema mismatch.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"All fields of the Structured Streaming's file source schema will be forced to be nullable since Spark 3.0.0. This protects users from corruptions when the specified or inferred schema is not compatible with actual data. If you would like the original behavior, you can set the SQL conf ""spark.sql.streaming.fileSource.schema.forceNullable"" to ""false"". This flag is added to reduce the migration work when upgrading to Spark 3.0.0 and will be removed in future. Please update your codes to work with the new behavior as soon as possible.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 09:55:27 UTC 2019,,,,,,,,,,"0|z05g14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Aug/19 09:55;gurwls223;Issue resolved by pull request 25382
[https://github.com/apache/spark/pull/25382];;;",,,,,,,,,,,,,,,,,,,,,,,
Recover additional metric feature and remove additional-metrics.js,SPARK-28647,13249506,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,07/Aug/19 18:24,19/Aug/19 02:56,13/Jul/23 08:49,13/Aug/19 00:06,3.0.0,,,,,,,,,,2.4.4,3.0.0,,Web UI,,,,,0,,,,"After stagepage.js was introduced, additional-metrics.js is no longer used so let's remove it. It's just a cleanup so I don't think it's worth filing. But if you think it's needed, please let me know.",,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 13 00:06:33 UTC 2019,,,,,,,,,,"0|z05fw0:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"13/Aug/19 00:06;dongjoon;This is resolved via https://github.com/apache/spark/pull/25374;;;",,,,,,,,,,,,,,,,,,,,,,,
Hide credentials in show create table,SPARK-28642,13249321,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,07/Aug/19 10:25,01/Sep/19 06:28,13/Jul/23 08:49,08/Aug/19 23:25,3.0.0,,,,,,,,,,2.4.4,3.0.0,,SQL,,,,,0,,,,"{code:sql}
spark-sql> show create table mysql_federated_sample;
CREATE TABLE `mysql_federated_sample` (`TBL_ID` BIGINT, `CREATE_TIME` INT, `DB_ID` BIGINT, `LAST_ACCESS_TIME` INT, `OWNER` STRING, `RETENTION` INT, `SD_ID` BIGINT, `TBL_NAME` STRING, `TBL_TYPE` STRING, `VIEW_EXPANDED_TEXT` STRING, `VIEW_ORIGINAL_TEXT` STRING, `IS_REWRITE_ENABLED` BOOLEAN)
USING org.apache.spark.sql.jdbc
OPTIONS (
`url` 'jdbc:mysql://localhost/hive?user=root&password=mypasswd',
`driver` 'com.mysql.jdbc.Driver',
`dbtable` 'TBLS'
)
{code}",,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17783,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 23:25:02 UTC 2019,,,,,,,,,,"0|z05eqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Aug/19 23:25;dongjoon;Issue resolved by pull request 25375
[https://github.com/apache/spark/pull/25375];;;",,,,,,,,,,,,,,,,,,,,,,,
Task summary metrics are wrong when there are running tasks,SPARK-28638,13249175,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,06/Aug/19 17:01,12/Aug/19 18:54,13/Jul/23 08:49,12/Aug/19 18:54,3.0.0,,,,,,,,,,2.4.4,3.0.0,,Web UI,,,,,0,,,,"Currently, on requesting summary metrics, cached data are returned if the current number of successful tasks is the same as the cached data.
However, the number of successful tasks is wrong. In `AppStatusStore`, the KVStore is ElementTrackingStore, instead of InMemoryStore. This PR is to fix the class matching.

",,Gengliang.Wang,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 12 18:54:47 UTC 2019,,,,,,,,,,"0|z05dug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Aug/19 18:54;vanzin;Issue resolved by pull request 25369
[https://github.com/apache/spark/pull/25369];;;",,,,,,,,,,,,,,,,,,,,,,,
Failed to start SparkSession with Keytab file ,SPARK-28634,13249049,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,yumwang,yumwang,06/Aug/19 08:28,17/May/20 18:14,13/Jul/23 08:49,19/Aug/19 18:06,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,YARN,,,,0,,,,"{noformat}
[user-etl@hermesdevour002-700165 spark-3.0.0-SNAPSHOT-bin-2.7.4]$ bin/spark-sql --master yarn --conf spark.yarn.keytab=/apache/spark-2.3.0-bin-2.7.3/conf/user-etl.keytab --conf spark.yarn.principal=user-etl@PROD.EXAMPLE.COM
log4j:WARN No such property [maxFileSize] in org.apache.log4j.rolling.RollingFileAppender.
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.rolling.RollingFileAppender.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1564558112805_1794 failed 2 times due to AM Container for appattempt_1564558112805_1794_000002 exited with  exitCode: 1
For more detailed output, check the application tracking page: https://0.0.0.0:8190/applicationhistory/app/application_1564558112805_1794 Then click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_e1987_1564558112805_1794_02_000001
Exit code: 1
Shell output: main : command provided 1
main : run as user is user-etl
main : requested yarn user is user-etl
Getting exit code file...
Creating script paths...
Writing pid file...
Writing to tmp file /hadoop/2/yarn/local/nmPrivate/application_1564558112805_1794/container_e1987_1564558112805_1794_02_000001/container_e1987_1564558112805_1794_02_000001.pid.tmp
Writing to cgroup task files...
Creating local dirs...
Launching container...
Getting exit code file...
Creating script paths...


Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
log4j:WARN No such property [maxFileSize] in org.apache.log4j.rolling.RollingFileAppender.
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.rolling.RollingFileAppender.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/2/yarn/local/usercache/user-etl/filecache/58/__spark_libs__4358879230136591830.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/apache/releases/hbase-1.1.2.2.6.4.1/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/apache/releases/hadoop-2.7.3.2.6.4.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread ""main"" org.apache.spark.SparkException: Keytab file: /apache/spark-2.3.0-bin-2.7.3/conf/user-etl.keytab does not exist
	at org.apache.spark.deploy.SparkHadoopUtil.loginUserFromKeytab(SparkHadoopUtil.scala:131)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:846)
	at org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:889)
	at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)

Failing this attempt. Failing the application.
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:95)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:185)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2466)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$5(SparkSession.scala:948)
	at scala.Option.getOrElse(Option.scala:138)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:315)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:853)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:168)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:196)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:87)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:932)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:941)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}

The keytab file is exist and can work in Spark 2.3 and 2.4.",,Steven Rand,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 19 18:06:27 UTC 2019,,,,,,,,,,"0|z05d2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Aug/19 08:32;yumwang;cc [~vanzin] I will provide more information if you need it.;;;","07/Aug/19 16:48;vanzin;Ah. If you use {{\-\-principal}} and {{\-\-keytab}} this works.

The config name has changed in master and you're using the deprecated ones; the YARN client code removes them from the config in client mode, but only the new names:
https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L769

For proper backwards compatibility it needs to remove the old names too. (Or make a change in the AM instead to ignore the keytab when running in client mode, which avoids the above hack.);;;","08/Aug/19 05:51;yumwang;Thank you [~vanzin] It works.;;;","15/Aug/19 20:25;vanzin;I think it's still worth it to fix it, so that users with old configuration are not surprised by this.;;;","19/Aug/19 18:06;vanzin;Issue resolved by pull request 25467
[https://github.com/apache/spark/pull/25467];;;",,,,,,,,,,,,,,,,,,,
Support namespaces in V2SessionCatalog,SPARK-28628,13248910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,05/Aug/19 16:03,03/Sep/19 20:19,13/Jul/23 08:49,03/Sep/19 20:15,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,V2SessionCatalog should implement SupportsNamespaces.,,brkyvz,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27661,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 20:15:12 UTC 2019,,,,,,,,,,"0|z05c7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Sep/19 20:15;brkyvz;Resolved by [https://github.com/apache/spark/pull/25363];;;",,,,,,,,,,,,,,,,,,,,,,,
"Spark leaves unencrypted data on local disk, even with encryption turned on (CVE-2019-10099)",SPARK-28626,13248898,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,irashid,irashid,05/Aug/19 14:51,19/Mar/20 02:07,13/Jul/23 08:49,05/Aug/19 14:51,2.3.2,,,,,,,,,,2.3.3,2.4.0,,Security,,,,,0,,,,"Severity: Important

 

Vendor: The Apache Software Foundation

 

Versions affected:

All Spark 1.x, Spark 2.0.x, Spark 2.1.x, and 2.2.x versions

Spark 2.3.0 to 2.3.2

 

Description:

Prior to Spark 2.3.3, in certain situations Spark would write user data to local disk unencrypted, even if spark.io.encryption.enabled=true.  This includes cached blocks that are fetched to disk (controlled by spark.maxRemoteBlockSizeFetchToMem); in SparkR, using parallelize; in Pyspark, using broadcast and parallelize; and use of python udfs.

 

 

Mitigation:

1.x, 2.0.x, 2.1.x, 2.2.x, 2.3.x  users should upgrade to 2.3.3 or newer, including 2.4.x

 

Credit:

This issue was reported by Thomas Graves of NVIDIA.

 

References:

[https://spark.apache.org/security.html]

 

The following commits were used to fix this issue, in branch-2.3 (there may be other commits in master / branch-2.4, that are equivalent.)
{noformat}
commit 575fea120e25249716e3f680396580c5f9e26b5b
Author: Imran Rashid <irashid@cloudera.com>
Date:   Wed Aug 22 16:38:28 2018 -0500

    [CORE] Updates to remote cache reads

    Covered by tests in DistributedSuite

 
commit 6d742d1bd71aa3803dce91a830b37284cb18cf70
Author: Imran Rashid <irashid@cloudera.com>
Date:   Thu Sep 6 12:11:47 2018 -0500

    [PYSPARK][SQL] Updates to RowQueue

    Tested with updates to RowQueueSuite

 
commit 09dd34cb1706f2477a89174d6a1a0f17ed5b0a65
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Aug 13 21:35:34 2018 -0500 

    [PYSPARK] Updates to pyspark broadcast

 
commit 12717ba0edfa5459c9ac2085f46b1ecc0ee759aa
Author: hyukjinkwon <gurwls223@apache.org>
Date:   Mon Sep 24 19:25:02 2018 +0800 

    [SPARKR] Match pyspark features in SparkR communication protocol
{noformat}",,irashid,wypoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 02:07:42 UTC 2020,,,,,,,,,,"0|z05c54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Mar/20 02:07;wypoon;For the record, to assist folks who need to backport this:
From branch-2.3, we also need [https://github.com/apache/spark/commit/323dc3ad02e63a7c99b5bd6da618d6020657ecba]
[PYSPARK] Update py4j to version 0.10.7.
For the SPARKR change, there is a preceding change that is needed
[https://github.com/apache/spark/commit/dad5c48b2a229bf6f9e6b8548f9335f04a15c818]
[MINOR][PYTHON] Use a helper in `PythonUtils` instead of direct accessing Scala package
;;;",,,,,,,,,,,,,,,,,,,,,,,
CheckCartesianProducts may throw some error which mismatch generated physical plan,SPARK-28621,13248850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,05/Aug/19 09:55,27/Aug/19 13:55,13/Jul/23 08:49,27/Aug/19 13:55,2.4.3,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"CheckCartesianProducts check logical plan which mismatch the physical plan. So when option ""spark.sql.crossJoin.enabled"" turn off (by default), it may throw some mismatching error which make user confusing.

 

There're some cases:

1) The sql optimizer will possibly do join reordering. So that if one join marked by ""CROSS JOIN"" syntax, it may be reordered in the physical plan. So here CheckCartesianProducts only check logical plan which will possibly mismatch the physical plan.

2) Other places which may be inconsistent with physical plan:

providing:
{code:java}
spark.range(2).createOrReplaceTempView(""sm1"") // can be broadcast
spark.range(50000000).createOrReplaceTempView(""bg1"") // cannot be broadcast
spark.range(60000000).createOrReplaceTempView(""bg2"") // cannot be broadcast
{code}
and suppose `spark.sql.crossJoin.enabled=false` (by default)

1) Some join could be convert to broadcast nested loop join, but CheckCartesianProducts raise error. e.g.
{code:java}
select sm1.id, bg1.id from bg1 join sm1 where sm1.id < bg1.id
{code}
2) Some join will run by CartesianJoin but CheckCartesianProducts DO NOT raise error. e.g.
{code:java}
select bg1.id, bg2.id from bg1 join bg2 where bg1.id < bg2.id
{code}",,cloud_fan,mdidonna,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 13:55:06 UTC 2019,,,,,,,,,,"0|z05bug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Aug/19 13:55;cloud_fan;Issue resolved by pull request 25520
[https://github.com/apache/spark/pull/25520];;;",,,,,,,,,,,,,,,,,,,,,,,
Update CRAN key to recover docker image generation,SPARK-28606,13248675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,02/Aug/19 22:48,19/Aug/19 03:22,13/Jul/23 08:49,02/Aug/19 23:41,2.4.4,3.0.0,,,,,,,,,2.4.4,3.0.0,,Project Infra,,,,,0,,,,"CRAN repo changed the key.
- https://cran.r-project.org/bin/linux/ubuntu/README.html
{code}
Err:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9
...
W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9
E: The repository 'https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease' is not signed.
{code}

Although they changed the key, but it reuses `cran35` for R 3.6.
{code}
Even though R has moved to version 3.6, for compatibility the sources.list entry still uses the cran3.5 designation.
{code}

This issue aims to recover the docker image generation first. We will verify the R doc generation in a separate issue.",,dbtsai,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 23:41:15 UTC 2019,,,,,,,,,,"0|z05ark:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/19 22:54;dongjoon;This is a release blocker for 2.4.4 and 3.0.0 since we should use that script and docker image to generate the release.
;;;","02/Aug/19 23:41;dbtsai;Issue resolved by pull request 25339
[https://github.com/apache/spark/pull/25339];;;",,,,,,,,,,,,,,,,,,,,,,
Fix `Execution Time` and `Duration` column sorting for ThriftServerSessionPage,SPARK-28599,13248469,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,yumwang,yumwang,02/Aug/19 02:41,23/Sep/19 22:09,13/Jul/23 08:49,22/Sep/19 21:18,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,,,2.4.5,3.0.0,,SQL,Web UI,,,,0,,,,Support sorting `Execution Time` and `Duration` columns for ThriftServerSessionPage,,dongjoon,shahid,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25566,SPARK-25567,SPARK-29053,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 22 21:18:44 UTC 2019,,,,,,,,,,"0|z059hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/19 02:56;yumwang;cc [~shahid] Would you like to pick up this?;;;","02/Aug/19 06:44;shahid;Thanks [~yumwang]. I will analyze this.
In SQL tab, we supported pagination, mainly because of the performance issue. Like crashing the page when loading large number of queries etc. I would like to know if any similar issue is there in the thriftserver tab.;;;","22/Sep/19 21:18;dongjoon;This is resolved via https://github.com/apache/spark/pull/25892;;;",,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.scheduler.TaskSchedulerImplSuite,SPARK-28584,13248153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,31/Jul/19 16:51,02/Aug/19 16:46,13/Jul/23 08:49,01/Aug/19 17:38,3.0.0,,,,,,,,,,3.0.0,,,Tests,,,,,0,,,,"This is another of those tests that don't seem to fail in PRs here, but fail more often than we'd like in our build machines. In this case it fails in several different ways, e.g.:

{noformat}
org.scalatest.exceptions.TestFailedException: Map(org.apache.spark.scheduler.TaskSetManager$$EnhancerByMockitoWithCGLIB$$c676cf51@412f9d43 -> 1550579875956) did not contain key org.apache.spark.scheduler.TaskSetManager$$EnhancerByMockitoWithCGLIB$$c676cf51@1945f15f
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
      at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
      at org.apache.spark.scheduler.TaskSchedulerImplSuite$$anonfun$21.apply(TaskSchedulerImplSuite.scala:635)
      at org.apache.spark.scheduler.TaskSchedulerImplSuite$$anonfun$21.apply(TaskSchedulerImplSuite.scala:591)
{noformat}

Or:

{noformat}
The code passed to eventually never returned normally. Attempted 40 times over 503.217543 milliseconds. Last failure message: tsm.isZombie was false.

Error message:
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 40 times over 503.217543 milliseconds. Last failure message: tsm.isZombie was false.
      at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:421)
      at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:439)
      at org.apache.spark.scheduler.TaskSchedulerImplSuite.eventually(TaskSchedulerImplSuite.scala:44)
      at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:337)
      at org.apache.spark.scheduler.TaskSchedulerImplSuite.eventually(TaskSchedulerImplSuite.scala:44)
      at org.apache.spark.scheduler.TaskSchedulerImplSuite$$anonfun$18.apply(TaskSchedulerImplSuite.scala:543)
      at org.apache.spark.scheduler.TaskSchedulerImplSuite$$anonfun$18.apply(TaskSchedulerImplSuite.scala:511)
{noformat}

There's a race condition in the test that can cause these different failures.",,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 16:46:52 UTC 2019,,,,,,,,,,"0|z057jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Aug/19 17:38;dongjoon;Issue resolved by pull request 25317
[https://github.com/apache/spark/pull/25317];;;","01/Aug/19 17:42;vanzin;Please keep the original title. It describes the problem that lead to the fix. And makes searching easier.;;;","02/Aug/19 16:46;dongjoon;Oh, got it. Thank you for fixing that, [~vanzin].;;;",,,,,,,,,,,,,,,,,,,,,
Subqueries should not call `onUpdatePlan` in Adaptive Query Execution,SPARK-28583,13248145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,31/Jul/19 16:16,03/Oct/19 22:14,13/Jul/23 08:49,07/Aug/19 20:11,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Subqueries do not have their own execution id, thus when calling {{AdaptiveSparkPlanExec.onUpdatePlan}}, it will actually get the {{QueryExecution}} instance of the main query, which is wasteful and problematic. It could cause issues like stack overflow or dead locks in some circumstances.",,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-07-31 16:16:07.0,,,,,,,,,,"0|z057hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark daemon exit failed when receive SIGTERM on py3.7,SPARK-28582,13248139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,31/Jul/19 15:45,12/Dec/22 18:10,13/Jul/23 08:49,03/Aug/19 01:36,2.4.3,,,,,,,,,,2.3.4,2.4.4,3.0.0,PySpark,,,,,0,,,,"Pyspark daemon exit failed when receive SIGTERM on py3.7.

We can run test on py3.7 like
{code}
python/run-tests --python-executables=python3.7 --testname ""pyspark.tests.test_daemon DaemonTests""
{code}

Will fail on test ""test_termination_sigterm"". And we can see daemon process do not exit.

This issue happen on py3.7 but lower version python works fine.
",,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 03 01:36:54 UTC 2019,,,,,,,,,,"0|z057gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Aug/19 01:36;gurwls223;Issue resolved by pull request 25343
[https://github.com/apache/spark/pull/25343];;;",,,,,,,,,,,,,,,,,,,,,,,
Add simple analysis checks to the V2 Create Table code paths,SPARK-28572,13247974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,31/Jul/19 01:01,09/Aug/19 04:09,13/Jul/23 08:49,09/Aug/19 04:05,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Currently, the V2 Create Table code paths don't have any checks around:
 # The existence of transforms in the table schema
 # Duplications of transforms
 # Case sensitivity checks around column names

Having these rudimentary checks would simplify V2 Catalog development.

Note that the goal of this JIRA is to not make V2 Create Table Hive Compatible.",,brkyvz,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 04:05:17 UTC 2019,,,,,,,,,,"0|z056g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Aug/19 04:05;cloud_fan;Issue resolved by pull request 25305
[https://github.com/apache/spark/pull/25305];;;",,,,,,,,,,,,,,,,,,,,,,,
Error should also be sent to QueryExecutionListener.onFailure,SPARK-28556,13247733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,29/Jul/19 20:33,12/Dec/22 18:10,13/Jul/23 08:49,30/Jul/19 02:48,2.4.3,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Right now Error is not sent to QueryExecutionListener.onFailure. If there is any Error when running a query, QueryExecutionListener.onFailure cannot be triggered.",,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31144,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 14 07:51:28 UTC 2020,,,,,,,,,,"0|z054yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Jul/19 02:48;gurwls223;Issue resolved by pull request 25292
[https://github.com/apache/spark/pull/25292];;;","29/Jan/20 06:39;dongjoon;[~smilegator]. How do we need to handle this? Do you want to add a deprecation note at 2.4.5?
2.4.5 is the last version before 3.0.0.;;;","29/Jan/20 07:28;zsxwing;This is not deprecating the API. We just fixed an issue in an experimental API. Hence, I don't think we need any deprecation note.;;;","29/Jan/20 17:41;dongjoon;Got it. Thanks for the confirmation, [~zsxwing].;;;","29/Jan/20 18:29;zsxwing;This also reminds me that we should also review all public APIs to see if there is any similar issue, since 3.0.0 is a good chance to fix API bugs.;;;","29/Jan/20 19:30;dongjoon;That sounds great!;;;","14/Mar/20 07:51;zsxwing;This change has been reverted. See SPARK-31144 for the new fix.;;;",,,,,,,,,,,,,,,,,
"CTAS SQL with LOCATION properties won't check location path, cause data under this location lose",SPARK-28551,13247628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vinodkc,angerszhuuu,angerszhuuu,29/Jul/19 11:10,21/May/21 09:34,13/Jul/23 08:49,20/May/21 06:13,2.4.0,,,,,,,,,,3.2.0,,,SQL,,,,,1,,,,"When we run SQL like 
{code:java}
CRETE TABLE TBL 
LOCATION 'PATH_URI'
AS
SELECT QUERY{code}
It won't check PATH_URI status, if there is some data under this path, it will just overwrite this path.

If user careless write a path with important data store under the path, it will be a disaster。 

Thought about add a checker in Catalyst for this situation.

In addition, if it fails to delete the old path completely, old data and new data coexists. ",,angerszhuuu,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 09:34:28 UTC 2021,,,,,,,,,,"0|z054bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/May/21 05:46;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/32411;;;","01/May/21 05:46;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/32411;;;","20/May/21 06:13;cloud_fan;Issue resolved by pull request 32411
[https://github.com/apache/spark/pull/32411];;;","21/May/21 09:34;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32618;;;",,,,,,,,,,,,,,,,,,,,
DebugExec cannot debug broadcast or columnar related queries.,SPARK-28537,13247482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,27/Jul/19 17:07,09/Aug/19 04:31,13/Jul/23 08:49,05/Aug/19 23:29,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"DebugExec does not implement doExecuteBroadcast and doExecuteColumnar so we can't debug broadcast or columnar related query.

One example for broadcast is here.
{code}
val df1 = Seq(1, 2, 3).toDF
val df2 = Seq(1, 2, 3).toDF
val joined = df1.join(df2, df1(""value"") === df2(""value""))
joined.debug()

java.lang.UnsupportedOperationException: Debug does not implement doExecuteBroadcast
...
{code}

Another for columnar is here.
{code}
val df = Seq(1, 2, 3).toDF
df.persist
df.debug()

java.lang.IllegalStateException: Internal Error class org.apache.spark.sql.execution.debug.package$DebugExec has column support mismatch:
...
{code}",,maropu,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 23:29:20 UTC 2019,,,,,,,,,,"0|z053f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Aug/19 23:29;maropu;Resolved by [https://github.com/apache/spark/pull/25274];;;",,,,,,,,,,,,,,,,,,,,,,,
WholeStageCodegen does not work property for LocalTableScanExec,SPARK-28520,13247232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,26/Jul/19 07:30,30/Jul/19 23:58,13/Jul/23 08:49,28/Jul/19 23:40,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Code is not generated for LocalTableScanExec although proper situations.

If a LocalTableScanExec plan has the direct parent plan which supports WholeStageCodegen,
the LocalTableScanExec plan also should be within a WholeStageCodegen domain.
But code is not generated for LocalTableScanExec and InputAdapter is inserted for now.

{code}
val df1 = spark.createDataset(1 to 10).toDF
val df2 = spark.createDataset(1 to 10).toDF
val df3 = df1.join(df2, df1(""value"") === df2(""value""))
df3.explain(true)

...

== Physical Plan ==
*(1) BroadcastHashJoin [value#1], [value#6], Inner, BuildRight
:- LocalTableScan [value#1]                                             // LocalTableScanExec is not within a WholeStageCodegen domain
+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
   +- LocalTableScan [value#6]
{code}

{code}
scala> df3.queryExecution.executedPlan.children.head.children.head.getClass
res4: Class[_ <: org.apache.spark.sql.execution.SparkPlan] = class org.apache.spark.sql.execution.InputAdapter
{code}


For the current implementation of LocalTableScanExec, codegen is enabled in case `parent` is not null
but `parent` is set in `consume`, which is called after `insertInputAdapter` so it doesn't work as intended.
",,maropu,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 28 23:40:05 UTC 2019,,,,,,,,,,"0|z051vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Jul/19 23:40;maropu;Resolved by [https://github.com/apache/spark/pull/25260|https://github.com/apache/spark/pull/25260#issuecomment-515752501];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix StatisticsCollectionTestBase#getDataSize refer to ChecksumFileSystem#isChecksumFile,SPARK-28518,13247220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,26/Jul/19 06:07,26/Jul/19 21:52,13/Jul/23 08:49,26/Jul/19 21:48,3.0.0,,,,,,,,,,3.0.0,,,SQL,Tests,,,,0,,,,"StatisticsCollectionTestBase.getDataSize is incorrect. We should refer to ChecksumFileSystem.isChecksumFile.

https://github.com/apache/spark/pull/25014#discussion_r307050435",,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28216,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 26 21:48:44 UTC 2019,,,,,,,,,,"0|z051sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jul/19 21:48;dongjoon;Issue resolved by pull request 25259
[https://github.com/apache/spark/pull/25259];;;",,,,,,,,,,,,,,,,,,,,,,,
Get REV from RELEASE_VERSION instead of VERSION,SPARK-28511,13246983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,25/Jul/19 07:28,25/Jul/19 17:59,13/Jul/23 08:49,25/Jul/19 17:54,3.0.0,,,,,,,,,,3.0.0,,,Project Infra,,,,,0,,,,"Unlike the other versions, `x.x.0-SNAPSHOT` causes `x.x.-1`. Although this will not happen in the tags (there is no `SNAPSHOT` postfix), we had better fix this.
{code}
$ dev/create-release/do-release-docker.sh -d /tmp/spark-3.0.0 -n
Output directory already exists. Overwrite and continue? [y/n] y
Branch [branch-2.4]: master
Current branch version is 3.0.0-SNAPSHOT.
Release [3.0.-1]:
{code}

The following is the expected behavior.
{code}
$ dev/create-release/do-release-docker.sh -d /tmp/spark-3.0.0 -n
Branch [branch-2.4]: master
Current branch version is 3.0.0-SNAPSHOT.
Release [3.0.0]:
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 17:54:45 UTC 2019,,,,,,,,,,"0|z050c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jul/19 17:54;dongjoon;Issue resolved by pull request 25254
[https://github.com/apache/spark/pull/25254];;;",,,,,,,,,,,,,,,,,,,,,,,
K8S integration tests are failing,SPARK-28509,13246895,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,vanzin,vanzin,24/Jul/19 18:46,17/May/20 18:24,13/Jul/23 08:49,24/Jul/19 21:47,3.0.0,,,,,,,,,,,,,Kubernetes,Spark Core,Tests,,,0,,,,"I've been seeing lots of failures in master. e.g. https://amplab.cs.berkeley.edu/jenkins/job/testing-k8s-prb-make-spark-distribution-unified/13180/console

{noformat}
- Start pod creation from template *** FAILED ***
  io.fabric8.kubernetes.client.KubernetesClientException: 404 page not found
  at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$2.onFailure(WatchConnectionManager.java:201)
  at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
  at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198)
  at okhttp3.RealCall$AsyncCall.execute(RealCall.java:206)
  at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
  ...
- PVs with local storage *** FAILED ***
  io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.39.112:8443/api/v1/persistentvolumes. Message: PersistentVolume ""test-local-pv"" is invalid: [spec.local: Forbidden: Local volumes are disabled by feature-gate, metadata.annotations: Required value: Local volume requires node affinity]. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=spec.local, message=Forbidden: Local volumes are disabled by feature-gate, reason=FieldValueForbidden, additionalProperties={}), StatusCause(field=metadata.annotations, message=Required value: Local volume requires node affinity, reason=FieldValueRequired, additionalProperties={})], group=null, kind=PersistentVolume, name=test-local-pv, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=PersistentVolume ""test-local-pv"" is invalid: [spec.local: Forbidden: Local volumes are disabled by feature-gate, metadata.annotations: Required value: Local volume requires node affinity], metadata=ListMeta(_continue=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).
  at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:478)
  at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:417)
  at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:381)
  at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:344)
  at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:227)
  at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:787)
  at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:357)
  at org.apache.spark.deploy.k8s.integrationtest.PVTestsSuite.setupLocalStorage(PVTestsSuite.scala:87)
  at org.apache.spark.deploy.k8s.integrationtest.PVTestsSuite.$anonfun$$init$$1(PVTestsSuite.scala:137)
  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
  ...
- Launcher client dependencies *** FAILED ***
  The code passed to eventually never returned normally. Attempted 1 times over 6.673903200033333 minutes. Last failure message: assertion failed: 
{noformat}
",,dongjoon,shaneknapp,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 18:13:55 UTC 2019,,,,,,,,,,"0|z04zso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jul/19 18:46;vanzin;[~shaneknapp] in case this is an infra issue.;;;","24/Jul/19 20:03;shaneknapp;as a precautionary step, on all ubuntu workers, i:

1) minikube stop && minikube delete
2) rm -rf .minikube .kube
3) rebooting the workers once all jobs are done.

things are now passing...  i'm hoping this clears it up.  the minikube/k8s versions haven't changed, but i did find a couple of dead pods that needed cleaning up on amp-jenkins-staging-worker-02.  the dead pods were in a completely different namespace, so that shouldn't impact the tests.

i will keep a close eye on this and see if i can track the failures down to one specific worker...  that doesn't seem to be the case tho.  :\;;;","24/Jul/19 20:16;shaneknapp;ah, got it!

it's research-jenkins-worker-09!

{noformat}
Starting local Kubernetes v1.9.0 cluster...
Starting VM...
{noformat}

we need k8s to be 1.13.3...  this is now fixed.;;;","24/Jul/19 20:38;shaneknapp;the entire k8s config on worker-09 is completely borked.  working on getting that fixed now.;;;","24/Jul/19 21:47;shaneknapp;ok, all fixed and builds are passing on this worker!

https://amplab.cs.berkeley.edu/jenkins/job/testing-k8s-prb-make-spark-distribution-unified/13238/;;;","25/Jul/19 18:13;shaneknapp;checked that worker today and all k8s builds are running successfully:
https://amplab.cs.berkeley.edu/jenkins/job/testing-k8s-prb-make-spark-distribution-unified/13265/console;;;",,,,,,,,,,,,,,,,,,
Error with struct conversion while using pandas_udf,SPARK-28502,13246855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,nasirali,nasirali,24/Jul/19 16:01,12/Dec/22 18:10,13/Jul/23 08:49,18/Dec/19 22:13,2.4.3,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"What I am trying to do: Group data based on time intervals (e.g., 15 days window) and perform some operations on dataframe using (pandas) UDFs. I don't know if there is a better/cleaner way to do it.

Below is the sample code that I tried and error message I am getting.

 
{code:java}
df = sparkSession.createDataFrame([(17.00, ""2018-03-10T15:27:18+00:00""),
                            (13.00, ""2018-03-11T12:27:18+00:00""),
                            (25.00, ""2018-03-12T11:27:18+00:00""),
                            (20.00, ""2018-03-13T15:27:18+00:00""),
                            (17.00, ""2018-03-14T12:27:18+00:00""),
                            (99.00, ""2018-03-15T11:27:18+00:00""),
                            (156.00, ""2018-03-22T11:27:18+00:00""),
                            (17.00, ""2018-03-31T11:27:18+00:00""),
                            (25.00, ""2018-03-15T11:27:18+00:00""),
                            (25.00, ""2018-03-16T11:27:18+00:00"")
                            ],
                           [""id"", ""ts""])
df = df.withColumn('ts', df.ts.cast('timestamp'))

schema = StructType([
    StructField(""id"", IntegerType()),
    StructField(""ts"", TimestampType())
])


@pandas_udf(schema, PandasUDFType.GROUPED_MAP)
def some_udf(df):
    # some computation
    return df

df.groupby('id', F.window(""ts"", ""15 days"")).apply(some_udf).show()
{code}
This throws following exception:
{code:java}
TypeError: Unsupported type in conversion from Arrow: struct<start: timestamp[us, tz=America/Chicago], end: timestamp[us, tz=America/Chicago]>
{code}
 

However, if I use builtin agg method then it works all fine. For example,
{code:java}
df.groupby('id', F.window(""ts"", ""15 days"")).mean().show(truncate=False)
{code}
Output
{code:java}
+-----+------------------------------------------+-------+
|id   |window                                    |avg(id)|
+-----+------------------------------------------+-------+
|13.0 |[2018-03-05 00:00:00, 2018-03-20 00:00:00]|13.0   |
|17.0 |[2018-03-20 00:00:00, 2018-04-04 00:00:00]|17.0   |
|156.0|[2018-03-20 00:00:00, 2018-04-04 00:00:00]|156.0  |
|99.0 |[2018-03-05 00:00:00, 2018-03-20 00:00:00]|99.0   |
|20.0 |[2018-03-05 00:00:00, 2018-03-20 00:00:00]|20.0   |
|17.0 |[2018-03-05 00:00:00, 2018-03-20 00:00:00]|17.0   |
|25.0 |[2018-03-05 00:00:00, 2018-03-20 00:00:00]|25.0   |
+-----+------------------------------------------+-------+
{code}","OS: Ubuntu

Python: 3.6",blakec,bryanc,icexelloss,nasirali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29402,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 01:25:04 UTC 2019,,,,,,,,,,"0|z04zjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jul/19 18:05;bryanc;I'm not sure, but I don't think you can use the same syntax as the normal groupby with window. [~icexelloss] could you check the syntax of the above example?;;;","26/Jul/19 21:09;icexelloss;Hmm.. I think this has sth to do with timezone, can you try setting the {{""spark.sql.session.timeZone"" to ""UTC""?}};;;","27/Jul/19 02:47;nasirali;I tried to set timzone to UTC as suggested by [~icexelloss] but it didn't solve the problem. It is throwing following error (same error but with tz=UTC).

 
{code:java}
TypeError: Unsupported type in conversion from Arrow: struct<start: timestamp[us, tz=UTC], end: timestamp[us, tz=UTC]>
{code}
I think, this is a conversion error, maybe? Below is the complete trace:

 
{code:java}
df.groupby('id', F.window(""ts"", ""15 days"")).apply(some_udf).show()
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 378, in show
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o207.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 5.0 failed 1 times, most recent failure: Lost task 13.0 in stage 5.0 (TID 32, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py"", line 372, in main
process()
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py"", line 367, in process
serializer.dump_stream(func(split_index, iterator), outfile)
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 283, in dump_stream
for series in iterator:
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 301, in load_stream
yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()]
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 301, in <listcomp>
yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()]
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 271, in arrow_to_pandas
s = _check_series_convert_date(s, from_arrow_type(arrow_column.type))
File ""/home/ali/spark/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py"", line 1672, in from_arrow_type
raise TypeError(""Unsupported type in conversion from Arrow: "" + str(at))
TypeError: Unsupported type in conversion from Arrow: struct<start: timestamp[us, tz=UTC], end: timestamp[us, tz=UTC]>
{code};;;","29/Jul/19 20:10;blakec; too am having the same issue and am interested in how I can perform this query (or a similar time window'ed query);;;","21/Sep/19 15:17;nasirali;any update?;;;","24/Sep/19 22:29;bryanc;I was able to reproduce in Spark 2.4.3. The problem was introducing a window in a PandasUDFType.GROUPED_MAP adds an Arrow column that is a StructType to the UDF. From the above example it is:

{noformat}
pyarrow.Table
window: struct<start: timestamp[us, tz=America/Los_Angeles], end: timestamp[us, tz=America/Los_Angeles]> not null
  child 0, start: timestamp[us, tz=America/Los_Angeles]
  child 1, end: timestamp[us, tz=America/Los_Angeles]
id: double
ts: timestamp[us, tz=America/Los_Angeles]
{noformat}

 And Spark 2.4.3 is not able to handle this column.

In current master, it seems to work and the column is treated as part of the key.  The example above will just ignore it by default, but you can get it as part of th e UDF input like so:

{code}
@pandas_udf(schema, PandasUDFType.GROUPED_MAP)
def some_udf(key, df):
    print(""Key: {}"".format(key))
    print(""DF:\n{}"".format(df))
    # some computation
    return df
{code}

Would print out for ts 13.0:

{noformat}
Key: (13.0, {'start': datetime.datetime(2018, 3, 5, 0, 0), 'end': datetime.datetime(2018, 3, 20, 0, 0)})
DF:
     id                  ts
0  13.0 2018-03-11 05:27:18
{noformat}

Someone should verify this is the correct way to handle it and the result is right. Can you do this [~blakec] or [~nasirali] ? If so, we can close this but should follow up and add proper testing.;;;","09/Oct/19 00:29;bryanc;I'm closing this since it is working in master and will mark fixed version as 3.0.0. I also created SPARK-29402 to add proper testing for this use case.;;;","09/Oct/19 00:30;bryanc;This was fixed once support for StructType was added in pandas_udf because the window range sent to the udf is a struct column;;;","10/Oct/19 18:06;nasirali;[~bryanc] I tested it and it works fine with master branch. Is there any expected release date for version 3? Or could this bug be fix be integrated in next release?;;;","10/Oct/19 22:35;bryanc;Thanks for testing it out [~nasirali]! It's unlikely that this would make it to the 2.4.x line, but the next Spark release will be 3.0.0 anyway, which will roughly be early 2020.;;;","06/Nov/19 00:19;bryanc;That's strange, I added your example as a unit test in SPARK-29402. I'll try to take a look at what's going on with the 3.0 preview.;;;","06/Nov/19 00:36;nasirali;[~bryanc] Sorry I had to remove my previous comment as I was in the middle of debugging this issue. I found the culprit package. My code works all fine with pyarrow==0.14.1. However, with the latest pyarrow release (0.15.1), my example code throws following exception. Please let me know if you need more information.

 
{code:java}
// code placeholder
19/11/05 18:23:17 ERROR Executor: Exception in task 13.0 in stage 5.0 (TID 13)
java.lang.IllegalArgumentException
 at java.nio.ByteBuffer.allocate(ByteBuffer.java:334)
 at org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:547)
 at org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)
 at org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)
 at org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:178)
 at org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:169)
 at org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:62)
 at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:89)
 at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
 at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)
 at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)
 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
 at org.apache.spark.scheduler.Task.run(Task.scala:127)
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
19/11/05 18:23:17 ERROR TaskSetManager: Task 13 in stage 5.0 failed 1 times; aborting job
{code};;;","06/Nov/19 20:52;nasirali;[~bryanc] If I perform any agg (e.g.,  _df.groupBy(""id"",F.window(""ts"", ""15 days"")).agg(\{""id"":""avg""}).show()_ ) on grouped data, pyspark returns me the key (e.g., id, window) with the avg for each group. However, in the above example, when udf returns the struct, it does not automatically return the key. I have to manually add window to returning dataframe. Is there a way to automatically concatenate results of udf?;;;","06/Nov/19 21:42;bryanc;Ahh, so Arrow 0.15.0+ had a change in the IPC format that requires pyspark to set an env var. See [https://github.com/apache/spark/blob/master/docs/sql-pyspark-pandas-with-arrow.md#compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x,] that should fix the problem with the Spark preview and once SPARK-29376 is merged in 3.0, you won't need to do this.

{quote} I have to manually add window to returning dataframe. Is there a way to automatically concatenate results of udf?  {quote}

I don't believe there is a way to add the key/window in the DataFrame automatically, you will have to manually add it in the udf.;;;","13/Dec/19 18:45;nasirali;{code:java}
import numpy as np
import pandas as pd
import json
from geopy.distance import great_circle
from pyspark.sql.functions import pandas_udf, PandasUDFType
from shapely.geometry.multipoint import MultiPoint
from sklearn.cluster import DBSCAN
from pyspark.sql.types import StructField, StructType, StringType, FloatType, MapType
from pyspark.sql.types import StructField, StructType, StringType, FloatType, TimestampType, IntegerType,DateType,TimestampTypeschema = StructType([
   StructField(""timestamp"", TimestampType()),
   StructField(""window"", StructType([
   StructField(""start"", TimestampType()),
   StructField(""end"", TimestampType())])),
   StructField(""some_val"", StringType())
   ])@pandas_udf(schema, PandasUDFType.GROUPED_MAP)
def get_win_col(key, user_data):
    all_vals = []
    for index, row in user_data.iterrows():
        all_vals.append([row[""timestamp""],key[2],""tesss""])
        
    return pd.DataFrame(all_vals,columns=['timestamp','window','some_val'])
{code}
I am not even able to manually return window column. It throws error
{code:java}
Traceback (most recent call last):
  File ""/usr/local/spark-3.0.0-preview/python/pyspark/sql/udf.py"", line 139, in returnType
    to_arrow_type(self._returnType_placeholder)
  File ""/usr/local/spark-3.0.0-preview/python/pyspark/sql/types.py"", line 1641, in to_arrow_type
    raise TypeError(""Nested StructType not supported in conversion to Arrow"")
TypeError: Nested StructType not supported in conversion to ArrowDuring handling of the above exception, another exception occurred:Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/spark-3.0.0-preview/python/pyspark/sql/udf.py"", line 79, in _create_udf
    return udf_obj._wrapped()
  File ""/usr/local/spark-3.0.0-preview/python/pyspark/sql/udf.py"", line 234, in _wrapped
    wrapper.returnType = self.returnType
  File ""/usr/local/spark-3.0.0-preview/python/pyspark/sql/udf.py"", line 143, in returnType
    ""%s is not supported"" % str(self._returnType_placeholder))
NotImplementedError: Invalid returnType with grouped map Pandas UDFs: StructType(List(StructField(timestamp,TimestampType,true),StructField(window,StructType(List(StructField(start,TimestampType,true),StructField(end,TimestampType,true))),true),StructField(some_val,StringType,true))) is not supported
{code}
However, if I manually run *to_arrow_schema(schema)*. It works all fine and there is no exception. [https://github.com/apache/spark/blob/master/python/pyspark/sql/udf.py#L139]
{code:java}
from pyspark.sql.types import to_arrow_schema
to_arrow_schema(schema)
{code};;;","18/Dec/19 22:13;bryanc;The problem is that returning nested StructTypes is not currently supported, so you will need to flatten the fields of the window separate columns. Since adding support for that is a new feature and not a bug, I'll close this.;;;","19/Dec/19 01:25;gurwls223;+1 for [~bryanc]'s  ;;;",,,,,,,
Disallow upcasting complex data types to string type,SPARK-28497,13246749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,24/Jul/19 08:06,12/Dec/22 18:10,13/Jul/23 08:49,25/Jul/19 11:55,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"In the current implementation. complex types like Array/Map/StructType are allowed to upcast as StringType.
This is not safe casting. We should disallow it.",,dongjoon,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 11:55:41 UTC 2019,,,,,,,,,,"0|z04yw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jul/19 02:49;dongjoon;Is this issue targeting `master` only, [~Gengliang.Wang]?;;;","25/Jul/19 11:55;gurwls223;Issue resolved by pull request 25242
[https://github.com/apache/spark/pull/25242];;;",,,,,,,,,,,,,,,,,,,,,,
KafkaOffsetRangeCalculator.getRanges may drop offsets,SPARK-28489,13246695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,24/Jul/19 00:09,25/Jun/20 18:22,13/Jul/23 08:49,26/Jul/19 07:12,2.4.0,2.4.1,2.4.2,2.4.3,,,,,,,2.4.4,3.0.0,,Structured Streaming,,,,,0,correctness,dataloss,,"KafkaOffsetRangeCalculator.getRanges may drop offsets due to round off errors.

 

This only affects queries using ""minPartitions"" option. A workaround is just removing the ""minPartitions"" option from the query.",,dongjoon,klaustelenius,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23541,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 18:22:28 UTC 2020,,,,,,,,,,"0|z04yk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jul/19 07:12;dongjoon;This is resolved via https://github.com/apache/spark/pull/25237;;;","25/Jun/20 10:23;klaustelenius;Does this affect non-structured streaming?;;;","25/Jun/20 18:22;zsxwing;[~klaustelenius] No. The ""minPartitions"" option for batch queries is added in Spark 3.0. Batch queries before Spark 3.0 will ignore the ""minPartitions"" option.;;;",,,,,,,,,,,,,,,,,,,,,
PythonBroadcast may delete the broadcast file while a Python worker still needs it,SPARK-28486,13246669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,zsxwing,zsxwing,23/Jul/19 20:56,12/Dec/22 18:10,13/Jul/23 08:49,05/Aug/19 11:19,2.4.3,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,"Steps to reproduce:
 * Run ""bin/pyspark --master local[1,1] --conf spark.memory.fraction=0.0001"" to start PySpark
 * Run the following codes:

{code:java}
b = sc.broadcast([100])
sc.parallelize([0],1).map(lambda x: 0 if x == 0 else b.value[0]).collect()
sc._jvm.java.lang.System.gc()
import time
time.sleep(5)
sc._jvm.java.lang.System.gc()
time.sleep(5)
sc.parallelize([1],1).map(lambda x: 0 if x == 0 else b.value[0]).collect()
{code}
* Error:

{code}
IOError: [Errno 2] No such file or directory: u'.../spark-ee2a0da1-7d2e-48fd-be9a-fdcc89c5076c/broadcast4970491472715621982'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
{code}",,Ngone51,smilegator,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 11:19:49 UTC 2019,,,,,,,,,,"0|z04yeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Jul/19 21:02;smilegator;[~Ngone51] Please submit a PR to address this issue. Thanks!;;;","05/Aug/19 11:19;gurwls223;Issue resolved by pull request 25262
[https://github.com/apache/spark/pull/25262];;;",,,,,,,,,,,,,,,,,,,,,,
Canceling a spark job using barrier mode but barrier tasks do not exit,SPARK-28483,13246569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,23/Jul/19 13:37,17/Sep/19 13:41,13/Jul/23 08:49,20/Aug/19 06:26,2.4.3,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,"Reproduce code:
{code:java}
import time
from pyspark import BarrierTaskContext

n = 4

def  task(x):
  context = BarrierTaskContext.get()
  this = next(x)
  if (this % 2 == 0):
    time.sleep(10000)
  context.barrier()
  return []

sc.setLogLevel(""INFO"")
sc.parallelize(list(range(n)), n).barrier().mapPartitions(task).collect(){code}


Run above code in pyspark shell and then print Ctrl + C to exit the job.

Get logging like:

{code}
19/02/05 01:07:42 INFO BarrierTaskContext: Task 3 from Stage 0(Attempt 0) has entered the global sync, current barrier epoch is 0.
19/02/05 01:07:42 INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) has entered the global sync, current barrier epoch is 0.
19/02/05 01:07:47 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2), reason: Stage cancelled
19/02/05 01:07:47 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled
19/02/05 01:07:47 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled
19/02/05 01:07:47 INFO Executor: Executor is trying to kill task 3.0 in stage 0.0 (TID 3), reason: Stage cancelled
19/02/05 01:07:50 WARN PythonRunner: Incomplete task 0.0 in stage 0 (TID 0) interrupted: Attempting to kill Python Worker
19/02/05 01:07:50 INFO Executor: Executor killed task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled
19/02/05 01:07:50 WARN PythonRunner: Incomplete task 3.3 in stage 0 (TID 3) interrupted: Attempting to kill Python Worker
19/02/05 01:07:50 INFO Executor: Executor killed task 3.0 in stage 0.0 (TID 3), reason: Stage cancelled
19/02/05 01:07:50 WARN PythonRunner: Incomplete task 2.2 in stage 0 (TID 2) interrupted: Attempting to kill Python Worker
19/02/05 01:07:50 INFO Executor: Executor killed task 2.0 in stage 0.0 (TID 2), reason: Stage cancelled
19/02/05 01:07:50 WARN PythonRunner: Incomplete task 1.1 in stage 0 (TID 1) interrupted: Attempting to kill Python Worker
19/02/05 01:07:50 INFO Executor: Executor killed task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled
19/02/05 01:08:42 INFO BarrierTaskContext: Task 3 from Stage 0(Attempt 0) waiting under the global sync since 1549328862443, has been waiting for 60 seconds, current barrier epoch is 0.
19/02/05 01:08:42 INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) waiting under the global sync since 1549328862522, has been waiting for 60 seconds, current barrier epoch is 0.
19/02/05 01:09:42 INFO BarrierTaskContext: Task 3 from Stage 0(Attempt 0) waiting under the global sync since 1549328862443, has been waiting for 120 seconds, current barrier epoch is 0.
19/02/05 01:09:42 INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) waiting under the global sync since 1549328862522, has been waiting for 120 seconds, current barrier epoch is 0.
19/02/05 01:10:42 INFO BarrierTaskContext: Task 3 from Stage 0(Attempt 0) waiting under the global sync since 1549328862443, has been waiting for 180 seconds, current barrier epoch is 0.
19/02/05 01:10:42 INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) waiting under the global sync since 1549328862522, has been waiting for 180 seconds, current barrier epoch is 0.
19/02/05 01:11:42 INFO BarrierTaskContext: Task 3 from Stage 0(Attempt 0) waiting under the global sync since 1549328862443, has been waiting for 240 seconds, current barrier epoch is 0.
19/02/05 01:11:42 INFO BarrierTaskContext: Task 1 from Stage 0(Attempt 0) waiting under the global sync since 1549328862522, has been waiting for 240 seconds, current barrier epoch is 0.
{code}
",,cloud_fan,holden,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 20 06:26:14 UTC 2019,,,,,,,,,,"0|z04xsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Aug/19 06:26;cloud_fan;Issue resolved by pull request 25235
[https://github.com/apache/spark/pull/25235];;;",,,,,,,,,,,,,,,,,,,,,,,
Lower JDBC client cannot read binary type,SPARK-28474,13246350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,22/Jul/19 15:15,12/Dec/22 18:10,13/Jul/23 08:49,08/Aug/19 08:04,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Logs:
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: [B incompatible with java.lang.String
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:83)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
	at java.security.AccessController.doPrivileged(AccessController.java:770)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
	at com.sun.proxy.$Proxy26.fetchResults(Unknown Source)
	at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:455)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:621)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:53)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:819)
Caused by: java.lang.ClassCastException: [B incompatible with java.lang.String
	at org.apache.hive.service.cli.ColumnValue.toTColumnValue(ColumnValue.java:198)
	at org.apache.hive.service.cli.RowBasedSet.addRow(RowBasedSet.java:60)
	at org.apache.hive.service.cli.RowBasedSet.addRow(RowBasedSet.java:32)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getNextRowSet(SparkExecuteStatementOperation.scala:148)
	at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:220)
	at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:785)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
	... 18 more
{noformat}
",,huaxingao,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 08:04:35 UTC 2019,,,,,,,,,,"0|z04wfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Aug/19 08:04;gurwls223;Issue resolved by pull request 25379
[https://github.com/apache/spark/pull/25379];;;",,,,,,,,,,,,,,,,,,,,,,,
Honor spark.sql.decimalOperations.nullOnOverflow in Cast,SPARK-28470,13246249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,cloud_fan,cloud_fan,22/Jul/19 07:41,08/Aug/19 02:43,13/Jul/23 08:49,07/Aug/19 23:12,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"cast long to decimal or decimal to decimal can overflow, we should respect the new config if overflow happens.",,cloud_fan,maropu,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 07 23:12:59 UTC 2019,,,,,,,,,,"0|z04vtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jul/19 07:41;cloud_fan;cc [~mgaido];;;","22/Jul/19 07:51;mgaido;Thanks for checking this Wenchen! I will work on this ASAP. Thanks.;;;","07/Aug/19 23:12;maropu;Resolved by [https://github.com/apache/spark/pull/25253];;;",,,,,,,,,,,,,,,,,,,,,
Upgrade pip to fix `sphinx` install error,SPARK-28468,13246232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,22/Jul/19 05:58,22/Jul/19 17:46,13/Jul/23 08:49,22/Jul/19 17:46,2.4.4,,,,,,,,,,2.4.4,,,Project Infra,,,,,0,,,,"`do-release-docker.sh` fails at `sphinx` installation to `Python 2.7`.
{code}
$ dev/create-release/do-release-docker.sh -d /tmp/spark-2.4.4 -n
{code}

The following is the same reproducible step.
{code}
$ docker build -t spark-rm-test2 --build-arg UID=501 dev/create-release/spark-rm
{code}

This happens in `branch-2.4` only.
{code}
root@4e196b3d7611:/# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 16.04.6 LTS
Release:        16.04
Codename:       xenial

root@4e196b3d7611:/# pip install sphinx
Collecting sphinx
  Downloading https://files.pythonhosted.org/packages/89/1e/64c77163706556b647f99d67b42fced9d39ae6b1b86673965a2cd28037b5/Sphinx-2.1.2.tar.gz (6.3MB)
    100% |################################| 6.3MB 316kB/s
    Complete output from command python setup.py egg_info:
    ERROR: Sphinx requires at least Python 3.5 to run.

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-7usNN9/sphinx/
You are using pip version 8.1.1, however version 19.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 17:46:25 UTC 2019,,,,,,,,,,"0|z04vpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jul/19 17:46;dongjoon;This is resolved via https://github.com/apache/spark/pull/25226;;;",,,,,,,,,,,,,,,,,,,,,,,
K8s integration tests fail due to missing ceph-nano image,SPARK-28465,13246175,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,skonto,skonto,21/Jul/19 16:27,17/May/20 18:23,13/Jul/23 08:49,24/Jul/19 21:58,3.0.0,,,,,,,,,,3.0.0,,,Kubernetes,Spark Core,,,,0,,,,"Image added here: [https://github.com/lightbend/spark/blob/72c80ee81ca4c3c9569749b54e2db0ec91b128a5/resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DepsTestsSuite.scala#L66] needs to be updated to the latest as it was removed from dockerhub.
{quote}docker pull ceph/daemon:v4.0.0-stable-4.0-master-centos-7-x86_64
 Error response from daemon: manifest for ceph/daemon:v4.0.0-stable-4.0-master-centos-7-x86_64 not found
{quote}
Also we need to apply this fix: [https://github.com/ceph/cn/issues/115#issuecomment-497384369]

I will create a PR shortly.",,skonto,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30738,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 21:58:42 UTC 2019,,,,,,,,,,"0|z04vcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jul/19 21:58;vanzin;Issue resolved by pull request 25222
[https://github.com/apache/spark/pull/25222];;;",,,,,,,,,,,,,,,,,,,,,,,
curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: ,SPARK-28457,13246109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,shaneknapp,smilegator,smilegator,20/Jul/19 06:44,22/Jan/20 22:03,13/Jul/23 08:49,22/Jul/19 15:56,3.0.0,,,,,,,,,,3.0.0,,,Project Infra,,,,,0,,,," 

Build broke since this afternoon.

[spark-master-compile-maven-hadoop-2.7 #10224 (broken since this build)|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-compile-maven-hadoop-2.7/10224/]
 [spark-master-compile-maven-hadoop-3.2 #171 (broken since this build)|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-compile-maven-hadoop-3.2/171/]
 [spark-master-lint #10599 (broken since this build)|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-lint/10599/]
  
{code:java}

  
 https://www.apache.org/dyn/closer.lua?action=download&filename=/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz
 curl: (60) SSL certificate problem: unable to get local issuer certificate
 More details here: 
 https://curl.haxx.se/docs/sslcerts.html
 curl performs SSL certificate verification by default, using a ""bundle""
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
 If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
 If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.
gzip: stdin: unexpected end of file
 tar: Child returned status 1
 tar: Error is not recoverable: exiting now
 Using `mvn` from path: /home/jenkins/workspace/spark-master-compile-maven-hadoop-2.7/build/apache-maven-3.6.1/bin/mvn
 build/mvn: line 163: /home/jenkins/workspace/spark-master-compile-maven-hadoop-2.7/build/apache-maven-3.6.1/bin/mvn: No such file or directory
 Build step 'Execute shell' marked build as failure
 Finished: FAILURE
{code}
 ",,heuermh,shaneknapp,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 16:00:31 UTC 2019,,,,,,,,,,"0|z04uy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Jul/19 06:46;smilegator;[~shaneknapp], Could you help take a look at this?;;;","20/Jul/19 19:11;heuermh;We're also seeing this in [our CI builds|https://amplab.cs.berkeley.edu/jenkins/job/ADAM/HADOOP_VERSION=2.7.5,SCALAVER=2.12,SPARK_VERSION=2.4.3,label=ubuntu/4351/console] on older Spark versions

{{{color:#222222}+ curl -L '{color}[https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop-scala-2.12.tgz]{color:#222222}' -o spark-2.4.3-bin-without-hadoop{color}{color:#222222}-scala-2.12.tgz{color}}}
{{{color:#222222}curl: (60) SSL certificate problem: unable to get local issuer 
{color}}};;;","22/Jul/19 15:25;shaneknapp;looking in to it now.;;;","22/Jul/19 15:46;shaneknapp;ok, curl was unhappy w/the old the cacert.pem, so i updated to the latest from [https://curl.haxx.se/ca/cacert.pem] and things look to be better, tho the lint job is failing.

once i get that sorted i will mark this as resolved.;;;","22/Jul/19 15:55;shaneknapp;ok, the error i'm seeing in the lint job is most definitely not related to the SSL certs:

[https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-lint/10613/console]
{noformat}
starting python compilation test...
python compilation succeeded.

downloading pycodestyle from https://raw.githubusercontent.com/PyCQA/pycodestyle/2.4.0/pycodestyle.py...
starting pycodestyle test...
pycodestyle checks failed:
  File ""/home/jenkins/workspace/spark-master-lint/dev/pycodestyle-2.4.0.py"", line 1
    500: Internal Server Error
       ^
SyntaxError: invalid syntax{noformat}

i went to PyCQA's repo on github and i'm seeing a LOT of 500 errors.  this is out of scope of this ticket, and actually not a localized (to our jenkins) issue, so i will notify dev@ and mark this as resolved.;;;","22/Jul/19 16:00;smilegator;[~shaneknapp] Thanks for fixing it! ;;;",,,,,,,,,,,,,,,,,,
Executor may be timed out too soon because of overflow in tracking code,SPARK-28455,13246089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,19/Jul/19 22:19,22/Jul/19 21:36,13/Jul/23 08:49,22/Jul/19 21:32,3.0.0,,,,,,,,,,3.0.0,,,Spark Core,,,,,0,,,,This affects the new code added in SPARK-27963 (so normal dynamic allocation is fine). There's an overflow issue in that code that may cause executors to be timed out early with the default configuration.,,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 21:32:18 UTC 2019,,,,,,,,,,"0|z04uts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jul/19 21:32;dongjoon;Issue resolved by pull request 25208
[https://github.com/apache/spark/pull/25208];;;",,,,,,,,,,,,,,,,,,,,,,,
Validate LongType in _make_type_verifier,SPARK-28454,13246087,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,simplylizz,simplylizz,simplylizz,19/Jul/19 22:06,12/Dec/22 18:10,13/Jul/23 08:49,08/Aug/19 02:49,2.3.3,,,,,,,,,,3.0.0,,,PySpark,,,,,0,,,,{{pyspark.sql.types._make_type_verifier doesn't validate LongType values range.}},,simplylizz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 02:49:50 UTC 2019,,,,,,,,,,"0|z04utc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Jul/19 22:11;simplylizz;[https://github.com/apache/spark/pull/25117] - related PR.;;;","08/Aug/19 02:49;gurwls223;Issue resolved by pull request 25117
[https://github.com/apache/spark/pull/25117];;;",,,,,,,,,,,,,,,,,,,,,,
Inconsistency between Scala and Python/Panda udfs when groupby with udf() is used,SPARK-28445,13245976,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,skonto,skonto,19/Jul/19 11:51,12/Dec/22 18:10,13/Jul/23 08:49,02/Aug/19 10:48,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"Python:

{code}
from pyspark.sql.functions import pandas_udf, PandasUDFType

@pandas_udf(""int"", PandasUDFType.SCALAR)
def noop(x):
 return x

spark.udf.register(""udf"", noop)

sql(""""""
 CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES
 (1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2), (null, 1), (3, null), (null, null)
 AS testData(a, b)"""""")

sql(""""""SELECT udf(a + 1), udf(COUNT(b)) FROM testData GROUP BY udf(a + 1)"""""").show()
{code}

{code}
: org.apache.spark.sql.AnalysisException: expression 'testdata.`a`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
Aggregate [udf((a#0 + 1))], [udf((a#0 + 1)) AS udf((a + 1))#10, udf(count(b#1)) AS udf(count(b))#12]
+- SubqueryAlias `testdata`
 +- Project [a#0, b#1]
 +- SubqueryAlias `testData`
 +- LocalRelation [a#0, b#1]
{code}


Scala:

{code}
spark.udf.register(""udf"", (input: Int) => input)

sql(""""""
 CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES
 (1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2), (null, 1), (3, null), (null, null)
 AS testData(a, b)"""""")

sql(""""""SELECT udf(a + 1), udf(COUNT(b)) FROM testData GROUP BY udf(a + 1)"""""").show()
{code}

{code}
+------------+-------------+
|udf((a + 1))|udf(count(b))|
+------------+-------------+
|        null|            1|
|           3|            2|
|           4|            2|
|           2|            2|
+------------+-------------+
{code}
",,skonto,Udbhav Agrawal,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28280,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 10:48:43 UTC 2019,,,,,,,,,,"0|z04u4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Aug/19 10:48;gurwls223;Issue resolved by pull request 25215
[https://github.com/apache/spark/pull/25215];;;",,,,,,,,,,,,,,,,,,,,,,,
PythonUDF used in correlated scalar subquery causes UnsupportedOperationException ,SPARK-28441,13245899,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,huaxingao,huaxingao,19/Jul/19 00:35,28/Jul/19 22:51,13/Jul/23 08:49,27/Jul/19 02:39,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"I found this when doing https://issues.apache.org/jira/browse/SPARK-28277

 
{code:java}
>>> @pandas_udf(""string"", PandasUDFType.SCALAR)
... def noop(x):
...     return x.apply(str)
... 
>>> spark.udf.register(""udf"", noop)
<function noop at 0x111b5f9d8>
>>> spark.sql(""CREATE OR REPLACE TEMPORARY VIEW t1 as select * from values (\""one\"", 1), (\""two\"", 2),(\""three\"", 3),(\""one\"", NULL) as t1(k, v)"")
DataFrame[]
>>> spark.sql(""CREATE OR REPLACE TEMPORARY VIEW t2 as select * from values (\""one\"", 1), (\""two\"", 22),(\""one\"", 5),(\""one\"", NULL), (NULL, 5) as t2(k, v)"")
DataFrame[]
>>> spark.sql(""SELECT t1.k FROM t1 WHERE  t1.v <= (SELECT   udf(max(udf(t2.v))) FROM     t2 WHERE    udf(t2.k) = udf(t1.k))"").show()
py4j.protocol.Py4JJavaError: An error occurred while calling o65.showString.
: java.lang.UnsupportedOperationException: Cannot evaluate expression: udf(null)
 at org.apache.spark.sql.catalyst.expressions.Unevaluable.eval(Expression.scala:296)
 at org.apache.spark.sql.catalyst.expressions.Unevaluable.eval$(Expression.scala:295)
 at org.apache.spark.sql.catalyst.expressions.PythonUDF.eval(PythonUDF.scala:52)
{code}
 

 

 ",,cloud_fan,huaxingao,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28277,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 27 02:39:43 UTC 2019,,,,,,,,,,"0|z04tnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Jul/19 02:39;cloud_fan;Issue resolved by pull request 25204
[https://github.com/apache/spark/pull/25204];;;",,,,,,,,,,,,,,,,,,,,,,,
pyspark.sql.functions.array_repeat should support Column as count argument,SPARK-28439,13245759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,18/Jul/19 11:42,17/Apr/20 17:36,13/Jul/23 08:49,18/Jul/19 19:59,2.4.0,3.0.0,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"In Scala, Spark supports (https://github.com/apache/spark/blob/c3e32bf06c35ba2580d46150923abfa795b4446a/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L3777)

 
{code:java}
(Column, Column) => Column
{code}
variant of array_repeat, however PySpark doesn't
{code:java}
>>> import pyspark                                               
>>> from pyspark.sql import functions as f
>>> pyspark.__version__
'3.0.0.dev0'
 
>>> f.array_repeat(f.col(""foo""), f.col(""bar""))
...
TypeError: Column is not iterable


{code}
 

 ",,dongjoon,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 19:59:15 UTC 2019,,,,,,,,,,"0|z04ssg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Jul/19 19:59;dongjoon;Issue resolved by pull request 25193
[https://github.com/apache/spark/pull/25193];;;",,,,,,,,,,,,,,,,,,,,,,,
Some stage table rows render wrong number of columns if tasks are missing metrics ,SPARK-28430,13245640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,18/Jul/19 00:03,17/Dec/19 01:51,13/Jul/23 08:49,18/Jul/19 20:16,2.4.0,3.0.0,,,,,,,,,2.3.4,2.4.4,3.0.0,Web UI,,,,,0,,,,"The Spark UI's stages table renders too few columns for some tasks if a subset of the tasks are missing their metrics. This is due to an inconsistency in how we render certain columns: some columns gracefully handle this case, but others do not. See attached screenshot below

 !ui-screenshot.png! ",,dongjoon,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30233,,,,,SPARK-20657,,,,,,,,,,,,,,,"18/Jul/19 00:04;joshrosen;ui-screenshot.png;https://issues.apache.org/jira/secure/attachment/12975110/ui-screenshot.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 20:16:43 UTC 2019,,,,,,,,,,"0|z04s2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Jul/19 20:16;dongjoon;Issue resolved by pull request 25183
[https://github.com/apache/spark/pull/25183];;;",,,,,,,,,,,,,,,,,,,,,,,
GROUPED_AGG pandas_udf doesn't with spark.sql() without group by clause,SPARK-28422,13245532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,icexelloss,icexelloss,17/Jul/19 14:24,12/Dec/22 18:11,13/Jul/23 08:49,13/Aug/19 15:37,2.4.0,2.4.1,2.4.2,2.4.3,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,," 
{code:python}
from pyspark.sql.functions import pandas_udf, PandasUDFType
@pandas_udf('double', PandasUDFType.GROUPED_AGG)
def max_udf(v):
    return v.max()

df = spark.range(0, 100)
spark.udf.register('max_udf', max_udf)
df.createTempView('table')

# A. This works
df.agg(max_udf(df['id'])).show()

# B. This doesn't work
spark.sql(""select max_udf(id) from table"").show(){code}
 

 

Query plan:

A:
{code:java}
== Parsed Logical Plan ==

'Aggregate [max_udf('id) AS max_udf(id)#140]

+- Range (0, 1000, step=1, splits=Some(4))




== Analyzed Logical Plan ==

max_udf(id): double

Aggregate [max_udf(id#64L) AS max_udf(id)#140]

+- Range (0, 1000, step=1, splits=Some(4))




== Optimized Logical Plan ==

Aggregate [max_udf(id#64L) AS max_udf(id)#140]

+- Range (0, 1000, step=1, splits=Some(4))




== Physical Plan ==

!AggregateInPandas [max_udf(id#64L)], [max_udf(id)#138 AS max_udf(id)#140]

+- Exchange SinglePartition

   +- *(1) Range (0, 1000, step=1, splits=4)
{code}
B:
{code:java}
== Parsed Logical Plan ==

'Project [unresolvedalias('max_udf('id), None)]

+- 'UnresolvedRelation [table]




== Analyzed Logical Plan ==

max_udf(id): double

Project [max_udf(id#0L) AS max_udf(id)#136]

+- SubqueryAlias `table`

   +- Range (0, 100, step=1, splits=Some(4))




== Optimized Logical Plan ==

Project [max_udf(id#0L) AS max_udf(id)#136]

+- Range (0, 100, step=1, splits=Some(4))




== Physical Plan ==

*(1) Project [max_udf(id#0L) AS max_udf(id)#136]

+- *(1) Range (0, 100, step=1, splits=4)
{code}
 ",,dongjoon,icexelloss,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 13 15:37:14 UTC 2019,,,,,,,,,,"0|z04reg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Aug/19 05:45;dongjoon;Thank you for reporting, [~icexelloss]. And, thank you for making a PR, [~viirya]. Since this is not supported from 2.4.0, I updated the affected versions, too.;;;","05/Aug/19 05:56;viirya;Thanks [~dongjoon]!
;;;","13/Aug/19 15:37;gurwls223;Issue resolved by pull request 25352
[https://github.com/apache/spark/pull/25352];;;",,,,,,,,,,,,,,,,,,,,,
sizeInByte is Not updated for parquet datasource on Next Insert.,SPARK-28413,13245248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,Bjangir,Bjangir,16/Jul/19 16:02,17/May/20 18:57,13/Jul/23 08:49,03/Feb/20 10:28,2.3.2,2.4.1,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"In  SPARK-21237 (link SPARK-21237)  it is fix when Appending data using  write.mode(""append"") . But when create same type of parquet table using SQL and  Insert data ,stats shows in-correct (not updated).

*+Correct Stats  Example (SPARK-21237)+*

scala> spark.range(100).write.saveAsTable(""tab1"")

scala> spark.sql(""explain cost select * from tab1"").show(false)
 +------------------------------------------------------------------------
|plan
 +------------------------------------------------------------------------|
|== Optimized Logical Plan ==
 Relation[id#10L|#10L] parquet, Statistics(*sizeInBytes=784.0 B*, hints=none)|

== Physical Plan ==
 FileScan parquet default.tab1[id#10L|#10L] Batched: false, Format: Parquet, 

scala> spark.range(100).write.mode(""append"").saveAsTable(""tab1"")

scala> spark.sql(""explain cost select * from tab1"").show(false)
 +----------------------------------------------------------------------
|plan
 +----------------------------------------------------------------------|
|== Optimized Logical Plan ==
 Relation[id#23L|#23L] parquet, Statistics(*sizeInBytes=1568.0 B*, hints=none)|

== Physical Plan ==
 FileScan parquet default.tab1[id#23L|#23L] Batched: false, Format: Parquet,

 

 

+*Incorrect Stats Example*+

scala> spark.sql(""create table tab2(id bigint) using parquet"")
 res6: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""explain cost select * from tab2"").show(false)
 +----------------------------------------------------------------------
|plan
 +----------------------------------------------------------------------|
|== Optimized Logical Plan ==
 Relation[id#30L|#30L] parquet, Statistics(*sizeInBytes=374.0 B,* hints=none)|

== Physical Plan ==
 FileScan parquet default.tab2[id#30L|#30L] Batched: false, Format: Parquet,

 

scala> spark.sql(""insert into tab2 select 1"")
 res9: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""explain cost select * from tab2"").show(false)
 +----------------------------------------------------------------------
|plan
 +----------------------------------------------------------------------|
|== Optimized Logical Plan ==
 Relation[id#30L|#30L] parquet, Statistics(*sizeInBytes={color:#ff0000}374.0 B{color}*, hints=none)|

== Physical Plan ==
 FileScan parquet default.tab2[id#30L|#30L] Batched: false, Format: Parquet,

 

 

Both table are same type of table

scala> spark.sql(""desc formatted tab1"").show(2000,false)
 +-----------------------------+-------------------------------------------------------------+
|col_name|data_type|

+-----------------------------+-------------------------------------------------------------+
|id|bigint|
| | |
| # Detailed Table Information| |
|Database|default|
|Table|tab1|
|Owner|Administrator|
|Created Time|Tue Jul 16 21:08:35 IST 2019|
|Last Access|Thu Jan 01 05:30:00 IST 1970|
|Created By|Spark 2.3.2|
|Type|MANAGED|
|Provider|parquet|
|Table Properties|[transient_lastDdlTime=1563291579]|
|Statistics|1568 bytes|
|Location|file:/x/2|
|Serde Library|org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe|
|InputFormat|org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat|
|OutputFormat|org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|

 

scala> spark.sql(""desc formatted tab2"").show(2000,false)
 +-----------------------------+-------------------------------------------------------------
|col_name|data_type
 +-----------------------------+-------------------------------------------------------------|
|id|bigint|
| |
| # Detailed Table Information|
|Database|default|
|Table|tab2|
|Owner|Administrator|
|Created Time|Tue Jul 16 21:10:24 IST 2019|
|Last Access|Thu Jan 01 05:30:00 IST 1970|
|Created By|Spark 2.3.2|
|Type|MANAGED|
|Provider|parquet|
|Table Properties|[transient_lastDdlTime=1563291624]|
|Location|file:/x/1|
|Serde Library|org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe|
|InputFormat|org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat|
|OutputFormat|org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|",,Bjangir,huaxingao,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19784,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 03 10:28:56 UTC 2020,,,,,,,,,,"0|z04q3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Aug/19 15:16;yumwang;Could you test this patch? [https://github.com/apache/spark/pull/22721];;;","03/Feb/20 10:28;yumwang;The issue fixed by https://github.com/apache/spark/commit/17881a467a1ac4224a50247458107f8b141850d2:
{noformat}

scala> spark.sql(""create table tab2(id bigint) using parquet"")
res4: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""explain cost select * from tab2"").show(false)
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|plan                                                                                                                                                                                                                                                                                                                                                                                                           |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|== Optimized Logical Plan ==
Relation[id#37L] parquet, Statistics(sizeInBytes=0.0 B)

== Physical Plan ==
*(1) ColumnarToRow
+- FileScan parquet default.tab2[id#37L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-27176/spark-3.0.0-SNAPSHOT-bin-2.7.4/spark-ware..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>

|
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


scala> spark.sql(""insert into tab2 select 1"")
res6: org.apache.spark.sql.DataFrame = []

scala>  spark.sql(""explain cost select * from tab2"").show(false)
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|plan                                                                                                                                                                                                                                                                                                                                                                                                             |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|== Optimized Logical Plan ==
Relation[id#51L] parquet, Statistics(sizeInBytes=457.0 B)

== Physical Plan ==
*(1) ColumnarToRow
+- FileScan parquet default.tab2[id#51L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-27176/spark-3.0.0-SNAPSHOT-bin-2.7.4/spark-ware..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>

|
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


{noformat};;;",,,,,,,,,,,,,,,,,,,,,,
insertInto with overwrite inconsistent behaviour Python/Scala,SPARK-28411,13245140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,vapira,vapira,16/Jul/19 08:57,12/Dec/22 18:11,13/Jul/23 08:49,18/Jul/19 04:38,2.2.1,2.4.0,,,,,,,,,3.0.0,,,PySpark,SQL,,,,1,release-notes,,,"The df.write.mode(""overwrite"").insertInto(""table"") has inconsistent behaviour between Scala and Python. In Python, insertInto ignores ""mode"" parameter and appends by default. Only when changing syntax to df.write.insertInto(""table"", overwrite=True) we get expected behaviour.

This is a native Spark syntax, expected to be the same between languages... Also, in other write methods, like saveAsTable or write.parquet ""mode"" seem to be respected.

Reproduce, Python, ignore ""overwrite"":
{code:java}
df = spark.createDataFrame(sc.parallelize([(1, 2),(3,4)]),['i','j'])

# create the table and load data
df.write.saveAsTable(""spark_overwrite_issue"")

# insert overwrite, expected result - 2 rows
df.write.mode(""overwrite"").insertInto(""spark_overwrite_issue"")

spark.sql(""select * from spark_overwrite_issue"").count()
# result - 4 rows, insert appended data instead of overwrite{code}
Reproduce, Scala, works as expected:
{code:java}
val df = Seq((1, 2),(3,4)).toDF(""i"",""j"")

df.write.mode(""overwrite"").insertInto(""spark_overwrite_issue"")

spark.sql(""select * from spark_overwrite_issue"").count()
# result - 2 rows{code}
Tested on Spark 2.2.1 (EMR) and 2.4.0 (Databricks)",,huaxingao,vapira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 07:42:51 UTC 2019,,,,,,,,,,"0|z04pfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Jul/19 18:48;huaxingao;I am working on this. Will submit a PR soon. ;;;","18/Jul/19 04:38;gurwls223;Issue resolved by pull request 25175
[https://github.com/apache/spark/pull/25175];;;","18/Jul/19 07:42;vapira;Great, thank you!;;;",,,,,,,,,,,,,,,,,,,,,
Fix negative timeout value in RateStreamContinuousPartitionReader,SPARK-28404,13244996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,15/Jul/19 14:41,15/Jul/19 22:01,13/Jul/23 08:49,15/Jul/19 18:01,2.1.3,2.2.3,2.3.3,2.4.3,3.0.0,,,,,,2.3.4,2.4.4,3.0.0,Structured Streaming,,,,,0,,,,"The issue came here: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/107676/testReport/

{code:java}
org.scalatest.exceptions.TestFailedException:  Assert on query failed: Execute: Query [id = 9be3f258-2f2a-430d-91c9-b8f395f9907b, runId = 6421d80a-fac0-4dae-8029-7fbc0b047601] terminated with exception: Writing job aborted. org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:353)  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)   Caused by:  Writing job aborted.  org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec.doExecute(WriteToContinuousDataSourceExec.scala:63)   org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)   org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)   org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)   org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)   org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)   org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.$anonfun$runContinuous$4(ContinuousExecution.scala:257)   org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)   org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)   org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)    Caused by:   Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.spark.SparkException: Data read failed  at org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader.next(ContinuousQueuedDataReader.scala:102)  at org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:93)  at org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:91)  at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)  at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$1(ContinuousWriteRDD.scala:58)  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1384)  at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:51)  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)  at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)  at org.apache.spark.scheduler.Task.run(Task.scala:126)  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:426)  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1350)  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:429)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)  at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.IllegalArgumentException: timeout value is negative  at java.lang.Thread.sleep(Native Method)  at org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousPartitionReader.next(ContinuousRateStreamSource.scala:138)  at org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader$DataReaderThread.run(ContinuousQueuedDataReader.scala:143)  Driver stacktrace:   org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1952)    org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1940)    org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1939)    scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)    scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)    scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)    org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1939)    org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:943)    org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:943)    scala.Option.foreach(Option.scala:274)     Caused by:    Data read failed    org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader.next(ContinuousQueuedDataReader.scala:102)     org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:93)     org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:91)     org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)     scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)     org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)     org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)     org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)     org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$1(ContinuousWriteRDD.scala:58)     scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)      Caused by:     timeout value is negative     java.lang.Thread.sleep(Native Method)      org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousPartitionReader.next(ContinuousRateStreamSource.scala:138)      org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader$DataReaderThread.run(ContinuousQueuedDataReader.scala:143)   == Progress ==    StartStream(ContinuousTrigger(3600000),org.apache.spark.util.SystemClock@7923a22b,Map(),null) => AssertOnQuery(<condition>, Execute)    AssertOnQuery(<condition>, Execute)    AssertOnQuery(<condition>, Execute)    StopStream    CheckAnswerContains: [0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47],[48],[49],[50],[51],[52],[53],[54],[55],[56],[57],[58],[59],[60],[61],[62],[63],[64],[65],[66],[67],[68],[69],[70],[71],[72],[73],[74],[75],[76],[77],[78],[79],[80],[81],[82],[83],[84],[85],[86],[87],[88],[89],[90],[91],[92],[93],[94],[95],[96],[97],[98],[99],[100],[101],[102],[103],[104],[105],[106],[107],[108],[109],[110],[111],[112],[113],[114],[115],[116],[117],[118],[119],[120],[121],[122],[123],[124],[125],[126],[127],[128],[129],[130],[131],[132],[133],[134],[135],[136],[137],[138],[139],[140],[141],[142],[143],[144],[145],[146],[147],[148],[149],[150],[151],[152],[153],[154],[155],[156],[157],[158],[159],[160],[161],[162],[163],[164],[165],[166],[167],[168],[169],[170],[171],[172],[173],[174],[175],[176],[177],[178],[179],[180],[181],[182],[183],[184],[185],[186],[187],[188],[189],[190],[191],[192],[193],[194],[195],[196],[197],[198],[199],[200],[201],[202],[203],[204],[205],[206],[207],[208],[209],[210],[211],[212],[213],[214],[215],[216],[217],[218],[219],[220],[221],[222],[223],[224],[225],[226],[227],[228],[229],[230],[231],[232],[233],[234],[235],[236],[237],[238],[239],[240],[241],[242],[243],[244],[245],[246],[247],[248],[249],[250],[251],[252],[253],[254],[255],[256],[257],[258],[259],[260],[261],[262],[263],[264],[265],[266],[267],[268],[269],[270],[271],[272],[273],[274],[275],[276],[277],[278],[279],[280],[281],[282],[283],[284],[285],[286],[287],[288],[289],[290],[291],[292],[293],[294],[295],[296],[297],[298],[299],[300],[301],[302],[303],[304],[305],[306],[307],[308],[309],[310],[311],[312],[313],[314],[315],[316],[317],[318],[319],[320],[321],[322],[323],[324],[325],[326],[327],[328],[329],[330],[331],[332],[333],[334],[335],[336],[337],[338],[339],[340],[341],[342],[343],[344],[345],[346],[347],[348],[349],[350],[351],[352],[353],[354],[355],[356],[357],[358],[359],[360],[361],[362],[363],[364],[365],[366],[367],[368],[369],[370],[371],[372],[373],[374],[375],[376],[377],[378],[379],[380],[381],[382],[383],[384],[385],[386],[387],[388],[389],[390],[391],[392],[393],[394],[395],[396],[397],[398],[399],[400],[401],[402],[403],[404],[405],[406],[407],[408],[409],[410],[411],[412],[413],[414],[415],[416],[417],[418],[419],[420],[421],[422],[423],[424],[425],[426],[427],[428],[429],[430],[431],[432],[433],[434],[435],[436],[437],[438],[439],[440],[441],[442],[443],[444],[445],[446],[447],[448],[449],[450],[451],[452],[453],[454],[455],[456],[457],[458],[459],[460],[461],[462],[463],[464],[465],[466],[467],[468],[469],[470],[471],[472],[473],[474],[475],[476],[477],[478],[479],[480],[481],[482],[483],[484],[485],[486],[487],[488],[489],[490],[491],[492],[493],[494],[495],[496],[497],[498],[499],[500],[501],[502],[503],[504],[505],[506],[507],[508],[509],[510],[511],[512],[513],[514],[515],[516],[517],[518],[519],[520],[521],[522],[523],[524],[525],[526],[527],[528],[529],[530],[531],[532],[533],[534],[535],[536],[537],[538],[539],[540],[541],[542],[543],[544],[545],[546],[547],[548],[549],[550],[551],[552],[553],[554],[555],[556],[557],[558],[559],[560],[561],[562],[563],[564],[565],[566],[567],[568],[569],[570],[571],[572],[573],[574],[575],[576],[577],[578],[579],[580],[581],[582],[583],[584],[585],[586],[587],[588],[589],[590],[591],[592],[593],[594],[595],[596],[597],[598],[599],[600],[601],[602],[603],[604],[605],[606],[607],[608],[609],[610],[611],[612],[613],[614],[615],[616],[617],[618],[619],[620],[621],[622],[623],[624],[625],[626],[627],[628],[629],[630],[631],[632],[633],[634],[635],[636],[637],[638],[639],[640],[641],[642],[643],[644],[645],[646],[647],[648],[649],[650],[651],[652],[653],[654],[655],[656],[657],[658],[659],[660],[661],[662],[663],[664],[665],[666],[667],[668],[669],[670],[671],[672],[673],[674],[675],[676],[677],[678],[679],[680],[681],[682],[683],[684],[685],[686],[687],[688],[689],[690],[691],[692],[693],[694],[695],[696],[697],[698],[699],[700],[701],[702],[703],[704],[705],[706],[707],[708],[709],[710],[711],[712],[713],[714],[715],[716],[717],[718],[719],[720],[721],[722],[723],[724],[725],[726],[727],[728],[729],[730],[731],[732],[733],[734],[735],[736],[737],[738],[739],[740],[741],[742],[743],[744],[745],[746],[747],[748],[749],[750],[751],[752],[753],[754],[755],[756],[757],[758],[759],[760],[761],[762],[763],[764],[765],[766],[767],[768],[769],[770],[771],[772],[773],[774],[775],[776],[777],[778],[779],[780],[781],[782],[783],[784],[785],[786],[787],[788],[789],[790],[791],[792],[793],[794],[795],[796],[797],[798],[799],[800],[801],[802],[803],[804],[805],[806],[807],[808],[809],[810],[811],[812],[813],[814],[815],[816],[817],[818],[819],[820],[821],[822],[823],[824],[825],[826],[827],[828],[829],[830],[831],[832],[833],[834],[835],[836],[837],[838],[839],[840],[841],[842],[843],[844],[845],[846],[847],[848],[849],[850],[851],[852],[853],[854],[855],[856],[857],[858],[859],[860],[861],[862],[863],[864],[865],[866],[867],[868],[869],[870],[871],[872],[873],[874],[875],[876],[877],[878],[879],[880],[881],[882],[883],[884],[885],[886],[887],[888],[889],[890],[891],[892],[893],[894],[895],[896],[897],[898],[899],[900],[901],[902],[903],[904],[905],[906],[907],[908],[909],[910],[911],[912],[913],[914],[915],[916],[917],[918],[919],[920],[921],[922],[923],[924],[925],[926],[927],[928],[929],[930],[931],[932],[933],[934],[935],[936],[937],[938],[939],[940],[941],[942],[943],[944],[945],[946],[947],[948],[949],[950],[951],[952],[953],[954],[955],[956],[957],[958],[959],[960],[961],[962],[963],[964],[965],[966],[967],[968],[969],[970],[971],[972],[973],[974],[975],[976],[977],[978],[979],[980],[981],[982],[983],[984],[985],[986],[987],[988],[989],[990],[991],[992],[993],[994],[995],[996],[997],[998],[999],[1000],[1001],[1002],[1003],[1004],[1005],[1006],[1007],[1008],[1009],[1010],[1011],[1012],[1013],[1014],[1015],[1016],[1017],[1018],[1019],[1020],[1021],[1022],[1023],[1024],[1025],[1026],[1027],[1028],[1029],[1030],[1031],[1032],[1033],[1034],[1035],[1036],[1037],[1038],[1039],[1040],[1041],[1042],[1043],[1044],[1045],[1046],[1047],[1048],[1049],[1050],[1051],[1052],[1053],[1054],[1055],[1056],[1057],[1058],[1059],[1060],[1061],[1062],[1063],[1064],[1065],[1066],[1067],[1068],[1069],[1070],[1071],[1072],[1073],[1074],[1075],[1076],[1077],[1078],[1079],[1080],[1081],[1082],[1083],[1084],[1085],[1086],[1087],[1088],[1089],[1090],[1091],[1092],[1093],[1094],[1095],[1096],[1097],[1098],[1099],[1100],[1101],[1102],[1103],[1104],[1105],[1106],[1107],[1108],[1109],[1110],[1111],[1112],[1113],[1114],[1115],[1116],[1117],[1118],[1119],[1120],[1121],[1122],[1123],[1124],[1125],[1126],[1127],[1128],[1129],[1130],[1131],[1132],[1133],[1134],[1135],[1136],[1137],[1138],[1139],[1140],[1141],[1142],[1143],[1144],[1145],[1146],[1147],[1148],[1149],[1150],[1151],[1152],[1153],[1154],[1155],[1156],[1157],[1158],[1159],[1160],[1161],[1162],[1163],[1164],[1165],[1166],[1167],[1168],[1169],[1170],[1171],[1172],[1173],[1174],[1175],[1176],[1177],[1178],[1179],[1180],[1181],[1182],[1183],[1184],[1185],[1186],[1187],[1188],[1189],[1190],[1191],[1192],[1193],[1194],[1195],[1196],[1197],[1198],[1199],[1200],[1201],[1202],[1203],[1204],[1205],[1206],[1207],[1208],[1209],[1210],[1211],[1212],[1213],[1214],[1215],[1216],[1217],[1218],[1219],[1220],[1221],[1222],[1223],[1224],[1225],[1226],[1227],[1228],[1229],[1230],[1231],[1232],[1233],[1234],[1235],[1236],[1237],[1238],[1239],[1240],[1241],[1242],[1243],[1244],[1245],[1246],[1247],[1248],[1249],[1250],[1251],[1252],[1253],[1254],[1255],[1256],[1257],[1258],[1259],[1260],[1261],[1262],[1263],[1264],[1265],[1266],[1267],[1268],[1269],[1270],[1271],[1272],[1273],[1274],[1275],[1276],[1277],[1278],[1279],[1280],[1281],[1282],[1283],[1284],[1285],[1286],[1287],[1288],[1289],[1290],[1291],[1292],[1293],[1294],[1295],[1296],[1297],[1298],[1299],[1300],[1301],[1302],[1303],[1304],[1305],[1306],[1307],[1308],[1309],[1310],[1311],[1312],[1313],[1314],[1315],[1316],[1317],[1318],[1319],[1320],[1321],[1322],[1323],[1324],[1325],[1326],[1327],[1328],[1329],[1330],[1331],[1332],[1333],[1334],[1335],[1336],[1337],[1338],[1339],[1340],[1341],[1342],[1343],[1344],[1345],[1346],[1347],[1348],[1349],[1350],[1351],[1352],[1353],[1354],[1355],[1356],[1357],[1358],[1359],[1360],[1361],[1362],[1363],[1364],[1365],[1366],[1367],[1368],[1369],[1370],[1371],[1372],[1373],[1374],[1375],[1376],[1377],[1378],[1379],[1380],[1381],[1382],[1383],[1384],[1385],[1386],[1387],[1388],[1389],[1390],[1391],[1392],[1393],[1394],[1395],[1396],[1397],[1398],[1399],[1400],[1401],[1402],[1403],[1404],[1405],[1406],[1407],[1408],[1409],[1410],[1411],[1412],[1413],[1414],[1415],[1416],[1417],[1418],[1419],[1420],[1421],[1422],[1423],[1424],[1425],[1426],[1427],[1428],[1429],[1430],[1431],[1432],[1433],[1434],[1435],[1436],[1437],[1438],[1439],[1440],[1441],[1442],[1443],[1444],[1445],[1446],[1447],[1448],[1449],[1450],[1451],[1452],[1453],[1454],[1455],[1456],[1457],[1458],[1459],[1460],[1461],[1462],[1463],[1464],[1465],[1466],[1467],[1468],[1469],[1470],[1471],[1472],[1473],[1474],[1475],[1476],[1477],[1478],[1479],[1480],[1481],[1482],[1483],[1484],[1485],[1486],[1487],[1488],[1489],[1490],[1491],[1492],[1493],[1494],[1495],[1496],[1497],[1498],[1499],[1500],[1501],[1502],[1503],[1504],[1505],[1506],[1507],[1508],[1509],[1510],[1511],[1512],[1513],[1514],[1515],[1516],[1517],[1518],[1519],[1520],[1521],[1522],[1523],[1524],[1525],[1526],[1527],[1528],[1529],[1530],[1531],[1532],[1533],[1534],[1535],[1536],[1537],[1538],[1539],[1540],[1541],[1542],[1543],[1544],[1545],[1546],[1547],[1548],[1549],[1550],[1551],[1552],[1553],[1554],[1555],[1556],[1557],[1558],[1559],[1560],[1561],[1562],[1563],[1564],[1565],[1566],[1567],[1568],[1569],[1570],[1571],[1572],[1573],[1574],[1575],[1576],[1577],[1578],[1579],[1580],[1581],[1582],[1583],[1584],[1585],[1586],[1587],[1588],[1589],[1590],[1591],[1592],[1593],[1594],[1595],[1596],[1597],[1598],[1599],[1600],[1601],[1602],[1603],[1604],[1605],[1606],[1607],[1608],[1609],[1610],[1611],[1612],[1613],[1614],[1615],[1616],[1617],[1618],[1619],[1620],[1621],[1622],[1623],[1624],[1625],[1626],[1627],[1628],[1629],[1630],[1631],[1632],[1633],[1634],[1635],[1636],[1637],[1638],[1639],[1640],[1641],[1642],[1643],[1644],[1645],[1646],[1647],[1648],[1649],[1650],[1651],[1652],[1653],[1654],[1655],[1656],[1657],[1658],[1659],[1660],[1661],[1662],[1663],[1664],[1665],[1666],[1667],[1668],[1669],[1670],[1671],[1672],[1673],[1674],[1675],[1676],[1677],[1678],[1679],[1680],[1681],[1682],[1683],[1684],[1685],[1686],[1687],[1688],[1689],[1690],[1691],[1692],[1693],[1694],[1695],[1696],[1697],[1698],[1699],[1700],[1701],[1702],[1703],[1704],[1705],[1706],[1707],[1708],[1709],[1710],[1711],[1712],[1713],[1714],[1715],[1716],[1717],[1718],[1719],[1720],[1721],[1722],[1723],[1724],[1725],[1726],[1727],[1728],[1729],[1730],[1731],[1732],[1733],[1734],[1735],[1736],[1737],[1738],[1739],[1740],[1741],[1742],[1743],[1744],[1745],[1746],[1747],[1748],[1749],[1750],[1751],[1752],[1753],[1754],[1755],[1756],[1757],[1758],[1759],[1760],[1761],[1762],[1763],[1764],[1765],[1766],[1767],[1768],[1769],[1770],[1771],[1772],[1773],[1774],[1775],[1776],[1777],[1778],[1779],[1780],[1781],[1782],[1783],[1784],[1785],[1786],[1787],[1788],[1789],[1790],[1791],[1792],[1793],[1794],[1795],[1796],[1797],[1798],[1799],[1800],[1801],[1802],[1803],[1804],[1805],[1806],[1807],[1808],[1809],[1810],[1811],[1812],[1813],[1814],[1815],[1816],[1817],[1818],[1819],[1820],[1821],[1822],[1823],[1824],[1825],[1826],[1827],[1828],[1829],[1830],[1831],[1832],[1833],[1834],[1835],[1836],[1837],[1838],[1839],[1840],[1841],[1842],[1843],[1844],[1845],[1846],[1847],[1848],[1849],[1850],[1851],[1852],[1853],[1854],[1855],[1856],[1857],[1858],[1859],[1860],[1861],[1862],[1863],[1864],[1865],[1866],[1867],[1868],[1869],[1870],[1871],[1872],[1873],[1874],[1875],[1876],[1877],[1878],[1879],[1880],[1881],[1882],[1883],[1884],[1885],[1886],[1887],[1888],[1889],[1890],[1891],[1892],[1893],[1894],[1895],[1896],[1897],[1898],[1899],[1900],[1901],[1902],[1903],[1904],[1905],[1906],[1907],[1908],[1909],[1910],[1911],[1912],[1913],[1914],[1915],[1916],[1917],[1918],[1919],[1920],[1921],[1922],[1923],[1924],[1925],[1926],[1927],[1928],[1929],[1930],[1931],[1932],[1933],[1934],[1935],[1936],[1937],[1938],[1939],[1940],[1941],[1942],[1943],[1944],[1945],[1946],[1947],[1948],[1949],[1950],[1951],[1952],[1953],[1954],[1955],[1956],[1957],[1958],[1959],[1960],[1961],[1962],[1963],[1964],[1965],[1966],[1967],[1968],[1969],[1970],[1971],[1972],[1973],[1974],[1975],[1976],[1977],[1978],[1979],[1980],[1981],[1982],[1983],[1984],[1985],[1986],[1987],[1988],[1989],[1990],[1991],[1992],[1993],[1994],[1995],[1996],[1997],[1998],[1999],[2000],[2001],[2002],[2003],[2004],[2005],[2006],[2007],[2008],[2009],[2010],[2011],[2012],[2013],[2014],[2015],[2016],[2017],[2018],[2019],[2020],[2021],[2022],[2023],[2024],[2025],[2026],[2027],[2028],[2029],[2030],[2031],[2032],[2033],[2034],[2035],[2036],[2037],[2038],[2039],[2040],[2041],[2042],[2043],[2044],[2045],[2046],[2047],[2048],[2049],[2050],[2051],[2052],[2053],[2054],[2055],[2056],[2057],[2058],[2059],[2060],[2061],[2062],[2063],[2064],[2065],[2066],[2067],[2068],[2069],[2070],[2071],[2072],[2073],[2074],[2075],[2076],[2077],[2078],[2079],[2080],[2081],[2082],[2083],[2084],[2085],[2086],[2087],[2088],[2089],[2090],[2091],[2092],[2093],[2094],[2095],[2096],[2097],[2098],[2099],[2100],[2101],[2102],[2103],[2104],[2105],[2106],[2107],[2108],[2109],[2110],[2111],[2112],[2113],[2114],[2115],[2116],[2117],[2118],[2119],[2120],[2121],[2122],[2123],[2124],[2125],[2126],[2127],[2128],[2129],[2130],[2131],[2132],[2133],[2134],[2135],[2136],[2137],[2138],[2139],[2140],[2141],[2142],[2143],[2144],[2145],[2146],[2147],[2148],[2149],[2150],[2151],[2152],[2153],[2154],[2155],[2156],[2157],[2158],[2159],[2160],[2161],[2162],[2163],[2164],[2165],[2166],[2167],[2168],[2169],[2170],[2171],[2172],[2173],[2174],[2175],[2176],[2177],[2178],[2179],[2180],[2181],[2182],[2183],[2184],[2185],[2186],[2187],[2188],[2189],[2190],[2191],[2192],[2193],[2194],[2195],[2196],[2197],[2198],[2199],[2200],[2201],[2202],[2203],[2204],[2205],[2206],[2207],[2208],[2209],[2210],[2211],[2212],[2213],[2214],[2215],[2216],[2217],[2218],[2219],[2220],[2221],[2222],[2223],[2224],[2225],[2226],[2227],[2228],[2229],[2230],[2231],[2232],[2233],[2234],[2235],[2236],[2237],[2238],[2239],[2240],[2241],[2242],[2243],[2244],[2245],[2246],[2247],[2248],[2249],[2250],[2251],[2252],[2253],[2254],[2255],[2256],[2257],[2258],[2259],[2260],[2261],[2262],[2263],[2264],[2265],[2266],[2267],[2268],[2269],[2270],[2271],[2272],[2273],[2274],[2275],[2276],[2277],[2278],[2279],[2280],[2281],[2282],[2283],[2284],[2285],[2286],[2287],[2288],[2289],[2290],[2291],[2292],[2293],[2294],[2295],[2296],[2297],[2298],[2299],[2300],[2301],[2302],[2303],[2304],[2305],[2306],[2307],[2308],[2309],[2310],[2311],[2312],[2313],[2314],[2315],[2316],[2317],[2318],[2319],[2320],[2321],[2322],[2323],[2324],[2325],[2326],[2327],[2328],[2329],[2330],[2331],[2332],[2333],[2334],[2335],[2336],[2337],[2338],[2339],[2340],[2341],[2342],[2343],[2344],[2345],[2346],[2347],[2348],[2349],[2350],[2351],[2352],[2353],[2354],[2355],[2356],[2357],[2358],[2359],[2360],[2361],[2362],[2363],[2364],[2365],[2366],[2367],[2368],[2369],[2370],[2371],[2372],[2373],[2374],[2375],[2376],[2377],[2378],[2379],[2380],[2381],[2382],[2383],[2384],[2385],[2386],[2387],[2388],[2389],[2390],[2391],[2392],[2393],[2394],[2395],[2396],[2397],[2398],[2399],[2400],[2401],[2402],[2403],[2404],[2405],[2406],[2407],[2408],[2409],[2410],[2411],[2412],[2413],[2414],[2415],[2416],[2417],[2418],[2419],[2420],[2421],[2422],[2423],[2424],[2425],[2426],[2427],[2428],[2429],[2430],[2431],[2432],[2433],[2434],[2435],[2436],[2437],[2438],[2439],[2440],[2441],[2442],[2443],[2444],[2445],[2446],[2447],[2448],[2449],[2450],[2451],[2452],[2453],[2454],[2455],[2456],[2457],[2458],[2459],[2460],[2461],[2462],[2463],[2464],[2465],[2466],[2467],[2468],[2469],[2470],[2471],[2472],[2473],[2474],[2475],[2476],[2477],[2478],[2479],[2480],[2481],[2482],[2483],[2484],[2485],[2486],[2487],[2488],[2489],[2490],[2491],[2492],[2493],[2494],[2495],[2496],[2497],[2498],[2499]  == Stream == Output Mode: Append Stream state: {} Thread state: dead    == Sink ==    == Plan == == Parsed Logical Plan == WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247 +- Project [value#396017L]    +- StreamingDataSourceV2Relation [timestamp#396016, value#396017L], org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1@78923774, org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousStream@5fbced94, {""0"":{""value"":-5,""runTimeMs"":1563183778277},""1"":{""value"":-4,""runTimeMs"":1563183778277},""2"":{""value"":-3,""runTimeMs"":1563183778277},""3"":{""value"":-2,""runTimeMs"":1563183778277},""4"":{""value"":-1,""runTimeMs"":1563183778277}}  == Analyzed Logical Plan ==  WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247 +- Project [value#396017L]    +- StreamingDataSourceV2Relation [timestamp#396016, value#396017L], org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1@78923774, org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousStream@5fbced94, {""0"":{""value"":-5,""runTimeMs"":1563183778277},""1"":{""value"":-4,""runTimeMs"":1563183778277},""2"":{""value"":-3,""runTimeMs"":1563183778277},""3"":{""value"":-2,""runTimeMs"":1563183778277},""4"":{""value"":-1,""runTimeMs"":1563183778277}}  == Optimized Logical Plan == WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247 +- Project [value#396017L]    +- StreamingDataSourceV2Relation [timestamp#396016, value#396017L], org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1@78923774, org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousStream@5fbced94, {""0"":{""value"":-5,""runTimeMs"":1563183778277},""1"":{""value"":-4,""runTimeMs"":1563183778277},""2"":{""value"":-3,""runTimeMs"":1563183778277},""3"":{""value"":-2,""runTimeMs"":1563183778277},""4"":{""value"":-1,""runTimeMs"":1563183778277}}  == Physical Plan == WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247 +- *(1) Project [value#396017L]    +- *(1) Project [timestamp#396016, value#396017L]       +- ContinuousScan[timestamp#396016, value#396017L] class org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1                     
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 
Assert on query failed: Execute: Query [id = 9be3f258-2f2a-430d-91c9-b8f395f9907b, runId = 6421d80a-fac0-4dae-8029-7fbc0b047601] terminated with exception: Writing job aborted.
org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:353)
	org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)

	Caused by: 	Writing job aborted.
	org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec.doExecute(WriteToContinuousDataSourceExec.scala:63)
		org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
		org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
		org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
		org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
		org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
		org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.$anonfun$runContinuous$4(ContinuousExecution.scala:257)
		org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)
		org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
		org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)

		Caused by: 		Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost, executor driver): org.apache.spark.SparkException: Data read failed
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader.next(ContinuousQueuedDataReader.scala:102)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:93)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:91)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$1(ContinuousWriteRDD.scala:58)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1384)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:126)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:426)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1350)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:429)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: timeout value is negative
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousPartitionReader.next(ContinuousRateStreamSource.scala:138)
	at org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader$DataReaderThread.run(ContinuousQueuedDataReader.scala:143)

Driver stacktrace:
		org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1952)
			org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1940)
			org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1939)
			scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
			scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
			scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
			org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1939)
			org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:943)
			org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:943)
			scala.Option.foreach(Option.scala:274)

			Caused by: 			Data read failed
			org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader.next(ContinuousQueuedDataReader.scala:102)
				org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:93)
				org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD$$anon$1.getNext(ContinuousDataSourceRDD.scala:91)
				org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
				scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
				org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
				org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
				org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:731)
				org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$1(ContinuousWriteRDD.scala:58)
				scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

				Caused by: 				timeout value is negative
				java.lang.Thread.sleep(Native Method)
					org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousPartitionReader.next(ContinuousRateStreamSource.scala:138)
					org.apache.spark.sql.execution.streaming.continuous.ContinuousQueuedDataReader$DataReaderThread.run(ContinuousQueuedDataReader.scala:143)


== Progress ==
   StartStream(ContinuousTrigger(3600000),org.apache.spark.util.SystemClock@7923a22b,Map(),null)
=> AssertOnQuery(<condition>, Execute)
   AssertOnQuery(<condition>, Execute)
   AssertOnQuery(<condition>, Execute)
   StopStream
   CheckAnswerContains: [0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47],[48],[49],[50],[51],[52],[53],[54],[55],[56],[57],[58],[59],[60],[61],[62],[63],[64],[65],[66],[67],[68],[69],[70],[71],[72],[73],[74],[75],[76],[77],[78],[79],[80],[81],[82],[83],[84],[85],[86],[87],[88],[89],[90],[91],[92],[93],[94],[95],[96],[97],[98],[99],[100],[101],[102],[103],[104],[105],[106],[107],[108],[109],[110],[111],[112],[113],[114],[115],[116],[117],[118],[119],[120],[121],[122],[123],[124],[125],[126],[127],[128],[129],[130],[131],[132],[133],[134],[135],[136],[137],[138],[139],[140],[141],[142],[143],[144],[145],[146],[147],[148],[149],[150],[151],[152],[153],[154],[155],[156],[157],[158],[159],[160],[161],[162],[163],[164],[165],[166],[167],[168],[169],[170],[171],[172],[173],[174],[175],[176],[177],[178],[179],[180],[181],[182],[183],[184],[185],[186],[187],[188],[189],[190],[191],[192],[193],[194],[195],[196],[197],[198],[199],[200],[201],[202],[203],[204],[205],[206],[207],[208],[209],[210],[211],[212],[213],[214],[215],[216],[217],[218],[219],[220],[221],[222],[223],[224],[225],[226],[227],[228],[229],[230],[231],[232],[233],[234],[235],[236],[237],[238],[239],[240],[241],[242],[243],[244],[245],[246],[247],[248],[249],[250],[251],[252],[253],[254],[255],[256],[257],[258],[259],[260],[261],[262],[263],[264],[265],[266],[267],[268],[269],[270],[271],[272],[273],[274],[275],[276],[277],[278],[279],[280],[281],[282],[283],[284],[285],[286],[287],[288],[289],[290],[291],[292],[293],[294],[295],[296],[297],[298],[299],[300],[301],[302],[303],[304],[305],[306],[307],[308],[309],[310],[311],[312],[313],[314],[315],[316],[317],[318],[319],[320],[321],[322],[323],[324],[325],[326],[327],[328],[329],[330],[331],[332],[333],[334],[335],[336],[337],[338],[339],[340],[341],[342],[343],[344],[345],[346],[347],[348],[349],[350],[351],[352],[353],[354],[355],[356],[357],[358],[359],[360],[361],[362],[363],[364],[365],[366],[367],[368],[369],[370],[371],[372],[373],[374],[375],[376],[377],[378],[379],[380],[381],[382],[383],[384],[385],[386],[387],[388],[389],[390],[391],[392],[393],[394],[395],[396],[397],[398],[399],[400],[401],[402],[403],[404],[405],[406],[407],[408],[409],[410],[411],[412],[413],[414],[415],[416],[417],[418],[419],[420],[421],[422],[423],[424],[425],[426],[427],[428],[429],[430],[431],[432],[433],[434],[435],[436],[437],[438],[439],[440],[441],[442],[443],[444],[445],[446],[447],[448],[449],[450],[451],[452],[453],[454],[455],[456],[457],[458],[459],[460],[461],[462],[463],[464],[465],[466],[467],[468],[469],[470],[471],[472],[473],[474],[475],[476],[477],[478],[479],[480],[481],[482],[483],[484],[485],[486],[487],[488],[489],[490],[491],[492],[493],[494],[495],[496],[497],[498],[499],[500],[501],[502],[503],[504],[505],[506],[507],[508],[509],[510],[511],[512],[513],[514],[515],[516],[517],[518],[519],[520],[521],[522],[523],[524],[525],[526],[527],[528],[529],[530],[531],[532],[533],[534],[535],[536],[537],[538],[539],[540],[541],[542],[543],[544],[545],[546],[547],[548],[549],[550],[551],[552],[553],[554],[555],[556],[557],[558],[559],[560],[561],[562],[563],[564],[565],[566],[567],[568],[569],[570],[571],[572],[573],[574],[575],[576],[577],[578],[579],[580],[581],[582],[583],[584],[585],[586],[587],[588],[589],[590],[591],[592],[593],[594],[595],[596],[597],[598],[599],[600],[601],[602],[603],[604],[605],[606],[607],[608],[609],[610],[611],[612],[613],[614],[615],[616],[617],[618],[619],[620],[621],[622],[623],[624],[625],[626],[627],[628],[629],[630],[631],[632],[633],[634],[635],[636],[637],[638],[639],[640],[641],[642],[643],[644],[645],[646],[647],[648],[649],[650],[651],[652],[653],[654],[655],[656],[657],[658],[659],[660],[661],[662],[663],[664],[665],[666],[667],[668],[669],[670],[671],[672],[673],[674],[675],[676],[677],[678],[679],[680],[681],[682],[683],[684],[685],[686],[687],[688],[689],[690],[691],[692],[693],[694],[695],[696],[697],[698],[699],[700],[701],[702],[703],[704],[705],[706],[707],[708],[709],[710],[711],[712],[713],[714],[715],[716],[717],[718],[719],[720],[721],[722],[723],[724],[725],[726],[727],[728],[729],[730],[731],[732],[733],[734],[735],[736],[737],[738],[739],[740],[741],[742],[743],[744],[745],[746],[747],[748],[749],[750],[751],[752],[753],[754],[755],[756],[757],[758],[759],[760],[761],[762],[763],[764],[765],[766],[767],[768],[769],[770],[771],[772],[773],[774],[775],[776],[777],[778],[779],[780],[781],[782],[783],[784],[785],[786],[787],[788],[789],[790],[791],[792],[793],[794],[795],[796],[797],[798],[799],[800],[801],[802],[803],[804],[805],[806],[807],[808],[809],[810],[811],[812],[813],[814],[815],[816],[817],[818],[819],[820],[821],[822],[823],[824],[825],[826],[827],[828],[829],[830],[831],[832],[833],[834],[835],[836],[837],[838],[839],[840],[841],[842],[843],[844],[845],[846],[847],[848],[849],[850],[851],[852],[853],[854],[855],[856],[857],[858],[859],[860],[861],[862],[863],[864],[865],[866],[867],[868],[869],[870],[871],[872],[873],[874],[875],[876],[877],[878],[879],[880],[881],[882],[883],[884],[885],[886],[887],[888],[889],[890],[891],[892],[893],[894],[895],[896],[897],[898],[899],[900],[901],[902],[903],[904],[905],[906],[907],[908],[909],[910],[911],[912],[913],[914],[915],[916],[917],[918],[919],[920],[921],[922],[923],[924],[925],[926],[927],[928],[929],[930],[931],[932],[933],[934],[935],[936],[937],[938],[939],[940],[941],[942],[943],[944],[945],[946],[947],[948],[949],[950],[951],[952],[953],[954],[955],[956],[957],[958],[959],[960],[961],[962],[963],[964],[965],[966],[967],[968],[969],[970],[971],[972],[973],[974],[975],[976],[977],[978],[979],[980],[981],[982],[983],[984],[985],[986],[987],[988],[989],[990],[991],[992],[993],[994],[995],[996],[997],[998],[999],[1000],[1001],[1002],[1003],[1004],[1005],[1006],[1007],[1008],[1009],[1010],[1011],[1012],[1013],[1014],[1015],[1016],[1017],[1018],[1019],[1020],[1021],[1022],[1023],[1024],[1025],[1026],[1027],[1028],[1029],[1030],[1031],[1032],[1033],[1034],[1035],[1036],[1037],[1038],[1039],[1040],[1041],[1042],[1043],[1044],[1045],[1046],[1047],[1048],[1049],[1050],[1051],[1052],[1053],[1054],[1055],[1056],[1057],[1058],[1059],[1060],[1061],[1062],[1063],[1064],[1065],[1066],[1067],[1068],[1069],[1070],[1071],[1072],[1073],[1074],[1075],[1076],[1077],[1078],[1079],[1080],[1081],[1082],[1083],[1084],[1085],[1086],[1087],[1088],[1089],[1090],[1091],[1092],[1093],[1094],[1095],[1096],[1097],[1098],[1099],[1100],[1101],[1102],[1103],[1104],[1105],[1106],[1107],[1108],[1109],[1110],[1111],[1112],[1113],[1114],[1115],[1116],[1117],[1118],[1119],[1120],[1121],[1122],[1123],[1124],[1125],[1126],[1127],[1128],[1129],[1130],[1131],[1132],[1133],[1134],[1135],[1136],[1137],[1138],[1139],[1140],[1141],[1142],[1143],[1144],[1145],[1146],[1147],[1148],[1149],[1150],[1151],[1152],[1153],[1154],[1155],[1156],[1157],[1158],[1159],[1160],[1161],[1162],[1163],[1164],[1165],[1166],[1167],[1168],[1169],[1170],[1171],[1172],[1173],[1174],[1175],[1176],[1177],[1178],[1179],[1180],[1181],[1182],[1183],[1184],[1185],[1186],[1187],[1188],[1189],[1190],[1191],[1192],[1193],[1194],[1195],[1196],[1197],[1198],[1199],[1200],[1201],[1202],[1203],[1204],[1205],[1206],[1207],[1208],[1209],[1210],[1211],[1212],[1213],[1214],[1215],[1216],[1217],[1218],[1219],[1220],[1221],[1222],[1223],[1224],[1225],[1226],[1227],[1228],[1229],[1230],[1231],[1232],[1233],[1234],[1235],[1236],[1237],[1238],[1239],[1240],[1241],[1242],[1243],[1244],[1245],[1246],[1247],[1248],[1249],[1250],[1251],[1252],[1253],[1254],[1255],[1256],[1257],[1258],[1259],[1260],[1261],[1262],[1263],[1264],[1265],[1266],[1267],[1268],[1269],[1270],[1271],[1272],[1273],[1274],[1275],[1276],[1277],[1278],[1279],[1280],[1281],[1282],[1283],[1284],[1285],[1286],[1287],[1288],[1289],[1290],[1291],[1292],[1293],[1294],[1295],[1296],[1297],[1298],[1299],[1300],[1301],[1302],[1303],[1304],[1305],[1306],[1307],[1308],[1309],[1310],[1311],[1312],[1313],[1314],[1315],[1316],[1317],[1318],[1319],[1320],[1321],[1322],[1323],[1324],[1325],[1326],[1327],[1328],[1329],[1330],[1331],[1332],[1333],[1334],[1335],[1336],[1337],[1338],[1339],[1340],[1341],[1342],[1343],[1344],[1345],[1346],[1347],[1348],[1349],[1350],[1351],[1352],[1353],[1354],[1355],[1356],[1357],[1358],[1359],[1360],[1361],[1362],[1363],[1364],[1365],[1366],[1367],[1368],[1369],[1370],[1371],[1372],[1373],[1374],[1375],[1376],[1377],[1378],[1379],[1380],[1381],[1382],[1383],[1384],[1385],[1386],[1387],[1388],[1389],[1390],[1391],[1392],[1393],[1394],[1395],[1396],[1397],[1398],[1399],[1400],[1401],[1402],[1403],[1404],[1405],[1406],[1407],[1408],[1409],[1410],[1411],[1412],[1413],[1414],[1415],[1416],[1417],[1418],[1419],[1420],[1421],[1422],[1423],[1424],[1425],[1426],[1427],[1428],[1429],[1430],[1431],[1432],[1433],[1434],[1435],[1436],[1437],[1438],[1439],[1440],[1441],[1442],[1443],[1444],[1445],[1446],[1447],[1448],[1449],[1450],[1451],[1452],[1453],[1454],[1455],[1456],[1457],[1458],[1459],[1460],[1461],[1462],[1463],[1464],[1465],[1466],[1467],[1468],[1469],[1470],[1471],[1472],[1473],[1474],[1475],[1476],[1477],[1478],[1479],[1480],[1481],[1482],[1483],[1484],[1485],[1486],[1487],[1488],[1489],[1490],[1491],[1492],[1493],[1494],[1495],[1496],[1497],[1498],[1499],[1500],[1501],[1502],[1503],[1504],[1505],[1506],[1507],[1508],[1509],[1510],[1511],[1512],[1513],[1514],[1515],[1516],[1517],[1518],[1519],[1520],[1521],[1522],[1523],[1524],[1525],[1526],[1527],[1528],[1529],[1530],[1531],[1532],[1533],[1534],[1535],[1536],[1537],[1538],[1539],[1540],[1541],[1542],[1543],[1544],[1545],[1546],[1547],[1548],[1549],[1550],[1551],[1552],[1553],[1554],[1555],[1556],[1557],[1558],[1559],[1560],[1561],[1562],[1563],[1564],[1565],[1566],[1567],[1568],[1569],[1570],[1571],[1572],[1573],[1574],[1575],[1576],[1577],[1578],[1579],[1580],[1581],[1582],[1583],[1584],[1585],[1586],[1587],[1588],[1589],[1590],[1591],[1592],[1593],[1594],[1595],[1596],[1597],[1598],[1599],[1600],[1601],[1602],[1603],[1604],[1605],[1606],[1607],[1608],[1609],[1610],[1611],[1612],[1613],[1614],[1615],[1616],[1617],[1618],[1619],[1620],[1621],[1622],[1623],[1624],[1625],[1626],[1627],[1628],[1629],[1630],[1631],[1632],[1633],[1634],[1635],[1636],[1637],[1638],[1639],[1640],[1641],[1642],[1643],[1644],[1645],[1646],[1647],[1648],[1649],[1650],[1651],[1652],[1653],[1654],[1655],[1656],[1657],[1658],[1659],[1660],[1661],[1662],[1663],[1664],[1665],[1666],[1667],[1668],[1669],[1670],[1671],[1672],[1673],[1674],[1675],[1676],[1677],[1678],[1679],[1680],[1681],[1682],[1683],[1684],[1685],[1686],[1687],[1688],[1689],[1690],[1691],[1692],[1693],[1694],[1695],[1696],[1697],[1698],[1699],[1700],[1701],[1702],[1703],[1704],[1705],[1706],[1707],[1708],[1709],[1710],[1711],[1712],[1713],[1714],[1715],[1716],[1717],[1718],[1719],[1720],[1721],[1722],[1723],[1724],[1725],[1726],[1727],[1728],[1729],[1730],[1731],[1732],[1733],[1734],[1735],[1736],[1737],[1738],[1739],[1740],[1741],[1742],[1743],[1744],[1745],[1746],[1747],[1748],[1749],[1750],[1751],[1752],[1753],[1754],[1755],[1756],[1757],[1758],[1759],[1760],[1761],[1762],[1763],[1764],[1765],[1766],[1767],[1768],[1769],[1770],[1771],[1772],[1773],[1774],[1775],[1776],[1777],[1778],[1779],[1780],[1781],[1782],[1783],[1784],[1785],[1786],[1787],[1788],[1789],[1790],[1791],[1792],[1793],[1794],[1795],[1796],[1797],[1798],[1799],[1800],[1801],[1802],[1803],[1804],[1805],[1806],[1807],[1808],[1809],[1810],[1811],[1812],[1813],[1814],[1815],[1816],[1817],[1818],[1819],[1820],[1821],[1822],[1823],[1824],[1825],[1826],[1827],[1828],[1829],[1830],[1831],[1832],[1833],[1834],[1835],[1836],[1837],[1838],[1839],[1840],[1841],[1842],[1843],[1844],[1845],[1846],[1847],[1848],[1849],[1850],[1851],[1852],[1853],[1854],[1855],[1856],[1857],[1858],[1859],[1860],[1861],[1862],[1863],[1864],[1865],[1866],[1867],[1868],[1869],[1870],[1871],[1872],[1873],[1874],[1875],[1876],[1877],[1878],[1879],[1880],[1881],[1882],[1883],[1884],[1885],[1886],[1887],[1888],[1889],[1890],[1891],[1892],[1893],[1894],[1895],[1896],[1897],[1898],[1899],[1900],[1901],[1902],[1903],[1904],[1905],[1906],[1907],[1908],[1909],[1910],[1911],[1912],[1913],[1914],[1915],[1916],[1917],[1918],[1919],[1920],[1921],[1922],[1923],[1924],[1925],[1926],[1927],[1928],[1929],[1930],[1931],[1932],[1933],[1934],[1935],[1936],[1937],[1938],[1939],[1940],[1941],[1942],[1943],[1944],[1945],[1946],[1947],[1948],[1949],[1950],[1951],[1952],[1953],[1954],[1955],[1956],[1957],[1958],[1959],[1960],[1961],[1962],[1963],[1964],[1965],[1966],[1967],[1968],[1969],[1970],[1971],[1972],[1973],[1974],[1975],[1976],[1977],[1978],[1979],[1980],[1981],[1982],[1983],[1984],[1985],[1986],[1987],[1988],[1989],[1990],[1991],[1992],[1993],[1994],[1995],[1996],[1997],[1998],[1999],[2000],[2001],[2002],[2003],[2004],[2005],[2006],[2007],[2008],[2009],[2010],[2011],[2012],[2013],[2014],[2015],[2016],[2017],[2018],[2019],[2020],[2021],[2022],[2023],[2024],[2025],[2026],[2027],[2028],[2029],[2030],[2031],[2032],[2033],[2034],[2035],[2036],[2037],[2038],[2039],[2040],[2041],[2042],[2043],[2044],[2045],[2046],[2047],[2048],[2049],[2050],[2051],[2052],[2053],[2054],[2055],[2056],[2057],[2058],[2059],[2060],[2061],[2062],[2063],[2064],[2065],[2066],[2067],[2068],[2069],[2070],[2071],[2072],[2073],[2074],[2075],[2076],[2077],[2078],[2079],[2080],[2081],[2082],[2083],[2084],[2085],[2086],[2087],[2088],[2089],[2090],[2091],[2092],[2093],[2094],[2095],[2096],[2097],[2098],[2099],[2100],[2101],[2102],[2103],[2104],[2105],[2106],[2107],[2108],[2109],[2110],[2111],[2112],[2113],[2114],[2115],[2116],[2117],[2118],[2119],[2120],[2121],[2122],[2123],[2124],[2125],[2126],[2127],[2128],[2129],[2130],[2131],[2132],[2133],[2134],[2135],[2136],[2137],[2138],[2139],[2140],[2141],[2142],[2143],[2144],[2145],[2146],[2147],[2148],[2149],[2150],[2151],[2152],[2153],[2154],[2155],[2156],[2157],[2158],[2159],[2160],[2161],[2162],[2163],[2164],[2165],[2166],[2167],[2168],[2169],[2170],[2171],[2172],[2173],[2174],[2175],[2176],[2177],[2178],[2179],[2180],[2181],[2182],[2183],[2184],[2185],[2186],[2187],[2188],[2189],[2190],[2191],[2192],[2193],[2194],[2195],[2196],[2197],[2198],[2199],[2200],[2201],[2202],[2203],[2204],[2205],[2206],[2207],[2208],[2209],[2210],[2211],[2212],[2213],[2214],[2215],[2216],[2217],[2218],[2219],[2220],[2221],[2222],[2223],[2224],[2225],[2226],[2227],[2228],[2229],[2230],[2231],[2232],[2233],[2234],[2235],[2236],[2237],[2238],[2239],[2240],[2241],[2242],[2243],[2244],[2245],[2246],[2247],[2248],[2249],[2250],[2251],[2252],[2253],[2254],[2255],[2256],[2257],[2258],[2259],[2260],[2261],[2262],[2263],[2264],[2265],[2266],[2267],[2268],[2269],[2270],[2271],[2272],[2273],[2274],[2275],[2276],[2277],[2278],[2279],[2280],[2281],[2282],[2283],[2284],[2285],[2286],[2287],[2288],[2289],[2290],[2291],[2292],[2293],[2294],[2295],[2296],[2297],[2298],[2299],[2300],[2301],[2302],[2303],[2304],[2305],[2306],[2307],[2308],[2309],[2310],[2311],[2312],[2313],[2314],[2315],[2316],[2317],[2318],[2319],[2320],[2321],[2322],[2323],[2324],[2325],[2326],[2327],[2328],[2329],[2330],[2331],[2332],[2333],[2334],[2335],[2336],[2337],[2338],[2339],[2340],[2341],[2342],[2343],[2344],[2345],[2346],[2347],[2348],[2349],[2350],[2351],[2352],[2353],[2354],[2355],[2356],[2357],[2358],[2359],[2360],[2361],[2362],[2363],[2364],[2365],[2366],[2367],[2368],[2369],[2370],[2371],[2372],[2373],[2374],[2375],[2376],[2377],[2378],[2379],[2380],[2381],[2382],[2383],[2384],[2385],[2386],[2387],[2388],[2389],[2390],[2391],[2392],[2393],[2394],[2395],[2396],[2397],[2398],[2399],[2400],[2401],[2402],[2403],[2404],[2405],[2406],[2407],[2408],[2409],[2410],[2411],[2412],[2413],[2414],[2415],[2416],[2417],[2418],[2419],[2420],[2421],[2422],[2423],[2424],[2425],[2426],[2427],[2428],[2429],[2430],[2431],[2432],[2433],[2434],[2435],[2436],[2437],[2438],[2439],[2440],[2441],[2442],[2443],[2444],[2445],[2446],[2447],[2448],[2449],[2450],[2451],[2452],[2453],[2454],[2455],[2456],[2457],[2458],[2459],[2460],[2461],[2462],[2463],[2464],[2465],[2466],[2467],[2468],[2469],[2470],[2471],[2472],[2473],[2474],[2475],[2476],[2477],[2478],[2479],[2480],[2481],[2482],[2483],[2484],[2485],[2486],[2487],[2488],[2489],[2490],[2491],[2492],[2493],[2494],[2495],[2496],[2497],[2498],[2499]

== Stream ==
Output Mode: Append
Stream state: {}
Thread state: dead



== Sink ==



== Plan ==
== Parsed Logical Plan ==
WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247
+- Project [value#396017L]
   +- StreamingDataSourceV2Relation [timestamp#396016, value#396017L], org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1@78923774, org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousStream@5fbced94, {""0"":{""value"":-5,""runTimeMs"":1563183778277},""1"":{""value"":-4,""runTimeMs"":1563183778277},""2"":{""value"":-3,""runTimeMs"":1563183778277},""3"":{""value"":-2,""runTimeMs"":1563183778277},""4"":{""value"":-1,""runTimeMs"":1563183778277}}

== Analyzed Logical Plan ==

WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247
+- Project [value#396017L]
   +- StreamingDataSourceV2Relation [timestamp#396016, value#396017L], org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1@78923774, org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousStream@5fbced94, {""0"":{""value"":-5,""runTimeMs"":1563183778277},""1"":{""value"":-4,""runTimeMs"":1563183778277},""2"":{""value"":-3,""runTimeMs"":1563183778277},""3"":{""value"":-2,""runTimeMs"":1563183778277},""4"":{""value"":-1,""runTimeMs"":1563183778277}}

== Optimized Logical Plan ==
WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247
+- Project [value#396017L]
   +- StreamingDataSourceV2Relation [timestamp#396016, value#396017L], org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1@78923774, org.apache.spark.sql.execution.streaming.continuous.RateStreamContinuousStream@5fbced94, {""0"":{""value"":-5,""runTimeMs"":1563183778277},""1"":{""value"":-4,""runTimeMs"":1563183778277},""2"":{""value"":-3,""runTimeMs"":1563183778277},""3"":{""value"":-2,""runTimeMs"":1563183778277},""4"":{""value"":-1,""runTimeMs"":1563183778277}}

== Physical Plan ==
WriteToContinuousDataSource org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@729b2247
+- *(1) Project [value#396017L]
   +- *(1) Project [timestamp#396016, value#396017L]
      +- ContinuousScan[timestamp#396016, value#396017L] class org.apache.spark.sql.execution.streaming.sources.RateStreamTable$$anon$1

         
         
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions.fail(Assertions.scala:1089)
	at org.scalatest.Assertions.fail$(Assertions.scala:1085)
	at org.scalatest.FunSuite.fail(FunSuite.scala:1560)
	at org.apache.spark.sql.streaming.StreamTest.failTest$1(StreamTest.scala:444)
	at org.apache.spark.sql.streaming.StreamTest.executeAction$1(StreamTest.scala:646)
	at org.apache.spark.sql.streaming.StreamTest.$anonfun$testStream$56(StreamTest.scala:770)
	at org.apache.spark.sql.streaming.StreamTest.$anonfun$testStream$56$adapted(StreamTest.scala:757)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:39)
	at org.apache.spark.sql.streaming.StreamTest.liftedTree1$1(StreamTest.scala:757)
	at org.apache.spark.sql.streaming.StreamTest.testStream(StreamTest.scala:756)
	at org.apache.spark.sql.streaming.StreamTest.testStream$(StreamTest.scala:326)
	at org.apache.spark.sql.streaming.continuous.ContinuousSuiteBase.testStream(ContinuousSuite.scala:32)
	at org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.$anonfun$new$21(ContinuousSuite.scala:276)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
",,dongjoon,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 18:01:53 UTC 2019,,,,,,,,,,"0|z04ojc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Jul/19 18:01;dongjoon;Issue resolved by pull request 25162
[https://github.com/apache/spark/pull/25162];;;",,,,,,,,,,,,,,,,,,,,,,,
Enforce idempotence on the PullupCorrelatedPredicates optimizer rule,SPARK-28375,13244720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,manifoldQAQ,manifoldQAQ,12/Jul/19 22:50,02/Mar/20 21:31,13/Jul/23 08:49,30/Jul/19 23:31,2.2.0,2.3.0,2.4.0,3.0.0,,,,,,,3.0.0,,,SQL,,,,,0,correctness,,,"The current PullupCorrelatedPredicates implementation can accidentally remove predicates for multiple runs.

For example, for the following logical plan, one more optimizer run can remove the predicate in the SubqueryExpresssion.
{code:java}
# Optimized
Project [a#0]
+- Filter a#0 IN (list#4 [(b#1 < d#3)])
   :  +- Project [c#2, d#3]
   :     +- LocalRelation <empty>, [c#2, d#3]
   +- LocalRelation <empty>, [a#0, b#1]

# Double optimized
Project [a#0]
+- Filter a#0 IN (list#4 [])
   :  +- Project [c#2, d#3]
   :     +- LocalRelation <empty>, [c#2, d#3]
   +- LocalRelation <empty>, [a#0, b#1]
{code}
 

 ",,dongjoon,joshrosen,manifoldQAQ,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,SPARK-28529,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 21:31:18 UTC 2020,,,,,,,,,,"0|z04mug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Jul/19 03:41;joshrosen;Adding the 'correctness' label so we remember to backport this fix to 2.4.x.;;;","15/Jul/19 17:55;smilegator;[~joshrosen] The issue is only triggered if we run the rule more than once. Is it still a correctness issue?;;;","15/Jul/19 19:09;joshrosen;I'm not sure; is it possible to trigger double-optimization using Spark's public APIs? Or only via relying on private / internal APIs?;;;","30/Jul/19 23:31;smilegator;This is possible, if users add the rule into postHocOptimizationBatches;;;","25/Jan/20 09:35;dongjoon;Hi, [~joshrosen] and [~smilegator].
This is still has `correctness`. Do we need to backport this?;;;","26/Jan/20 00:23;maropu;IMO PullupCorrelatedPredicates is in the catalyst package and the object seems to be internal. If so, I think its less worth backpoiting this to branch-2.4.;;;","26/Jan/20 01:25;dongjoon;[~maropu]. So, this doesn't have any effect to user sides?;;;","26/Jan/20 01:28;maropu;The same comment here, too: https://github.com/apache/spark/pull/26173#issuecomment-578458400;;;","02/Mar/20 21:31;dongjoon;Although we don't backport this, I updated this issue type as `Bug` and updated the affected version due to `postHocOptimizationBatches`.;;;",,,,,,,,,,,,,,,
"Parquet ""starts with"" filter is not null-safe",SPARK-28371,13244658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,12/Jul/19 17:26,13/Jul/19 18:44,13/Jul/23 08:49,13/Jul/19 18:39,3.0.0,,,,,,,,,,2.4.4,3.0.0,,SQL,,,,,0,,,,"I ran into this when running unit tests with Parquet 1.11. It seems that 1.10 has the same behavior in a few places but Spark somehow doesn't trigger those code paths.

Basically, {{UserDefinedPredicate.keep}} should be null-safe, and Spark's implementation is not. This was clarified in Parquet's documentation in PARQUET-1489.

Failure I was getting:

{noformat}
Job aborted due to stage failure: Task 0 in stage 1304.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1304.0 (TID 2528, localhost, executor driver): java.lang.NullPointerException&#010;
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$createFilter$16$$anon$1.keep(ParquetFilters.scala:544)&#010;
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$createFilter$16$$anon$1.keep(ParquetFilters.scala:523)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:152)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)&#010;
  at org.apache.parquet.filter2.predicate.Operators$UserDefined.accept(Operators.java:377)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:181)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)&#010;
  at org.apache.parquet.filter2.predicate.Operators$And.accept(Operators.java:309)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)&#010;
  at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)&#010;
  at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)&#010;
  at org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:954)&#010;
  at org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:759)&#010;
  at org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:207)&#010;
  at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:182)&#010;
  at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)&#010;
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:439)&#010;
  ... 
{noformat}",,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 13 18:39:50 UTC 2019,,,,,,,,,,"0|z04mgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Jul/19 18:39;dongjoon;Issue resolved by pull request 25140
[https://github.com/apache/spark/pull/25140];;;",,,,,,,,,,,,,,,,,,,,,,,
Check overflow in decimal UDF,SPARK-28369,13244616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,mickjermsurawong-stripe,mickjermsurawong-stripe,12/Jul/19 13:46,22/Jul/19 07:01,13/Jul/23 08:49,22/Jul/19 02:47,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Udf resulting in overflowing BigDecimal currently returns null. This is inconsistent with new behavior allow option to check and throw overflow introduced in https://issues.apache.org/jira/browse/SPARK-23179
{code:java}
import spark.implicits._
val tenFold: java.math.BigDecimal => java.math.BigDecimal = 
  _.multiply(new java.math.BigDecimal(""10""))
val tenFoldUdf = udf(tenFold)
val ds = spark
  .createDataset(Seq(BigDecimal(""12345678901234567890.123"")))
  .select(tenFoldUdf(col(""value"")))
  .as[BigDecimal]
ds.collect shouldEqual Seq(null){code}
The problem is at the {{CatalystTypeConverters}} where {{toPrecision}} gets converted to null

[https://github.com/apache/spark/blob/13ae9ebb38ba357aeb3f1e3fe497b322dff8eb35/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L344-L356]",,cloud_fan,maropu,mickjermsurawong-stripe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 02:47:46 UTC 2019,,,,,,,,,,"0|z04m7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jul/19 02:47;cloud_fan;Issue resolved by pull request 25144
[https://github.com/apache/spark/pull/25144];;;",,,,,,,,,,,,,,,,,,,,,,,
Kafka connector infinite wait because metadata never updated,SPARK-28367,13244596,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gsomogyi,gsomogyi,gsomogyi,12/Jul/19 12:02,09/Feb/21 01:51,13/Jul/23 08:49,11/Dec/20 10:18,2.1.3,2.2.3,2.3.3,2.4.3,3.0.0,3.1.0,,,,,3.1.0,,,Structured Streaming,,,,,2,,,,"Spark uses an old and deprecated API named poll(long) which never returns and stays in live lock if metadata is not updated (for instance when broker disappears at consumer creation).
",,Bartalos,gsomogyi,ijuma,jincheng,kabhwan,prashant,Tagar,,,,,,,,,,,,,,,,,,,,SPARK-30169,SPARK-30311,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 10:18:05 UTC 2020,,,,,,,,,,"0|z04m34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Aug/19 08:30;gsomogyi;It has been turned out new API from Kafka side is needed for the clean solution. The discussion has been initiated. I'm actively tracking the progress and intended to create a new PR when it's available.;;;","05/Sep/19 07:28;prashant;Hi, can you please provide the link to the kafka discussion as well?;;;","05/Sep/19 08:33;gsomogyi;The main discussion on Kafka side started here: https://lists.apache.org/thread.html/017cf631ef981ab1b494b1249be5c11d7edfe5f4867770a18188ebdc@%3Cdev.kafka.apache.org%3E

I've made a summary in my closed PR to describe the solution going to be made on Spark side.;;;","24/Oct/19 18:20;gsomogyi;The new Kafka API is merged to master which will be available in 2.5.;;;","20/Dec/19 06:09;kabhwan;I just realized Kafka 2.4 doesn't contain KIP-396... Gabor is right. The code just got merged 4 days later than code freeze of Kafka 2.4 release plan. Sorry for the confusion.;;;","19/Apr/20 22:53;ijuma;This should be unblocked since Spark has upgraded to Kafka 2.5.0:

[https://github.com/apache/spark/pull/28235];;;","20/Apr/20 08:40;gsomogyi;It is unblocked and started to work on this...;;;","21/Apr/20 10:02;gsomogyi;I've taken a look at the possibilities given by the new API in [KIP-396|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97551484]. I've found the following problems:
* Consumer properties don't match 100% with AdminClient so Consumer properties can't be used for instantiation (at the first glance I think adding this to the Spark API would be an overkill)
* With the new API by using AdminClient Spark looses the possibility to use the assign, subscribe and subscribePattern APIs (implementing this logic would be feasible since Kafka consumer does this on client side as well but would be ugly).

My main conclusion is that adding AdminClient and using Consumer in a parallel way would be super hacky. I would use either Consumer (which doesn't provide metadata only at the moment) or AdminClient (where it must be checked whether all existing features can be filled + how to add properties).
;;;","19/Jun/20 08:04;gsomogyi;I think we can split the problem into 2 pieces. Driver and executor side.
The executor side is not problematic and can be done w/o new API.
The driver side requires further consideration and effort. Creating subtasks and PR for executor side.;;;","16/Jul/20 14:21;gsomogyi;I'm going to have a chat w/ the Flink guys how they've overcome offset fetch issue w/ the new poll(Duration) API. Maybe we learn something.;;;","13/Aug/20 17:34;Tagar;[~gsomogyi] thanks! yep would be great to learn how this is done on the Flink side. ;;;","11/Dec/20 10:18;gsomogyi;The issue solved in subtasks so closing this.;;;",,,,,,,,,,,,
Fallback locale to en_US in StopWordsRemover if system default locale isn't in available locales in JVM,SPARK-28365,13244585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,12/Jul/19 11:09,12/Dec/22 18:10,13/Jul/23 08:49,26/Jul/19 03:13,3.0.0,,,,,,,,,,3.0.0,,,ML,,,,,0,,,,"Because the local default locale isn't in available locales at {{Locale}}, when I did some tests locally with python code, {{StopWordsRemover}} related python test hits some errors, like:

{code}
Traceback (most recent call last):
  File ""/spark-1/python/pyspark/ml/tests/test_feature.py"", line 87, in test_stopwordsremover
    stopWordRemover = StopWordsRemover(inputCol=""input"", outputCol=""output"")
  File ""/spark-1/python/pyspark/__init__.py"", line 111, in wrapper
    return func(self, **kwargs)
  File ""/spark-1/python/pyspark/ml/feature.py"", line 2646, in __init__
    self.uid)
  File ""/spark-1/python/pyspark/ml/wrapper.py"", line 67, in _new_java_obj
    return java_obj(*java_args)
  File /spark-1/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1554, in __call__
    answer, self._gateway_client, None, self._fqn)
  File ""/spark-1/python/pyspark/sql/utils.py"", line 93, in deco
    raise converted
pyspark.sql.utils.IllegalArgumentException: 'StopWordsRemover_4598673ee802 parameter locale given invalid value en_TW.'
{code}

As per [~hyukjin.kwon]'s advice, instead of setting up locale to pass test, it is better to have a workable locale if system default locale can't be found in available locales in JVM. Otherwise, users have to manually change system locale or accessing a private property _jvm in PySpark.",,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 26 03:13:39 UTC 2019,,,,,,,,,,"0|z04m0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jul/19 03:13;gurwls223;Issue resolved by pull request 25133
[https://github.com/apache/spark/pull/25133];;;",,,,,,,,,,,,,,,,,,,,,,,
Make integrated UDF tests robust by making them no-op,SPARK-28359,13244519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,12/Jul/19 05:29,12/Dec/22 17:51,13/Jul/23 08:49,17/Jul/19 14:42,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"Current UDFs available in `IntegratedUDFTestUtils` are not exactly no-op. It converts input column to strings and outputs to strings.

This causes many issues, for instance, https://github.com/apache/spark/pull/25128 or https://github.com/apache/spark/pull/25110

Ideally we should make this UDF virtually noop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27921,SPARK-27893,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-07-12 05:29:43.0,,,,,,,,,,"0|z04lm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use JIRA user name instead of JIRA user key,SPARK-28354,13244458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/Jul/19 21:15,12/Dec/22 18:10,13/Jul/23 08:49,12/Jul/19 09:44,2.3.4,2.4.4,3.0.0,,,,,,,,3.0.0,,,Project Infra,,,,,0,,,,"`dev/merge_spark_pr.py` script always fail for some users (e.g. [~yumwang]) because they have different `name` and `key`.

- https://issues.apache.org/jira/rest/api/2/user?username=yumwang

JIRA Client expects `name`, but we are using `key`.
{code}
    def assign_issue(self, issue, assignee):
        """"""Assign an issue to a user. None will set it to unassigned. -1 will set it to Automatic.

        :param issue: the issue ID or key to assign
        :param assignee: the user to assign the issue to

        :type issue: int or str
        :type assignee: str

        :rtype: bool
        """"""
        url = self._options['server'] + \
            '/rest/api/latest/issue/' + str(issue) + '/assignee'
        payload = {'name': assignee}
        r = self._session.put(
            url, data=json.dumps(payload))
        raise_on_error(r)
        return True
{code}

This issue fixes it.
{code}
-                asf_jira.assign_issue(issue.key, assignee.key)
+                asf_jira.assign_issue(issue.key, assignee.name)
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 12 09:44:51 UTC 2019,,,,,,,,,,"0|z04l8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Jul/19 09:44;gurwls223;Issue resolved by pull request 25120
[https://github.com/apache/spark/pull/25120];;;",,,,,,,,,,,,,,,,,,,,,,,
"clone the query plan between analyzer, optimizer and planner",SPARK-28346,13244230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Jul/19 06:46,23/Jul/19 16:06,13/Jul/23 08:49,23/Jul/19 16:01,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,,,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-07-11 06:46:06.0,,,,,,,,,,"0|z04juw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fail the query if detect ambiguous self join,SPARK-28344,13244218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Jul/19 04:31,28/Sep/21 08:44,13/Jul/23 08:49,06/Aug/19 02:08,1.4.1,1.5.2,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,3.0.0,,,SQL,,,,,0,correctness,,,,,apachespark,cloud_fan,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30218,SPARK-10892,,SPARK-27547,SPARK-36874,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 15:50:02 UTC 2021,,,,,,,,,,"0|z04jug:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,"06/Aug/19 02:08;cloud_fan;Issue resolved by pull request 25107
[https://github.com/apache/spark/pull/25107];;;","22/Jan/20 18:18;dongjoon;Hi, [~cloud_fan].
This improvement prevents the correctness issues. So, I switched this to `Bug` . 
Could you make a backport this to `branch-2.4`?;;;","22/Jan/20 19:58;dongjoon;cc [~tgraves];;;","01/Feb/20 05:18;dongjoon;Please refer the discussion on our backporting efforts.
- https://github.com/apache/spark/pull/27417

I removed `Target Version: 2.4.5` for now. If we need this in `branch-2.4` later, `Target Version` will be `2.4.6`.;;;","02/Mar/20 21:26;dongjoon;I updated `Affected Versions` because SPARK-10892 is resolved via this.;;;","01/Jun/20 14:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28695;;;","01/Jun/20 14:12;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28695;;;","10/Jun/20 12:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28783;;;","10/Jun/20 12:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28783;;;","09/Feb/21 15:49;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/27417;;;","09/Feb/21 15:50;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/27417;;;",,,,,,,,,,,,,
Replace REL_12_BETA1 to REL_12_BETA2 in PostgresSQL SQL tests,SPARK-28342,13244202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gurwls223,gurwls223,,11/Jul/19 01:49,12/Dec/22 17:51,13/Jul/23 08:49,11/Jul/19 02:03,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"See [https://github.com/apache/spark/pull/25086#discussion_r302208451]

We should replace REL_12_BETA1 to REL_12_BETA2.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27921,SPARK-27763,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 11 02:03:19 UTC 2019,,,,,,,,,,"0|z04jqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jul/19 02:03;dongjoon;This is resolved via https://github.com/apache/spark/pull/25105;;;",,,,,,,,,,,,,,,,,,,,,,,
SQLMetric wrong initValue ,SPARK-28332,13244069,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,EdisonWang,windpiger,windpiger,10/Jul/19 10:00,26/Dec/19 04:10,13/Jul/23 08:49,23/Dec/19 07:46,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Currently SQLMetrics.createSizeMetric create a SQLMetric with initValue set to -1.

If there is a ShuffleMapStage with lots of Tasks which read 0 bytes data, these tasks will send the metric(the metric value still be the initValue with -1) to Driver,  then Driver do metric merge for this Stage in DAGScheduler.updateAccumulators, this will cause the merged metric value of this Stage set to be a negative value. 

This is incorrect， we should set the initValue to 0 .

Another same case in SQLMetrics.createTimingMetric.",,cloud_fan,EdisonWang,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 08:20:22 UTC 2019,,,,,,,,,,"0|z04ixk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Dec/19 07:46;cloud_fan;Issue resolved by pull request 26899
[https://github.com/apache/spark/pull/26899];;;","23/Dec/19 08:20;EdisonWang;I've taken it [~cloud_fan];;;",,,,,,,,,,,,,,,,,,,,,,
Catalogs.load always throws CatalogNotFoundException on loading built-in catalogs,SPARK-28331,13244067,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,10/Jul/19 09:59,07/Aug/19 23:19,13/Jul/23 08:49,07/Aug/19 23:16,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"In `Catalogs.load`, the `pluginClassName` in the following code 
```
String pluginClassName = conf.getConfString(""spark.sql.catalog."" + name, null);
```
is always null for built-in catalogs, e.g there is a SQLConf entry for `spark.sql.catalog.session`.

This is because of https://github.com/apache/spark/pull/18852: SQLConf.conf.getConfString(key, null)  always returns null.

",,brkyvz,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 07 23:16:20 UTC 2019,,,,,,,,,,"0|z04ix4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Aug/19 23:16;brkyvz;Resolved with [https://github.com/apache/spark/pull/25348];;;",,,,,,,,,,,,,,,,,,,,,,,
PythonUDF should be able to use in join condition,SPARK-28323,13244009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,10/Jul/19 04:16,12/Dec/22 18:10,13/Jul/23 08:49,11/Jul/19 00:39,3.0.0,,,,,,,,,,3.0.0,,,PySpark,SQL,,,,0,,,,"There is a bug in {{ExtractPythonUDFs}} that produces wrong result attributes. It causes a failure when using PythonUDFs among multiple child plans, e.g., join. An example is using PythonUDFs in join condition.",,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28278,SPARK-28276,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 11 00:39:09 UTC 2019,,,,,,,,,,"0|z04ik8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Jul/19 04:24;viirya;I found this bug when doing SPARK-28276.;;;","11/Jul/19 00:39;gurwls223;Fixed at [https://github.com/apache/spark/pull/25091];;;",,,,,,,,,,,,,,,,,,,,,,
"functions.udf(UDF0, DataType) produces unexpected results",SPARK-28321,13243961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,netvl,netvl,09/Jul/19 20:35,12/Dec/22 17:34,13/Jul/23 08:49,12/Jul/19 09:04,2.3.2,2.4.3,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"It looks like that the `f.udf(UDF0, DataType)` variant of the UDF Column-creating methods is wrong ([https://github.com/apache/spark/blob/c3e32bf06c35ba2580d46150923abfa795b4446a/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L4061|https://github.com/apache/spark/blob/c3e32bf06c35ba2580d46150923abfa795b4446a/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L4061):]):

 
{code:java}
def udf(f: UDF0[_], returnType: DataType): UserDefinedFunction = {
  val func = f.asInstanceOf[UDF0[Any]].call()
  SparkUserDefinedFunction.create(() => func, returnType, inputSchemas = Seq.fill(0)(None))
}
{code}
Here the UDF passed as the first argument will be called *right inside the `udf` method* on the driver, rather than at the dataframe computation time on executors. One of the major issues here is that non-deterministic UDFs (e.g. generating a random value) will produce unexpected results:

 

 
{code:java}
val scalaudf = f.udf { () => scala.util.Random.nextInt() }.asNondeterministic()
val javaudf = f.udf(new UDF0[Int] { override def call(): Int = scala.util.Random.nextInt() }, IntegerType).asNondeterministic()

(1 to 100).toDF().select(scalaudf().as(""scala""), javaudf().as(""java"")).show()

// prints

+-----------+---------+
|      scala|     java|
+-----------+---------+
|  934190385|478543809|
|-1082102515|478543809|
|  774466710|478543809|
| 1883582103|478543809|
|-1959743031|478543809|
| 1534685218|478543809|
| 1158899264|478543809|
|-1572590653|478543809|
| -309451364|478543809|
| -906574467|478543809|
| -436584308|478543809|
| 1598340674|478543809|
|-1331343156|478543809|
|-1804177830|478543809|
|-1682906106|478543809|
| -197444289|478543809|
|  260603049|478543809|
|-1993515667|478543809|
|-1304685845|478543809|
|  481017016|478543809|
+-----------+---------{code}
Note that the version which relies on a different overload of the `functions.udf` method works correctly.

 ",,netvl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-07-09 20:35:38.0,,,,,,,,,,"0|z04i9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AppVeyor fails to install testthat (1.0.2) due to previously installed testthat (2.1.0) due to devtools,SPARK-28309,13243792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,09/Jul/19 01:11,12/Dec/22 18:10,13/Jul/23 08:49,09/Jul/19 03:07,3.0.0,,,,,,,,,,3.0.0,,,Project Infra,SparkR,,,,0,,,,"Looks like devtools 2.1.0 is released and then our AppVeyor users the latest one.
The problem is, they added testthat 2.1.1+ as its dependency - https://github.com/r-lib/devtools/blob/master/DESCRIPTION#L35

Usually it should remove and reinstall it properly; however, seems it's being failed in AppVeyor due to the previous installation.

{code}
[00:01:41] > devtools::install_version('testthat', version = '1.0.2', repos='https://cloud.r-project.org/')
[00:01:44] Downloading package from url: https://cloud.r-project.org//src/contrib/Archive/testthat/testthat_1.0.2.tar.gz
...
[00:02:25] WARNING: moving package to final location failed, copying instead
[00:02:25] Warning in file.copy(instdir, dirname(final_instdir), recursive = TRUE,  :
[00:02:25]   problem copying c:\RLibrary\00LOCK-testthat\00new\testthat\libs\i386\testthat.dll to c:\RLibrary\testthat\libs\i386\testthat.dll: Permission denied
[00:02:25] ** testing if installed package can be loaded from final location
[00:02:25] *** arch - i386
[00:02:26] Error: package or namespace load failed for 'testthat' in FUN(X[[i]], ...):
[00:02:26]  no such symbol find_label_ in package c:/RLibrary/testthat/libs/i386/testthat.dll
[00:02:26] Error: loading failed
[00:02:26] Execution halted
[00:02:26] *** arch - x64
[00:02:26] ERROR: loading failed for 'i386'
[00:02:26] * removing 'c:/RLibrary/testthat'
[00:02:26] * restoring previous 'c:/RLibrary/testthat'
[00:02:26] Warning in file.copy(lp, dirname(pkgdir), recursive = TRUE, copy.date = TRUE) :
[00:02:26]   problem copying c:\RLibrary\00LOCK-testthat\testthat\libs\i386\testthat.dll to c:\RLibrary\testthat\libs\i386\testthat.dll: Permission denied
[00:02:26] Warning message:
[00:02:26] In i.p(...) :
[00:02:26]   installation of package 'C:/Users/appveyor/AppData/Local/Temp/1/RtmpIx25hi/remotes5743d4a9b1/testthat' had non-zero exit status
{code}

devtools was installed at SPARK-22817 to pin the testthat version.

Therefore, we might have to work around this by directly installing from the archive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 09 03:07:46 UTC 2019,,,,,,,,,,"0|z04h80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Jul/19 03:07;gurwls223;Issue resolved by pull request 25081
[https://github.com/apache/spark/pull/25081];;;",,,,,,,,,,,,,,,,,,,,,,,
CalendarInterval sub-second part should be padded before parsing,SPARK-28308,13243777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,08/Jul/19 23:16,09/Jul/19 06:19,13/Jul/23 08:49,09/Jul/19 06:18,2.0.2,2.1.3,2.2.3,2.3.3,2.4.3,,,,,,2.3.4,2.4.4,3.0.0,Spark Core,,,,,0,correctness,,,"{code}
spark-sql> select interval '0 0:0:0.123456789' day to second;
interval 123 milliseconds 456 microseconds

spark-sql> select interval '0 0:0:0.12345678' day to second;
interval 12 milliseconds 345 microseconds

spark-sql> select interval '0 0:0:0.1234' day to second;
interval 1 microseconds
Time taken: 0.024 seconds, Fetched 1 row(s)
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 09 06:19:29 UTC 2019,,,,,,,,,,"0|z04h4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Jul/19 06:19;dongjoon;This is resolved via https://github.com/apache/spark/pull/25079;;;",,,,,,,,,,,,,,,,,,,,,,,
SparkLauncher: The process cannot access the file because it is being used by another process,SPARK-28302,13243653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,08/Jul/19 11:40,12/Dec/22 18:10,13/Jul/23 08:49,09/Jul/19 06:51,2.4.0,,,,,,,,,,2.3.4,2.4.4,3.0.0,Spark Core,,,,09/Jul/19 00:00,0,,,,"When using SparkLauncher to submit applications concurrently with a thread pool under *Windows*, some apps would show that ""The process cannot access the file because it is being used by another process"" and remains in LOST state at the end. (Issue can be reproduced with attach file.)

 

After digging into the code, I find that, Windows cmd %RANDOM% would return the same number if we call it  instantly( < 500ms) after last call. As a result, SparkLauncher would get same output file(spark-class-launcher-output-%RANDOM%.txt) for apps. Then, the following app would hit the issue when it try to write the same file which has already been occupied by another app.

 

 ",Windows 7,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/19 11:45;Ngone51;Main.scala;https://issues.apache.org/jira/secure/attachment/12973920/Main.scala",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Tue Jul 09 06:51:29 UTC 2019,,,,,,,,,,"0|z04gd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Jul/19 06:51;gurwls223;Issue resolved by pull request 25076
[https://github.com/apache/spark/pull/25076];;;",,,,,,,,,,,,,,,,,,,,,,,
data duplication when `path` serde property is present,SPARK-28266,13243448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shardulm,Tagar,Tagar,05/Jul/19 23:29,18/Oct/21 18:10,13/Jul/23 08:49,21/Jul/21 14:42,2.2.0,2.2.1,2.2.2,,,,,,,,3.0.4,3.1.3,3.2.0,Spark Core,,,,,0,correctness,,,"Spark duplicates returned datasets when `path` serde is present in a parquet table. 

Confirmed versions affected: Spark 2.2, Spark 2.3, Spark 2.4.

Confirmed unaffected versions: Spark 2.1 and earlier (tested with Spark 1.6 at least).

Reproducer:

{code:python}
>>> spark.sql(""create table ruslan_test.test55 as select 1 as id"")
DataFrame[]

>>> spark.table(""ruslan_test.test55"").explain()

== Physical Plan ==
HiveTableScan [id#16], HiveTableRelation `ruslan_test`.`test55`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#16]

>>> spark.table(""ruslan_test.test55"").count()
1

{code}

(all is good at this point, now exist session and run in Hive for example - )

{code:sql}
ALTER TABLE ruslan_test.test55 SET SERDEPROPERTIES ( 'path'='hdfs://epsdatalake/hivewarehouse/ruslan_test.db/test55' )
{code}

So LOCATION and serde `path` property would point to the same location.
Now see count returns two records instead of one:

{code:python}
>>> spark.table(""ruslan_test.test55"").count()
2

>>> spark.table(""ruslan_test.test55"").explain()
== Physical Plan ==
*(1) FileScan parquet ruslan_test.test55[id#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://epsdatalake/hivewarehouse/ruslan_test.db/test55, hdfs://epsdatalake/hive..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>
>>>

{code}

Also notice that the presence of `path` serde property makes TABLE location 
show up twice - 
{quote}
InMemoryFileIndex[hdfs://epsdatalake/hivewarehouse/ruslan_test.db/test55, hdfs://epsdatalake/hive..., 
{quote}

We have some applications that create parquet tables in Hive with `path` serde property
and it makes data duplicate in query results. 

Hive, Impala etc and Spark version 2.1 and earlier read such tables fine, but not Spark 2.2 and later releases.
",,apachespark,cloud_fan,dongjoon,shardulm,Tagar,Tonix517,xkrogen,,,,,,,,,,,,,,,,,,,,,SPARK-37027,,,,,,,,HIVE-21952,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 21 14:42:39 UTC 2021,,,,,,,,,,"0|z04f3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Jul/19 15:35;Tagar;Suspecting change in SPARK-22158 causes this ;;;","10/Jul/19 19:36;Tagar;This issue happens `spark.sql.sources.provider` table property is NOT present, and `path` serde property is present -

Spark duplicates records in this case.

 ;;;","11/Jul/19 17:54;Tagar;Another interesting side Spark bug found while was trying to fix this issue.

If `spark.sql.sources.provider` table property IS present and `path` serde property IS NOT present,
then Spark will happily always return 0 (zero) records irrespective of all the files that `LOCATION` points at.

 ;;;","13/Jan/20 05:56;dongjoon;Hi, [~Tagar]. 
For now, I cannot reproduce this in `3.0.0-preview` and `3.0.0-preview2`. How did you test this for master branch?

{code}
spark-sql> select version();
3.0.0 007c873ae34f58651481ccba30e8e2ba38a692c4
Time taken: 3.309 seconds, Fetched 1 row(s)

spark-sql> create table t as select 1 as id;
Time taken: 1.303 seconds

spark-sql> select * from t;
1
Time taken: 0.247 seconds, Fetched 1 row(s)

spark-sql> desc formatted t;
id	int	NULL

# Detailed Table Information
Database	default
Table	t
Owner	dongjoon
Created Time	Sun Jan 12 21:50:55 PST 2020
Last Access	UNKNOWN
Created By	Spark 3.0.0-preview
Type	MANAGED
Provider	hive
Table Properties	[transient_lastDdlTime=1578894656]
Statistics	2 bytes
Location	file:/Users/dongjoon/APACHE/spark-release/spark-3.0.0-preview-bin-hadoop3.2/spark-warehouse/t
Serde Library	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat	org.apache.hadoop.mapred.TextInputFormat
OutputFormat	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties	[serialization.format=1]
Partition Provider	Catalog
Time taken: 0.098 seconds, Fetched 19 row(s)

spark-sql> ALTER TABLE t SET SERDEPROPERTIES ( 'path'='file:/Users/dongjoon/APACHE/spark-release/spark-3.0.0-preview-bin-hadoop3.2/spark-warehouse/t' );
Time taken: 0.093 seconds

spark-sql> select * from t;
1
Time taken: 0.092 seconds, Fetched 1 row(s)

spark-sql> explain select * from t;
== Physical Plan ==
Scan hive default.t [id#29], HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#29]

Time taken: 0.064 seconds, Fetched 1 row(s)
{code};;;","13/Jan/20 06:04;dongjoon;So, the observation was that `InMemoryFileIndex` has duplicated entries?
And, why `spark.sql.sources.provider` property is not present there? Is it removed intentionally?
> This issue happens `spark.sql.sources.provider` table property is NOT present, and `path` serde property is present -;;;","13/Jan/20 06:19;dongjoon;BTW, SPARK-22158 is irrelevant to this one because it affects only orc and parquet table property containing `.orc` or `.parquet`.;;;","13/Jan/20 06:27;dongjoon;In Apache Spark 2.4.4, I also cannot reproduce this issue.
{code}
scala> spark.version
res0: String = 2.4.4

scala> sql(""create table t as select 1 as id"")
res1: org.apache.spark.sql.DataFrame = []

scala> sql(""desc extended t"").show(false)
+----------------------------+----------------------------------------------------------+-------+
|col_name                    |data_type                                                 |comment|
+----------------------------+----------------------------------------------------------+-------+
|id                          |int                                                       |null   |
|                            |                                                          |       |
|# Detailed Table Information|                                                          |       |
|Database                    |default                                                   |       |
|Table                       |t                                                         |       |
|Owner                       |dongjoon                                                  |       |
|Created Time                |Sun Jan 12 22:26:18 PST 2020                              |       |
|Last Access                 |Wed Dec 31 16:00:00 PST 1969                              |       |
|Created By                  |Spark 2.4.4                                               |       |
|Type                        |MANAGED                                                   |       |
|Provider                    |hive                                                      |       |
|Table Properties            |[transient_lastDdlTime=1578896780]                        |       |
|Statistics                  |2 bytes                                                   |       |
|Location                    |file:/Users/dongjoon/spark-warehouse/t                    |       |
|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe        |       |
|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                  |       |
|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat|       |
|Storage Properties          |[serialization.format=1]                                  |       |
|Partition Provider          |Catalog                                                   |       |
+----------------------------+----------------------------------------------------------+-------+

scala> sql(""ALTER TABLE t SET SERDEPROPERTIES ( 'path'='file:/Users/dongjoon/spark-warehouse/t' )"")
res3: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
| id|
+---+
|  1|
+---+
{code};;;","13/Jan/20 06:35;dongjoon;For 2.3.4 and 2.2.3, it's the same. I cannot reproduce this issue.
{code}
scala> spark.version
res0: String = 2.3.4

scala> sql(""create table t as select 1 as id"")
res1: org.apache.spark.sql.DataFrame = []

scala> sql(""desc extended t"").show(false)
+----------------------------+-------------------------------------------------------------------------------------+-------+
|col_name                    |data_type                                                                            |comment|
+----------------------------+-------------------------------------------------------------------------------------+-------+
|id                          |int                                                                                  |null   |
|                            |                                                                                     |       |
|# Detailed Table Information|                                                                                     |       |
|Database                    |default                                                                              |       |
|Table                       |t                                                                                    |       |
|Owner                       |dongjoon                                                                             |       |
|Created Time                |Sun Jan 12 22:34:16 PST 2020                                                         |       |
|Last Access                 |Wed Dec 31 16:00:00 PST 1969                                                         |       |
|Created By                  |Spark 2.3.4                                                                          |       |
|Type                        |MANAGED                                                                              |       |
|Provider                    |hive                                                                                 |       |
|Table Properties            |[transient_lastDdlTime=1578897257]                                                   |       |
|Statistics                  |2 bytes                                                                              |       |
|Location                    |file:/Users/dongjoon/APACHE/spark-release/spark-2.3.4-bin-hadoop2.7/spark-warehouse/t|       |
|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                   |       |
|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                             |       |
|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                           |       |
|Storage Properties          |[serialization.format=1]                                                             |       |
|Partition Provider          |Catalog                                                                              |       |
+----------------------------+-------------------------------------------------------------------------------------+-------+


scala> sql(""ALTER TABLE t SET SERDEPROPERTIES ( 'path'='file:/Users/dongjoon/APACHE/spark-release/spark-2.3.4-bin-hadoop2.7/spark-warehouse/t' )"")
res3: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
| id|
+---+
|  1|
+---+
{code};;;","13/Jan/20 06:37;dongjoon;Could you reopen this with more information please?;;;","13/Jan/20 16:46;Tagar;Thank you for checking [~dongjoon]

That may have been a Cloudera Distribution of Spark issue all along (I did have a support case with Cloudera last year on this and it did not go anywhere on Spark side - Cloudera were fixing that from another side, by fixing `path` correctly on tables that were replicated )

I have moved on and no longer have access to a Cloudera environment. ;;;","13/Jan/20 21:49;dongjoon;Oh, I see. Thank you for reply.;;;","13/Jul/21 18:01;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/33328;;;","13/Jul/21 21:11;shardulm;I would like to propose another angle to look at the issue.

In this case, Spark can be reading Hive tables created by other products like Hive, Trino or any external system. In such a case, Spark should not be interpreting {{path}} property as it can controlled by an end user in these system, it is not a restricted property and does not have a meaning in Hive itself.

So here, the issue is not that {{spark.sql.sources.provider}} is missing. It was never expected to be set since the table was not created through Spark.

An easy example to demonstrate this issue is by creating an empty table with {{path}} property through Hive, and then trying to read it through Spark.
{code:java}
hive (default)> CREATE TABLE test (id bigint)
              > ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
              > WITH SERDEPROPERTIES ('path'='someRandomValue')
              > STORED AS PARQUET;
OK
Time taken: 0.069 seconds
{code}
{code:java}
scala> spark.sql(""SELECT * FROM test"")
org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://user/username/someRandomValue
  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:803)
  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:800)
  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
  at scala.util.Success.$anonfun$map$1(Try.scala:255)
  at scala.util.Success.map(Try.scala:213)
  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
  at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)
  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
{code}
In the case the path property points to a valid location, it may result in incorrect data
{code:java}
 hive (default)> CREATE TABLE test1 (id bigint)
              > ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
              > WITH SERDEPROPERTIES ('path'='/user/username/test1')
              > STORED AS PARQUET LOCATION '/user/username/test1';
OK
Time taken: 0.046 seconds

hive (default)> INSERT INTO test1 VALUES (1);
1 Rows loaded to test1
OK
Time taken: 59.979 seconds
{code}
{code:java}
scala> spark.sql(""SELECT * FROM test1"").show()
+---+
| id|
+---+
|  1|
|  1|
+---+
{code};;;","13/Jul/21 21:18;xkrogen;Re-opening this issue based on [~shardulm]'s example above demonstrating that this is indeed a real-world issue. We have faced this issue internally on a number of occasions and have been operating a fix, on which [~shardulm]'s PR is based, for a few years.;;;","21/Jul/21 14:42;cloud_fan;Issue resolved by pull request 33328
[https://github.com/apache/spark/pull/33328];;;",,,,,,,,,
Fix error message of inserting into a non-existing table,SPARK-28251,13243221,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,petertoth,petertoth,petertoth,04/Jul/19 13:40,04/Jul/19 19:37,13/Jul/23 08:49,04/Jul/19 19:33,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"Inserting into a non-existing table returns an error message that could be a bit more user friendly:

{noformat}
scala> sql(""INSERT INTO test VALUES (1)"").show
org.apache.spark.sql.AnalysisException: unresolved operator 'InsertIntoTable 'UnresolvedRelation [test], false, false;;
'InsertIntoTable 'UnresolvedRelation [test], false, false
+- LocalRelation [col1#4]

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:45)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:44)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:97)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$35(CheckAnalysis.scala:461)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$35$adapted(CheckAnalysis.scala:459)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:150)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:459)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:85)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:97)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:114)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:63)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:61)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:61)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:53)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:87)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:653)
  ... 47 elided
{noformat}

",,dongjoon,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 04 19:33:23 UTC 2019,,,,,,,,,,"0|z04dpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Jul/19 13:42;petertoth;cc [~dongjoon];;;","04/Jul/19 19:33;dongjoon;This is resolved via https://github.com/apache/spark/pull/25054;;;",,,,,,,,,,,,,,,,,,,,,,
Upgrade maven-jar-plugin and maven-source-plugin,SPARK-28233,13242761,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,02/Jul/19 13:43,12/Dec/22 18:10,13/Jul/23 08:49,03/Jul/19 12:47,3.0.0,,,,,,,,,,3.0.0,,,Build,,,,,0,,,,"Upgrade {{maven-jar-plugin}} to 3.1.2 and {{maven-source-plugin}} to 3.1.0 to avoid:
 * MJAR-259 – Archiving to jar is very slow
 * MSOURCES-119 – Archiving to jar is very slow

Release notes:
[https://blogs.apache.org/maven/entry/apache-maven-source-plugin-version]
[https://blogs.apache.org/maven/entry/apache-maven-jar-plugin-version2]
[https://blogs.apache.org/maven/entry/apache-maven-jar-plugin-version1]",,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 03 12:47:21 UTC 2019,,,,,,,,,,"0|z04aw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jul/19 12:47;gurwls223;Issue resolved by pull request 25031
[https://github.com/apache/spark/pull/25031];;;",,,,,,,,,,,,,,,,,,,,,,,
Add groupIdPrefix for Kafka batch connector,SPARK-28232,13242700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,02/Jul/19 09:37,02/Jul/19 12:42,13/Jul/23 08:49,02/Jul/19 12:41,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"According to the documentation groupIdPrefix should be available for streaming and batch.
It is not the case because the batch part is missing.
",,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-07-02 09:37:53.0,,,,,,,,,,"0|z04aig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check overflow in decimal Sum aggregate,SPARK-28224,13242623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mickjermsurawong-stripe,mickjermsurawong-stripe,mickjermsurawong-stripe,01/Jul/19 23:51,20/Aug/19 00:49,13/Jul/23 08:49,20/Aug/19 00:49,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,,0,,,,"To reproduce:
{code:java}
import spark.implicits._
val ds = spark
  .createDataset(Seq(BigDecimal(""1"" * 20), BigDecimal(""9"" * 20)))
  .agg(sum(""value""))
  .as[BigDecimal]
ds.collect shouldEqual Seq(null){code}
Given the option to throw exception on overflow on, sum aggregation of overflowing bigdecimal still remain null. {{DecimalAggregates}} is only invoked when expression of the sum (not the elements to be operated) has sufficiently small precision. The fix seems to be in Sum expression itself. 

 ",,maropu,mickjermsurawong-stripe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 20 00:49:40 UTC 2019,,,,,,,,,,"0|z04a0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Jul/19 23:53;mickjermsurawong-stripe;To reproduce this: [https://github.com/apache/spark/commit/131679c1a6e08be96245a56830fea2416bedf285];;;","20/Aug/19 00:49;maropu;Resolved by [https://github.com/apache/spark/pull/25033|https://github.com/apache/spark/pull/25033#];;;",,,,,,,,,,,,,,,,,,,,,,
stream-stream joins should fail unsupported checker in update mode,SPARK-28223,13242617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joseph.torres,joseph.torres,01/Jul/19 22:27,09/Jul/19 15:17,13/Jul/23 08:49,02/Jul/19 17:00,2.4.3,,,,,,,,,,3.0.0,,,Structured Streaming,,,,,0,,,,"Right now they fail only for inner joins, because we implemented the check when that was the only supported type.",,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25834,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 02 17:00:14 UTC 2019,,,,,,,,,,"0|z049zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jul/19 17:00;joseph.torres;Issue resolved by pull request 25023
[https://github.com/apache/spark/pull/25023];;;",,,,,,,,,,,,,,,,,,,,,,,
